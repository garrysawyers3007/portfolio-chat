# Age-Classification
This is the repository for Age Classification on [data](https://www.kaggle.com/c/human-age-recognition/data) consisting of around 13,000 images of boys and girls belonging to age groups from 1 to 80.

Best accuracy on test dataset: 0.508
```python
import numpy as np
import scipy.sparse as sp
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from torch.autograd import Variable
import math
import pandas as pd
```

```python
def build_dense_graph(node_num):
    graph = 1. / (node_num - 1) * np.ones((node_num, node_num))
    np.fill_diagonal(graph, 0)
    graph = torch.from_numpy(graph).float()
    return graph


def sample_gumbel(shape, eps=1e-10):
    U = torch.rand(shape).float()
    return - torch.log(eps - torch.log(U + eps))


def gumbel_softmax_sample(logits, tau=1, eps=1e-10, dim=-1):
    """
    NOTE: Stolen from https://github.com/ethanfetaya/NRI/blob/master/utils.py
    Draw a sample from the Gumbel-Softmax distribution
    based on
    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb
    """
    gumbel_noise = sample_gumbel(logits.size(), eps=eps)
    if logits.is_cuda:
        gumbel_noise = gumbel_noise.cuda()
    y = logits + Variable(gumbel_noise)
    return F.softmax(y / tau, dim=dim)


def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):
    """
    NOTE: Stolen from https://github.com/ethanfetaya/NRI/blob/master/utils.py
    Sample from the Gumbel-Softmax distribution and optionally discretize.
    Args:
def gumbel_softmax(logits, tau=1, hard=False, eps=1e-10, dim=-1):
    """
    NOTE: Stolen from https://github.com/ethanfetaya/NRI/blob/master/utils.py
    Sample from the Gumbel-Softmax distribution and optionally discretize.
    Args:
      logits: [batch_size, n_class] unnormalized log-probs
      tau: non-negative scalar temperature
      hard: if True, take argmax, but differentiate w.r.t. soft sample y
    Returns:
      [batch_size, n_class] sample from the Gumbel-Softmax distribution.
      If hard=True, then the returned sample will be one-hot, otherwise it will
      be a probability distribution that sums to 1 across classes
    Constraints:
    - this implementation only works on batch_size x num_features tensor for now
    based on
    https://github.com/ericjang/gumbel-softmax/blob/3c8584924603869e90ca74ac20a6a03d99a91ef9/Categorical%20VAE.ipynb
    """
    y_soft = gumbel_softmax_sample(logits, tau=tau, eps=eps, dim=dim)
    if hard:
        shape = logits.size()
        _, k = y_soft.data.max(-1)
        # this bit is based on
        # https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5
        y_hard = torch.zeros(*shape)
        if y_soft.is_cuda:
            y_hard = y_hard.cuda()
        y_hard = y_hard.zero_().scatter_(-1, k.view(shape[:-1] + (1,)), 1.0)
        # this cool bit of code achieves two things:
# https://discuss.pytorch.org/t/stop-gradients-for-st-gumbel-softmax/530/5
        y_hard = torch.zeros(*shape)
        if y_soft.is_cuda:
            y_hard = y_hard.cuda()
        y_hard = y_hard.zero_().scatter_(-1, k.view(shape[:-1] + (1,)), 1.0)
        # this cool bit of code achieves two things:
        # - makes the output value exactly one-hot (since we add then
        #   subtract y_soft value)
        # - makes the gradient equal to y_soft gradient (since we strip
        #   all other gradients)
        y = Variable(y_hard - y_soft.data) + y_soft
    else:
        y = y_soft
    return y


def kl_categorical(preds, log_prior, concept_num, eps=1e-16):
    kl_div = preds * (torch.log(preds + eps) - log_prior)
    return kl_div.sum() / (concept_num * preds.size(0))


def kl_categorical_uniform(preds, concept_num, num_edge_types, add_const=False, eps=1e-16):
    kl_div = preds * torch.log(preds + eps)
    if add_const:
        const = np.log(num_edge_types)
        kl_div += const
    return kl_div.sum() / (concept_num * preds.size(0))


def nll_gaussian(preds, target, variance, add_const=False):
    # pred: [concept_num, embedding_dim]
    # target: [concept_num, embedding_dim]
    neg_log_p = ((preds - target) ** 2 / (2 * variance))
    if add_const:
        const = 0.5 * np.log(2 * np.pi * variance)
        neg_log_p += const
    return neg_log_p.mean()
if add_const:
        const = 0.5 * np.log(2 * np.pi * variance)
        neg_log_p += const
    return neg_log_p.mean()


# Calculate accuracy of prediction result and its corresponding label
# output: tensor, labels: tensor
def accuracy(output, labels):
    preds = output.max(1)[1].type_as(labels)
    correct = preds.eq(labels.reshape(-1)).double()
    correct = correct.sum()
    return correct / len(labels)
```

```python
class MLP(nn.Module):
    """Two-layer fully-connected ReLU net with batch norm."""

    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0., bias=True):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=bias)
        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=bias)
        self.norm = nn.BatchNorm1d(output_dim)
        # the paper said they added Batch Normalization for the output of MLPs, as shown in Section 4.2
        self.dropout = dropout
        self.output_dim = output_dim
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
                m.bias.data.fill_(0.1)
            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def batch_norm(self, inputs):
m.bias.data.fill_(0.1)
            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def batch_norm(self, inputs):
        if inputs.numel() == self.output_dim or inputs.numel() == 0:
            # batch_size == 1 or 0 will cause BatchNorm error, so return the input directly
            return inputs
        if len(inputs.size()) == 3:
            x = inputs.view(inputs.size(0) * inputs.size(1), -1)
            x = self.norm(x)
            return x.view(inputs.size(0), inputs.size(1), -1)
        else:  # len(input_size()) == 2
            return self.norm(inputs)

    def forward(self, inputs):
        x = F.relu(self.fc1(inputs))
        x = F.dropout(x, self.dropout, training=self.training)  # pay attention to add training=self.training
        x = F.relu(self.fc2(x))
        return self.batch_norm(x)


class EraseAddGate(nn.Module):
    """
    Erase & Add Gate module
    NOTE: this erase & add gate is a bit different from that in DKVMN.
    For more information about Erase & Add gate, please refer to the paper "Dynamic Key-Value Memory Networks for Knowledge Tracing"
    The paper can be found in https://arxiv.org/abs/1611.08108
    """

    def __init__(self, feature_dim, concept_num, bias=True):
        super(EraseAddGate, self).__init__()
        # weight
The paper can be found in https://arxiv.org/abs/1611.08108
    """

    def __init__(self, feature_dim, concept_num, bias=True):
        super(EraseAddGate, self).__init__()
        # weight
        self.weight = nn.Parameter(torch.rand(concept_num))
        self.reset_parameters()
        # erase gate
        self.erase = nn.Linear(feature_dim, feature_dim, bias=bias)
        # add gate
        self.add = nn.Linear(feature_dim, feature_dim, bias=bias)

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(0))
        self.weight.data.uniform_(-stdv, stdv)

    def forward(self, x):
        r"""
        Params:
            x: input feature matrix
        Shape:
            x: [batch_size, concept_num, feature_dim]
            res: [batch_size, concept_num, feature_dim]
        Return:
            res: returned feature matrix with old information erased and new information added
        The GKT paper didn't provide detailed explanation about this erase-add gate. As the erase-add gate in the GKT only has one input parameter,
        this gate is different with that of the DKVMN. We used the input matrix to build the erase and add gates, rather than $\mathbf{v}_{t}$ vector in the DKVMN.
        """
        erase_gate = torch.sigmoid(self.erase(x))  # [batch_size, concept_num, feature_dim]
        # self.weight.unsqueeze(dim=1) shape: [concept_num, 1]
res: returned feature matrix with old information erased and new information added
        The GKT paper didn't provide detailed explanation about this erase-add gate. As the erase-add gate in the GKT only has one input parameter,
        this gate is different with that of the DKVMN. We used the input matrix to build the erase and add gates, rather than $\mathbf{v}_{t}$ vector in the DKVMN.
        """
        erase_gate = torch.sigmoid(self.erase(x))  # [batch_size, concept_num, feature_dim]
        # self.weight.unsqueeze(dim=1) shape: [concept_num, 1]
        tmp_x = x - self.weight.unsqueeze(dim=1) * erase_gate * x
        add_feat = torch.tanh(self.add(x))  # [batch_size, concept_num, feature_dim]
        res = tmp_x + self.weight.unsqueeze(dim=1) * add_feat
        return res


class ScaledDotProductAttention(nn.Module):
    """
    Scaled Dot-Product Attention
    NOTE: Stole and modify from https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Modules.py
    """

    def __init__(self, temperature, attn_dropout=0.):
        super().__init__()
        self.temperature = temperature
        self.dropout = attn_dropout

    def forward(self, q, k, mask=None):
        r"""
        Parameters:
            q: multi-head query matrix
            k: multi-head key matrix
            mask: mask matrix
        Shape:
r"""
        Parameters:
            q: multi-head query matrix
            k: multi-head key matrix
            mask: mask matrix
        Shape:
            q: [n_head, mask_num, embedding_dim]
            k: [n_head, concept_num, embedding_dim]
        Return: attention score of all queries
        """
        attn = torch.matmul(q / self.temperature, k.transpose(1, 2))  # [n_head, mask_number, concept_num]
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)
        # pay attention to add training=self.training!
        attn = F.dropout(F.softmax(attn, dim=0), self.dropout, training=self.training)  # pay attention that dim=-1 is not as good as dim=0!
        return attn


class MLPEncoder(nn.Module):
    """
    MLP encoder module.
    NOTE: Stole and modify the code from https://github.com/ethanfetaya/NRI/blob/master/modules.py
    """
    def __init__(self, input_dim, hidden_dim, output_dim, factor=True, dropout=0., bias=True):
        super(MLPEncoder, self).__init__()
        self.factor = factor
        self.mlp = MLP(input_dim * 2, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.mlp2 = MLP(hidden_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        if self.factor:
            self.mlp3 = MLP(hidden_dim * 3, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        else:
self.factor = factor
        self.mlp = MLP(input_dim * 2, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.mlp2 = MLP(hidden_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        if self.factor:
            self.mlp3 = MLP(hidden_dim * 3, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        else:
            self.mlp3 = MLP(hidden_dim * 2, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
                m.bias.data.fill_(0.1)

    def node2edge(self, x, sp_send, sp_rec):
        # NOTE: Assumes that we have the same graph across all samples.
        receivers = torch.matmul(sp_rec, x)
        senders = torch.matmul(sp_send, x)
        edges = torch.cat([senders, receivers], dim=1)
        return edges

    def edge2node(self, x, sp_send_t, sp_rec_t):
        # NOTE: Assumes that we have the same graph across all samples.
        incoming = torch.matmul(sp_rec_t, x)
        return incoming

    def forward(self, inputs, sp_send, sp_rec, sp_send_t, sp_rec_t):
        r"""
        Parameters:
            inputs: input concept embedding matrix
return incoming

    def forward(self, inputs, sp_send, sp_rec, sp_send_t, sp_rec_t):
        r"""
        Parameters:
            inputs: input concept embedding matrix
            sp_send: one-hot encoded send-node index(sparse tensor)
            sp_rec: one-hot encoded receive-node index(sparse tensor)
            sp_send_t: one-hot encoded send-node index(sparse tensor, transpose)
            sp_rec_t: one-hot encoded receive-node index(sparse tensor, transpose)
        Shape:
            inputs: [concept_num, embedding_dim]
            sp_send: [edge_num, concept_num]
            sp_rec: [edge_num, concept_num]
            sp_send_t: [concept_num, edge_num]
            sp_rec_t: [concept_num, edge_num]
        Return:
            output: [edge_num, edge_type_num]
        """
        x = self.node2edge(inputs, sp_send, sp_rec)  # [edge_num, 2 * embedding_dim]
        x = self.mlp(x)  # [edge_num, hidden_num]
        x_skip = x

        if self.factor:
            x = self.edge2node(x, sp_send_t, sp_rec_t)  # [concept_num, hidden_num]
            x = self.mlp2(x)  # [concept_num, hidden_num]
            x = self.node2edge(x, sp_send, sp_rec)  # [edge_num, 2 * hidden_num]
            x = torch.cat((x, x_skip), dim=1)  # Skip connection  shape: [edge_num, 3 * hidden_num]
            x = self.mlp3(x)  # [edge_num, hidden_num]
        else:
x = self.edge2node(x, sp_send_t, sp_rec_t)  # [concept_num, hidden_num]
            x = self.mlp2(x)  # [concept_num, hidden_num]
            x = self.node2edge(x, sp_send, sp_rec)  # [edge_num, 2 * hidden_num]
            x = torch.cat((x, x_skip), dim=1)  # Skip connection  shape: [edge_num, 3 * hidden_num]
            x = self.mlp3(x)  # [edge_num, hidden_num]
        else:
            x = self.mlp2(x)  # [edge_num, hidden_num]
            x = torch.cat((x, x_skip), dim=1)  # Skip connection  shape: [edge_num, 2 * hidden_num]
            x = self.mlp3(x)  # [edge_num, hidden_num]
        output = self.fc_out(x)  # [edge_num, output_dim]
        return output


class MLPDecoder(nn.Module):
    """
    MLP decoder module.
    NOTE: Stole and modify the code from https://github.com/ethanfetaya/NRI/blob/master/modules.py
    """

    def __init__(self, input_dim, msg_hidden_dim, msg_output_dim, hidden_dim, edge_type_num, dropout=0., bias=True):
        super(MLPDecoder, self).__init__()
        self.msg_out_dim = msg_output_dim
        self.edge_type_num = edge_type_num
        self.dropout = dropout

        self.msg_fc1 = nn.ModuleList([nn.Linear(2 * input_dim, msg_hidden_dim, bias=bias) for _ in range(edge_type_num)])
        self.msg_fc2 = nn.ModuleList([nn.Linear(msg_hidden_dim, msg_output_dim, bias=bias) for _ in range(edge_type_num)])
self.msg_out_dim = msg_output_dim
        self.edge_type_num = edge_type_num
        self.dropout = dropout

        self.msg_fc1 = nn.ModuleList([nn.Linear(2 * input_dim, msg_hidden_dim, bias=bias) for _ in range(edge_type_num)])
        self.msg_fc2 = nn.ModuleList([nn.Linear(msg_hidden_dim, msg_output_dim, bias=bias) for _ in range(edge_type_num)])
        self.out_fc1 = nn.Linear(msg_output_dim, hidden_dim, bias=bias)
        self.out_fc2 = nn.Linear(hidden_dim, hidden_dim, bias=bias)
        self.out_fc3 = nn.Linear(hidden_dim, input_dim, bias=bias)

    def node2edge(self, x, sp_send, sp_rec):
        receivers = torch.matmul(sp_rec, x)  # [edge_num, embedding_dim]
        senders = torch.matmul(sp_send, x)  # [edge_num, embedding_dim]
        edges = torch.cat([senders, receivers], dim=-1)  # [edge_num, 2 * embedding_dim]
        return edges

    def edge2node(self, x, sp_send_t, sp_rec_t):
        # NOTE: Assumes that we have the same graph across all samples.
        incoming = torch.matmul(sp_rec_t, x)
        return incoming

    def forward(self, inputs, rel_type, sp_send, sp_rec, sp_send_t, sp_rec_t):
        r"""
        Parameters:
            inputs: input concept embedding matrix
            rel_type: inferred edge weights for all edge types from MLPEncoder
            sp_send: one-hot encoded send-node index(sparse tensor)
def forward(self, inputs, rel_type, sp_send, sp_rec, sp_send_t, sp_rec_t):
        r"""
        Parameters:
            inputs: input concept embedding matrix
            rel_type: inferred edge weights for all edge types from MLPEncoder
            sp_send: one-hot encoded send-node index(sparse tensor)
            sp_rec: one-hot encoded receive-node index(sparse tensor)
            sp_send_t: one-hot encoded send-node index(sparse tensor, transpose)
            sp_rec_t: one-hot encoded receive-node index(sparse tensor, transpose)
        Shape:
            inputs: [concept_num, embedding_dim]
            sp_send: [edge_num, concept_num]
            sp_rec: [edge_num, concept_num]
            sp_send_t: [concept_num, edge_num]
            sp_rec_t: [concept_num, edge_num]
        Return:
            output: [edge_num, edge_type_num]
        """
        # NOTE: Assumes that we have the same graph across all samples.
        # Node2edge
        pre_msg = self.node2edge(inputs, sp_send, sp_rec)
        all_msgs = Variable(torch.zeros(pre_msg.size(0), self.msg_out_dim, device=inputs.device))  # [edge_num, msg_out_dim]
        for i in range(self.edge_type_num):
            msg = F.relu(self.msg_fc1[i](pre_msg))
            msg = F.dropout(msg, self.dropout, training=self.training)
            msg = F.relu(self.msg_fc2[i](msg))
            msg = msg * rel_type[:, i:i + 1]
all_msgs = Variable(torch.zeros(pre_msg.size(0), self.msg_out_dim, device=inputs.device))  # [edge_num, msg_out_dim]
        for i in range(self.edge_type_num):
            msg = F.relu(self.msg_fc1[i](pre_msg))
            msg = F.dropout(msg, self.dropout, training=self.training)
            msg = F.relu(self.msg_fc2[i](msg))
            msg = msg * rel_type[:, i:i + 1]
            all_msgs += msg

        # Aggregate all msgs to receiver
        agg_msgs = self.edge2node(all_msgs, sp_send_t, sp_rec_t)  # [concept_num, msg_out_dim]
        # Output MLP
        pred = F.dropout(F.relu(self.out_fc1(agg_msgs)), self.dropout, training=self.training)  # [concept_num, hidden_dim]
        pred = F.dropout(F.relu(self.out_fc2(pred)), self.dropout, training=self.training)  # [concept_num, hidden_dim]
        pred = self.out_fc3(pred)  # [concept_num, embedding_dim]
        return pred
```

```python
class GKT(nn.Module):

    def __init__(self, concept_num, hidden_dim, embedding_dim, edge_type_num, graph_type, graph=None, graph_model=None, dropout=0.5, bias=True, binary=False, has_cuda=False):
        super(GKT, self).__init__()
        self.concept_num = concept_num
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.edge_type_num = edge_type_num

        self.res_len = 2 if binary else 12
        self.has_cuda = has_cuda
self.embedding_dim = embedding_dim
        self.edge_type_num = edge_type_num

        self.res_len = 2 if binary else 12
        self.has_cuda = has_cuda

        assert graph_type in ['Dense', 'Transition', 'DKT', 'PAM', 'MHA', 'VAE']
        self.graph_type = graph_type
        if graph_type in ['Dense', 'Transition', 'DKT']:
            assert edge_type_num == 2
            assert graph is not None and graph_model is None
            self.graph = nn.Parameter(graph)  # [concept_num, concept_num]
            self.graph.requires_grad = False  # fix parameter
            self.graph_model = graph_model
        else:  # ['PAM', 'MHA', 'VAE']
            assert graph is None
            self.graph = graph  # None
            if graph_type == 'PAM':
                assert graph_model is None
                self.graph = nn.Parameter(torch.rand(concept_num, concept_num))
            else:
                assert graph_model is not None
            self.graph_model = graph_model

        # one-hot feature and question
        one_hot_feat = torch.eye(self.res_len * self.concept_num)
        self.one_hot_feat = one_hot_feat.cuda() if self.has_cuda else one_hot_feat
        self.one_hot_q = torch.eye(self.concept_num, device=self.one_hot_feat.device)
        zero_padding = torch.zeros(1, self.concept_num, device=self.one_hot_feat.device)
# one-hot feature and question
        one_hot_feat = torch.eye(self.res_len * self.concept_num)
        self.one_hot_feat = one_hot_feat.cuda() if self.has_cuda else one_hot_feat
        self.one_hot_q = torch.eye(self.concept_num, device=self.one_hot_feat.device)
        zero_padding = torch.zeros(1, self.concept_num, device=self.one_hot_feat.device)
        self.one_hot_q = torch.cat((self.one_hot_q, zero_padding), dim=0)
        # concept and concept & response embeddings
        self.emb_x = nn.Embedding(self.res_len * concept_num, embedding_dim)
        # last embedding is used for padding, so dim + 1
        self.emb_c = nn.Embedding(concept_num + 1, embedding_dim, padding_idx=-1)

        # f_self function and f_neighbor functions
        mlp_input_dim = hidden_dim + embedding_dim
        self.f_self = MLP(mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.f_neighbor_list = nn.ModuleList()
        if graph_type in ['Dense', 'Transition', 'DKT', 'PAM']:
            # f_in and f_out functions
            self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))
            self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))
        else:  # ['MHA', 'VAE']
            for i in range(edge_type_num):
if graph_type in ['Dense', 'Transition', 'DKT', 'PAM']:
            # f_in and f_out functions
            self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))
            self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))
        else:  # ['MHA', 'VAE']
            for i in range(edge_type_num):
                self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))

        # Erase & Add Gate
        self.erase_add_gate = EraseAddGate(hidden_dim, concept_num)
        # Gate Recurrent Unit
        self.gru = nn.GRUCell(hidden_dim, hidden_dim, bias=bias)
        # prediction layer
        self.predict = nn.Linear(hidden_dim, 1, bias=bias)

    # Aggregate step, as shown in Section 3.2.1 of the paper
    def _aggregate(self, xt, qt, ht, batch_size):
        r"""
        Parameters:
            xt: input one-hot question answering features at the current timestamp
            qt: question indices for all students in a batch at the current timestamp
            ht: hidden representations of all concepts at the current timestamp
            batch_size: the size of a student batch
        Shape:
            xt: [batch_size]
            qt: [batch_size]
            ht: [batch_size, concept_num, hidden_dim]
ht: hidden representations of all concepts at the current timestamp
            batch_size: the size of a student batch
        Shape:
            xt: [batch_size]
            qt: [batch_size]
            ht: [batch_size, concept_num, hidden_dim]
            tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
        Return:
            tmp_ht: aggregation results of concept hidden knowledge state and concept(& response) embedding
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        x_idx_mat = torch.arange(self.res_len * self.concept_num, device=xt.device)
        x_embedding = self.emb_x(x_idx_mat)  # [res_len * concept_num, embedding_dim]
        masked_feat = F.embedding(xt[qt_mask], self.one_hot_feat)  # [mask_num, res_len * concept_num]
        res_embedding = masked_feat.mm(x_embedding)  # [mask_num, embedding_dim]
        mask_num = res_embedding.shape[0]

        concept_idx_mat = self.concept_num * torch.ones((batch_size, self.concept_num), device=xt.device).long()
        concept_idx_mat[qt_mask, :] = torch.arange(self.concept_num, device=xt.device)
        concept_embedding = self.emb_c(concept_idx_mat)  # [batch_size, concept_num, embedding_dim]

        index_tuple = (torch.arange(mask_num, device=xt.device), qt[qt_mask].long())
concept_idx_mat = self.concept_num * torch.ones((batch_size, self.concept_num), device=xt.device).long()
        concept_idx_mat[qt_mask, :] = torch.arange(self.concept_num, device=xt.device)
        concept_embedding = self.emb_c(concept_idx_mat)  # [batch_size, concept_num, embedding_dim]

        index_tuple = (torch.arange(mask_num, device=xt.device), qt[qt_mask].long())
        concept_embedding[qt_mask] = concept_embedding[qt_mask].index_put(index_tuple, res_embedding)
        tmp_ht = torch.cat((ht, concept_embedding), dim=-1)  # [batch_size, concept_num, hidden_dim + embedding_dim]
        return tmp_ht

    # GNN aggregation step, as shown in 3.3.2 Equation 1 of the paper
    def _agg_neighbors(self, tmp_ht, qt):
        r"""
        Parameters:
            tmp_ht: temporal hidden representations of all concepts after the aggregate step
            qt: question indices for all students in a batch at the current timestamp
        Shape:
            tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
            qt: [batch_size]
            m_next: [batch_size, concept_num, hidden_dim]
        Return:
            m_next: hidden representations of all concepts aggregating neighboring representations at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
qt: [batch_size]
            m_next: [batch_size, concept_num, hidden_dim]
        Return:
            m_next: hidden representations of all concepts aggregating neighboring representations at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
            z_prob: probability distribution of latent variable z in VAE (optional)
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        masked_qt = qt[qt_mask]  # [mask_num, ]
        masked_tmp_ht = tmp_ht[qt_mask]  # [mask_num, concept_num, hidden_dim + embedding_dim]
        mask_num = masked_tmp_ht.shape[0]
        self_index_tuple = (torch.arange(mask_num, device=qt.device), masked_qt.long())
        self_ht = masked_tmp_ht[self_index_tuple]  # [mask_num, hidden_dim + embedding_dim]
        self_features = self.f_self(self_ht)  # [mask_num, hidden_dim]
        expanded_self_ht = self_ht.unsqueeze(dim=1).repeat(1, self.concept_num, 1)  #[mask_num, concept_num, hidden_dim + embedding_dim]
        neigh_ht = torch.cat((expanded_self_ht, masked_tmp_ht), dim=-1)  #[mask_num, concept_num, 2 * (hidden_dim + embedding_dim)]
        concept_embedding, rec_embedding, z_prob = None, None, None

        if self.graph_type in ['Dense', 'Transition', 'DKT', 'PAM']:
self_features = self.f_self(self_ht)  # [mask_num, hidden_dim]
        expanded_self_ht = self_ht.unsqueeze(dim=1).repeat(1, self.concept_num, 1)  #[mask_num, concept_num, hidden_dim + embedding_dim]
        neigh_ht = torch.cat((expanded_self_ht, masked_tmp_ht), dim=-1)  #[mask_num, concept_num, 2 * (hidden_dim + embedding_dim)]
        concept_embedding, rec_embedding, z_prob = None, None, None

        if self.graph_type in ['Dense', 'Transition', 'DKT', 'PAM']:
            adj = self.graph[masked_qt.long(), :].unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
            reverse_adj = self.graph[:, masked_qt.long()].transpose(0, 1).unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
            # self.f_neighbor_list[0](neigh_ht) shape: [mask_num, concept_num, hidden_dim]
            neigh_features = adj * self.f_neighbor_list[0](neigh_ht) + reverse_adj * self.f_neighbor_list[1](neigh_ht)
        else:  # ['MHA', 'VAE']
            concept_index = torch.arange(self.concept_num, device=qt.device)
            concept_embedding = self.emb_c(concept_index)  # [concept_num, embedding_dim]
            if self.graph_type == 'MHA':
                query = self.emb_c(masked_qt)
                key = concept_embedding
                att_mask = Variable(torch.ones(self.edge_type_num, mask_num, self.concept_num, device=qt.device))
                for k in range(self.edge_type_num):
concept_embedding = self.emb_c(concept_index)  # [concept_num, embedding_dim]
            if self.graph_type == 'MHA':
                query = self.emb_c(masked_qt)
                key = concept_embedding
                att_mask = Variable(torch.ones(self.edge_type_num, mask_num, self.concept_num, device=qt.device))
                for k in range(self.edge_type_num):
                    index_tuple = (torch.arange(mask_num, device=qt.device), masked_qt.long())
                    att_mask[k] = att_mask[k].index_put(index_tuple, torch.zeros(mask_num, device=qt.device))
                graphs = self.graph_model(masked_qt, query, key, att_mask)
            else:  # self.graph_type == 'VAE'
                sp_send, sp_rec, sp_send_t, sp_rec_t = self._get_edges(masked_qt)
                graphs, rec_embedding, z_prob = self.graph_model(concept_embedding, sp_send, sp_rec, sp_send_t, sp_rec_t)
            neigh_features = 0
            for k in range(self.edge_type_num):
                adj = graphs[k][masked_qt, :].unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
                if k == 0:
                    neigh_features = adj * self.f_neighbor_list[k](neigh_ht)
                else:
                    neigh_features = neigh_features + adj * self.f_neighbor_list[k](neigh_ht)
            if self.graph_type == 'MHA':
adj = graphs[k][masked_qt, :].unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
                if k == 0:
                    neigh_features = adj * self.f_neighbor_list[k](neigh_ht)
                else:
                    neigh_features = neigh_features + adj * self.f_neighbor_list[k](neigh_ht)
            if self.graph_type == 'MHA':
                neigh_features = 1. / self.edge_type_num * neigh_features
        # neigh_features: [mask_num, concept_num, hidden_dim]
        m_next = tmp_ht[:, :, :self.hidden_dim]
        m_next[qt_mask] = neigh_features
        m_next[qt_mask] = m_next[qt_mask].index_put(self_index_tuple, self_features)
        return m_next, concept_embedding, rec_embedding, z_prob

    # Update step, as shown in Section 3.3.2 of the paper
    def _update(self, tmp_ht, ht, qt):
        r"""
        Parameters:
            tmp_ht: temporal hidden representations of all concepts after the aggregate step
            ht: hidden representations of all concepts at the current timestamp
            qt: question indices for all students in a batch at the current timestamp
        Shape:
            tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
            ht: [batch_size, concept_num, hidden_dim]
            qt: [batch_size]
            h_next: [batch_size, concept_num, hidden_dim]
        Return:
Shape:
            tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
            ht: [batch_size, concept_num, hidden_dim]
            qt: [batch_size]
            h_next: [batch_size, concept_num, hidden_dim]
        Return:
            h_next: hidden representations of all concepts at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
            z_prob: probability distribution of latent variable z in VAE (optional)
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        mask_num = qt_mask.nonzero().shape[0]
        # GNN Aggregation
        m_next, concept_embedding, rec_embedding, z_prob = self._agg_neighbors(tmp_ht, qt)  # [batch_size, concept_num, hidden_dim]
        # Erase & Add Gate
        m_next[qt_mask] = self.erase_add_gate(m_next[qt_mask])  # [mask_num, concept_num, hidden_dim]
        # GRU
        h_next = m_next
        res = self.gru(m_next[qt_mask].reshape(-1, self.hidden_dim), ht[qt_mask].reshape(-1, self.hidden_dim))  # [mask_num * concept_num, hidden_num]
        index_tuple = (torch.arange(mask_num, device=qt_mask.device), )
        h_next[qt_mask] = h_next[qt_mask].index_put(index_tuple, res.reshape(-1, self.concept_num, self.hidden_dim))
        return h_next, concept_embedding, rec_embedding, z_prob
h_next = m_next
        res = self.gru(m_next[qt_mask].reshape(-1, self.hidden_dim), ht[qt_mask].reshape(-1, self.hidden_dim))  # [mask_num * concept_num, hidden_num]
        index_tuple = (torch.arange(mask_num, device=qt_mask.device), )
        h_next[qt_mask] = h_next[qt_mask].index_put(index_tuple, res.reshape(-1, self.concept_num, self.hidden_dim))
        return h_next, concept_embedding, rec_embedding, z_prob

    # Predict step, as shown in Section 3.3.3 of the paper
    def _predict(self, h_next, qt):
        r"""
        Parameters:
            h_next: hidden representations of all concepts at the next timestamp after the update step
            qt: question indices for all students in a batch at the current timestamp
        Shape:
            h_next: [batch_size, concept_num, hidden_dim]
            qt: [batch_size]
            y: [batch_size, concept_num]
        Return:
            y: predicted correct probability of all concepts at the next timestamp
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        y = self.predict(h_next).squeeze(dim=-1)  # [batch_size, concept_num]
        y[qt_mask] = torch.sigmoid(y[qt_mask])  # [batch_size, concept_num]
        return y

    def _get_next_pred(self, yt, q_next):
        r"""
        Parameters:
            yt: predicted correct probability of all concepts at the next timestamp
return y

    def _get_next_pred(self, yt, q_next):
        r"""
        Parameters:
            yt: predicted correct probability of all concepts at the next timestamp
            q_next: question index matrix at the next timestamp
            batch_size: the size of a student batch
        Shape:
            y: [batch_size, concept_num]
            questions: [batch_size, seq_len]
            pred: [batch_size, ]
        Return:
            pred: predicted correct probability of the question answered at the next timestamp
        """
        next_qt = q_next
        next_qt = torch.where(next_qt != -1, next_qt, self.concept_num * torch.ones_like(next_qt, device=yt.device))
        one_hot_qt = F.embedding(next_qt.long(), self.one_hot_q)  # [batch_size, concept_num]
        # dot product between yt and one_hot_qt
        pred = (yt * one_hot_qt).sum(dim=1)  # [batch_size, ]
        return pred

    # Get edges for edge inference in VAE
    def _get_edges(self, masked_qt):
        r"""
        Parameters:
            masked_qt: qt index with -1 padding values removed
        Shape:
            masked_qt: [mask_num, ]
            rel_send: [edge_num, concept_num]
            rel_rec: [edge_num, concept_num]
        Return:
            rel_send: from nodes in edges which send messages to other nodes
Shape:
            masked_qt: [mask_num, ]
            rel_send: [edge_num, concept_num]
            rel_rec: [edge_num, concept_num]
        Return:
            rel_send: from nodes in edges which send messages to other nodes
            rel_rec:  to nodes in edges which receive messages from other nodes
        """
        mask_num = masked_qt.shape[0]
        row_arr = masked_qt.cpu().numpy().reshape(-1, 1)  # [mask_num, 1]
        row_arr = np.repeat(row_arr, self.concept_num, axis=1)  # [mask_num, concept_num]
        col_arr = np.arange(self.concept_num).reshape(1, -1)  # [1, concept_num]
        col_arr = np.repeat(col_arr, mask_num, axis=0)  # [mask_num, concept_num]
        # add reversed edges
        new_row = np.vstack((row_arr, col_arr))  # [2 * mask_num, concept_num]
        new_col = np.vstack((col_arr, row_arr))  # [2 * mask_num, concept_num]
        row_arr = new_row.flatten()  # [2 * mask_num * concept_num, ]
        col_arr = new_col.flatten()  # [2 * mask_num * concept_num, ]
        data_arr = np.ones(2 * mask_num * self.concept_num)
        init_graph = sp.coo_matrix((data_arr, (row_arr, col_arr)), shape=(self.concept_num, self.concept_num))
        init_graph.setdiag(0)  # remove self-loop edges
        row_arr, col_arr, _ = sp.find(init_graph)
        row_tensor = torch.from_numpy(row_arr).long()
col_arr = new_col.flatten()  # [2 * mask_num * concept_num, ]
        data_arr = np.ones(2 * mask_num * self.concept_num)
        init_graph = sp.coo_matrix((data_arr, (row_arr, col_arr)), shape=(self.concept_num, self.concept_num))
        init_graph.setdiag(0)  # remove self-loop edges
        row_arr, col_arr, _ = sp.find(init_graph)
        row_tensor = torch.from_numpy(row_arr).long()
        col_tensor = torch.from_numpy(col_arr).long()
        one_hot_table = torch.eye(self.concept_num, self.concept_num)
        rel_send = F.embedding(row_tensor, one_hot_table)  # [edge_num, concept_num]
        rel_rec = F.embedding(col_tensor, one_hot_table)  # [edge_num, concept_num]
        sp_rec, sp_send = rel_rec.to_sparse(), rel_send.to_sparse()
        sp_rec_t, sp_send_t = rel_rec.T.to_sparse(), rel_send.T.to_sparse()
        sp_send = sp_send.to(device=masked_qt.device)
        sp_rec = sp_rec.to(device=masked_qt.device)
        sp_send_t = sp_send_t.to(device=masked_qt.device)
        sp_rec_t = sp_rec_t.to(device=masked_qt.device)
        return sp_send, sp_rec, sp_send_t, sp_rec_t

    def forward(self, features, questions):
        r"""
        Parameters:
            features: input one-hot matrix
            questions: question index matrix
        seq_len dimension needs padding, because different students may have learning sequences with different lengths.
def forward(self, features, questions):
        r"""
        Parameters:
            features: input one-hot matrix
            questions: question index matrix
        seq_len dimension needs padding, because different students may have learning sequences with different lengths.
        Shape:
            features: [batch_size, seq_len]
            questions: [batch_size, seq_len]
            pred_res: [batch_size, seq_len - 1]
        Return:
            pred_res: the correct probability of questions answered at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
            z_prob: probability distribution of latent variable z in VAE (optional)
        """
        batch_size, seq_len = features.shape
        ht = Variable(torch.zeros((batch_size, self.concept_num, self.hidden_dim), device=features.device))
        pred_list = []
        ec_list = []  # concept embedding list in VAE
        rec_list = []  # reconstructed embedding list in VAE
        z_prob_list = []  # probability distribution of latent variable z in VAE
        for i in range(seq_len):
            xt = features[:, i]  # [batch_size]
            qt = questions[:, i]  # [batch_size]
            qt_mask = torch.ne(qt, -1)  # [batch_size], next_qt != -1
rec_list = []  # reconstructed embedding list in VAE
        z_prob_list = []  # probability distribution of latent variable z in VAE
        for i in range(seq_len):
            xt = features[:, i]  # [batch_size]
            qt = questions[:, i]  # [batch_size]
            qt_mask = torch.ne(qt, -1)  # [batch_size], next_qt != -1
            tmp_ht = self._aggregate(xt, qt, ht, batch_size)  # [batch_size, concept_num, hidden_dim + embedding_dim]
            h_next, concept_embedding, rec_embedding, z_prob = self._update(tmp_ht, ht, qt)  # [batch_size, concept_num, hidden_dim]
            ht[qt_mask] = h_next[qt_mask]  # update new ht
            yt = self._predict(h_next, qt)  # [batch_size, concept_num]
            if i < seq_len - 1:
                pred = self._get_next_pred(yt, questions[:, i + 1])
                pred_list.append(pred)
            ec_list.append(concept_embedding)
            rec_list.append(rec_embedding)
            z_prob_list.append(z_prob)
        pred_res = torch.stack(pred_list, dim=1)  # [batch_size, seq_len - 1]
        return pred_res, ec_list, rec_list, z_prob_list
```

```python
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention module
    NOTE: Stole and modify from https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py
    """
class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention module
    NOTE: Stole and modify from https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py
    """

    def __init__(self, n_head, concept_num, input_dim, d_k, dropout=0.):
        super(MultiHeadAttention, self).__init__()
        self.n_head = n_head
        self.concept_num = concept_num
        self.d_k = d_k
        self.w_qs = nn.Linear(input_dim, n_head * d_k, bias=False)
        self.w_ks = nn.Linear(input_dim, n_head * d_k, bias=False)
        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5, attn_dropout=dropout)
        # inferred latent graph, used for saving and visualization
        self.graphs = nn.Parameter(torch.zeros(n_head, concept_num, concept_num))
        self.graphs.requires_grad = False

    def _get_graph(self, attn_score, qt):
        r"""
        Parameters:
            attn_score: attention score of all queries
            qt: masked question index
        Shape:
            attn_score: [n_head, mask_num, concept_num]
            qt: [mask_num]
        Return:
            graphs: n_head types of inferred graphs
        """
        graphs = Variable(torch.zeros(self.n_head, self.concept_num, self.concept_num, device=qt.device))
        for k in range(self.n_head):
            index_tuple = (qt.long(), )
Return:
            graphs: n_head types of inferred graphs
        """
        graphs = Variable(torch.zeros(self.n_head, self.concept_num, self.concept_num, device=qt.device))
        for k in range(self.n_head):
            index_tuple = (qt.long(), )
            graphs[k] = graphs[k].index_put(index_tuple, attn_score[k])  # used for calculation
            #############################
            # here, we need to detach edges when storing it into self.graphs in case memory leak!
            self.graphs.data[k] = self.graphs.data[k].index_put(index_tuple, attn_score[k].detach())  # used for saving and visualization
            #############################
        return graphs

    def forward(self, qt, query, key, mask=None):
        r"""
        Parameters:
            qt: masked question index
            query: answered concept embedding for a student batch
            key: concept embedding matrix
            mask: mask matrix
        Shape:
            qt: [mask_num]
            query: [mask_num, embedding_dim]
            key: [concept_num, embedding_dim]
        Return:
            graphs: n_head types of inferred graphs
        """
        d_k, n_head = self.d_k, self.n_head
        len_q, len_k = query.size(0), key.size(0)

        # Pass through the pre-attention projection: lq x (n_head *dk)
        # Separate different heads: lq x n_head x dk
"""
        d_k, n_head = self.d_k, self.n_head
        len_q, len_k = query.size(0), key.size(0)

        # Pass through the pre-attention projection: lq x (n_head *dk)
        # Separate different heads: lq x n_head x dk
        q = self.w_qs(query).view(len_q, n_head, d_k)
        k = self.w_ks(key).view(len_k, n_head, d_k)

        # Transpose for attention dot product: n_head x lq x dk
        q, k = q.transpose(0, 1), k.transpose(0, 1)
        attn_score = self.attention(q, k, mask=mask)  # [n_head, mask_num, concept_num]
        graphs = self._get_graph(attn_score, qt)
        return graphs
```

```python
class KTLoss(nn.Module):

    def __init__(self):
        super(KTLoss, self).__init__()

    def forward(self, pred_answers, real_answers):
        r"""
        Parameters:
            pred_answers: the correct probability of questions answered at the next timestamp
            real_answers: the real results(0 or 1) of questions answered at the next timestamp
        Shape:
            pred_answers: [batch_size, seq_len - 1]
            real_answers: [batch_size, seq_len]
        Return:
        """
        real_answers = real_answers[:, 1:]  # timestamp=1 ~ T
        # real_answers shape: [batch_size, seq_len - 1]
        # Here we can directly use nn.BCELoss, but this loss doesn't have ignore_index function
        answer_mask = torch.ne(real_answers, -1)
Return:
        """
        real_answers = real_answers[:, 1:]  # timestamp=1 ~ T
        # real_answers shape: [batch_size, seq_len - 1]
        # Here we can directly use nn.BCELoss, but this loss doesn't have ignore_index function
        answer_mask = torch.ne(real_answers, -1)
        pred_one, pred_zero = pred_answers, 1.0 - pred_answers  # [batch_size, seq_len - 1]

        # calculate auc and accuracy metrics
        try:
            y_true = real_answers[answer_mask].cpu().detach().numpy()
            y_pred = pred_one[answer_mask].cpu().detach().numpy()
            auc = roc_auc_score(y_true, y_pred)  # may raise ValueError
            output = torch.cat((pred_zero[answer_mask].reshape(-1, 1), pred_one[answer_mask].reshape(-1, 1)), dim=1)
            label = real_answers[answer_mask].reshape(-1, 1)
            acc = accuracy(output, label)
            acc = float(acc.cpu().detach().numpy())
        except ValueError as e:
            auc, acc = -1, -1

        # calculate NLL loss
        pred_one[answer_mask] = torch.log(pred_one[answer_mask])
        pred_zero[answer_mask] = torch.log(pred_zero[answer_mask])
        pred_answers = torch.cat((pred_zero.unsqueeze(dim=1), pred_one.unsqueeze(dim=1)), dim=1)
        # pred_answers shape: [batch_size, 2, seq_len - 1]
        nll_loss = nn.NLLLoss(ignore_index=-1)  # ignore masked values in real_answers
# calculate NLL loss
        pred_one[answer_mask] = torch.log(pred_one[answer_mask])
        pred_zero[answer_mask] = torch.log(pred_zero[answer_mask])
        pred_answers = torch.cat((pred_zero.unsqueeze(dim=1), pred_one.unsqueeze(dim=1)), dim=1)
        # pred_answers shape: [batch_size, 2, seq_len - 1]
        nll_loss = nn.NLLLoss(ignore_index=-1)  # ignore masked values in real_answers
        loss = nll_loss(pred_answers, real_answers.long())
        return loss, auc, acc


class VAELoss(nn.Module):

    def __init__(self, concept_num, edge_type_num=2, prior=False, var=5e-5):
        super(VAELoss, self).__init__()
        self.concept_num = concept_num
        self.edge_type_num = edge_type_num
        self.prior = prior
        self.var = var

    def forward(self, ec_list, rec_list, z_prob_list, log_prior=None):
        time_stamp_num = len(ec_list)
        loss = 0
        for time_idx in range(time_stamp_num):
            output = rec_list[time_idx]
            target = ec_list[time_idx]
            prob = z_prob_list[time_idx]
            loss_nll = nll_gaussian(output, target, self.var)
            if self.prior:
                assert log_prior is not None
                loss_kl = kl_categorical(prob, log_prior, self.concept_num)
            else:
                loss_kl = kl_categorical_uniform(prob, self.concept_num, self.edge_type_num)
loss_nll = nll_gaussian(output, target, self.var)
            if self.prior:
                assert log_prior is not None
                loss_kl = kl_categorical(prob, log_prior, self.concept_num)
            else:
                loss_kl = kl_categorical_uniform(prob, self.concept_num, self.edge_type_num)
            if time_idx == 0:
                loss = loss_nll + loss_kl
            else:
                loss = loss + loss_nll + loss_kl
        return loss / time_stamp_num
```

```python
import os
from torch.utils.data import Dataset, TensorDataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

class KTDataset(Dataset):
    def __init__(self, features, questions, answers):
        super(KTDataset, self).__init__()
        self.features = features
        self.questions = questions
        self.answers = answers

    def __getitem__(self, index):
        return self.features[index], self.questions[index], self.answers[index]

    def __len__(self):
        return len(self.features)


def pad_collate(batch):
    (features, questions, answers) = zip(*batch)
    features = [torch.LongTensor(feat) for feat in features]
    questions = [torch.LongTensor(qt) for qt in questions]
    answers = [torch.LongTensor(ans) for ans in answers]
    feature_pad = pad_sequence(features, batch_first=True, padding_value=-1)
def pad_collate(batch):
    (features, questions, answers) = zip(*batch)
    features = [torch.LongTensor(feat) for feat in features]
    questions = [torch.LongTensor(qt) for qt in questions]
    answers = [torch.LongTensor(ans) for ans in answers]
    feature_pad = pad_sequence(features, batch_first=True, padding_value=-1)
    question_pad = pad_sequence(questions, batch_first=True, padding_value=-1)
    answer_pad = pad_sequence(answers, batch_first=True, padding_value=-1)
    return feature_pad, question_pad, answer_pad


def load_dataset(file_path, batch_size, graph_type, dkt_graph_path=None, train_ratio=0.7, val_ratio=0.2, shuffle=True, model_type='GKT', use_binary=True, res_len=2, use_cuda=True):
    r"""
    Parameters:
        file_path: input file path of knowledge tracing data
        batch_size: the size of a student batch
        graph_type: the type of the concept graph
        shuffle: whether to shuffle the dataset or not
        use_cuda: whether to use GPU to accelerate training speed
    Return:
        concept_num: the number of all concepts(or questions)
        graph: the static graph is graph type is in ['Dense', 'Transition', 'DKT'], otherwise graph is None
        train_data_loader: data loader of the training dataset
        valid_data_loader: data loader of the validation dataset
        test_data_loader: data loader of the test dataset
    """
concept_num: the number of all concepts(or questions)
        graph: the static graph is graph type is in ['Dense', 'Transition', 'DKT'], otherwise graph is None
        train_data_loader: data loader of the training dataset
        valid_data_loader: data loader of the validation dataset
        test_data_loader: data loader of the test dataset
    """
    df = pd.read_csv(file_path)
    if "skill_id" not in df.columns:
        raise KeyError(f"The column 'skill_id' was not found on {file_path}")
    if "correct" not in df.columns:
        raise KeyError(f"The column 'correct' was not found on {file_path}")
    if "user_id" not in df.columns:
        raise KeyError(f"The column 'user_id' was not found on {file_path}")

    # if not (df['correct'].isin([0, 1])).all():
    #     raise KeyError(f"The values of the column 'correct' must be 0 or 1.")

    # Step 1.1 - Remove questions without skill
    df.dropna(subset=['skill_id'], inplace=True)

    # Step 1.2 - Remove users with a single answer
    df = df.groupby('user_id').filter(lambda q: len(q) > 1).copy()

    # Step 2 - Enumerate skill id
    df['skill'], _ = pd.factorize(df['skill_id'], sort=True)  # we can also use problem_id to represent exercises

    # Step 3 - Cross skill id with answer to form a synthetic feature
df = df.groupby('user_id').filter(lambda q: len(q) > 1).copy()

    # Step 2 - Enumerate skill id
    df['skill'], _ = pd.factorize(df['skill_id'], sort=True)  # we can also use problem_id to represent exercises

    # Step 3 - Cross skill id with answer to form a synthetic feature
    # use_binary: (0,1); !use_binary: (1,2,3,4,5,6,7,8,9,10,11,12). Either way, the correct result index is guaranteed to be 1
    if use_binary:
        df['skill_with_answer'] = df['skill'] * 2 + df['correct']
    else:
        df['skill_with_answer'] = df['skill'] * res_len + df['correct'] - 1


    # Step 4 - Convert to a sequence per user id and shift features 1 timestep
    feature_list = []
    question_list = []
    answer_list = []
    seq_len_list = []

    def get_data(series):
        feature_list.append(series['skill_with_answer'].tolist())
        question_list.append(series['skill'].tolist())
        answer_list.append(series['correct'].eq(1).astype('int').tolist())
        seq_len_list.append(series['correct'].shape[0])

    df.groupby('user_id').apply(get_data)
    max_seq_len = np.max(seq_len_list)
    print('max seq_len: ', max_seq_len)
    student_num = len(seq_len_list)
    print('student num: ', student_num)
    feature_dim = int(df['skill_with_answer'].max() + 1)
    print('feature_dim: ', feature_dim)
    question_dim = int(df['skill'].max() + 1)
print('max seq_len: ', max_seq_len)
    student_num = len(seq_len_list)
    print('student num: ', student_num)
    feature_dim = int(df['skill_with_answer'].max() + 1)
    print('feature_dim: ', feature_dim)
    question_dim = int(df['skill'].max() + 1)
    print('question_dim: ', question_dim)
    concept_num = question_dim

    # print('feature_dim:', feature_dim, 'res_len*question_dim:', res_len*question_dim)
    # assert feature_dim == res_len * question_dim

    kt_dataset = KTDataset(feature_list, question_list, answer_list)
    train_size = int(train_ratio * student_num)
    val_size = int(val_ratio * student_num)
    test_size = student_num - train_size - val_size
    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(kt_dataset, [train_size, val_size, test_size])
    print('train_size: ', train_size, 'val_size: ', val_size, 'test_size: ', test_size)

    train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=pad_collate)
    valid_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=pad_collate)
    test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=pad_collate)

    graph = None
    if model_type == 'GKT':
        if graph_type == 'Dense':
            graph = build_dense_graph(concept_num)
test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=pad_collate)

    graph = None
    if model_type == 'GKT':
        if graph_type == 'Dense':
            graph = build_dense_graph(concept_num)
        elif graph_type == 'Transition':
            graph = build_transition_graph(question_list, seq_len_list, train_dataset.indices, student_num, concept_num)
        elif graph_type == 'DKT':
            graph = build_dkt_graph(dkt_graph_path, concept_num)
        if use_cuda and graph_type in ['Dense', 'Transition', 'DKT']:
            graph = graph.cuda()
    return concept_num, graph, train_data_loader, valid_data_loader, test_data_loader


def build_transition_graph(question_list, seq_len_list, indices, student_num, concept_num):
    graph = np.zeros((concept_num, concept_num))
    student_dict = dict(zip(indices, np.arange(student_num)))
    for i in range(student_num):
        if i not in student_dict:
            continue
        questions = question_list[i]
        seq_len = seq_len_list[i]
        for j in range(seq_len - 1):
            pre = questions[j]
            next = questions[j + 1]
            graph[pre, next] += 1
    np.fill_diagonal(graph, 0)
    # row normalization
    rowsum = np.array(graph.sum(1))
    def inv(x):
        if x == 0:
            return x
        return 1. / x
    inv_func = np.vectorize(inv)
rowsum = np.array(graph.sum(1))
    def inv(x):
        if x == 0:
            return x
        return 1. / x
    inv_func = np.vectorize(inv)
    r_inv = inv_func(rowsum).flatten()
    r_mat_inv = np.diag(r_inv)
    graph = r_mat_inv.dot(graph)
    # covert to tensor
    graph = torch.from_numpy(graph).float()
    return graph


def build_dkt_graph(file_path, concept_num):
    graph = np.loadtxt(file_path)
    assert graph.shape[0] == concept_num and graph.shape[1] == concept_num
    graph = torch.from_numpy(graph).float()
    return graph
```

```python
args = {
    'seed':42,
    'data-dir': '/content/drive/MyDrive/GKT-data',
    'data-file': 'assistment_test15.csv',
    'save-dir': 'save',
    'graph-save-dir': '',
    'load-dir': '',
    'dkt-graph-dir': '',
    'dkt-graph': 'dkt-graph',
    'model': 'GKT',
    'hid-dim': 32,
    'emb-dim': 32,
    'attn-dim': 32,
    'vae-encoder-dim': 32,
    'vae-decoder-dim': 32,
    'edge-types': 2,
    'graph-type': 'MHA',
    'dropout': 0,
    'bias': True,
    'binary': True,
    'result-type': 12,
    'temp': 0.5,
    'hard': False,
    'no-factor': False,
    'prior': True,
    'var': 1,
    'epochs': 50,
    'batch-size': 128,
    'train-ratio': 0.6,
    'val-ratio': 0.2,
    'shuffle': True,
    'lr': 0.001,
    'lr-decay': 200,
    'gamma': 0.5,
    'test': False,
    'test-model-dir': ''
}
```

```python
import random
'test-model-dir': ''
}
```

```python
import random
args['cuda'] = torch.cuda.is_available()
args['factor'] = not args['no-factor']

random.seed(args['seed'])
np.random.seed(args['seed'])
torch.manual_seed(args['seed'])

if args['cuda']:
  torch.cuda.manual_seed(args['seed'])
  torch.cuda.manual_seed_all(args['seed'])
  torch.backends.cudnn.benchmark = False
  torch.backends.cudnn.deterministic = True

res_len = 2 if args['binary'] else args['result_type']

log = None
save_dir = args['save-dir']

exp_counter = 0
now = datetime.datetime.now()
# timestamp = now.isoformat()
timestamp = now.strftime('%Y-%m-%d %H-%M-%S')
    
model_file_name = 'GKT' + '-' + args['graph-type']
save_dir = '{}/exp{}/'.format(args['save-dir'], model_file_name + timestamp)
if not os.path.exists(save_dir):
  os.makedirs(save_dir)
meta_file = os.path.join(save_dir, 'metadata.pkl')
model_file = os.path.join(save_dir, model_file_name + '.pt')
optimizer_file = os.path.join(save_dir, model_file_name + '-Optimizer.pt')
scheduler_file = os.path.join(save_dir, model_file_name + '-Scheduler.pt')
log_file = os.path.join(save_dir, 'log.txt')
log = open(log_file, 'w')
pickle.dump({'args': args}, open(meta_file, "wb"))

dataset_path = os.path.join(args['data-dir'], args['data-file'])
dkt_graph_path = os.path.join(args['dkt-graph-dir'], args['dkt-graph'])
if not os.path.exists(dkt_graph_path):
log = open(log_file, 'w')
pickle.dump({'args': args}, open(meta_file, "wb"))

dataset_path = os.path.join(args['data-dir'], args['data-file'])
dkt_graph_path = os.path.join(args['dkt-graph-dir'], args['dkt-graph'])
if not os.path.exists(dkt_graph_path):
    dkt_graph_path = None
concept_num, graph, train_loader, valid_loader, test_loader = load_dataset(dataset_path, args['batch-size'], args['graph-type'], dkt_graph_path=dkt_graph_path,
                                                                           train_ratio=args['train-ratio'], val_ratio=args['val-ratio'], shuffle=args['shuffle'],
                                                                           model_type=args['model'], use_cuda=args['cuda'])

if args['graph-type'] == 'MHA':
  graph_model = MultiHeadAttention(args['edge-types'], concept_num, args['emb-dim'], args['attn-dim'], dropout=args['dropout'])
elif args['graph-type'] == 'VAE':
  graph_model = VAE(args['emb_dim'], args['vae-encoder-dim'], args['edge-types'], args['vae-decoder-dim'], args['vae-decoder-dim'], concept_num,
                          edge_type_num=args['edge-types'], tau=args['temp'], factor=args['factor'], dropout=args['dropout'], bias=args['bias'])
  vae_loss = VAELoss(concept_num, edge_type_num=args['edge-types'], prior=args['prior'], var=args['var'])
  if args.cuda:
      vae_loss = vae_loss.cuda()
elif args['graph-type'] == 'VAE':
  graph_model = VAE(args['emb_dim'], args['vae-encoder-dim'], args['edge-types'], args['vae-decoder-dim'], args['vae-decoder-dim'], concept_num,
                          edge_type_num=args['edge-types'], tau=args['temp'], factor=args['factor'], dropout=args['dropout'], bias=args['bias'])
  vae_loss = VAELoss(concept_num, edge_type_num=args['edge-types'], prior=args['prior'], var=args['var'])
  if args.cuda:
      vae_loss = vae_loss.cuda()
if args['cuda'] and args['graph-type'] in ['MHA', 'VAE']:
  graph_model = graph_model.cuda()
model = GKT(concept_num, args['hid-dim'], args['emb-dim'], args['edge-types'], args['graph-type'], graph=graph, graph_model=graph_model,
                dropout=args['dropout'], bias=args['bias'], has_cuda=args['cuda'])

kt_loss = KTLoss()

# build optimizer
optimizer = optim.Adam(model.parameters(), lr=args['lr'])
scheduler = lr_scheduler.StepLR(optimizer, step_size=args['lr-decay'], gamma=args['gamma'])

optimizer = optim.Adam(model.parameters(), lr=args['lr'])
scheduler = lr_scheduler.StepLR(optimizer, step_size=args['lr-decay'], gamma=args['gamma'])

if args['model'] == 'GKT' and args['prior']:
    prior = np.array([0.91, 0.03, 0.03, 0.03])  # TODO: hard coded for now
    print("Using prior")
    print(prior)
    log_prior = torch.FloatTensor(np.log(prior))
    log_prior = torch.unsqueeze(log_prior, 0)
if args['model'] == 'GKT' and args['prior']:
    prior = np.array([0.91, 0.03, 0.03, 0.03])  # TODO: hard coded for now
    print("Using prior")
    print(prior)
    log_prior = torch.FloatTensor(np.log(prior))
    log_prior = torch.unsqueeze(log_prior, 0)
    log_prior = torch.unsqueeze(log_prior, 0)
    log_prior = Variable(log_prior)
    if args['cuda']:
        log_prior = log_prior.cuda()

if args['cuda']:
    model = model.cuda()
    kt_loss = KTLoss()
```

```python
def train(epoch, best_val_loss):
    t = time.time()
    loss_train = []
    kt_train = []
    vae_train = []
    auc_train = []
    acc_train = []
    if graph_model is not None:
        graph_model.train()
    model.train()
    for batch_idx, (features, questions, answers) in enumerate(train_loader):
        t1 = time.time()
        if args['cuda']:
            features, questions, answers = features.cuda(), questions.cuda(), answers.cuda()
        ec_list, rec_list, z_prob_list = None, None, None
        pred_res, ec_list, rec_list, z_prob_list = model(features, questions)
        loss_kt, auc, acc = kt_loss(pred_res, answers)
        kt_train.append(float(loss_kt.cpu().detach().numpy()))
        if auc != -1 and acc != -1:
            auc_train.append(auc)
            acc_train.append(acc)

        
        loss = loss_kt
if auc != -1 and acc != -1:
            auc_train.append(auc)
            acc_train.append(acc)

        
        loss = loss_kt
        print('batch idx: ', batch_idx, 'loss kt: ', loss_kt.item(), 'auc: ', auc, 'acc: ', acc, end=' ')
        loss_train.append(float(loss.cpu().detach().numpy()))
        loss.backward()
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()
        del loss
        print('cost time: ', str(time.time() - t1))

    loss_val = []
    kt_val = []
    vae_val = []
    auc_val = []
    acc_val = []

    if graph_model is not None:
        graph_model.eval()
    model.eval()
    with torch.no_grad():
        for batch_idx, (features, questions, answers) in enumerate(valid_loader):
            if args['cuda']:
                features, questions, answers = features.cuda(), questions.cuda(), answers.cuda()
            ec_list, rec_list, z_prob_list = None, None, None
            pred_res, ec_list, rec_list, z_prob_list = model(features, questions)

            loss_kt, auc, acc = kt_loss(pred_res, answers)
            loss_kt = float(loss_kt.cpu().detach().numpy())
            kt_val.append(loss_kt)
            if auc != -1 and acc != -1:
                auc_val.append(auc)
                acc_val.append(acc)

            loss = loss_kt
            loss_val.append(loss)
            del loss
acc_val.append(acc)

            loss = loss_kt
            loss_val.append(loss)
            del loss
    
    print('Epoch: {:04d}'.format(epoch),
              'loss_train: {:.10f}'.format(np.mean(loss_train)),
              'auc_train: {:.10f}'.format(np.mean(auc_train)),
              'acc_train: {:.10f}'.format(np.mean(acc_train)),
              'loss_val: {:.10f}'.format(np.mean(loss_val)),
              'auc_val: {:.10f}'.format(np.mean(auc_val)),
              'acc_val: {:.10f}'.format(np.mean(acc_val)),
              'time: {:.4f}s'.format(time.time() - t))
    if args['save-dir'] and np.mean(loss_val) < best_val_loss:
        print('Best model so far, saving...')
        torch.save(model.state_dict(), model_file)
        torch.save(optimizer.state_dict(), optimizer_file)
        torch.save(scheduler.state_dict(), scheduler_file)
        print('Epoch: {:04d}'.format(epoch),
                  'loss_train: {:.10f}'.format(np.mean(loss_train)),
                  'auc_train: {:.10f}'.format(np.mean(auc_train)),
                  'acc_train: {:.10f}'.format(np.mean(acc_train)),
                  'loss_val: {:.10f}'.format(np.mean(loss_val)),
                  'auc_val: {:.10f}'.format(np.mean(auc_val)),
                  'acc_val: {:.10f}'.format(np.mean(acc_val)),
                  'time: {:.4f}s'.format(time.time() - t), file=log)
        log.flush()
'acc_train: {:.10f}'.format(np.mean(acc_train)),
                  'loss_val: {:.10f}'.format(np.mean(loss_val)),
                  'auc_val: {:.10f}'.format(np.mean(auc_val)),
                  'acc_val: {:.10f}'.format(np.mean(acc_val)),
                  'time: {:.4f}s'.format(time.time() - t), file=log)
        log.flush()
    res = np.mean(loss_val)
    del loss_train
    del auc_train
    del acc_train
    del loss_val
    del auc_val
    del acc_val
    gc.collect()
    if args['cuda']:
        torch.cuda.empty_cache()
    return res
```

```python
import numpy as np
import time
import random
import argparse
import pickle
import os
import gc
import datetime
import torch
import torch.optim as optim
from torch.optim import lr_scheduler
from sklearn.metrics import roc_auc_score
def test():
    loss_test = []
    kt_test = []
    vae_test = []
    auc_test = []
    acc_test = []

    if graph_model is not None:
        graph_model.eval()
    model.eval()
    model.load_state_dict(torch.load(model_file))
    with torch.no_grad():
        for batch_idx, (features, questions, answers) in enumerate(test_loader):
            if args.cuda:
                features, questions, answers = features.cuda(), questions.cuda(), answers.cuda()
            ec_list, rec_list, z_prob_list = None, None, None
model.load_state_dict(torch.load(model_file))
    with torch.no_grad():
        for batch_idx, (features, questions, answers) in enumerate(test_loader):
            if args.cuda:
                features, questions, answers = features.cuda(), questions.cuda(), answers.cuda()
            ec_list, rec_list, z_prob_list = None, None, None
            pred_res, ec_list, rec_list, z_prob_list = model(features, questions)
            
            loss_kt, auc, acc = kt_loss(pred_res, answers)
            loss_kt = float(loss_kt.cpu().detach().numpy())
            if auc != -1 and acc != -1:
                auc_test.append(auc)
                acc_test.append(acc)
            kt_test.append(loss_kt)
            loss = loss_kt
            loss_test.append(loss)
            del loss
    print('--------------------------------')
    print('--------Testing-----------------')
    print('--------------------------------')
    
    print('loss_test: {:.10f}'.format(np.mean(loss_test)),
              'auc_test: {:.10f}'.format(np.mean(auc_test)),
              'acc_test: {:.10f}'.format(np.mean(acc_test)))
    if args['save_dir']:
        print('--------------------------------', file=log)
        print('--------Testing-----------------', file=log)
        print('--------------------------------', file=log)
        
        print('loss_test: {:.10f}'.format(np.mean(loss_test)),
if args['save_dir']:
        print('--------------------------------', file=log)
        print('--------Testing-----------------', file=log)
        print('--------------------------------', file=log)
        
        print('loss_test: {:.10f}'.format(np.mean(loss_test)),
                  'auc_test: {:.10f}'.format(np.mean(auc_test)),
                  'acc_test: {:.10f}'.format(np.mean(acc_test)), file=log)
        log.flush()
    del loss_test
    del auc_test
    del acc_test
    gc.collect()
    if args['cuda']:
        torch.cuda.empty_cache()

if args['test'] is False:
    # Train model
    print('start training!')
    t_total = time.time()
    best_val_loss = np.inf
    best_epoch = 0
    for epoch in range(args['epochs']):
        val_loss = train(epoch, best_val_loss)
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            best_epoch = epoch
    print("Optimization Finished!")
    print("Best Epoch: {:04d}".format(best_epoch))
    if args['save-dir']:
        print("Best Epoch: {:04d}".format(best_epoch), file=log)
        log.flush()
```

```python
torch.save(model, '/content/model_50.pt')
```

```python
new_model = torch.load('/content/model_50.pt')
```
# AI-Assignment
PATH: layers.py
LINES: 1-13

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable

# Graph-based Knowledge Tracing: Modeling Student Proficiency Using Graph Neural Network.
# For more information, please refer to https://dl.acm.org/doi/10.1145/3350546.3352513
# Author: jhljx
# Email: jhljx8918@gmail.com


# Multi-Layer Perceptron(MLP) layer
PATH: layers.py
LINES: 14-51

class MLP(nn.Module):
    """Two-layer fully-connected ReLU net with batch norm."""

    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0., bias=True):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=bias)
        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=bias)
        self.norm = nn.BatchNorm1d(output_dim)
        # the paper said they added Batch Normalization for the output of MLPs, as shown in Section 4.2
        self.dropout = dropout
        self.output_dim = output_dim
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
                m.bias.data.fill_(0.1)
            elif isinstance(m, nn.BatchNorm1d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def batch_norm(self, inputs):
        if inputs.numel() == self.output_dim or inputs.numel() == 0:
            # batch_size == 1 or 0 will cause BatchNorm error, so return the input directly
            return inputs
        if len(inputs.size()) == 3:
            x = inputs.view(inputs.size(0) * inputs.size(1), -1)
            x = self.norm(x)
            return x.view(inputs.size(0), inputs.size(1), -1)
        else:  # len(input_size()) == 2
            return self.norm(inputs)

    def forward(self, inputs):
        x = F.relu(self.fc1(inputs))
        x = F.dropout(x, self.dropout, training=self.training)  # pay attention to add training=self.training
        x = F.relu(self.fc2(x))
        return self.batch_norm(x)
PATH: layers.py
LINES: 54-93

class EraseAddGate(nn.Module):
    """
    Erase & Add Gate module
    NOTE: this erase & add gate is a bit different from that in DKVMN.
    For more information about Erase & Add gate, please refer to the paper "Dynamic Key-Value Memory Networks for Knowledge Tracing"
    The paper can be found in https://arxiv.org/abs/1611.08108
    """

    def __init__(self, feature_dim, concept_num, bias=True):
        super(EraseAddGate, self).__init__()
        # weight
        self.weight = nn.Parameter(torch.rand(concept_num))
        self.reset_parameters()
        # erase gate
        self.erase = nn.Linear(feature_dim, feature_dim, bias=bias)
        # add gate
        self.add = nn.Linear(feature_dim, feature_dim, bias=bias)

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.weight.size(0))
        self.weight.data.uniform_(-stdv, stdv)

    def forward(self, x):
        r"""
        Params:
            x: input feature matrix
        Shape:
            x: [batch_size, concept_num, feature_dim]
            res: [batch_size, concept_num, feature_dim]
        Return:
            res: returned feature matrix with old information erased and new information added
        The GKT paper didn't provide detailed explanation about this erase-add gate. As the erase-add gate in the GKT only has one input parameter,
        this gate is different with that of the DKVMN. We used the input matrix to build the erase and add gates, rather than $\mathbf{v}_{t}$ vector in the DKVMN.
        """
        erase_gate = torch.sigmoid(self.erase(x))  # [batch_size, concept_num, feature_dim]
        # self.weight.unsqueeze(dim=1) shape: [concept_num, 1]
        tmp_x = x - self.weight.unsqueeze(dim=1) * erase_gate * x
        add_feat = torch.tanh(self.add(x))  # [batch_size, concept_num, feature_dim]
        res = tmp_x + self.weight.unsqueeze(dim=1) * add_feat
        return res
PATH: layers.py
LINES: 96-123

class ScaledDotProductAttention(nn.Module):
    """
    Scaled Dot-Product Attention
    NOTE: Stole and modify from https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Modules.py
    """

    def __init__(self, temperature, attn_dropout=0.):
        super().__init__()
        self.temperature = temperature
        self.dropout = attn_dropout

    def forward(self, q, k, mask=None):
        r"""
        Parameters:
            q: multi-head query matrix
            k: multi-head key matrix
            mask: mask matrix
        Shape:
            q: [n_head, mask_num, embedding_dim]
            k: [n_head, concept_num, embedding_dim]
        Return: attention score of all queries
        """
        attn = torch.matmul(q / self.temperature, k.transpose(1, 2))  # [n_head, mask_number, concept_num]
        if mask is not None:
            attn = attn.masked_fill(mask == 0, -1e9)
        # pay attention to add training=self.training!
        attn = F.dropout(F.softmax(attn, dim=0), self.dropout, training=self.training)  # pay attention that dim=-1 is not as good as dim=0!
        return attn
PATH: layers.py
LINES: 126-156

class MLPEncoder(nn.Module):
    """
    MLP encoder module.
    NOTE: Stole and modify the code from https://github.com/ethanfetaya/NRI/blob/master/modules.py
    """
    def __init__(self, input_dim, hidden_dim, output_dim, factor=True, dropout=0., bias=True):
        super(MLPEncoder, self).__init__()
        self.factor = factor
        self.mlp = MLP(input_dim * 2, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.mlp2 = MLP(hidden_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        if self.factor:
            self.mlp3 = MLP(hidden_dim * 3, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        else:
            self.mlp3 = MLP(hidden_dim * 2, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.init_weights()

    def init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_normal_(m.weight.data)
                m.bias.data.fill_(0.1)

    def node2edge(self, x, sp_send, sp_rec):
        # NOTE: Assumes that we have the same graph across all samples.
        receivers = torch.matmul(sp_rec, x)
        senders = torch.matmul(sp_send, x)
        edges = torch.cat([senders, receivers], dim=1)
        return edges

    def edge2node(self, x, sp_send_t, sp_rec_t):
PATH: layers.py
LINES: 157-185

# NOTE: Assumes that we have the same graph across all samples.
        incoming = torch.matmul(sp_rec_t, x)
        return incoming

    def forward(self, inputs, sp_send, sp_rec, sp_send_t, sp_rec_t):
        r"""
        Parameters:
            inputs: input concept embedding matrix
            sp_send: one-hot encoded send-node index(sparse tensor)
            sp_rec: one-hot encoded receive-node index(sparse tensor)
            sp_send_t: one-hot encoded send-node index(sparse tensor, transpose)
            sp_rec_t: one-hot encoded receive-node index(sparse tensor, transpose)
        Shape:
            inputs: [concept_num, embedding_dim]
            sp_send: [edge_num, concept_num]
            sp_rec: [edge_num, concept_num]
            sp_send_t: [concept_num, edge_num]
            sp_rec_t: [concept_num, edge_num]
        Return:
            output: [edge_num, edge_type_num]
        """
        x = self.node2edge(inputs, sp_send, sp_rec)  # [edge_num, 2 * embedding_dim]
        x = self.mlp(x)  # [edge_num, hidden_num]
        x_skip = x

        if self.factor:
            x = self.edge2node(x, sp_send_t, sp_rec_t)  # [concept_num, hidden_num]
            x = self.mlp2(x)  # [concept_num, hidden_num]
            x = self.node2edge(x, sp_send, sp_rec)  # [edge_num, 2 * hidden_num]
PATH: layers.py
LINES: 186-193

x = torch.cat((x, x_skip), dim=1)  # Skip connection  shape: [edge_num, 3 * hidden_num]
            x = self.mlp3(x)  # [edge_num, hidden_num]
        else:
            x = self.mlp2(x)  # [edge_num, hidden_num]
            x = torch.cat((x, x_skip), dim=1)  # Skip connection  shape: [edge_num, 2 * hidden_num]
            x = self.mlp3(x)  # [edge_num, hidden_num]
        output = self.fc_out(x)  # [edge_num, output_dim]
        return output
PATH: layers.py
LINES: 196-224

class MLPDecoder(nn.Module):
    """
    MLP decoder module.
    NOTE: Stole and modify the code from https://github.com/ethanfetaya/NRI/blob/master/modules.py
    """

    def __init__(self, input_dim, msg_hidden_dim, msg_output_dim, hidden_dim, edge_type_num, dropout=0., bias=True):
        super(MLPDecoder, self).__init__()
        self.msg_out_dim = msg_output_dim
        self.edge_type_num = edge_type_num
        self.dropout = dropout

        self.msg_fc1 = nn.ModuleList([nn.Linear(2 * input_dim, msg_hidden_dim, bias=bias) for _ in range(edge_type_num)])
        self.msg_fc2 = nn.ModuleList([nn.Linear(msg_hidden_dim, msg_output_dim, bias=bias) for _ in range(edge_type_num)])
        self.out_fc1 = nn.Linear(msg_output_dim, hidden_dim, bias=bias)
        self.out_fc2 = nn.Linear(hidden_dim, hidden_dim, bias=bias)
        self.out_fc3 = nn.Linear(hidden_dim, input_dim, bias=bias)

    def node2edge(self, x, sp_send, sp_rec):
        receivers = torch.matmul(sp_rec, x)  # [edge_num, embedding_dim]
        senders = torch.matmul(sp_send, x)  # [edge_num, embedding_dim]
        edges = torch.cat([senders, receivers], dim=-1)  # [edge_num, 2 * embedding_dim]
        return edges

    def edge2node(self, x, sp_send_t, sp_rec_t):
        # NOTE: Assumes that we have the same graph across all samples.
        incoming = torch.matmul(sp_rec_t, x)
        return incoming
PATH: layers.py
LINES: 225-251

def forward(self, inputs, rel_type, sp_send, sp_rec, sp_send_t, sp_rec_t):
        r"""
        Parameters:
            inputs: input concept embedding matrix
            rel_type: inferred edge weights for all edge types from MLPEncoder
            sp_send: one-hot encoded send-node index(sparse tensor)
            sp_rec: one-hot encoded receive-node index(sparse tensor)
            sp_send_t: one-hot encoded send-node index(sparse tensor, transpose)
            sp_rec_t: one-hot encoded receive-node index(sparse tensor, transpose)
        Shape:
            inputs: [concept_num, embedding_dim]
            sp_send: [edge_num, concept_num]
            sp_rec: [edge_num, concept_num]
            sp_send_t: [concept_num, edge_num]
            sp_rec_t: [concept_num, edge_num]
        Return:
            output: [edge_num, edge_type_num]
        """
        # NOTE: Assumes that we have the same graph across all samples.
        # Node2edge
        pre_msg = self.node2edge(inputs, sp_send, sp_rec)
        all_msgs = Variable(torch.zeros(pre_msg.size(0), self.msg_out_dim, device=inputs.device))  # [edge_num, msg_out_dim]
        for i in range(self.edge_type_num):
            msg = F.relu(self.msg_fc1[i](pre_msg))
            msg = F.dropout(msg, self.dropout, training=self.training)
            msg = F.relu(self.msg_fc2[i](msg))
            msg = msg * rel_type[:, i:i + 1]
PATH: layers.py
LINES: 252-260

all_msgs += msg

        # Aggregate all msgs to receiver
        agg_msgs = self.edge2node(all_msgs, sp_send_t, sp_rec_t)  # [concept_num, msg_out_dim]
        # Output MLP
        pred = F.dropout(F.relu(self.out_fc1(agg_msgs)), self.dropout, training=self.training)  # [concept_num, hidden_dim]
        pred = F.dropout(F.relu(self.out_fc2(pred)), self.dropout, training=self.training)  # [concept_num, hidden_dim]
        pred = self.out_fc3(pred)  # [concept_num, embedding_dim]
        return pred
PATH: model.py
LINES: 1-7

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import scipy.sparse as sp
from torch.autograd import Variable
from layers import MLP, EraseAddGate, ScaledDotProductAttention
PATH: model.py
LINES: 8-38

class GKT(nn.Module):

    def __init__(self, concept_num, hidden_dim, embedding_dim, edge_type_num, graph_type, graph=None, graph_model=None, dropout=0.5, bias=True, binary=False, has_cuda=False):
        super(GKT, self).__init__()
        self.concept_num = concept_num
        self.hidden_dim = hidden_dim
        self.embedding_dim = embedding_dim
        self.edge_type_num = edge_type_num

        self.res_len = 2 if binary else 12
        self.has_cuda = has_cuda

        assert graph_type in ['Dense', 'Transition', 'DKT', 'PAM', 'MHA', 'VAE']
        self.graph_type = graph_type
        if graph_type in ['Dense', 'Transition', 'DKT']:
            assert edge_type_num == 2
            assert graph is not None and graph_model is None
            self.graph = nn.Parameter(graph)  # [concept_num, concept_num]
            self.graph.requires_grad = False  # fix parameter
            self.graph_model = graph_model
        else:  # ['PAM', 'MHA', 'VAE']
            assert graph is None
            self.graph = graph  # None
            if graph_type == 'PAM':
                assert graph_model is None
                self.graph = nn.Parameter(torch.rand(concept_num, concept_num))
            else:
                assert graph_model is not None
            self.graph_model = graph_model

        # one-hot feature and question
PATH: model.py
LINES: 39-58

one_hot_feat = torch.eye(self.res_len * self.concept_num)
        self.one_hot_feat = one_hot_feat.cuda() if self.has_cuda else one_hot_feat
        self.one_hot_q = torch.eye(self.concept_num, device=self.one_hot_feat.device)
        zero_padding = torch.zeros(1, self.concept_num, device=self.one_hot_feat.device)
        self.one_hot_q = torch.cat((self.one_hot_q, zero_padding), dim=0)
        # concept and concept & response embeddings
        self.emb_x = nn.Embedding(self.res_len * concept_num, embedding_dim)
        # last embedding is used for padding, so dim + 1
        self.emb_c = nn.Embedding(concept_num + 1, embedding_dim, padding_idx=-1)

        # f_self function and f_neighbor functions
        mlp_input_dim = hidden_dim + embedding_dim
        self.f_self = MLP(mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias)
        self.f_neighbor_list = nn.ModuleList()
        if graph_type in ['Dense', 'Transition', 'DKT', 'PAM']:
            # f_in and f_out functions
            self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))
            self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))
        else:  # ['MHA', 'VAE']
            for i in range(edge_type_num):
PATH: model.py
LINES: 59-85

self.f_neighbor_list.append(MLP(2 * mlp_input_dim, hidden_dim, hidden_dim, dropout=dropout, bias=bias))

        # Erase & Add Gate
        self.erase_add_gate = EraseAddGate(hidden_dim, concept_num)
        # Gate Recurrent Unit
        self.gru = nn.GRUCell(hidden_dim, hidden_dim, bias=bias)
        # prediction layer
        self.predict = nn.Linear(hidden_dim, 1, bias=bias)

    # Aggregate step, as shown in Section 3.2.1 of the paper
    def _aggregate(self, xt, qt, ht, batch_size):
        r"""
        Parameters:
            xt: input one-hot question answering features at the current timestamp
            qt: question indices for all students in a batch at the current timestamp
            ht: hidden representations of all concepts at the current timestamp
            batch_size: the size of a student batch
        Shape:
            xt: [batch_size]
            qt: [batch_size]
            ht: [batch_size, concept_num, hidden_dim]
            tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
        Return:
            tmp_ht: aggregation results of concept hidden knowledge state and concept(& response) embedding
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        x_idx_mat = torch.arange(self.res_len * self.concept_num, device=xt.device)
PATH: model.py
LINES: 86-108

x_embedding = self.emb_x(x_idx_mat)  # [res_len * concept_num, embedding_dim]
        masked_feat = F.embedding(xt[qt_mask], self.one_hot_feat)  # [mask_num, res_len * concept_num]
        res_embedding = masked_feat.mm(x_embedding)  # [mask_num, embedding_dim]
        mask_num = res_embedding.shape[0]

        concept_idx_mat = self.concept_num * torch.ones((batch_size, self.concept_num), device=xt.device).long()
        concept_idx_mat[qt_mask, :] = torch.arange(self.concept_num, device=xt.device)
        concept_embedding = self.emb_c(concept_idx_mat)  # [batch_size, concept_num, embedding_dim]

        index_tuple = (torch.arange(mask_num, device=xt.device), qt[qt_mask].long())
        concept_embedding[qt_mask] = concept_embedding[qt_mask].index_put(index_tuple, res_embedding)
        tmp_ht = torch.cat((ht, concept_embedding), dim=-1)  # [batch_size, concept_num, hidden_dim + embedding_dim]
        return tmp_ht

    # GNN aggregation step, as shown in 3.3.2 Equation 1 of the paper
    def _agg_neighbors(self, tmp_ht, qt):
        r"""
        Parameters:
            tmp_ht: temporal hidden representations of all concepts after the aggregate step
            qt: question indices for all students in a batch at the current timestamp
        Shape:
            tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
            qt: [batch_size]
PATH: model.py
LINES: 109-127

m_next: [batch_size, concept_num, hidden_dim]
        Return:
            m_next: hidden representations of all concepts aggregating neighboring representations at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
            z_prob: probability distribution of latent variable z in VAE (optional)
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        masked_qt = qt[qt_mask]  # [mask_num, ]
        masked_tmp_ht = tmp_ht[qt_mask]  # [mask_num, concept_num, hidden_dim + embedding_dim]
        mask_num = masked_tmp_ht.shape[0]
        self_index_tuple = (torch.arange(mask_num, device=qt.device), masked_qt.long())
        self_ht = masked_tmp_ht[self_index_tuple]  # [mask_num, hidden_dim + embedding_dim]
        self_features = self.f_self(self_ht)  # [mask_num, hidden_dim]
        expanded_self_ht = self_ht.unsqueeze(dim=1).repeat(1, self.concept_num, 1)  #[mask_num, concept_num, hidden_dim + embedding_dim]
        neigh_ht = torch.cat((expanded_self_ht, masked_tmp_ht), dim=-1)  #[mask_num, concept_num, 2 * (hidden_dim + embedding_dim)]
        concept_embedding, rec_embedding, z_prob = None, None, None

        if self.graph_type in ['Dense', 'Transition', 'DKT', 'PAM']:
PATH: model.py
LINES: 128-144

adj = self.graph[masked_qt.long(), :].unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
            reverse_adj = self.graph[:, masked_qt.long()].transpose(0, 1).unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
            # self.f_neighbor_list[0](neigh_ht) shape: [mask_num, concept_num, hidden_dim]
            neigh_features = adj * self.f_neighbor_list[0](neigh_ht) + reverse_adj * self.f_neighbor_list[1](neigh_ht)
        else:  # ['MHA', 'VAE']
            concept_index = torch.arange(self.concept_num, device=qt.device)
            concept_embedding = self.emb_c(concept_index)  # [concept_num, embedding_dim]
            if self.graph_type == 'MHA':
                query = self.emb_c(masked_qt)
                key = concept_embedding
                att_mask = Variable(torch.ones(self.edge_type_num, mask_num, self.concept_num, device=qt.device))
                for k in range(self.edge_type_num):
                    index_tuple = (torch.arange(mask_num, device=qt.device), masked_qt.long())
                    att_mask[k] = att_mask[k].index_put(index_tuple, torch.zeros(mask_num, device=qt.device))
                graphs = self.graph_model(masked_qt, query, key, att_mask)
            else:  # self.graph_type == 'VAE'
                sp_send, sp_rec, sp_send_t, sp_rec_t = self._get_edges(masked_qt)
PATH: model.py
LINES: 145-168

graphs, rec_embedding, z_prob = self.graph_model(concept_embedding, sp_send, sp_rec, sp_send_t, sp_rec_t)
            neigh_features = 0
            for k in range(self.edge_type_num):
                adj = graphs[k][masked_qt, :].unsqueeze(dim=-1)  # [mask_num, concept_num, 1]
                if k == 0:
                    neigh_features = adj * self.f_neighbor_list[k](neigh_ht)
                else:
                    neigh_features = neigh_features + adj * self.f_neighbor_list[k](neigh_ht)
            if self.graph_type == 'MHA':
                neigh_features = 1. / self.edge_type_num * neigh_features
        # neigh_features: [mask_num, concept_num, hidden_dim]
        m_next = tmp_ht[:, :, :self.hidden_dim]
        m_next[qt_mask] = neigh_features
        m_next[qt_mask] = m_next[qt_mask].index_put(self_index_tuple, self_features)
        return m_next, concept_embedding, rec_embedding, z_prob

    # Update step, as shown in Section 3.3.2 of the paper
    def _update(self, tmp_ht, ht, qt):
        r"""
        Parameters:
            tmp_ht: temporal hidden representations of all concepts after the aggregate step
            ht: hidden representations of all concepts at the current timestamp
            qt: question indices for all students in a batch at the current timestamp
        Shape:
PATH: model.py
LINES: 169-191

tmp_ht: [batch_size, concept_num, hidden_dim + embedding_dim]
            ht: [batch_size, concept_num, hidden_dim]
            qt: [batch_size]
            h_next: [batch_size, concept_num, hidden_dim]
        Return:
            h_next: hidden representations of all concepts at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
            z_prob: probability distribution of latent variable z in VAE (optional)
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        mask_num = qt_mask.nonzero().shape[0]
        # GNN Aggregation
        m_next, concept_embedding, rec_embedding, z_prob = self._agg_neighbors(tmp_ht, qt)  # [batch_size, concept_num, hidden_dim]
        # Erase & Add Gate
        m_next[qt_mask] = self.erase_add_gate(m_next[qt_mask])  # [mask_num, concept_num, hidden_dim]
        # GRU
        h_next = m_next
        res = self.gru(m_next[qt_mask].reshape(-1, self.hidden_dim), ht[qt_mask].reshape(-1, self.hidden_dim))  # [mask_num * concept_num, hidden_num]
        index_tuple = (torch.arange(mask_num, device=qt_mask.device), )
        h_next[qt_mask] = h_next[qt_mask].index_put(index_tuple, res.reshape(-1, self.concept_num, self.hidden_dim))
        return h_next, concept_embedding, rec_embedding, z_prob
PATH: model.py
LINES: 192-223

# Predict step, as shown in Section 3.3.3 of the paper
    def _predict(self, h_next, qt):
        r"""
        Parameters:
            h_next: hidden representations of all concepts at the next timestamp after the update step
            qt: question indices for all students in a batch at the current timestamp
        Shape:
            h_next: [batch_size, concept_num, hidden_dim]
            qt: [batch_size]
            y: [batch_size, concept_num]
        Return:
            y: predicted correct probability of all concepts at the next timestamp
        """
        qt_mask = torch.ne(qt, -1)  # [batch_size], qt != -1
        y = self.predict(h_next).squeeze(dim=-1)  # [batch_size, concept_num]
        y[qt_mask] = torch.sigmoid(y[qt_mask])  # [batch_size, concept_num]
        return y

    def _get_next_pred(self, yt, q_next):
        r"""
        Parameters:
            yt: predicted correct probability of all concepts at the next timestamp
            q_next: question index matrix at the next timestamp
            batch_size: the size of a student batch
        Shape:
            y: [batch_size, concept_num]
            questions: [batch_size, seq_len]
            pred: [batch_size, ]
        Return:
            pred: predicted correct probability of the question answered at the next timestamp
        """
        next_qt = q_next
PATH: model.py
LINES: 224-250

next_qt = torch.where(next_qt != -1, next_qt, self.concept_num * torch.ones_like(next_qt, device=yt.device))
        one_hot_qt = F.embedding(next_qt.long(), self.one_hot_q)  # [batch_size, concept_num]
        # dot product between yt and one_hot_qt
        pred = (yt * one_hot_qt).sum(dim=1)  # [batch_size, ]
        return pred

    # Get edges for edge inference in VAE
    def _get_edges(self, masked_qt):
        r"""
        Parameters:
            masked_qt: qt index with -1 padding values removed
        Shape:
            masked_qt: [mask_num, ]
            rel_send: [edge_num, concept_num]
            rel_rec: [edge_num, concept_num]
        Return:
            rel_send: from nodes in edges which send messages to other nodes
            rel_rec:  to nodes in edges which receive messages from other nodes
        """
        mask_num = masked_qt.shape[0]
        row_arr = masked_qt.cpu().numpy().reshape(-1, 1)  # [mask_num, 1]
        row_arr = np.repeat(row_arr, self.concept_num, axis=1)  # [mask_num, concept_num]
        col_arr = np.arange(self.concept_num).reshape(1, -1)  # [1, concept_num]
        col_arr = np.repeat(col_arr, mask_num, axis=0)  # [mask_num, concept_num]
        # add reversed edges
        new_row = np.vstack((row_arr, col_arr))  # [2 * mask_num, concept_num]
        new_col = np.vstack((col_arr, row_arr))  # [2 * mask_num, concept_num]
PATH: model.py
LINES: 251-274

row_arr = new_row.flatten()  # [2 * mask_num * concept_num, ]
        col_arr = new_col.flatten()  # [2 * mask_num * concept_num, ]
        data_arr = np.ones(2 * mask_num * self.concept_num)
        init_graph = sp.coo_matrix((data_arr, (row_arr, col_arr)), shape=(self.concept_num, self.concept_num))
        init_graph.setdiag(0)  # remove self-loop edges
        row_arr, col_arr, _ = sp.find(init_graph)
        row_tensor = torch.from_numpy(row_arr).long()
        col_tensor = torch.from_numpy(col_arr).long()
        one_hot_table = torch.eye(self.concept_num, self.concept_num)
        rel_send = F.embedding(row_tensor, one_hot_table)  # [edge_num, concept_num]
        rel_rec = F.embedding(col_tensor, one_hot_table)  # [edge_num, concept_num]
        sp_rec, sp_send = rel_rec.to_sparse(), rel_send.to_sparse()
        sp_rec_t, sp_send_t = rel_rec.T.to_sparse(), rel_send.T.to_sparse()
        sp_send = sp_send.to(device=masked_qt.device)
        sp_rec = sp_rec.to(device=masked_qt.device)
        sp_send_t = sp_send_t.to(device=masked_qt.device)
        sp_rec_t = sp_rec_t.to(device=masked_qt.device)
        return sp_send, sp_rec, sp_send_t, sp_rec_t

    def forward(self, features, questions):
        r"""
        Parameters:
            features: input one-hot matrix
            questions: question index matrix
PATH: model.py
LINES: 275-296

seq_len dimension needs padding, because different students may have learning sequences with different lengths.
        Shape:
            features: [batch_size, seq_len]
            questions: [batch_size, seq_len]
            pred_res: [batch_size, seq_len - 1]
        Return:
            pred_res: the correct probability of questions answered at the next timestamp
            concept_embedding: input of VAE (optional)
            rec_embedding: reconstructed input of VAE (optional)
            z_prob: probability distribution of latent variable z in VAE (optional)
        """
        batch_size, seq_len = features.shape
        ht = Variable(torch.zeros((batch_size, self.concept_num, self.hidden_dim), device=features.device))
        pred_list = []
        ec_list = []  # concept embedding list in VAE
        rec_list = []  # reconstructed embedding list in VAE
        z_prob_list = []  # probability distribution of latent variable z in VAE
        for i in range(seq_len):
            xt = features[:, i]  # [batch_size]
            qt = questions[:, i]  # [batch_size]
            qt_mask = torch.ne(qt, -1)  # [batch_size], next_qt != -1
            tmp_ht = self._aggregate(xt, qt, ht, batch_size)  # [batch_size, concept_num, hidden_dim + embedding_dim]
PATH: model.py
LINES: 297-307

h_next, concept_embedding, rec_embedding, z_prob = self._update(tmp_ht, ht, qt)  # [batch_size, concept_num, hidden_dim]
            ht[qt_mask] = h_next[qt_mask]  # update new ht
            yt = self._predict(h_next, qt)  # [batch_size, concept_num]
            if i < seq_len - 1:
                pred = self._get_next_pred(yt, questions[:, i + 1])
                pred_list.append(pred)
            ec_list.append(concept_embedding)
            rec_list.append(rec_embedding)
            z_prob_list.append(z_prob)
        pred_res = torch.stack(pred_list, dim=1)  # [batch_size, seq_len - 1]
        return pred_res, ec_list, rec_list, z_prob_list
PATH: model.py
LINES: 309-340

class MultiHeadAttention(nn.Module):
    """
    Multi-Head Attention module
    NOTE: Stole and modify from https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/SubLayers.py
    """

    def __init__(self, n_head, concept_num, input_dim, d_k, dropout=0.):
        super(MultiHeadAttention, self).__init__()
        self.n_head = n_head
        self.concept_num = concept_num
        self.d_k = d_k
        self.w_qs = nn.Linear(input_dim, n_head * d_k, bias=False)
        self.w_ks = nn.Linear(input_dim, n_head * d_k, bias=False)
        self.attention = ScaledDotProductAttention(temperature=d_k ** 0.5, attn_dropout=dropout)
        # inferred latent graph, used for saving and visualization
        self.graphs = nn.Parameter(torch.zeros(n_head, concept_num, concept_num))
        self.graphs.requires_grad = False

    def _get_graph(self, attn_score, qt):
        r"""
        Parameters:
            attn_score: attention score of all queries
            qt: masked question index
        Shape:
            attn_score: [n_head, mask_num, concept_num]
            qt: [mask_num]
        Return:
            graphs: n_head types of inferred graphs
        """
        graphs = Variable(torch.zeros(self.n_head, self.concept_num, self.concept_num, device=qt.device))
        for k in range(self.n_head):
            index_tuple = (qt.long(), )
PATH: model.py
LINES: 341-371

graphs[k] = graphs[k].index_put(index_tuple, attn_score[k])  # used for calculation
            #############################
            # here, we need to detach edges when storing it into self.graphs in case memory leak!
            self.graphs.data[k] = self.graphs.data[k].index_put(index_tuple, attn_score[k].detach())  # used for saving and visualization
            #############################
        return graphs

    def forward(self, qt, query, key, mask=None):
        r"""
        Parameters:
            qt: masked question index
            query: answered concept embedding for a student batch
            key: concept embedding matrix
            mask: mask matrix
        Shape:
            qt: [mask_num]
            query: [mask_num, embedding_dim]
            key: [concept_num, embedding_dim]
        Return:
            graphs: n_head types of inferred graphs
        """
        d_k, n_head = self.d_k, self.n_head
        len_q, len_k = query.size(0), key.size(0)

        # Pass through the pre-attention projection: lq x (n_head *dk)
        # Separate different heads: lq x n_head x dk
        q = self.w_qs(query).view(len_q, n_head, d_k)
        k = self.w_ks(key).view(len_k, n_head, d_k)

        # Transpose for attention dot product: n_head x lq x dk
        q, k = q.transpose(0, 1), k.transpose(0, 1)
PATH: model.py
LINES: 372-374

attn_score = self.attention(q, k, mask=mask)  # [n_head, mask_num, concept_num]
        graphs = self._get_graph(attn_score, qt)
        return graphs
PATH: server.py
LINES: 1-11

from flask import Flask, request, jsonify, abort, Response, render_template, session, url_for
from werkzeug.utils import redirect
from model import GKT, MultiHeadAttention, ScaledDotProductAttention, MLP, EraseAddGate
import torch
from utils import *
import pandas as pd
app = Flask(__name__)

model = None 

@app.route('/', methods=['POST', 'GET'])
PATH: server.py
LINES: 12-21

def get_probs():
    if (request.method == 'GET'):
        return render_template('index.html')

    if(request.method == 'POST'):
        user_id = int(request.form['user_id'])
        skill_id = int(request.form['skill_id'])
        avg_prob = get_prob(user_id, skill_id)
        session['avg_prob'] = avg_prob
        return redirect(url_for('get_result'))
PATH: server.py
LINES: 26-37

def get_prob(user_id, skill_id):
    df = pd.read_csv('assistment_test15.csv')
    data_loader, id_map = load_data(df[(df['user_id']==user_id)])
    for _, (features, questions) in enumerate(data_loader):
        pred_res, _, _, _ = model(features, questions)
    try:
        temp = torch.where(questions==id_map[skill_id])[1]
    except KeyError as error: 
        abort(Response('Invalid Skill ID - {}'.format(error), status= 400))
    x = [pred_res[0][t-1].item() for t in temp]
    avg_prob = sum(x)/len(x)
    return avg_prob
PATH: server.py
LINES: 40-43

def get_result():
    if (request.method == 'GET'):
        print(session['avg_prob'])
        return render_template('result.html', context={'avg_prob': session['avg_prob']})
PATH: static/index.js
LINES: 1-44

$(document).ready(function() {
    $('#contact_form').bootstrapValidator({
        // To use feedback icons, ensure that you use Bootstrap v3.1.0 or later
        feedbackIcons: {
            valid: 'glyphicon glyphicon-ok',
            invalid: 'glyphicon glyphicon-remove',
            validating: 'glyphicon glyphicon-refresh'
        },
        fields: {
            first_name: {
                validators: {
                        stringLength: {
                        min: 2,
                    },
                        notEmpty: {
                        message: 'Please enter your First Name'
                    }
                }
            },
             last_name: {
                validators: {
                     stringLength: {
                        min: 2,
                    },
                    notEmpty: {
                        message: 'Please enter your Last Name'
                    }
                }
            },
			 user_name: {
                validators: {
                     stringLength: {
                        min: 8,
                    },
                    notEmpty: {
                        message: 'Please enter your Username'
                    }
                }
            },
			 user_password: {
                validators: {
                     stringLength: {
                        min: 8,
                    },
PATH: static/index.js
LINES: 45-90

notEmpty: {
                        message: 'Please enter your Password'
                    }
                }
            },
			confirm_password: {
                validators: {
                     stringLength: {
                        min: 8,
                    },
                    notEmpty: {
                        message: 'Please confirm your Password'
                    }
                }
            },
            email: {
                validators: {
                    notEmpty: {
                        message: 'Please enter your Email Address'
                    },
                    emailAddress: {
                        message: 'Please enter a valid Email Address'
                    }
                }
            },
            contact_no: {
                validators: {
                  stringLength: {
                        min: 12, 
                        max: 12,
                    notEmpty: {
                        message: 'Please enter your Contact No.'
                     }
                }
            },
			 department: {
                validators: {
                    notEmpty: {
                        message: 'Please select your Department/Office'
                    }
                }
            },
                }
            }
        })
        .on('success.form.bv', function(e) {
PATH: static/index.js
LINES: 91-108

$('#success_message').slideDown({ opacity: "show" }, "slow") // Do something ...
                $('#contact_form').data('bootstrapValidator').resetForm();

            // Prevent form submission
            e.preventDefault();

            // Get the form instance
            var $form = $(e.target);

            // Get the BootstrapValidator instance
            var bv = $form.data('bootstrapValidator');

            // Use Ajax to submit form data
            $.post($form.attr('action'), $form.serialize(), function(result) {
                console.log(result);
            }, 'json');
        });
});
<!--
Author: Colorlib
Author URL: https://colorlib.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->
<!DOCTYPE html>
<html>
<head>
<title>Creative Colorlib SignUp Form</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
<!-- Custom Theme files -->
<link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}" type="text/css" media="all" />
<!-- //Custom Theme files -->
<!-- web font -->
<link href="//fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i" rel="stylesheet">
<!-- //web font -->
</head>
<body>
	<!-- main -->
	<div class="main-w3layouts wrapper">
		<h1>Student Performance Model</h1>
		<div class="main-agileinfo">
			<div class="agileits-top">
				<form action="/" method="POST">
					<input class="text" type="text" id="user_id" name="user_id" placeholder="Enter Student ID" required="">
					<input class="text email" type="text" id="skill_id" name="skill_id" placeholder="Enter Skill ID" required="">
					<input type="submit" value="Submit">
				</form>
			</div>
		</div>
		<!-- copyright -->
		<!-- //copyright -->
<input type="submit" value="Submit">
				</form>
			</div>
		</div>
		<!-- copyright -->
		<!-- //copyright -->
		<ul class="colorlib-bubbles">
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
		</ul>
	</div>
	<!-- //main -->
</body>
</html>
<!--
Author: Colorlib
Author URL: https://colorlib.com
License: Creative Commons Attribution 3.0 Unported
License URL: http://creativecommons.org/licenses/by/3.0/
-->
<!DOCTYPE html>
<html>
<head>
<title>Creative Colorlib SignUp Form</title>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<script type="application/x-javascript"> addEventListener("load", function() { setTimeout(hideURLbar, 0); }, false); function hideURLbar(){ window.scrollTo(0,1); } </script>
<!-- Custom Theme files -->
<link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}" type="text/css" media="all" />
<!-- //Custom Theme files -->
<!-- web font -->
<link href="//fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i" rel="stylesheet">
<!-- //web font -->
</head>
<body>
	<!-- main -->
	<div class="main-w3layouts wrapper">
		<h1>Student Performance Model</h1>
		<div class="main-agileinfo">
			<div class="agileits-top">
                <span>The average probability of getting a correct answer for the given skill ID: {{ session['avg_prob'] }} </span>
				<!-- <form action="/" method="POST">
					<input class="text" type="text" id="user_id" name="user_id" placeholder="Enter Student ID" required="">
<h1>Student Performance Model</h1>
		<div class="main-agileinfo">
			<div class="agileits-top">
                <span>The average probability of getting a correct answer for the given skill ID: {{ session['avg_prob'] }} </span>
				<!-- <form action="/" method="POST">
					<input class="text" type="text" id="user_id" name="user_id" placeholder="Enter Student ID" required="">
					<input class="text email" type="text" id="skill_id" name="skill_id" placeholder="Enter Skill ID" required="">
					<input type="submit" value="Submit">
				</form> -->
			</div>
		</div>
		<!-- copyright -->
		<!-- //copyright -->
		<ul class="colorlib-bubbles">
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
			<li></li>
		</ul>
	</div>
	<!-- //main -->
</body>
</html>
PATH: utils.py
LINES: 1-5

import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
PATH: utils.py
LINES: 6-41

def load_data(df, shuffle=True, model_type='GKT', use_binary=True, res_len=2, use_cuda=True):
  
    df.dropna(subset=['skill_id'], inplace=True)

    # Step 1.2 - Remove users with a single answer
    df = df.groupby('user_id').filter(lambda q: len(q) > 1).copy()

    # Step 2 - Enumerate skill id
    df['skill'], id_map = pd.factorize(df['skill_id'], sort=True)  # we can also use problem_id to represent exercises
    # Step 3 - Cross skill id with answer to form a synthetic feature
    # use_binary: (0,1); !use_binary: (1,2,3,4,5,6,7,8,9,10,11,12). Either way, the correct result index is guaranteed to be 1
    if use_binary:
        df['skill_with_answer'] = df['skill'] * 2 + df['correct']
    else:
        df['skill_with_answer'] = df['skill'] * res_len + df['correct'] - 1


    # Step 4 - Convert to a sequence per user id and shift features 1 timestep
    feature_list = []
    question_list = []
    seq_len_list = []

    def get_data(series):
        feature_list.append(series['skill_with_answer'].tolist())
        question_list.append(series['skill'].tolist())
        seq_len_list.append(series['correct'].shape[0])

    df.groupby('user_id').apply(get_data)

    # print('feature_dim:', feature_dim, 'res_len*question_dim:', res_len*question_dim)
    # assert feature_dim == res_len * question_dim

    data = KTTestDataset(feature_list, question_list)
    
    data_loader = DataLoader(data, batch_size=1, collate_fn=pad)
    return data_loader, {skill_id: idx for idx, skill_id in enumerate(id_map)}
PATH: utils.py
LINES: 43-49

def pad(batch):
    (features, questions) = zip(*batch)
    features = [torch.LongTensor(feat) for feat in features]
    questions = [torch.LongTensor(qt) for qt in questions]
    feature_pad = pad_sequence(features, batch_first=True, padding_value=-1)
    question_pad = pad_sequence(questions, batch_first=True, padding_value=-1)
    return feature_pad, question_pad
PATH: utils.py
LINES: 51-61

class KTTestDataset(Dataset):
    def __init__(self, features, questions):
        super(KTTestDataset, self).__init__()
        self.features = features
        self.questions = questions

    def __getitem__(self, index):
        return self.features[index], self.questions[index]

    def __len__(self):
        return len(self.features)
PATH: app/src/androidTest/java/com/bitspilani/apogeear/ExampleInstrumentedTest.java
LINES: 1-27

package com.bitspilani.apogeear;

import android.content.Context;

import androidx.test.platform.app.InstrumentationRegistry;
import androidx.test.ext.junit.runners.AndroidJUnit4;

import org.junit.Test;
import org.junit.runner.RunWith;

import static org.junit.Assert.*;

/**
 * Instrumented test, which will execute on an Android device.
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
@RunWith(AndroidJUnit4.class)
public class ExampleInstrumentedTest {
    @Test
    public void useAppContext() {
        // Context of the app under test.
        Context appContext = InstrumentationRegistry.getInstrumentation().getTargetContext();

        assertEquals("com.bitspilani.myapplication", appContext.getPackageName());
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/CharAdapter.java
LINES: 1-42

package com.bitspilani.apogeear.Adapters;

import android.app.Activity;
import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.viewpager.widget.PagerAdapter;

import com.bitspilani.apogeear.CharSelect;
import com.bitspilani.apogeear.MainActivity;
import com.bitspilani.apogeear.Models.CharacterModel;
import com.bitspilani.apogeear.R;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.SetOptions;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

import de.hdodenhof.circleimageview.CircleImageView;

import static android.content.Context.MODE_PRIVATE;

public class CharAdapter extends PagerAdapter {

    private List<CharacterModel> list;
    private Context context;
    private FirebaseAuth mAuth = FirebaseAuth.getInstance();
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    Intent fromLogin;
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/CharAdapter.java
LINES: 43-79

public CharAdapter(List<CharacterModel> list, Context context) {
        this.list = list;
        this.context = context;
    }

    @Override
    public int getCount() {
        return list.size();
    }

    @Override
    public boolean isViewFromObject(@NonNull View view, @NonNull Object object) {
        return view.equals(object);
    }

    @NonNull
    @Override
    public Object instantiateItem(@NonNull ViewGroup container, int position) {
        LayoutInflater layoutInflater = LayoutInflater.from(context);
        View view = layoutInflater.inflate(R.layout.char_select,container,false);

        TextView textView = view.findViewById(R.id.user_name1);
        ImageView imageView = view.findViewById(R.id.user_icon1);

        textView.setText(list.get(position).getName());
        imageView.setImageResource(list.get(position).getImage());

        mAuth = FirebaseAuth.getInstance();
        FirebaseUser user = mAuth.getCurrentUser();

        fromLogin = ((Activity)context).getIntent();

        view.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Map<String, Object> data = new HashMap<>();
                data.put("char",textView.getText().toString().trim());
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/CharAdapter.java
LINES: 80-105

db.collection("Users").document(user.getUid()).update(data).addOnCompleteListener(new OnCompleteListener<Void>() {
                    @Override
                    public void onComplete(@NonNull Task<Void> task) {

                        SharedPreferences sharedPref=context.getSharedPreferences("userinfo",MODE_PRIVATE);
                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("char",textView.getText().toString().trim());
                        editor.apply();
                        Intent i = new Intent(context, MainActivity.class);
                        i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        context.startActivity(i);
                        ((Activity) context).finish();
                    }
                });
            }
        });

        container.addView(view,0);
        return view;
    }

    @Override
    public void destroyItem(@NonNull ViewGroup container, int position, @NonNull Object object) {
        container.removeView((View)object);
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/DevelopersAdapter.java
LINES: 1-46

package com.bitspilani.apogeear.Adapters;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.Models.Developers;
import com.bitspilani.apogeear.R;

import java.util.ArrayList;
import java.util.ConcurrentModificationException;
import java.util.List;

public class DevelopersAdapter extends RecyclerView.Adapter<DevelopersAdapter.ViewHolder>{

    private List<Developers> list;
    private Context context;

    public DevelopersAdapter(Context context,List<Developers> list){
        this.context = context;
        this.list = list;
    }


    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view = LayoutInflater.from(parent.getContext()).inflate(R.layout.card_developers,parent,false);
        return new ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {
        holder.name.setText(list.get(position).getName());
        holder.desc.setText(list.get(position).getDesc());
        holder.image.setImageResource(list.get(position).getImg());
    }

    @Override
    public int getItemCount() {
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/DevelopersAdapter.java
LINES: 47-65

return list.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder{

        TextView name,desc;
        ImageView image;

        public ViewHolder(@NonNull View itemView) {
            super(itemView);

            name = itemView.findViewById(R.id.blaName);
            desc = itemView.findViewById(R.id.bladesc);
            image = itemView.findViewById(R.id.blapic);

        }
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/HorizontalAdapter.java
LINES: 1-44

package com.bitspilani.apogeear.Adapters;

import android.content.Context;
import android.graphics.Color;
import android.text.TextUtils;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.cardview.widget.CardView;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.Models.Event_Details;
import com.bitspilani.apogeear.R;

import java.util.ArrayList;

public class HorizontalAdapter extends RecyclerView.Adapter<HorizontalAdapter.ViewHolder> {

    private ArrayList<Event_Details> event_details;
    private String attend;
    private Context context;
    private static ClickListener clickListener;
    private boolean check,exists;


    public HorizontalAdapter(ArrayList<Event_Details> event_details, Context context,String attend,boolean exists) {
        this.event_details=event_details;
        this.context=context;
        this.attend=attend;
        check=false;
        this.exists=exists;
    }

    @NonNull
    @Override
    public HorizontalAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {

        View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.horizontalitem,parent,false);
        ViewHolder viewHolder=new ViewHolder(view);
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/HorizontalAdapter.java
LINES: 45-79

return viewHolder;
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {
        holder.text.setText(event_details.get(position).getName());
        holder.text.setEllipsize(TextUtils.TruncateAt.MARQUEE);
        holder.text.setSingleLine(true);
        holder.text.setSelected(true);
        holder.text.setMarqueeRepeatLimit(-1);

        Log.d("Event",event_details.get(position).getName());


            if (event_details.get(position).getName().equals(attend))
                check = true;

            if (event_details.size() == 1) {

                holder.or.setVisibility(View.INVISIBLE);
                holder.or1.setVisibility(View.INVISIBLE);

                if(exists) {
                    holder.hor.setBackgroundColor(Color.parseColor("#4ECE60"));
                    holder.hor1.setBackgroundColor(Color.parseColor("#4ECE60"));
                    //holder.text.setTextColor(Color.parseColor("#4ECE60"));
                    holder.Event.setBackground(context.getDrawable(R.drawable.attended_bg));
                    holder.or2.setBackgroundColor(Color.parseColor("#4ECE60"));
                }

            } else if (position == 0) {
                holder.or.setVisibility(View.INVISIBLE);

                if(exists) {
                    if (event_details.get(position).getName().equals(attend)) {
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/HorizontalAdapter.java
LINES: 80-100

holder.hor.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.hor1.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.or2.setBackgroundColor(Color.parseColor("#4ECE60"));
                        //holder.text.setTextColor(Color.parseColor("#4ECE60"));
                        holder.Event.setBackground(context.getDrawable(R.drawable.attended_bg));
                    } else if (!check) {
                        holder.or1.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.or2.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.hor.setBackgroundColor(Color.parseColor("#4ECE60"));
                    }
                }
            } else if (position == event_details.size() - 1) {
                holder.hor.setVisibility(View.INVISIBLE);
                holder.or1.setVisibility(View.INVISIBLE);

                if(exists) {
                    if (event_details.get(position).getName().equals(attend)) {
                        holder.or.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.or2.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.hor1.setBackgroundColor(Color.parseColor("#4ECE60"));
                       // holder.text.setTextColor(Color.parseColor("#4ECE60"));
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/HorizontalAdapter.java
LINES: 101-134

holder.Event.setBackground(context.getDrawable(R.drawable.attended_bg));
                    }
                }
            } else {
                holder.hor.setVisibility(View.INVISIBLE);
                if(exists) {
                    if (!check) {
                        holder.or.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.or2.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.or1.setBackgroundColor(Color.parseColor("#4ECE60"));
                    } else if (event_details.get(position).getName().equals(attend)) {
                        holder.or.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.or2.setBackgroundColor(Color.parseColor("#4ECE60"));
                        holder.hor1.setBackgroundColor(Color.parseColor("#4ECE60"));
                        //holder.text.setTextColor(Color.parseColor("#4ECE60"));
                        holder.Event.setBackground(context.getDrawable(R.drawable.attended_bg));
                    }
                }
            }

    }


    @Override
    public int getItemCount() {
        return event_details.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView text;
        CardView Event;
        View hor,hor1,or,or1,or2;
        public ViewHolder(@NonNull View itemView) {
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/HorizontalAdapter.java
LINES: 135-161

super(itemView);
            text=itemView.findViewById(R.id.horizontal_item_text);
            hor=itemView.findViewById(R.id.hor);
            hor1=itemView.findViewById(R.id.hor1);

            or=itemView.findViewById(R.id.or);
            or1=itemView.findViewById(R.id.or1);
            or2=itemView.findViewById(R.id.or2);
            Event=itemView.findViewById(R.id.Event);

            itemView.setOnClickListener(new View.OnClickListener() {
                @Override
                public void onClick(View view) {
                    clickListener.onItemClicked(getAdapterPosition(), view);
                }
            });
        }
    }

    public interface ClickListener {
        void onItemClicked(int position, View v);
    }

    public void setOnItemClickListener(ClickListener clickListener) {
        HorizontalAdapter.clickListener = clickListener;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/InterestAdapter.java
LINES: 1-46

package com.bitspilani.apogeear.Adapters;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.R;

import java.util.ArrayList;

public class InterestAdapter extends RecyclerView.Adapter<InterestAdapter.ViewHolder> {

    private ArrayList<String> types;

    public InterestAdapter(ArrayList<String> types) {
        this.types=types;
    }

    @NonNull
    @Override
    public InterestAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {

        View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.event_filter_item,parent,false);
        InterestAdapter.ViewHolder viewHolder=new InterestAdapter.ViewHolder(view);
        return viewHolder;
    }

    @Override
    public void onBindViewHolder(@NonNull InterestAdapter.ViewHolder holder, int position) {

    }

    @Override
    public int getItemCount() {
        return 0;
    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        public ViewHolder(@NonNull View itemView) {
            super(itemView);
        }
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/LeaderBoardAdapter.java
LINES: 1-42

package com.bitspilani.apogeear.Adapters;

import android.app.Activity;
import android.content.Context;
import android.graphics.Color;
import android.graphics.drawable.Drawable;
import android.net.Uri;
import android.text.TextUtils;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.LinearLayout;
import android.widget.RelativeLayout;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.Fragments.Leaderboard;
import com.bitspilani.apogeear.R;
import com.bitspilani.apogeear.Models.Rank;
import com.bumptech.glide.Glide;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;
import com.squareup.picasso.Picasso;

import java.util.ArrayList;

public class LeaderBoardAdapter extends RecyclerView.Adapter<LeaderBoardAdapter.ViewHolder> {

    private ArrayList<Rank> list;
    private Context context;
    private Activity activity;
    private View view;
    private StorageReference charRef;

    public LeaderBoardAdapter (ArrayList<Rank> list, Context context, Activity activity){
        this.list=list;
        this.context=context;
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/LeaderBoardAdapter.java
LINES: 43-74

this.activity=activity;
    }
    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {

        view= LayoutInflater.from(parent.getContext()).inflate(R.layout.leaderboard_new,parent,false);
        return new ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {

        holder.name.setEllipsize(TextUtils.TruncateAt.MARQUEE);
        holder.name.setSingleLine(true);
        holder.name.setSelected(true);
        holder.name.setMarqueeRepeatLimit(-1);

        switch (position){

            case 0:holder.name.setText(list.get(position).getUsername());
                holder.name.setTextColor(Color.parseColor("#000000"));
                holder.charName.setText(list.get(position).getCharName());
                holder.charName.setTextColor(Color.parseColor("#000000"));
                holder.score.setText(String.valueOf(list.get(position).getCoins()));
                holder.score.setTextColor(Color.parseColor("#000000"));
                holder.rank.setText(""+(position+1));
                getTagFromDatabase(holder.img);
                holder.relativeLayout.setBackground(context.getDrawable(R.drawable.leaderboard_rank1_bg));
                getImageFromDatabase(holder.img1,list.get(position).getCharName());
                break;
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/LeaderBoardAdapter.java
LINES: 75-95

case 1:holder.name.setText(list.get(position).getUsername());
                holder.charName.setText(list.get(position).getCharName());
                holder.name.setTextColor(Color.parseColor("#000000"));
                holder.charName.setTextColor(Color.parseColor("#000000"));
                holder.score.setText(String.valueOf(list.get(position).getCoins()));
                holder.score.setTextColor(Color.parseColor("#000000"));
                holder.rank.setText(""+(position+1));
                getTagFromDatabase(holder.img);
                holder.relativeLayout.setBackground(context.getDrawable(R.drawable.leaderboard_rank2_bg));
                holder.rank.setText(""+(position+1));
                getImageFromDatabase(holder.img1,list.get(position).getCharName());
                break;

            case 2:holder.name.setText(list.get(position).getUsername());
                holder.charName.setText(list.get(position).getCharName());
                holder.name.setTextColor(Color.parseColor("#000000"));
                holder.charName.setTextColor(Color.parseColor("#000000"));
                holder.score.setTextColor(Color.parseColor("#000000"));
                holder.score.setText(String.valueOf(list.get(position).getCoins()));
                holder.rank.setText(""+(position+1));
                getTagFromDatabase(holder.img);
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/LeaderBoardAdapter.java
LINES: 96-128

holder.relativeLayout.setBackground(context.getDrawable(R.drawable.leaderboard_rank3_bg));
                getImageFromDatabase(holder.img1,list.get(position).getCharName());
                break;

            default:  holder.name.setText(list.get(position).getUsername());
                holder.charName.setText(list.get(position).getCharName());
                holder.score.setText(String.valueOf(list.get(position).getCoins()));
                getTagFromDatabase(holder.img);
                holder.rank.setText(""+(position+1));
                getImageFromDatabase(holder.img1,list.get(position).getCharName());
                break;
        }
    }

    @Override
    public int getItemCount() {
        return 10;
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView name,rank,score,charName;
        ImageView img,img1;
        RelativeLayout relativeLayout;

        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            name=itemView.findViewById(R.id.player_name);
            rank=itemView.findViewById(R.id.player_rank);
            score=itemView.findViewById(R.id.player_score);
            img = itemView.findViewById(R.id.badge);
            img1 = itemView.findViewById(R.id.circleImageView);
            charName = itemView.findViewById(R.id.player_char_name);
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/LeaderBoardAdapter.java
LINES: 129-171

relativeLayout = itemView.findViewById(R.id.card_name);
        }
    }

    @Override
    public int getItemViewType(int position) {
        if (position == 0) {
            return 0;
        } else if (position == 1){
            return 1;
        }else if (position == 2){
            return 2;
        }else{
            return 3;
        }
    }

    private void getImageFromDatabase(ImageView v,String charName){
        FirebaseStorage storage = FirebaseStorage.getInstance();
        StorageReference storageRef = storage.getReference();

        switch (charName){
            case "The HackerMan": charRef = storageRef.child("Characters/Hackerman.png");
                break;
            case "Maestro": charRef = storageRef.child("Characters/Maestro.png");
                break;
            default: charRef = storageRef.child("Characters/Maestro.png");
                break;
        }


            charRef.getDownloadUrl().addOnSuccessListener(new OnSuccessListener<Uri>() {
                @Override
                public void onSuccess(Uri uri) {
                    if(context!=null && activity!=null)
                    Glide.with(activity).load(uri.toString()).into(v);
                }
            });

    }

    private void getTagFromDatabase(ImageView v){
        FirebaseStorage storage = FirebaseStorage.getInstance();
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/LeaderBoardAdapter.java
LINES: 172-188

StorageReference storageRef = storage.getReference();

        StorageReference charRef = storageRef.child("Badges/topimg.png");

            charRef.getDownloadUrl().addOnSuccessListener(new OnSuccessListener<Uri>() {
                @Override
                public void onSuccess(Uri uri) {
//                    if(context!=null && activity!=null)
//                    Glide.with(context).load(uri.toString()).into(v);

                    if(context!=null && activity!=null)
                        Picasso.get().load(uri.toString()).into(v);
                }
            });
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/MoreAdapter.java
LINES: 1-43

package com.bitspilani.apogeear.Adapters;

import android.content.Context;
import android.content.Intent;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.DevelopersActivity;
import com.bitspilani.apogeear.Models.MoreModel;
import com.bitspilani.apogeear.R;

import java.util.List;

public class MoreAdapter extends RecyclerView.Adapter<MoreAdapter.MoreViewHolder> {

    private Context context;
    private List<MoreModel> moreList;

    public MoreAdapter(Context context, List<MoreModel> moreList) {
        this.context = context;
        this.moreList = moreList;
    }

    @NonNull
    @Override
    public MoreViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        LayoutInflater inflater = LayoutInflater.from(context);
        View view = inflater.inflate(R.layout.more_item_view,null);
        return new MoreViewHolder(view);
    }

    @Override
    public void onBindViewHolder(MoreViewHolder holder, int position) {
        MoreModel moreModel = moreList.get(position);

        holder.textView.setText(moreModel.getName());
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/MoreAdapter.java
LINES: 44-91

holder.imageView.setImageDrawable(context.getResources().getDrawable(moreModel.getImage()));
        holder.textView2.setText(moreModel.getSubName());

        holder.itemView.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                TextView t = v.findViewById(R.id.more);
                String a = t.getText().toString();
                Toast.makeText(context, a, Toast.LENGTH_SHORT).show();

                switch (a){
                    case "Developers":
                        context.startActivity(new Intent(context,DevelopersActivity.class));
                        break;
                }

            }
        });

    }

    @Override
    public int getItemCount() {
        return moreList.size();
    }

    class MoreViewHolder extends RecyclerView.ViewHolder{

        TextView textView,textView2;
        ImageView imageView;

        MoreViewHolder(@NonNull View itemView) {
            super(itemView);

            textView = itemView.findViewById(R.id.more);
            imageView = itemView.findViewById(R.id.mark);
            textView2 = itemView.findViewById(R.id.sub_more);

        }
    }

    @Override
    public int getItemViewType(int position) {
        if (position == 0){
            return 0;
        }else if (position == 1){
            return 1;
        }else{
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/MoreAdapter.java
LINES: 92-95

return 2;
        }
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/MoreNestedAdapter.java
LINES: 1-39

package com.bitspilani.apogeear.Adapters;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.Models.MoreModel;
import com.bitspilani.apogeear.Models.MoreNestedModel;
import com.bitspilani.apogeear.R;

import java.util.List;

public class MoreNestedAdapter extends RecyclerView.Adapter<MoreNestedAdapter.MoreNestedViewHolder> {

    private Context context;
    private List<MoreNestedModel> moreNestedModelList;
    private RecyclerView.RecycledViewPool recycledViewPool;

    public MoreNestedAdapter(Context context, List<MoreNestedModel> moreNestedModelList) {
        this.context = context;
        this.moreNestedModelList = moreNestedModelList;
        recycledViewPool = new RecyclerView.RecycledViewPool();
    }

    @NonNull
    @Override
    public MoreNestedAdapter.MoreNestedViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.more_nested,parent,false);
        return new MoreNestedViewHolder(view);
    }

    @Override
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/MoreNestedAdapter.java
LINES: 40-67

public void onBindViewHolder(@NonNull MoreNestedAdapter.MoreNestedViewHolder holder, int position) {
        MoreNestedModel moreNestedModel = moreNestedModelList.get(position);

        holder.name.setText(moreNestedModel.getName());
        holder.recyclerView.setRecycledViewPool(recycledViewPool);
        holder.recyclerView.setHasFixedSize(true);
        holder.recyclerView.setLayoutManager(new LinearLayoutManager(context,RecyclerView.VERTICAL,false));
        MoreAdapter moreAdapter = new MoreAdapter(context,moreNestedModel.getMoreModelList());
        holder.recyclerView.setAdapter(moreAdapter);
    }

    @Override
    public int getItemCount() {
        return moreNestedModelList.size();
    }

    public class MoreNestedViewHolder extends RecyclerView.ViewHolder {
        TextView name;
        RecyclerView recyclerView;

        MoreNestedViewHolder(@NonNull View itemView) {
            super(itemView);

            name = itemView.findViewById(R.id.more_item_heading);
            recyclerView = itemView.findViewById(R.id.recycler_more1);
        }
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/VerticalAdapter.java
LINES: 1-39

package com.bitspilani.apogeear.Adapters;

import android.content.Context;
import android.content.SharedPreferences;
import android.graphics.Color;
import android.graphics.drawable.Drawable;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.cardview.widget.CardView;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.Models.Event_Details;
import com.bitspilani.apogeear.R;
import com.github.vipulasri.timelineview.TimelineView;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FirebaseFirestore;

import java.util.ArrayList;
import java.util.Calendar;

import static android.content.Context.MODE_PRIVATE;

public class VerticalAdapter extends RecyclerView.Adapter<VerticalAdapter.ViewHolder> {

    private ArrayList<ArrayList<Event_Details>> lists;
    private ArrayList<String> attended;
    private Context context;
    private String attend;
    private FirebaseFirestore db=FirebaseFirestore.getInstance();
    private RecyclerView.RecycledViewPool recycledViewPool;
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/VerticalAdapter.java
LINES: 40-71

public VerticalAdapter(ArrayList<ArrayList<Event_Details>> lists, Context context, ArrayList<String> attended) {
        this.lists=lists;
        this.context=context;
        recycledViewPool=new RecyclerView.RecycledViewPool();
        this.attended=attended;
    }

    public VerticalAdapter(){

    }


    @NonNull
    @Override
    public VerticalAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {

        View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.verticalitem,parent,false);
        ViewHolder viewHolder=new ViewHolder(view,viewType);
        return viewHolder;
    }

    @Override
    public void onBindViewHolder(@NonNull VerticalAdapter.ViewHolder holder, int position) {

        Calendar c1=Calendar.getInstance(),c2=Calendar.getInstance(),c3=Calendar.getInstance();
        c1.setTimeInMillis(lists.get(position).get(0).getTime().getSeconds()*1000);
        c2.setTimeInMillis(lists.get(position).get(lists.get(position).size()-1).getTime().getSeconds()*1000);
        c3.setTimeInMillis(lists.get(position>0?position-1:position).get(0).getTime().getSeconds()*1000);

        if(position==0 ||(position>0 && c1.get(Calendar.DATE)>c3.get(Calendar.DATE))) {
            holder.date.setText(c1.get(Calendar.DATE)+" March");
            holder.date.setVisibility(View.VISIBLE);
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/VerticalAdapter.java
LINES: 72-94

holder.datecard.setVisibility(View.VISIBLE);
            if(position!=0) {
                holder.timext.setVisibility(View.VISIBLE);
                holder.timext.setBackgroundColor(Color.parseColor("#262626"));
            }
            if(position>lists.size()-3)
                holder.timext.setBackgroundColor(Color.parseColor("#d0d3d4"));
        }

        if(lists.get(position).size()==1 || lists.get(position).get(0).getTime().compareTo(lists.get(position).get(lists.get(position).size()-1).getTime())==0)
        holder.text.setText(c1.get(Calendar.HOUR)+":"+(c1.get(Calendar.MINUTE)==0?"00":c1.get(Calendar.MINUTE))+" "+(c1.get(Calendar.AM_PM)==0?"AM":"PM"));

        else
            holder.text.setText(c1.get(Calendar.HOUR)+":"+(c1.get(Calendar.MINUTE)==0?"00":c1.get(Calendar.MINUTE))+" "+(c1.get(Calendar.AM_PM)==0?"AM":"PM")+"-"+c2.get(Calendar.HOUR)+":"+(c2.get(Calendar.MINUTE)==0?"00":c2.get(Calendar.MINUTE))+" "+(c2.get(Calendar.AM_PM)==0?"AM":"PM"));

        SharedPreferences sharedPref=context.getSharedPreferences("userinfo",MODE_PRIVATE);
        String user=sharedPref.getString("username","");

                attend="";
                for(int i=0;i<lists.get(position).size();i++)
                if(attended.contains(lists.get(position).get(i).getName()))
                    attend=lists.get(position).get(i).getName();
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/VerticalAdapter.java
LINES: 95-116

holder.horizontalrv.setRecycledViewPool(recycledViewPool);
                holder.horizontalrv.setHasFixedSize(true);
                holder.horizontalrv.setLayoutManager(new LinearLayoutManager(context,RecyclerView.VERTICAL,false));
                HorizontalAdapter horizontalAdapter = new HorizontalAdapter(lists.get(position),context,attend,!attend.equals(""));
                holder.horizontalrv.setAdapter(horizontalAdapter);
                horizontalAdapter.setOnItemClickListener(new HorizontalAdapter.ClickListener() {
                    @Override
                    public void onItemClicked(int position, View v) {
                        Toast.makeText(context,"Clicked",Toast.LENGTH_SHORT).show();
                    }
                });


                if(attend.equals(""))
                    holder.timelineView.setMarker(holder.fail);
                if(position==lists.size()-1 || position==lists.size()-2)
                    holder.timelineView.setMarker(holder.incomplete);
                if(position<lists.size()-3) {
                    holder.timelineView.setEndLineColor(Color.parseColor("#262626"), holder.getItemViewType());
                    holder.timelineView.setStartLineColor(Color.parseColor("#262626"), holder.getItemViewType());
                }
                if(position==lists.size()-3){
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/VerticalAdapter.java
LINES: 117-162

holder.timelineView.setStartLineColor(Color.parseColor("#262626"), holder.getItemViewType());
                }




        //Log.d("Hey","Yo"+lists.get(position).size());
    }

    @Override
    public int getItemCount() {
        return lists.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView text,date;
        CardView datecard;
        View timext;
        RecyclerView horizontalrv;
        public TimelineView timelineView;
        Drawable fail,incomplete;

        public ViewHolder(@NonNull View itemView,int viewType) {
            super(itemView);

            text=itemView.findViewById(R.id.course_item_name_tv);
            horizontalrv=itemView.findViewById(R.id.horizontal_list);
            timelineView=itemView.findViewById(R.id.timeline);
            date=itemView.findViewById(R.id.date);
            timelineView.initLine(viewType);
            timext=itemView.findViewById(R.id.timext);
            datecard=itemView.findViewById(R.id.datecard);

            fail=itemView.getResources().getDrawable(R.drawable.fail);
            incomplete=itemView.getResources().getDrawable(R.drawable.incomplete);

        }


    }
    @Override
    public int getItemViewType(int position) {
        return TimelineView.getTimeLineViewType(position, getItemCount());
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Adapters/ViewPagerAdapter.java
LINES: 1-52

package com.bitspilani.apogeear.Adapters;

import androidx.annotation.NonNull;
import androidx.fragment.app.Fragment;
import androidx.fragment.app.FragmentManager;
import androidx.fragment.app.FragmentStatePagerAdapter;

import com.bitspilani.apogeear.Fragments.Home;
import com.bitspilani.apogeear.Fragments.Leaderboard;
import com.bitspilani.apogeear.Fragments.Map;
import com.bitspilani.apogeear.Fragments.More;
import com.bitspilani.apogeear.Fragments.Profile;
import com.bitspilani.apogeear.R;

import java.util.ArrayList;
import java.util.List;

public class ViewPagerAdapter extends FragmentStatePagerAdapter {
    private List<Fragment> Fragment = new ArrayList<>();

    public ViewPagerAdapter(@NonNull FragmentManager fm) {
        super(fm);
    }

    @NonNull
    @Override
    public Fragment getItem(int position) {
        switch (position){
            case 0:
                return new Profile();
            case 1:
                return new Map();
            case 2 :
                return new Home();
            case 3:
                return new Leaderboard();
            case 4:
                return new More();
            default:
                return new Home();
        }
    }

    public void add(Fragment Frag) {
        Fragment.add(Frag);
    }

    @Override
    public int getCount() {
        return 5;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/CharSelect.java
LINES: 1-40

package com.bitspilani.apogeear;

import androidx.appcompat.app.AppCompatActivity;
import androidx.viewpager.widget.ViewPager;

import android.animation.ArgbEvaluator;
import android.os.Bundle;
import android.widget.Adapter;

import com.bitspilani.apogeear.Adapters.CharAdapter;
import com.bitspilani.apogeear.Models.CharacterModel;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.FirebaseFirestore;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class CharSelect extends AppCompatActivity {

    ViewPager viewPager;
    CharAdapter adapter;
    List<CharacterModel> list;
    Integer[] colors = null;
    ArgbEvaluator argbEvaluator = new ArgbEvaluator();
    private FirebaseAuth mAuth = FirebaseAuth.getInstance();
    private FirebaseFirestore db = FirebaseFirestore.getInstance();

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_char_select);

        list = new ArrayList<>();
        list.add(new CharacterModel("The HackerMan",R.drawable.hackerman_charpage));
        list.add(new CharacterModel("Maestro",R.drawable.maestro));
        list.add(new CharacterModel("Paint Slinger",R.drawable.coinimage));
        list.add(new CharacterModel("Grease Monkey",R.drawable.coinimage));
PATH: app/src/main/java/com/bitspilani/apogeear/CharSelect.java
LINES: 41-70

adapter = new CharAdapter(list,this);

        viewPager = findViewById(R.id.char_view_pager);
        viewPager.setAdapter(adapter);
        viewPager.setPadding(100,400,100,0);

        Integer[] colors_temp = {getResources().getColor(R.color.gold),getResources().getColor(R.color.BACKGROUND),getResources().getColor(R.color.PRIMARY_VARIANT),getResources().getColor(R.color.PRIMARY_VARIANT)};
        colors = colors_temp;
        viewPager.setOnPageChangeListener(new ViewPager.OnPageChangeListener() {
            @Override
            public void onPageScrolled(int position, float positionOffset, int positionOffsetPixels) {
                if(position<(adapter.getCount()-1)&&position<(colors.length-1)){
                    viewPager.setBackgroundColor((Integer)argbEvaluator.evaluate(positionOffset,colors[position],colors[position+1]));
                }else{
                    viewPager.setBackgroundColor(colors[colors.length-1]);
                }
            }

            @Override
            public void onPageSelected(int position) {

            }

            @Override
            public void onPageScrollStateChanged(int state) {

            }
        });
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Constants.java
LINES: 1-26

package com.bitspilani.apogeear;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

public class Constants {

    private static Map<String,String> chartointerest=new HashMap<>();
    private static ArrayList<String> Types=new ArrayList<>();

    public static Map<String,String> getChartointerest() {
        chartointerest.clear();
        chartointerest.put("The HackerMan","coding and fintech");
        chartointerest.put("Maestro","quizzing and strategy");

        return chartointerest;
    }

    public static ArrayList<String> getTypes() {
        Types.clear();
        Types.add("coding and fintech");
        Types.add("quizzing and strategy");
        return Types;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/DevelopersActivity.java
LINES: 1-35

package com.bitspilani.apogeear;

import androidx.appcompat.app.AppCompatActivity;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.os.Bundle;

import com.bitspilani.apogeear.Adapters.DevelopersAdapter;
import com.bitspilani.apogeear.Models.Developers;

import java.util.ArrayList;
import java.util.List;

public class DevelopersActivity extends AppCompatActivity {

    RecyclerView recyclerView;
    List<Developers> list;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_developers);

        recyclerView = findViewById(R.id.developers_rv);

        list = new ArrayList<>();
        list.add(new Developers("Anubhav","Developer",R.drawable.topimg));
        list.add(new Developers("Gauransh","Developer",R.drawable.topimg));
        list.add(new Developers("Naman","Developer",R.drawable.topimg));
        list.add(new Developers("Anubhav","Developer",R.drawable.topimg));
        list.add(new Developers("Gauransh","Developer",R.drawable.topimg));
        list.add(new Developers("Naman","Developer",R.drawable.topimg));
        list.add(new Developers("Anubhav","Developer",R.drawable.topimg));
        list.add(new Developers("Gauransh","Developer",R.drawable.topimg));
PATH: app/src/main/java/com/bitspilani/apogeear/DevelopersActivity.java
LINES: 36-45

list.add(new Developers("Naman","Developer",R.drawable.topimg));

        recyclerView.setHasFixedSize(true);
        recyclerView.setLayoutManager(new LinearLayoutManager(this));
        DevelopersAdapter developersAdapter = new DevelopersAdapter(this,list);
        recyclerView.setAdapter(developersAdapter);


    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/EventDialog.java
LINES: 1-46

package com.bitspilani.apogeear;

import android.app.Dialog;
import android.content.Context;
import android.graphics.Color;
import android.os.Bundle;
import android.view.View;
import android.widget.GridView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.GridLayoutManager;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.apogeear.Adapters.InterestAdapter;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FirebaseFirestore;

import java.lang.reflect.Field;
import java.util.ArrayList;


public class EventDialog extends Dialog {

    private Context context;
    RecyclerView gridView;
    private ArrayList<String> types;
    private FirebaseFirestore db=FirebaseFirestore.getInstance();
    private String userId;
    InterestAdapter interestAdapter;

    public EventDialog(@NonNull Context context, ArrayList<String> types,String userId) {
        super(context);
        this.context=context;
        this.types=types;
        this.userId=userId;

    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.event_filter);
PATH: app/src/main/java/com/bitspilani/apogeear/EventDialog.java
LINES: 47-82

gridView=findViewById(R.id.mygridview);

        GridLayoutManager llm = new GridLayoutManager(context,2);
        gridView.setLayoutManager(llm);
        interestAdapter=new InterestAdapter(types);

        gridView.setAdapter(interestAdapter);
    }

    private void set(TextView textView,String ele){
        if(types.contains(ele.toLowerCase())){

            types.remove(ele.toLowerCase());
            db.collection("Users").document(userId)
                    .update("Types",types)
                    .addOnSuccessListener(new OnSuccessListener<Void>() {
                        @Override
                        public void onSuccess(Void aVoid) {
                            textView.setTextColor(Color.parseColor("#fdfdfd"));
                        }
                    });
        }
        else {
            types.add(ele.toLowerCase());
            db.collection("Users").document(userId)
                    .update("Types",types)
                    .addOnSuccessListener(new OnSuccessListener<Void>() {
                        @Override
                        public void onSuccess(Void aVoid) {
                            textView.setTextColor(Color.parseColor("#d0d3d4"));
                        }
                    });

        }
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/EventList.java
LINES: 1-41

package com.bitspilani.apogeear;

import androidx.annotation.Nullable;
import androidx.appcompat.app.AppCompatActivity;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.os.Bundle;
import android.util.Log;

import com.bitspilani.apogeear.Adapters.VerticalAdapter;
import com.bitspilani.apogeear.Models.Event_Details;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;

import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.Locale;

public class EventList extends AppCompatActivity {

    VerticalAdapter adapter;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_eventlist);

        ArrayList<ArrayList<Event_Details>> lists=new ArrayList<>();
        ArrayList<String> ev=new ArrayList<>();
        ev.add("coding and fintech");
        ev.add("quizzing and strategy");
        ArrayList<Event_Details> list=new ArrayList<>();
PATH: app/src/main/java/com/bitspilani/apogeear/EventList.java
LINES: 42-66

RecyclerView vertical=findViewById(R.id.vertical1);
        FirebaseFirestore db=FirebaseFirestore.getInstance();

        vertical.setHasFixedSize(true);

        LinearLayoutManager llm=new LinearLayoutManager(this);
        vertical.setLayoutManager(llm);


        db.collection("Events").whereIn("Type",ev)
                .addSnapshotListener(new EventListener<QuerySnapshot>() {
                    @Override
                    public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {

                        for (QueryDocumentSnapshot document : queryDocumentSnapshots) {
                            Log.d("TAG", document.getId() + " => " + document.get("Name"));
                            if(document.get("Name")!=null && document.get("Type")!=null && document.getTimestamp("Time")!=null)
                                list.add(new Event_Details(document.get("Name").toString(),document.get("Type").toString(),document.getTimestamp("Time")));
                        }

                        Comparator<Event_Details> comparebyTime=(Event_Details e1, Event_Details e2)->{
                            try {
                                int i = e1.getTime().compareTo(e2.getTime());
                                return i;
                            } catch (Exception ex) {
PATH: app/src/main/java/com/bitspilani/apogeear/EventList.java
LINES: 67-94

ex.printStackTrace();
                            }
                            return 0;
                        };

                        Collections.sort(list,comparebyTime);
                        long seconds=86400;
                        int prev=0;
                        ArrayList<Event_Details> temp=new ArrayList<>();
                        temp.add(list.get(0));

//                        for(int i=0;i<list.size();i++){
//                            temp.add(list.get(i));
//                            if(i%2==1 || i==list.size()-1) {
//                                Log.d("Size",""+temp.size());
//                                ArrayList<Event_Details> temp1=new ArrayList<>();
//                                    temp1.addAll(temp);
//                                lists.add(temp1);
//                                temp.clear();
//                                Log.d("Size",""+lists.get(0).size());
//                            }
//                        }

                        for(int i=1;i<list.size();i++){
                            if(toDate(list.get(i).getTime().toDate()).equals(toDate(list.get(prev).getTime().toDate())))
                                seconds=list.get(i).getTime().getSeconds()-list.get(prev).getTime().getSeconds();

                            if(seconds<=3600)
PATH: app/src/main/java/com/bitspilani/apogeear/EventList.java
LINES: 95-127

temp.add(list.get(i));
                            else {

                                Log.d("Size",""+temp.size());
                                ArrayList<Event_Details> temp1=new ArrayList<>();
                                    temp1.addAll(temp);
                                lists.add(temp1);
                                temp.clear();
                               Log.d("Size",""+lists.get(0).size());
                                prev=i;
                                temp.add(list.get(i));
                                seconds=86400;
                            }

                            if(i==list.size()-1 ){

                                lists.add(temp);
                                break;
                            }
                            Log.d("Date",list.get(i).getTime().toDate().toString());
                            Log.d("Time",""+list.get(i).getTime().getSeconds());
                        }
                        //adapter=new VerticalAdapter(lists, EventList.this, attended);
                       // vertical.setAdapter(adapter);
                    }
                });

    }

    private String toDate(Date date) {
        SimpleDateFormat sdf = new SimpleDateFormat("dd-MM-yyyy", Locale.getDefault());
        String currentTime = sdf.format(new Date());
        return currentTime.trim();
PATH: app/src/main/java/com/bitspilani/apogeear/EventList.java
LINES: 128-130

}
}
PATH: app/src/main/java/com/bitspilani/apogeear/FadeOutTransformation.java
LINES: 1-15

package com.bitspilani.apogeear;

import android.view.View;

import androidx.annotation.NonNull;
import androidx.viewpager.widget.ViewPager;

public class FadeOutTransformation implements ViewPager.PageTransformer {
    @Override
    public void transformPage(@NonNull View page, float position) {
        page.setTranslationX(-position*page.getWidth());

        page.setAlpha(1-Math.abs(position));
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 1-38

package com.bitspilani.apogeear.Fragments;

import android.content.Context;
import android.os.Bundle;

import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;

import com.bitspilani.apogeear.Constants;
import com.bitspilani.apogeear.EventDialog;
import com.bitspilani.apogeear.Models.Event_Details;
import com.bitspilani.apogeear.R;
import com.bitspilani.apogeear.Adapters.VerticalAdapter;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentReference;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.ListenerRegistration;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;

import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.Locale;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 39-77

import java.util.Map;


/**
 * A simple {@link Fragment} subclass.
 * Activities that contain this fragment must implement the
 * to handle interaction events.
 */
public class Home extends Fragment {

    private VerticalAdapter adapter;
    private FirebaseAuth mAuth=FirebaseAuth.getInstance();
    private ImageView filter;
    private DocumentReference query;
    private ListenerRegistration listenerRegistration,listenerRegistration2;

    public Home() {
        // Required empty public constructor
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View view=inflater.inflate(R.layout.fragment_home, container, false);

        String userid=mAuth.getCurrentUser().getUid();
        ArrayList<String> types= Constants.getTypes();
        Map<String,String> charmap=Constants.getChartointerest();
        filter=view.findViewById(R.id.filter);

        Log.d("hello","hellp");
        FirebaseFirestore db=FirebaseFirestore.getInstance();

        filter.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                db.collection("Users").document(userid)
                        .get()
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 78-105

.addOnSuccessListener(new OnSuccessListener<DocumentSnapshot>() {
                            @Override
                            public void onSuccess(DocumentSnapshot documentSnapshot) {
                                if(documentSnapshot!=null) {

                                    ArrayList<String> types = (ArrayList<String>) documentSnapshot.get("Types");
                                    String character=documentSnapshot.get("char").toString();

                                    types.remove(charmap.get(character));

                                    EventDialog eventDialog = new EventDialog(getContext(), types, userid);
                                    eventDialog.show();
                                }
                            }
                        });
            }
        });
        Log.d("yoyo","yooyoyoyo");

        query=db.collection("Users").document(userid);

        listenerRegistration=query.addSnapshotListener(new EventListener<DocumentSnapshot>() {
                    @Override
                    public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {

                        if(documentSnapshot!=null) {

                            ArrayList<String> ev = (ArrayList<String>) documentSnapshot.get("Types");
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 106-122

ArrayList<String> attended = (ArrayList<String>) documentSnapshot.get("Attended");


                            if (ev.size() != 0) {
                                listenerRegistration2 = db.collection("Events").whereIn("Type", ev)
                                        .addSnapshotListener(new EventListener<QuerySnapshot>() {
                                            @Override
                                            public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
                                                ArrayList<ArrayList<Event_Details>> lists = new ArrayList<>();
                                                ArrayList<Event_Details> list = new ArrayList<>();

                                                for (QueryDocumentSnapshot document : queryDocumentSnapshots) {
                                                    Log.d("TAG", document.getId() + " => " + document.get("Name"));
                                                    if (document.get("Name") != null && document.get("Type") != null && document.getTimestamp("Time") != null)
                                                        list.add(new Event_Details(document.get("Name").toString(), document.get("Type").toString(), document.getTimestamp("Time")));
                                                }
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 123-143

Comparator<Event_Details> comparebyTime = (Event_Details e1, Event_Details e2) -> {
                                                    try {
                                                        int i = e1.getTime().compareTo(e2.getTime());
                                                        return i;
                                                    } catch (Exception ex) {
                                                        ex.printStackTrace();
                                                    }
                                                    return 0;
                                                };

                                                Collections.sort(list, comparebyTime);
                                                long seconds = 86400;
                                                int prev = 0;
                                                ArrayList<Event_Details> temp = new ArrayList<>();
                                                temp.add(list.get(0));

//                        for(int i=0;i<list.size();i++){
//                            temp.add(list.get(i));
//                            if(i%2==1 || i==list.size()-1) {
//                                Log.d("Size",""+temp.size());
//                                ArrayList<Event_Details> temp1=new ArrayList<>();
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 144-164

//                                    temp1.addAll(temp);
//                                lists.add(temp1);
//                                temp.clear();
//                                Log.d("Size",""+lists.get(0).size());
//                            }
//                        }

                                                for (int i = 1; i < list.size(); i++) {
                                                    if (toDate(list.get(i).getTime().toDate()).equals(toDate(list.get(prev).getTime().toDate())))
                                                        seconds = list.get(i).getTime().getSeconds() - list.get(prev).getTime().getSeconds();

                                                    if (seconds == 0)
                                                        temp.add(list.get(i));
                                                    else {

                                                        Log.d("Size", "" + temp.size());
                                                        ArrayList<Event_Details> temp1 = new ArrayList<>();
                                                        temp1.addAll(temp);
                                                        lists.add(temp1);
                                                        temp.clear();
                                                        Log.d("Size", "" + lists.get(0).size());
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 165-187

prev = i;
                                                        temp.add(list.get(i));
                                                        seconds = 86400;
                                                    }

                                                    if (i == list.size() - 1) {

                                                        lists.add(temp);
                                                        break;
                                                    }
                                                    Log.d("Date", list.get(i).getTime().toDate().toString());
                                                    Log.d("Time", "" + list.get(i).getTime().getSeconds());
                                                }


                                                RecyclerView vertical = view.findViewById(R.id.vertical);
                                                vertical.setHasFixedSize(true);

                                                LinearLayoutManager llm = new LinearLayoutManager(getContext());
                                                vertical.setLayoutManager(llm);

                                                adapter = new VerticalAdapter(lists, getContext(), attended);
                                                vertical.setAdapter(adapter);
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Home.java
LINES: 188-229

adapter.notifyDataSetChanged();
                                            }
                                        });
                            }
                        }

                    }
                });

        Log.d("hello","hellp");
        return view;
    }

    private String toDate(Date date) {
        SimpleDateFormat sdf = new SimpleDateFormat("dd-MM-yyyy", Locale.getDefault());
        String currentTime = sdf.format(new Date());
        return currentTime.trim();

    }

    @Override
    public void onAttach(Context context) {

        super.onAttach(context);
    }

    @Override
    public void onPause() {
//        if (listenerRegistration!= null) {
//            listenerRegistration.remove();
//            listenerRegistration = null;
//        }
//
//        if (listenerRegistration2!= null) {
//            listenerRegistration2.remove();
//            listenerRegistration2 = null;
//        }
        Log.d("pause","Homepaused");
        super.onPause();
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Leaderboard.java
LINES: 1-40

package com.bitspilani.apogeear.Fragments;

import android.net.Uri;
import android.os.Bundle;

import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;

import com.bitspilani.apogeear.Adapters.LeaderBoardAdapter;
import com.bitspilani.apogeear.R;
import com.bitspilani.apogeear.Models.Rank;
import com.bumptech.glide.Glide;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;

public class Leaderboard extends Fragment {

    private RecyclerView leaderboard;
    private FirebaseFirestore db;
    private ArrayList<Rank> list;
    private LeaderBoardAdapter leaderBoardAdapter;
    private FirebaseAuth mAuth=FirebaseAuth.getInstance();
    private TextView rank,name,coins,charName;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Leaderboard.java
LINES: 41-73

private ImageView userImage;
    private StorageReference charRef;
    private String userChar;

    public Leaderboard() {
        // Required empty public constructor
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View view= inflater.inflate(R.layout.fragment_leaderboard, container, false);
        leaderboard=view.findViewById(R.id.leader_rv);
        rank=view.findViewById(R.id.user_rank);
        name=view.findViewById(R.id.user_name);
        coins=view.findViewById(R.id.user_score);
        charName = view.findViewById(R.id.user_char_name);
        userImage = view.findViewById(R.id.user_image);
        db=FirebaseFirestore.getInstance();

        FirebaseStorage storage = FirebaseStorage.getInstance();
        StorageReference storageRef = storage.getReference();

        String userid=mAuth.getCurrentUser().getUid();
        list=new ArrayList<>();

        db.collection("Users").get().addOnSuccessListener(new OnSuccessListener<QuerySnapshot>() {
            @Override
            public void onSuccess(QuerySnapshot queryDocumentSnapshots) {
                list.clear();
                int i=1;
                for(QueryDocumentSnapshot document:queryDocumentSnapshots){
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Leaderboard.java
LINES: 74-101

Rank ob=new Rank(document.get("name").toString(),document.getDouble("score"),document.get("username").toString(),document.get("char").toString());
                    list.add(ob);
                    i++;
                }

                Comparator<Rank> comparebycoins=(Rank o1,Rank o2)-> (int)(o2.getCoins()-o1.getCoins());
                Collections.sort(list,comparebycoins);

                leaderBoardAdapter=new LeaderBoardAdapter(list,getContext(),getActivity());
                leaderboard.setLayoutManager(new LinearLayoutManager(getContext()));
                leaderboard.setAdapter(leaderBoardAdapter);

                for(Rank rc: list){
                    if(rc.getUserId().equals(userid)){
                        rank.setText((list.indexOf(rc)+1)+"");
                        name.setText(rc.getUsername());
                        coins.setText(rc.getCoins()+"");
                        charName.setText(rc.getCharName());
                    }
                }
            }
        });

        db.collection("Users").document(userid).get().addOnSuccessListener(new OnSuccessListener<DocumentSnapshot>() {
            @Override
            public void onSuccess(DocumentSnapshot documentSnapshot) {
                userChar = documentSnapshot.get("char").toString();
                switch (userChar){
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Leaderboard.java
LINES: 102-124

case "The HackerMan": charRef = storageRef.child("Characters/Hackerman.png");
                        break;
                    case "Maestro": charRef = storageRef.child("Characters/Maestro.png");
                        break;
                    default: charRef = storageRef.child("Characters/Hackerman.png");
                        break;
                }


                    charRef.getDownloadUrl().addOnSuccessListener(new OnSuccessListener<Uri>() {
                        @Override
                        public void onSuccess(Uri uri) {
                            if(getContext()!=null)
                            Glide.with(getContext()).load(uri.toString()).into(userImage);
                        }
                    });

            }
        });
        return view;
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 1-40

package com.bitspilani.apogeear.Fragments;

import android.Manifest;
import android.app.PendingIntent;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.content.pm.PackageManager;
import android.content.res.Resources;
import android.graphics.Color;
import android.location.Location;
import android.net.Uri;
import android.os.Bundle;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AlertDialog;
import androidx.core.app.ActivityCompat;
import androidx.fragment.app.Fragment;

import android.os.Handler;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.Button;
import android.widget.Toast;

import com.bitspilani.apogeear.MainActivity;
import com.bitspilani.apogeear.R;

import com.bitspilani.arapogee.UnityPlayerActivity;
import com.google.android.gms.common.api.GoogleApiClient;
import com.google.android.gms.common.api.ResultCallback;
import com.google.android.gms.common.api.Status;
import com.google.android.gms.location.FusedLocationProviderClient;
import com.google.android.gms.location.Geofence;
import com.google.android.gms.location.GeofencingRequest;
import com.google.android.gms.location.LocationServices;
import com.google.android.gms.maps.CameraUpdateFactory;
import com.google.android.gms.maps.GoogleMap;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 41-71

import com.google.android.gms.maps.OnMapReadyCallback;
import com.google.android.gms.maps.SupportMapFragment;
import com.google.android.gms.maps.model.BitmapDescriptorFactory;
import com.google.android.gms.maps.model.CameraPosition;
import com.google.android.gms.maps.model.Circle;
import com.google.android.gms.maps.model.CircleOptions;
import com.google.android.gms.maps.model.LatLng;
import com.google.android.gms.maps.model.MapStyleOptions;
import com.google.android.gms.maps.model.Marker;
import com.google.android.gms.maps.model.MarkerOptions;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;
import com.google.maps.android.SphericalUtil;
import com.vuforia.INIT_FLAGS;
import com.vuforia.Vuforia;

import java.util.ArrayList;
import java.util.List;
import java.util.Objects;
public class Map extends Fragment implements OnMapReadyCallback {

    private GoogleMap map;
    private Location currentLocation;
    private MarkerOptions markerOptions1;
    private boolean rmvMark = false;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 72-115

private List<Marker> m = new ArrayList<>();
    private double RADIUS = 2000f;
    private Button navBtn,removeMarkerBtn;
    private Boolean collected = false;
    FirebaseFirestore db;
    private int i,j,k;
    FirebaseAuth mAuth=FirebaseAuth.getInstance();

    private static final String TAG = "MapFragment";
    private static final int LOCATION_PERMISSION_REQUEST_CODE =1234;
    private static final float DEFAULT_ZOOM = 17.5f;

    private Boolean mLocationPermissionsGranted = false;
    public Map() {
        // Required empty public constructor
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        View rootView = inflater.inflate(R.layout.fragment_map, container, false);
        // Inflate the layout for this fragment
        navBtn = rootView.findViewById(R.id.nav_btn);
        removeMarkerBtn = rootView.findViewById(R.id.remove_marker);
        db=FirebaseFirestore.getInstance();

        db.collection("Events").get();

        getLocationPermission();
        return rootView;
    }

    @Override
    public void onAttach(Context context) {
        super.onAttach(context);
    }
    @Override
    public void onDetach() {
        super.onDetach();
    }

    @Override
    public void onMapReady(GoogleMap googleMap) {
        map = googleMap;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 116-150

// Theme customization
        try { // Customise the styling of the base map using a JSON object defined
            // in a raw resource file.
            boolean success = googleMap.setMapStyle(
                    MapStyleOptions.loadRawResourceStyle(
                            Objects.requireNonNull(getActivity()), R.raw.dark_json));
            if (!success) {
                Log.e(TAG, "Style parsing failed.");
            }
        } catch (Resources.NotFoundException e) {
            Log.e(TAG, "Can't find style. Error: ", e);
        }

        if (mLocationPermissionsGranted){
            getDeviceLocation();

            map.setMyLocationEnabled(true);
            map.getUiSettings().setMyLocationButtonEnabled(true);
            map.getUiSettings().setCompassEnabled(false);

           addCoinsForEvents();
           addUniversalCoins();
        }else{
            showGPSDisabledAlertToUser();
        }
        mapClickListener();
    }

    private void addUniversalCoins() {
        db.collection("Coins").document("Universal Coins").get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
            @Override
            public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                DocumentSnapshot documentSnapshot = task.getResult();
                List<Double> latA, lngA, latB, lngB, latC, lngC;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 151-172

latA = (List<Double>) documentSnapshot.get("latA");
                lngA = (List<Double>) documentSnapshot.get("lngA");
                latB = (List<Double>) documentSnapshot.get("latB");
                lngB = (List<Double>) documentSnapshot.get("lngB");
                latC = (List<Double>) documentSnapshot.get("latC");
                lngC = (List<Double>) documentSnapshot.get("lngC");

                int coinsa =  Integer.parseInt(documentSnapshot.get("CoinsA").toString().trim());
                int coinsb =  Integer.parseInt(documentSnapshot.get("CoinsB").toString().trim());
                int coinsc =  Integer.parseInt(documentSnapshot.get("CoinsC").toString().trim());

                for (i=0;i<coinsa;i++){
                    LatLng latLng = new LatLng(latA.get(i),lngA.get(i));
                    map.addMarker(new MarkerOptions()
                            .position(latLng)
                            .icon(BitmapDescriptorFactory.fromResource(R.drawable.coinimage)));
                //    map.moveCamera(CameraUpdateFactory.newLatLng(latLng));

                        map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
                            @Override
                            public boolean onMarkerClick(@NonNull Marker marker) {
                                Location l = new Location("");
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 173-195

l.setLatitude(marker.getPosition().latitude);
                                l.setLongitude(marker.getPosition().longitude);
                                double d = (double) currentLocation.distanceTo(l);
                                if (d<RADIUS&&!collected) {
                                    marker.setVisible(false);
                                    Intent intent = new Intent(getActivity(), UnityPlayerActivity.class);
                                    startActivity(intent);
                                    collected = true;
                                }else {
                                    Toast.makeText(getContext(), "Go close to coin", Toast.LENGTH_SHORT).show();
                                }
                                return true;
                            }
                        });
                }
                for (j=0;j<coinsb;j++){
                    LatLng latLng = new LatLng(latB.get(j),lngB.get(j));
                    map.addMarker(new MarkerOptions()
                            .position(latLng)
                            .icon(BitmapDescriptorFactory.fromResource(R.drawable.coin_green)));
                   // map.moveCamera(CameraUpdateFactory.newLatLng(latLng));

                        map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 196-218

@Override
                            public boolean onMarkerClick(@NonNull Marker marker) {
                                Location l = new Location("");
                                l.setLatitude(marker.getPosition().latitude);
                                l.setLongitude(marker.getPosition().longitude);
                                double d = (double) currentLocation.distanceTo(l);
                                if (d<RADIUS&&!collected) {
                                    marker.setVisible(false);
                                    Intent intent = new Intent(getActivity(), UnityPlayerActivity.class);
                                    startActivity(intent);
                                    collected = true;
                                }else {
                                    Toast.makeText(getContext(), "Go close to coin", Toast.LENGTH_SHORT).show();
                                }
                                return true;
                            }
                        });
                }
                for (k=0;k<coinsc;k++){
                    LatLng latLng = new LatLng(latC.get(k),lngC.get(k));
                    map.addMarker(new MarkerOptions()
                            .position(latLng)
                            .icon(BitmapDescriptorFactory.fromResource(R.drawable.coin_red)));
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 219-244

//   map.moveCamera(CameraUpdateFactory.newLatLng(latLng));

                        map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
                            @Override
                            public boolean onMarkerClick(@NonNull Marker marker) {
                                    Location l = new Location("");
                                    l.setLatitude(marker.getPosition().latitude);
                                    l.setLongitude(marker.getPosition().longitude);
                                    double d = (double) currentLocation.distanceTo(l);
                                    if (d<RADIUS&&!collected) {
                                        marker.setVisible(false);
                                        Intent intent = new Intent(getActivity(), UnityPlayerActivity.class);
                                        startActivity(intent);
                                        collected = true;
                                    }else {
                                        Toast.makeText(getContext(), "Go close to coin", Toast.LENGTH_SHORT).show();
                                    }
                                    return true;
                            }
                        });
                    }
                }
        });
    }

    private void mapClickListener() {
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 245-269

map.setOnMapClickListener(new GoogleMap.OnMapClickListener() {
            @Override
            public void onMapClick(final LatLng latLng) {
                // on click marker appereance
                markerOptions1 = new MarkerOptions();
                markerOptions1.position(latLng);
                m.add(0,map.addMarker(markerOptions1));
                for (int i=0;i<m.size();i++){
                    if (i==0) m.get(i).setVisible(true);
                    else m.get(i).setVisible(false);
                }
                rmvMark = true;
                if (rmvMark==true) {
                    removeMarkerBtn.setOnClickListener(new View.OnClickListener() {
                        @Override
                        public void onClick(View v) {
                            for (int i=0;i<m.size();i++){
                                m.get(i).setVisible(false);
                            }
                            rmvMark = false;
                            map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
                                @Override
                                public boolean onMarkerClick(@NonNull Marker marker) {
                                    Location l = new Location("");
                                    l.setLatitude(marker.getPosition().latitude);
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 270-294

l.setLongitude(marker.getPosition().longitude);
                                    double d = (double) currentLocation.distanceTo(l);
                                    if (d<RADIUS&&!collected) {
                                        Intent intent = new Intent(getActivity(), UnityPlayerActivity.class);
                                        startActivity(intent);
                                        collected = true;
                                    }else {
                                        Toast.makeText(getContext(), "Go close to coin", Toast.LENGTH_SHORT).show();
                                    }
                                    return true;
                                }
                            });
                        }
                    });
                }else{
                    removeMarkerBtn.setOnClickListener(new View.OnClickListener() {
                        @Override
                        public void onClick(View v) {
                            Toast.makeText(getContext(), "Please place a marker to remove", Toast.LENGTH_SHORT).show();
                        }
                    });
                }
                navBtn.setOnClickListener(new View.OnClickListener() {
                    @Override
                    public void onClick(View v) {
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 295-317

if (rmvMark) {
                            new Handler().postDelayed(new Runnable() {
                                @Override
                                public void run() {
                                    Uri gmmIntentUri = Uri.parse("http://maps.google.com/maps?daddr=" + latLng.latitude + "," + latLng.longitude); //// "geo:28.356478,75.583785?q=28.456388,75.583964"
                                    Intent mapIntent = new Intent(Intent.ACTION_VIEW, gmmIntentUri);
                                    mapIntent.setPackage("com.google.android.apps.maps");
                                    startActivity(mapIntent);
                                }
                            }, 1000);
                        }else{
                            navBtn.setOnClickListener(new View.OnClickListener() {
                                @Override
                                public void onClick(View v) {
                                    Toast.makeText(getActivity(), "Place the marker", Toast.LENGTH_SHORT).show();
                                }
                            });
                        }
                    }
                });
                map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
                    @Override
                    public boolean onMarkerClick(Marker marker) {
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 318-349

return false;
                    }
                });
            }
        });
    }


    private void addCoinsForEvents(){
        db.collection("Events").get().addOnSuccessListener(new OnSuccessListener<QuerySnapshot>() {
            @Override
            public void onSuccess(QuerySnapshot queryDocumentSnapshots) {
                for (QueryDocumentSnapshot document:queryDocumentSnapshots){
                    double lat = document.getDouble("lat");
                    double lng = document.getDouble("long");

                    LatLng latLng = new LatLng(lat,lng);
                    map.addMarker(new MarkerOptions()
                            .position(latLng)
                            .icon(BitmapDescriptorFactory.fromResource(R.drawable.coinimage)));
                 //   map.moveCamera(CameraUpdateFactory.newLatLng(latLng));

                    if (rmvMark==true){
                        map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
                            @Override
                            public boolean onMarkerClick(@NonNull Marker marker) {
                               return true;
                            }
                        });
                    }else{
                        map.setOnMarkerClickListener(new GoogleMap.OnMarkerClickListener() {
                            @Override
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 350-375

public boolean onMarkerClick(@NonNull Marker marker) {
                                Location l = new Location("");
                                l.setLatitude(marker.getPosition().latitude);
                                l.setLongitude(marker.getPosition().longitude);
                                double d = (double) currentLocation.distanceTo(l);
                                if (d<RADIUS&&!collected) {
                                    Intent intent = new Intent(getActivity(), UnityPlayerActivity.class);
                                    startActivity(intent);
                                    collected = true;
                                }else {
                                    Toast.makeText(getContext(), "Go close to coin", Toast.LENGTH_SHORT).show();
                                }
                                return true;
                            }
                        });
                    }
                }
            }
        });
    }

    private void getDeviceLocation(){
        FusedLocationProviderClient fusedLocationProviderClient = LocationServices.getFusedLocationProviderClient(Objects.requireNonNull(getActivity()));
        try{
            if (mLocationPermissionsGranted){
                final Task location = fusedLocationProviderClient.getLastLocation();
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 376-406

location.addOnCompleteListener(new OnCompleteListener() {
                    @Override
                    public void onComplete(@NonNull Task task) {
                        if (task.isSuccessful()){
                            Log.d(TAG,"onComplete: found location");
                            currentLocation = (Location)task.getResult();
                            if (currentLocation != null) {
                                cameraZoom(currentLocation);
                            }else{
                                showGPSDisabledAlertToUser();
                            }
                        }else{
                            Log.d(TAG,"onComplete: current Location is null");
                            Toast.makeText(getActivity(), "unable to get current location", Toast.LENGTH_SHORT).show();
                        }
                    }
                });
            }
        }catch (SecurityException e){
            Log.e(TAG,"getDeviceLocation: SecurityException: "+ e.getMessage());
        }
    }


    private void initMap(){
        // Obtain the SupportMapFragment and get notified when map is ready to be used
        assert getFragmentManager() != null;
        SupportMapFragment mapFragment = (SupportMapFragment) getChildFragmentManager().findFragmentById(R.id.map);
        mapFragment.getMapAsync(this);
    }
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 407-431

private void cameraZoom(Location location){
        CameraPosition position = new CameraPosition.Builder()
                .target(new LatLng(location.getLatitude(), location.getLongitude())) // Sets the new camera position
                .zoom(DEFAULT_ZOOM) // Sets the zoom
                .bearing(0) // Rotate the camera
                .tilt(70) // Set the camera tilt
                .build(); // Creates a CameraPosition from the builder
        map.animateCamera(CameraUpdateFactory
                .newCameraPosition(position), new GoogleMap.CancelableCallback() {
            @Override
            public void onFinish() {
                // Code to execute when the animateCamera task has finished
            }
            @Override
            public void onCancel() {
                // Code to execute when the user has canceled the animateCamera task
            }
        });
    }

    private void getLocationPermission(){
        Log.d("isnull","Null");

        String[] permissions = {Manifest.permission.ACCESS_FINE_LOCATION, android.Manifest.permission.ACCESS_COARSE_LOCATION};
        if (ActivityCompat.checkSelfPermission(getContext(), android.Manifest.permission.ACCESS_FINE_LOCATION) != PackageManager.PERMISSION_GRANTED &&
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 432-462

ActivityCompat.checkSelfPermission(getContext(), android.Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) {
            requestPermissions(permissions, LOCATION_PERMISSION_REQUEST_CODE);
        }else{
            mLocationPermissionsGranted = true;
            initMap();
        }
    }

    @Override
    public void onRequestPermissionsResult(int requestCode, @NonNull String[] permissions, @NonNull int[] grantResults) {
        mLocationPermissionsGranted = false;
        if (requestCode == LOCATION_PERMISSION_REQUEST_CODE) {
            if (grantResults.length > 0) {
                for (int grantResult : grantResults) {
                    if (grantResult != PackageManager.PERMISSION_GRANTED) {
                        mLocationPermissionsGranted = false;
                        return;
                    }
                }
                mLocationPermissionsGranted = true;
                // initialize our map
                initMap();
            }
        }
    }

    // gps dialog box
    private void showGPSDisabledAlertToUser() {
        AlertDialog.Builder alertDialogBuilder = new AlertDialog.Builder(Objects.requireNonNull(getActivity()));
        alertDialogBuilder.setMessage("GPS is disabled in your device. Would you like to enable it?")
                .setCancelable(false)
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Map.java
LINES: 463-481

.setPositiveButton("Goto Settings Page To Enable GPS",
                        new DialogInterface.OnClickListener() {
                            public void onClick(DialogInterface dialog, int id) {
                                Intent callGPSSettingIntent = new Intent(android.provider.Settings.ACTION_LOCATION_SOURCE_SETTINGS);
                                startActivity(callGPSSettingIntent);
                            }
                        });
        alertDialogBuilder.setNegativeButton("Cancel",
                new DialogInterface.OnClickListener() {
                    public void onClick(DialogInterface dialog, int id) {
                        dialog.cancel();
                        Intent intent = new Intent(getActivity(), MainActivity.class);
                        startActivity(intent);
                    }
                });
        AlertDialog alert = alertDialogBuilder.create();
        alert.show();
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/More.java
LINES: 1-43

package com.bitspilani.apogeear.Fragments;

import android.os.Bundle;

import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import com.bitspilani.apogeear.Adapters.MoreAdapter;
import com.bitspilani.apogeear.Adapters.MoreNestedAdapter;
import com.bitspilani.apogeear.Models.MoreModel;
import com.bitspilani.apogeear.Models.MoreNestedModel;
import com.bitspilani.apogeear.R;
import com.google.android.material.appbar.CollapsingToolbarLayout;
import com.ramotion.cardslider.CardSliderLayoutManager;
import com.ramotion.cardslider.CardSnapHelper;

import java.util.ArrayList;
import java.util.List;

public class More extends Fragment {
    // the fragment initialization parameters, e.g. ARG_ITEM_NUMBER
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";

    List<MoreNestedModel> list;
    List<MoreModel> list1,list2,list3;
    RecyclerView recyclerView;

    // TODO: Rename and change types of parameters
    private String mParam1;
    private String mParam2;

    public More() {
        // Required empty public constructor
    }

    public static More newInstance(String param1, String param2) {
        More fragment = new More();
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/More.java
LINES: 44-76

Bundle args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        args.putString(ARG_PARAM2, param2);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        View view = inflater.inflate(R.layout.fragment_more,container,false);

        list = new ArrayList<>();
        list1 = new ArrayList<>();
        list2 = new ArrayList<>();
        list3 = new ArrayList<>();

        list1.add(new MoreModel("Edit Character",R.drawable.next,"Change your Avatar"));
        list1.add(new MoreModel("Change Interests",R.drawable.next,"Select new Interests"));
        list2.add(new MoreModel("EPC Blog",R.drawable.next));
        list2.add(new MoreModel("HPC Blog",R.drawable.next));
        list3.add(new MoreModel("Developers",R.drawable.next,"Get to know our developers"));
        list3.add(new MoreModel("Instructions",R.drawable.next));
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/More.java
LINES: 77-99

list3.add(new MoreModel("About Us",R.drawable.next,"Get in touch with Coding Club !!!"));
        list3.add(new MoreModel("Privacy Policy",R.drawable.next));
        list3.add(new MoreModel("Terms And Conditions",R.drawable.next));
        list3.add(new MoreModel("Terms And Conditions",R.drawable.next));
        list3.add(new MoreModel("Terms And Conditions",R.drawable.next));

        list.add(new MoreNestedModel("My Account",list1));
        list.add(new MoreNestedModel("Notifications",list2));
        list.add(new MoreNestedModel("About",list3));

        recyclerView = view.findViewById(R.id.recycler_more2);

        recyclerView.setHasFixedSize(true);
        recyclerView.setLayoutManager(new LinearLayoutManager(getActivity()));

        MoreNestedAdapter moreNestedAdapter = new MoreNestedAdapter(getActivity(),list);

        recyclerView.setAdapter(moreNestedAdapter);
        // Inflate the layout for this fragment
        return view;
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 1-41

package com.bitspilani.apogeear.Fragments;

import android.content.Intent;
import android.net.Uri;
import android.os.Bundle;

import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;

import android.os.CountDownTimer;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.ProgressBar;
import android.widget.TextView;
import android.widget.Toast;

import com.bitspilani.apogeear.LoginActivity;
import com.bitspilani.apogeear.R;
import com.bumptech.glide.Glide;
import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;

import com.google.firebase.firestore.ListenerRegistration;

import java.util.Calendar;

public class Profile extends Fragment {

    private CountDownTimer countDownTimer;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 42-87

private long totalTimeCountInMilliseconds;
    private long startTime, endTime;
    private TextView showtime, coins, name, charName;
    private ImageView logout,bgProfile,usercharImage;
    private FirebaseAuth mAuth = FirebaseAuth.getInstance();
    private ProgressBar timer;
    ListenerRegistration listenerRegistration1,listenerRegistration2;
    private long timeBlinkInMilliseconds; // start time of start blinking
    private boolean blink;
    private Calendar c;
    private GoogleSignInOptions gso;
    FirebaseFirestore db ;
    private GoogleSignInClient googleSignInClient;
    private String userid;
    View view;


    public Profile() {
        // Required empty public constructor
    }
    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        //user=mAuth.getCurrentUser();

    }

    @Override
    public void onResume() {
        super.onResume();
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        view = inflater.inflate(R.layout.fragment_profile_green,container,false);

        initalize();

        bgProfile = view.findViewById(R.id.header_profile);
        usercharImage = view.findViewById(R.id.yyyy);


        userid = mAuth.getCurrentUser().getUid();
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 88-115

gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestEmail()
                .requestIdToken(getString(R.string.default_web_client_id))
                .build();
        googleSignInClient = GoogleSignIn.getClient(getActivity(), gso);

        initalize();
        setLogout();

        db= FirebaseFirestore.getInstance();

        db= FirebaseFirestore.getInstance();
        listenerRegistration1=db.collection("Users").document(userid)
                .addSnapshotListener(new EventListener<DocumentSnapshot>() {
                    @Override
                    public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {

                        if(documentSnapshot!=null) {
                            long coinval = Math.round(documentSnapshot.getDouble("score"));
                            coins.setText(coinval + "");
                            name.setText(documentSnapshot.get("name").toString());
                            charName.setText(documentSnapshot.get("char").toString());
                        }
                    }
                });
        listenerRegistration2=db.collection("Coins").document("Universal Coins")
                .addSnapshotListener(new EventListener<DocumentSnapshot>() {
                    @Override
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 116-139

public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {
                        if(documentSnapshot!=null) {

                            c = Calendar.getInstance();
                            totalTimeCountInMilliseconds = documentSnapshot.getTimestamp("Expire Time").getSeconds() - c.getTimeInMillis() / 1000;
                            startTime = documentSnapshot.getTimestamp("Start Time").getSeconds() * 1000;
                            endTime = documentSnapshot.getTimestamp("Expire Time").getSeconds() * 1000;
//                            if (startTime > c.getTimeInMillis())
//                                totalTimeCountInMilliseconds = 0;
                            setTimer();
                        }
                    }
                });

        db.collection("Users").document(userid).get().addOnSuccessListener(new OnSuccessListener<DocumentSnapshot>() {
            @Override
            public void onSuccess(DocumentSnapshot documentSnapshot) {
                String userCharacter = documentSnapshot.get("char").toString();

                FirebaseStorage storage = FirebaseStorage.getInstance();
                StorageReference storageRef = storage.getReference();
                StorageReference bgRef, charRef;

                switch (userCharacter) {
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 140-164

case "The HackerMan":
                        bgRef = storageRef.child("Backgrounds/backg_hackerman.png");
                        charRef = storageRef.child("Characters/Hackerman.png");
                        break;
                    case "Maestro":
                        bgRef = storageRef.child("Backgrounds/backg_hackerman.png");
                        charRef = storageRef.child("Characters/Maestro.png");
                        break;
                    default:
                        bgRef = storageRef.child("Backgrounds/backg_hackerman.png");
                        charRef = storageRef.child("Characters/Hackerman.png");
                        break;
                }
                bgRef.getDownloadUrl().addOnSuccessListener(new OnSuccessListener<Uri>() {
                    @Override
                    public void onSuccess(Uri uri) {
                        if (getContext() != null) {
                            Glide.with(getContext()).load(uri.toString()).into(bgProfile);
                        }
                    }
                });
                charRef.getDownloadUrl().addOnSuccessListener(new OnSuccessListener<Uri>() {
                    @Override
                    public void onSuccess(Uri uri) {
                        if (getContext() != null) {
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 165-205

Glide.with(getContext()).load(uri.toString()).into(usercharImage);
                        }
                    }
                });
            }
        });

        return view;
    }

    private void initalize(){
        showtime = view.findViewById(R.id.tvTimeCount);
        coins = view.findViewById(R.id.total);
        timer = view.findViewById(R.id.progressbar);
        logout = view.findViewById(R.id.logout);
        name = view.findViewById(R.id.zzzz);
        charName = view.findViewById(R.id.aaaa);

        setLogout();
    }

    private void setLogout(){
        logout.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {

                if (listenerRegistration1!= null) {
                    listenerRegistration1.remove();
                    listenerRegistration1 = null;
                }
                if (listenerRegistration2!= null) {
                    listenerRegistration2.remove();
                    listenerRegistration2 = null;
                }

                googleSignInClient.signOut();
                mAuth.signOut();
                Intent i=new Intent(getActivity(), LoginActivity.class);
                i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                startActivity(i);
                getActivity().finish();
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 206-242

Toast.makeText(getContext(), "Signed Out",
                        Toast.LENGTH_SHORT).show();
            }
        });
    }

    private void setTimer() {
        int time = 0;
        //Toast.makeText(getContext(), "Please Enter Minutes...",
        //      Toast.LENGTH_LONG).show();
        totalTimeCountInMilliseconds = totalTimeCountInMilliseconds * 1000;
        //totalTimeCountInMilliseconds = 60 * time * 1000;
        timeBlinkInMilliseconds = 30 * 1000;
        startTimer();
    }

    private void startTimer() {

        if(countDownTimer!=null){
            countDownTimer.cancel();
        }
        countDownTimer = new CountDownTimer(totalTimeCountInMilliseconds, 1000) {
            // 500 means, onTick function will be called at every 500
            // milliseconds
            @Override
            public void onFinish() {
                timer.setProgress(0);
                showtime.setVisibility(View.VISIBLE);

                showtime.setText(0 + " hrs " + 0 + " min " + 0 + " sec");
            }
            @Override
            public void onTick(long leftTimeInMilliseconds) {

                //Log.d("Time left",""+leftTimeInMilliseconds);
                int minutes = (int) ((leftTimeInMilliseconds / (1000 * 60)) % 60);
                int seconds = (int) (leftTimeInMilliseconds / 1000) % 60;
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 243-267

int hours = (int) ((leftTimeInMilliseconds / (1000 * 60 * 60)) % 24);
                //i++;
                //Setting the Progress Bar to decrease wih the timer
                Log.d("timeleft", "" + totalTimeCountInMilliseconds);
                Log.d("total count", "" + (endTime - startTime));
                Log.d("lefttt", "" + leftTimeInMilliseconds);
                if (endTime != startTime && endTime-startTime>leftTimeInMilliseconds) {
                    timer.setMax((int) (endTime - startTime));
                    timer.setProgress((int) (0.83*(endTime - startTime - leftTimeInMilliseconds)));
                    showtime.setText(hours + " hrs " + minutes + " min " + seconds + " sec");
                } else {
                    timer.setProgress(0);
                    showtime.setText(0 + " hrs " + 0 + " min " + 0 + " sec");
                }
//                if (leftTimeInMilliseconds < timeBlinkInMilliseconds) {
//                    // change the style of the textview .. giving a red
//                    // alert style
//
//                    if (blink) {
//                        showtime.setVisibility(View.VISIBLE);
//                        // if blink is true, textview will be visible
//                    } else {
//                        showtime.setVisibility(View.INVISIBLE);
//                    }
//
PATH: app/src/main/java/com/bitspilani/apogeear/Fragments/Profile.java
LINES: 268-294

//                    blink = !blink; // toggle the value of blink
//                }

                // format the textview to show the easily readable format

            }

        }.start();
    }


    @Override
    public void onPause() {
//
//        if (listenerRegistration1!= null) {
//            listenerRegistration1.remove();
//            listenerRegistration1 = null;
//        }
//        if (listenerRegistration2!= null) {
//            listenerRegistration2.remove();
//            listenerRegistration2 = null;
//        }
        Log.d("pause","profilepaused");
        super.onPause();
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 1-33

package com.bitspilani.apogeear;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import com.bitspilani.apogeear.Models.Event_Details;
import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInAccount;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.common.SignInButton;
import com.google.android.gms.common.api.ApiException;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.AuthCredential;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.auth.GoogleAuthProvider;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FieldValue;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreSettings;
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 34-71

import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.firestore.SetOptions;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.StringTokenizer;

public class LoginActivity extends AppCompatActivity {

    private EditText userEmail,userPassword;
    private Button loginBtn,button;
    private FirebaseAuth mAuth;
    private GoogleSignInClient mGoogleSignInClient;
    String email,password;

    private static final int RC_SIGN_IN = 9001;
    private static final String TAG = "SignInActivity";

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_login);

        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestEmail()
                .requestIdToken(getString(R.string.default_web_client_id))
                .build();

        mGoogleSignInClient = GoogleSignIn.getClient(this, gso);
        button = findViewById(R.id.googlelogin);
        userEmail=findViewById(R.id.user_email);
        userPassword=findViewById(R.id.password);
        loginBtn=findViewById(R.id.login_btn);

        mAuth = FirebaseAuth.getInstance();
        FirebaseUser user = mAuth.getCurrentUser();
        FirebaseFirestore db1=FirebaseFirestore.getInstance();
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 72-92

if (user != null) {

            SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
            SharedPreferences.Editor editor = sharedPref.edit();
            editor.putString("username", user.getUid());
            editor.apply();

            if(sharedPref.getString("char","").equals("")) {
                db1.collection("Users").document(user.getUid()).get()
                        .addOnSuccessListener(new OnSuccessListener<DocumentSnapshot>() {
                            @Override
                            public void onSuccess(DocumentSnapshot DocumentSnapshot) {
                                if(DocumentSnapshot.get("char")==null) {
                                    SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                                    SharedPreferences.Editor editor = sharedPref.edit();
                                    editor.putString("char", DocumentSnapshot.get("char").toString());
                                    editor.apply();
                                    startActivity(new Intent(LoginActivity.this, CharSelect.class));
                                    finish();
                                }
                                else if (DocumentSnapshot.get("char").toString().equals("none")){
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 93-119

SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                                    SharedPreferences.Editor editor = sharedPref.edit();
                                    editor.putString("char", DocumentSnapshot.get("char").toString());
                                    editor.apply();
                                    startActivity(new Intent(LoginActivity.this, CharSelect.class));
                                    finish();
                                }
                                else{
                                    SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                                    SharedPreferences.Editor editor = sharedPref.edit();
                                    editor.putString("char", DocumentSnapshot.get("char").toString());
                                    editor.apply();
                                    startActivity(new Intent(LoginActivity.this, MainActivity.class));
                                    finish();
                                }

                            }
                        });

            }
            else {
                startActivity(new Intent(LoginActivity.this, MainActivity.class));
                finish();
            }
        }

        button.setOnClickListener(
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 120-154

new View.OnClickListener() {
                    @Override
                    public void onClick(View v) {
                        signIn();
                        Log.d("here", "here");
                    }
                });
        loginBtn.setOnClickListener(
                new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                email=userEmail.getText().toString();
                password=userPassword.getText().toString();
                firebaseAuthwithemail();
            }
        });
    }

    private void signIn() {
        Intent signInIntent = mGoogleSignInClient.getSignInIntent();
        startActivityForResult(signInIntent, RC_SIGN_IN);
        Log.d("here", "here");
    }

    @Override
    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        // Result returned from launching the Intent from GoogleSignInApi.getSignInIntent(...);
        if (requestCode == RC_SIGN_IN) {
            Task<GoogleSignInAccount> task = GoogleSignIn.getSignedInAccountFromIntent(data);
            try {
                // Google Sign In was successful, authenticate with Firebase
                GoogleSignInAccount account = task.getResult(ApiException.class);
                String email1=account.getEmail();
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 155-182

if(email1.substring(email1.indexOf('@')+1).equals("pilani.bits-pilani.ac.in"))
                    firebaseAuthWithGoogle(account);
                else {
                    Toast.makeText(getApplicationContext(), "Invalid Email", Toast.LENGTH_LONG).show();
                    mGoogleSignInClient.signOut();
                    mAuth.signOut();
                }
            } catch (ApiException e) {
                // Google Sign In failed, update UI appropriately
                Log.w(TAG, "Google sign in failed", e);
                // ...
            }
        }
    }

    private void firebaseAuthWithGoogle(GoogleSignInAccount acct) {
        Log.d(TAG, "firebaseAuthWithGoogle:" + acct.getId());

        AuthCredential credential = GoogleAuthProvider.getCredential(acct.getIdToken(), null);
        mAuth.signInWithCredential(credential)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {

                            // Sign in success, update UI with the signed-in user's information
                                Log.d(TAG, "signInWithCredential:success");
                                FirebaseUser user = mAuth.getCurrentUser();
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 183-213

updateUI(user);


                        } else {
                            // If sign in fails, display a message to the user.
                            Log.w(TAG, "signInWithCredential:failure", task.getException());
                            Toast.makeText(getApplicationContext(), R.string.app_name, Toast.LENGTH_LONG).show();
                        }
                    }
                });
    }

    private void firebaseAuthwithemail()
    {
        mAuth.signInWithEmailAndPassword(email, password)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {
                            // Sign in success, update UI with the signed-in user's information
                            Log.d("TAG", "signInWithEmail:success");
                            FirebaseUser user = mAuth.getCurrentUser();
                            updateUIOut(user);
                        } else {
                            // If sign in fails, display a message to the user.
                            Log.w("TAG", "signInWithEmail:failure", task.getException());
                            updateUIOut(null);
                        }

                    }
                });
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 214-239

}

    private void updateUIOut(FirebaseUser user){
        if (user != null) {
            final FirebaseFirestore db = FirebaseFirestore.getInstance();
            FirebaseFirestoreSettings settings = new FirebaseFirestoreSettings.Builder()
                    .setPersistenceEnabled(true)
                    .build();
            db.setFirestoreSettings(settings);

            db.collection("Users").whereEqualTo("username", user.getUid()).get().addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                @Override
                public void onComplete(@NonNull Task<QuerySnapshot> task) {
                    if (task.getResult().getDocuments().isEmpty()) {

                        SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("username", user.getUid());
                        editor.putString("char","none");
                        editor.apply();

                        ArrayList<String> types=new ArrayList<>();
                        ArrayList<Event_Details> events=new ArrayList<>();
                        Map<String, Object> data = new HashMap<>();
                        email = email.trim();
                        int index = email.indexOf("@");
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 240-262

String name = email.substring(0,index);
                        data.put("email", email);
                        data.put("name", name);
                        data.put("char","none");
                        data.put("username", user.getUid());
                        data.put("score", 0.0);
                        data.put("slot_time", FieldValue.serverTimestamp());
                        StringTokenizer stringTokenizer = new StringTokenizer(user.getEmail(), "@");
                        String qrcode = stringTokenizer.nextToken();
                        data.put("qr_code", qrcode);
                        data.put("Types",types);
                        data.put("Attended",events);

                        db.collection("Users").document(user.getUid()).set(data, SetOptions.merge()).addOnCompleteListener(new OnCompleteListener<Void>() {
                            @Override
                            public void onComplete(@NonNull Task<Void> task) {
                                if (task.isSuccessful()) {
                                    Intent i = new Intent(LoginActivity.this,  CharSelect.class);
                                    i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                                    startActivity(i);

                                    finish();
                                } else {
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 263-292

Toast.makeText(LoginActivity.this, "Connection error!", Toast.LENGTH_SHORT).show();
                                }
                            }
                        });
                    }
                    else{
                        email = email.trim();
                        int index = email.indexOf("@");
                        String name = email.substring(0,index);
                        SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("username", name);
                        editor.putString("char","none");
                        editor.apply();
                        Intent i = new Intent(LoginActivity.this, CharSelect.class);
                        i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        startActivity(i);
                        finish();
                    }
                }
            });
        }
    }

    private void updateUI(FirebaseUser user) {
        if (user != null) {
            final FirebaseFirestore db = FirebaseFirestore.getInstance();
            FirebaseFirestoreSettings settings = new FirebaseFirestoreSettings.Builder()
                    .setPersistenceEnabled(true)
                    .build();
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 293-315

db.setFirestoreSettings(settings);

            db.collection("Users").whereEqualTo("username", user.getUid()).get().addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                @Override
                public void onComplete(@NonNull Task<QuerySnapshot> task) {
                    if (task.getResult().getDocuments().isEmpty()) {

                        SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("username", user.getUid());
                        editor.apply();

                        ArrayList<String> types=new ArrayList<>();
                        ArrayList<Event_Details> events=new ArrayList<>();
                        Map<String, Object> data = new HashMap<>();
                        data.put("email", user.getEmail());
                        data.put("name", user.getDisplayName());
                        data.put("char","none");
                        data.put("username", user.getUid());
                        data.put("score", 0.0);
                        data.put("slot_time", FieldValue.serverTimestamp());
                        StringTokenizer stringTokenizer = new StringTokenizer(user.getEmail(), "@");
                        String qrcode = stringTokenizer.nextToken();
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 316-342

data.put("qr_code", qrcode);
                        data.put("Types",types);
                        data.put("Attended",events);

                        db.collection("Users").document(user.getUid()).set(data, SetOptions.merge()).addOnCompleteListener(new OnCompleteListener<Void>() {
                            @Override
                            public void onComplete(@NonNull Task<Void> task) {
                                if (task.isSuccessful()) {
                                    Intent i = new Intent(LoginActivity.this,  CharSelect.class);
                                    i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                                    i.putExtra("user_id",user.getUid());
                                    startActivity(i);

                                    finish();
                                } else {

                                    Toast.makeText(LoginActivity.this, "Connection error!", Toast.LENGTH_SHORT).show();

                                }
                            }
                        });
                    }
                    else{

                        SharedPreferences sharedPref=getSharedPreferences("userinfo",MODE_PRIVATE);
                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("username", user.getUid());
PATH: app/src/main/java/com/bitspilani/apogeear/LoginActivity.java
LINES: 343-354

editor.putString("char","none");
                        editor.apply();
                        Intent i = new Intent(LoginActivity.this, CharSelect.class);
                        i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        startActivity(i);
                        finish();
                    }
                }
            });
        }
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/MainActivity.java
LINES: 1-45

package com.bitspilani.apogeear;

import androidx.appcompat.app.AppCompatActivity;
import androidx.coordinatorlayout.widget.CoordinatorLayout;
import androidx.viewpager.widget.ViewPager;

import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;
import android.content.IntentFilter;
import android.graphics.Color;
import android.net.ConnectivityManager;
import android.net.NetworkInfo;
import android.os.Bundle;

import android.telephony.TelephonyManager;
import android.view.View;
import android.widget.RelativeLayout;

import com.bitspilani.apogeear.Adapters.ViewPagerAdapter;
import com.gauravk.bubblenavigation.BubbleNavigationLinearView;
import com.gauravk.bubblenavigation.listener.BubbleNavigationChangeListener;
import com.google.android.material.snackbar.Snackbar;


public class MainActivity extends AppCompatActivity {

    ViewPager viewPager;
    Snackbar snackbar;
    ViewPagerAdapter viewPagerAdapter;
    CoordinatorLayout coordinatorLayout;

    public static int TYPE_SLOW = -1;
    public static int TYPE_WIFI = 1;
    public static int TYPE_MOBILE = 2;
    public static int TYPE_NOT_CONNECTED = 0;
    BubbleNavigationLinearView bubbleNavigationLinearView;


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
PATH: app/src/main/java/com/bitspilani/apogeear/MainActivity.java
LINES: 46-77

registerInternetCheckReceiver();
        bubbleNavigationLinearView = findViewById(R.id.bottom_nav);
        viewPager = findViewById(R.id.viewpager);
        coordinatorLayout=findViewById(R.id.myCoordinatorLayout);
        //  bottomNavigationView.setSelectedItemId(R.id.home);
        viewPagerAdapter = new ViewPagerAdapter(getSupportFragmentManager());
        viewPager.setAdapter(viewPagerAdapter);
        viewPager.setCurrentItem(2);

        viewPager.setOffscreenPageLimit(2);
        //viewPager.setPageTransformer(true,new FadeOutTransformation());

        bubbleNavigationLinearView.setCurrentActiveItem(2);
        bubbleNavigationLinearView.setNavigationChangeListener(new BubbleNavigationChangeListener() {
            @Override
            public void onNavigationChanged(View view, int position) {
                viewPager.setCurrentItem(position,true);
            }
        });

        viewPager.addOnPageChangeListener(new ViewPager.OnPageChangeListener() {
            @Override
            public void onPageScrolled(int position, float positionOffset, int positionOffsetPixels) {
            }
            @Override
            public void onPageSelected(int position) {
                bubbleNavigationLinearView.setCurrentActiveItem(position);
            }
            @Override
            public void onPageScrollStateChanged(int state) {

            }
PATH: app/src/main/java/com/bitspilani/apogeear/MainActivity.java
LINES: 78-110

});


//        bottomNavigationView.setOnNavigationItemSelectedListener(new BottomNavigationView.OnNavigationItemSelectedListener() {
//            @Override
//            public boolean onNavigationItemSelected(@NonNull MenuItem item) {
//                switch (item.getItemId()){
//                    case R.id.home :
//                        viewPager.setCurrentItem(2);
//                        break;
//                    case R.id.map:
//                        viewPager.setCurrentItem(1);
//                        break;
//                    case R.id.profile:
//                        viewPager.setCurrentItem(0);
//                        break;
//                    case R.id.more:
//                        viewPager.setCurrentItem(4);
//                        break;
//                    case R.id.leaderboard:
//                        viewPager.setCurrentItem(3);
//                        break;
//                }
//                return false;
//            }});
//
//        viewPager.addOnPageChangeListener(new ViewPager.OnPageChangeListener() {
//            @Override
//            public void onPageScrolled(int position, float positionOffset, int positionOffsetPixels) {
//            }
//            @Override
//            public void onPageSelected(int position) {
//                bottomNavigationView.getMenu().getItem(position).setChecked(true);
PATH: app/src/main/java/com/bitspilani/apogeear/MainActivity.java
LINES: 111-159

//            }
//            @Override
//            public void onPageScrollStateChanged(int state) {
//
//            }
//        });
//    }
//
//    private boolean loadFragment(Fragment fragment) {
//        //switching fragment
//        if (fragment != null) {
//            getSupportFragmentManager()
//                    .beginTransaction()
//                    .replace(R.id.fragment_container, fragment)
//                    .commit();
//            return true;
//        }
//        return false;
//    }
    }

    @Override
    protected void onDestroy() {
        super.onDestroy();
        unregisterReceiver(broadcastReceiver);
    }

    @Override
    public void onBackPressed() {

        if(viewPager.getCurrentItem()!=2)
            viewPager.setCurrentItem(2);
        else
            super.onBackPressed();
    }

    /**
     *  Method to register runtime broadcast receiver to show snackbar alert for internet connection..
     */
    private void registerInternetCheckReceiver() {
        IntentFilter internetFilter = new IntentFilter();
        internetFilter.addAction("android.net.wifi.STATE_CHANGE");
        internetFilter.addAction("android.net.conn.CONNECTIVITY_CHANGE");
        registerReceiver(broadcastReceiver, internetFilter);
    }

    /**
     *  Runtime Broadcast receiver inner class to capture internet connectivity events
     */
PATH: app/src/main/java/com/bitspilani/apogeear/MainActivity.java
LINES: 160-194

public BroadcastReceiver broadcastReceiver = new BroadcastReceiver() {
        @Override
        public void onReceive(Context context, Intent intent) {
            String status = getConnectivityStatusString(context);
            setSnackbarMessage(status,false);
        }
    };

    public static int getConnectivityStatus(Context context) {
        ConnectivityManager cm = (ConnectivityManager) context
                .getSystemService(Context.CONNECTIVITY_SERVICE);

        NetworkInfo activeNetwork = cm.getActiveNetworkInfo();
        if (null != activeNetwork) {
            if(activeNetwork.getType() == TYPE_WIFI) {
                if(activeNetwork.getSubtype()== TelephonyManager.NETWORK_TYPE_CDMA)
                    return TYPE_SLOW;
                return TYPE_WIFI;
            }

            if(activeNetwork.getType() == ConnectivityManager.TYPE_MOBILE) {
                if(activeNetwork.getSubtype()== TelephonyManager.NETWORK_TYPE_CDMA)
                    return TYPE_SLOW;
                return TYPE_MOBILE;
            }
        }
        return TYPE_NOT_CONNECTED;
    }

    public static String getConnectivityStatusString(Context context) {
        int conn = getConnectivityStatus(context);
        String status = null;
        if (conn == TYPE_WIFI) {
            status = "Wifi enabled";
        } else if (conn == TYPE_MOBILE) {
PATH: app/src/main/java/com/bitspilani/apogeear/MainActivity.java
LINES: 195-221

status = "Mobile data enabled";
        } else if (conn == TYPE_NOT_CONNECTED) {
            status = "Not connected to Internet";
        }else if (conn==TYPE_SLOW){
            status = "Poorly connected to Internet";
        }
        return status;
    }
    private void setSnackbarMessage(String status,boolean showBar) {
        String internetStatus="";
        if(status.equalsIgnoreCase("Wifi enabled")||status.equalsIgnoreCase("Mobile data enabled")){
            internetStatus="Online";
        }else if (status.equalsIgnoreCase("Poorly connected to Internet")){
            internetStatus="Poor Connection";
        }
        else {
            internetStatus="Offline";
        }
        snackbar = Snackbar
                .make(coordinatorLayout, internetStatus, Snackbar.LENGTH_SHORT);
        if(internetStatus.equals("Online"))
            snackbar.setBackgroundTint(Color.parseColor("#4ECE60"));

        snackbar.show();
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Models/CharacterModel.java
LINES: 1-27

package com.bitspilani.apogeear.Models;

public class CharacterModel {
    private String name;
    private int image;

    public CharacterModel(String name, int image) {
        this.name = name;
        this.image = image;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getImage() {
        return image;
    }

    public void setImage(int image) {
        this.image = image;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Models/Developers.java
LINES: 1-37

package com.bitspilani.apogeear.Models;

public class Developers {
    String name,desc;
    int img;

    public Developers(String name, String desc, int img) {
        this.name = name;
        this.desc = desc;
        this.img = img;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public String getDesc() {
        return desc;
    }

    public void setDesc(String desc) {
        this.desc = desc;
    }

    public int getImg() {
        return img;
    }

    public void setImg(int img) {
        this.img = img;
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Models/Event_Details.java
LINES: 1-26

package com.bitspilani.apogeear.Models;

import com.google.firebase.Timestamp;

public class Event_Details {
    private String Name,Type;
    private Timestamp Time;

    public Event_Details(String name, String type, Timestamp time) {
        Name = name;
        Type = type;
        Time = time;
    }

    public String getName() {
        return Name;
    }

    public String getType() {
        return Type;
    }

    public Timestamp getTime() {
        return Time;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Models/MoreModel.java
LINES: 1-43

package com.bitspilani.apogeear.Models;

public class MoreModel {
    private String name;
    private String subName;
    private int image;

    public MoreModel(String name, int image, String subName) {
        this.name = name;
        this.image = image;
        this.subName = subName;
    }

    public MoreModel(String name, int image) {
        this.name = name;
        this.image = image;
        this.subName = "";
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public int getImage() {
        return image;
    }

    public void setImage(int image) {
        this.image = image;
    }

    public String getSubName() {
        return subName;
    }

    public void setSubName(String subName) {
        this.subName = subName;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Models/MoreNestedModel.java
LINES: 1-29

package com.bitspilani.apogeear.Models;

import java.util.List;

public class MoreNestedModel {
    private String name;
    private List<MoreModel> moreModelList;

    public MoreNestedModel(String name, List<MoreModel> moreModelList) {
        this.name = name;
        this.moreModelList = moreModelList;
    }

    public String getName() {
        return name;
    }

    public void setName(String name) {
        this.name = name;
    }

    public List<MoreModel> getMoreModelList() {
        return moreModelList;
    }

    public void setMoreModelList(List<MoreModel> moreModelList) {
        this.moreModelList = moreModelList;
    }
}
PATH: app/src/main/java/com/bitspilani/apogeear/Models/Rank.java
LINES: 1-36

package com.bitspilani.apogeear.Models;

public class Rank {

    private String username,userId,charName;
    private double coins;
    private int rank;

    public Rank(String username, double coins,String userId,String charName) {
        this.username = username;
        this.coins = coins;
        this.userId=userId;
        this.charName=charName;
    }

    public String getUsername() {
        return username;
    }

    public double getCoins() {
        return coins;
    }

    public String getUserId() {
        return userId;
    }

    public String getCharName() {
        return charName;
    }

    public void setCharName(String charName) {
        this.charName = charName;
    }

}
PATH: app/src/main/java/com/bitspilani/apogeear/Services/MyFirebaseMessagingService.java
LINES: 1-29

package com.bitspilani.apogeear.Services;

import android.app.NotificationChannel;
import android.app.NotificationManager;
import android.app.PendingIntent;
import android.content.Intent;
import android.os.Build;
import android.util.Log;

import androidx.annotation.NonNull;
import androidx.core.app.NotificationCompat;

import com.bitspilani.apogeear.MainActivity;
import com.bitspilani.apogeear.R;
import com.google.firebase.messaging.FirebaseMessagingService;
import com.google.firebase.messaging.RemoteMessage;

public class MyFirebaseMessagingService extends FirebaseMessagingService {
    @Override
    public void onMessageReceived(@NonNull RemoteMessage remoteMessage) {
        super.onMessageReceived(remoteMessage);
        Log.d("msg", "onMessageReceived: " + remoteMessage.getData().get("message"));
        Intent intent = new Intent(this, getClass());
        intent.addFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
        PendingIntent pendingIntent = PendingIntent.getActivity(this, 0, intent, PendingIntent.FLAG_ONE_SHOT);
        String channelId = "Default";
        NotificationCompat.Builder builder = new  NotificationCompat.Builder(this, channelId)
                .setSmallIcon(R.mipmap.ic_launcher)
                .setContentTitle(remoteMessage.getNotification().getTitle())
PATH: app/src/main/java/com/bitspilani/apogeear/Services/MyFirebaseMessagingService.java
LINES: 30-38

.setContentText(remoteMessage.getNotification().getBody()).setAutoCancel(true).setContentIntent(pendingIntent);;
        NotificationManager manager = (NotificationManager) getSystemService(NOTIFICATION_SERVICE);
        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.O) {
            NotificationChannel channel = new NotificationChannel(channelId, "Default channel", NotificationManager.IMPORTANCE_DEFAULT);
            manager.createNotificationChannel(channel);
        }
        manager.notify(0, builder.build());
    }
}
PATH: app/src/test/java/com/bitspilani/apogeear/ExampleUnitTest.java
LINES: 1-17

package com.bitspilani.apogeear;

import org.junit.Test;

import static org.junit.Assert.*;

/**
 * Example local unit test, which will execute on the development machine (host).
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
public class ExampleUnitTest {
    @Test
    public void addition_isCorrect() {
        assertEquals(4, 2 + 2);
    }
}
# BOSM2019
BOSM Roulette: A betting app

Do you feel your adrenaline gushing? Can you hear the distant triumph of victory and the groans of defeat? Are you ready to join them in a battlefield like no other, where the rules are defined by the flip of a coin and the roll of a dice.

Coding Club brings to you, BOSM Roulette, the only betting app for BOSM 2019, redesigned and developed to bring the game to you on a new level. Experience the rush of BOSM, like you have never before.

1. Begin your journey this BOSM with 1000 BITSCoins and place your bets on matches and the sports you love.
2. Support your team and get rewarded for their victory.
3. Place bets on multiple matches and earn a bonus out of your opponent's wallet every time your team wins.
4. Track live scores on selected matches and observe patterns of betting on the currently popular and trending messages.
5. Spin the *Roulette Wheel* to win bonuses and unlock new features every two hours.
6. Accumulate your profits and fight your way to the top of the leaderboard to earn real and exciting prizes.

## Notes:-
Built upon **Firebase**
* **Google Sign In**
* **Firebase User Authentication**
* **Firebase Firetore**: Store list of matches, users and bets placed by each user
* **Firebase Cloud Messaging**: Send notifications
PATH: app/src/androidTest/java/com/bitspilani/bosmroulette/ExampleInstrumentedTest.java
LINES: 1-27

package com.bitspilani.bosmroulette;

import android.content.Context;

import androidx.test.InstrumentationRegistry;
import androidx.test.runner.AndroidJUnit4;

import org.junit.Test;
import org.junit.runner.RunWith;

import static org.junit.Assert.*;

/**
 * Instrumented test, which will execute on an Android device.
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
@RunWith(AndroidJUnit4.class)
public class ExampleInstrumentedTest {
    @Test
    public void useAppContext() {
        // Context of the app under test.
        Context appContext = InstrumentationRegistry.getTargetContext();

        assertEquals("com.bitspilani.bosm2019", appContext.getPackageName());
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 1-42

package com.bitspilani.bosmroulette;

import android.app.Dialog;
import android.content.Context;
import android.graphics.Color;
import android.os.Bundle;
import android.view.View;
import android.view.Window;
import android.widget.Button;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;

import com.bitspilani.bosmroulette.models.FixtureModel;
import com.bitspilani.bosmroulette.models.PlaceBetModel;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FieldValue;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.firestore.SetOptions;

import java.util.HashMap;
import java.util.List;
import java.util.Map;

public class BetDialog extends Dialog implements View.OnClickListener {
    String userId;
    FixtureModel fixture;
    Context context;

    TextView team1, team2, amt50, amt100, amt150, amt200, amt;
    Button placebet;
    int betAmount = 0, flag = -1, amtflag;
    double walletamount;
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    private FirebaseAuth mAuth;
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 43-72

public BetDialog(@NonNull Context context, FixtureModel fixture, double walletamount) {
        super(context);
        this.context = context;
        this.fixture = fixture;
        this.walletamount = walletamount;
        mAuth = FirebaseAuth.getInstance();
        db.collection("users").whereEqualTo("email", mAuth.getCurrentUser().getEmail()).get()
                .addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                    @Override
                    public void onComplete(@NonNull Task<QuerySnapshot> task) {
                        if (task.isSuccessful()) {
                            List<DocumentSnapshot> documents = task.getResult().getDocuments();
                            for (DocumentSnapshot document : documents) {
                                userId = document.get("username").toString();
                            }
                        }
                    }
                });
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        requestWindowFeature(Window.FEATURE_NO_TITLE);
        setContentView(R.layout.bet_dialog);
        team1 = findViewById(R.id.team11);
        team2 = findViewById(R.id.team22);
        placebet = findViewById(R.id.place_bet);
        amt50 = findViewById(R.id.amt50);
        amt100 = findViewById(R.id.amt100);
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 73-106

amt150 = findViewById(R.id.amt150);
        amt200 = findViewById(R.id.amt200);
        amt = findViewById(R.id.amt);
        team1.setText(fixture.getCollege1());
        team2.setText(fixture.getCollege2());
        team1.setOnClickListener(this);
        team2.setOnClickListener(this);
        placebet.setOnClickListener(this);
        amt50.setOnClickListener(this);
        amt100.setOnClickListener(this);
        amt150.setOnClickListener(this);
        amt200.setOnClickListener(this);
        betAmount = 0;
    }

    @Override
    public void onClick(View v) {
        switch (v.getId()) {
            case R.id.team11:
                flag = 0;
                team1.setBackgroundResource(R.drawable.amtselectedborder);
                team2.setBackgroundResource(R.drawable.amtborderdark);
                team1.setTextColor(Color.parseColor("#ffffff"));
                team2.setTextColor(Color.parseColor("#ffffff"));
                break;
            case R.id.team22:
                flag = 1;
                team2.setBackgroundResource(R.drawable.amtselectedborder);
                team1.setBackgroundResource(R.drawable.amtborderdark);
                team2.setTextColor(Color.parseColor("#ffffff"));
                team1.setTextColor(Color.parseColor("#ffffff"));
                break;

            case R.id.place_bet:
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 107-132

if ((flag == 0 || flag == 1) && betAmount > 0&&walletamount - betAmount >= 0) {
                    placebet();
                    dismiss();
                } else if ((flag == 0 || flag == 1)&&walletamount - betAmount <= 0)
                    Toast.makeText(context, "Not enough balance!!", Toast.LENGTH_SHORT).show();
                else if (betAmount == 0)
                    Toast.makeText(context, "Select amount", Toast.LENGTH_SHORT).show();
                else
                    Toast.makeText(context, "Select a team", Toast.LENGTH_SHORT).show();
                break;
            case R.id.amt50:
                amtflag = 1;
                betAmount += 50;
                amt.setText(String.valueOf(betAmount));
                break;

            case R.id.amt100:
                betAmount += 100;
                /*amt100.setBackgroundResource(R.drawable.amtselectedborder);
                amt100.setTextColor(Color.parseColor("#ffffff"));
                amt50.setBackgroundResource(R.drawable.amtborder);
                amt50.setTextColor(Color.parseColor("#000000"));
                amt150.setBackgroundResource(R.drawable.amtborder);
                amt150.setTextColor(Color.parseColor("#000000"));
                amt200.setBackgroundResource(R.drawable.amtborder);
                amt200.setTextColor(Color.parseColor("#000000"));*/
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 133-173

amt.setText(String.valueOf(betAmount));
                amtflag = 1;
                break;

            case R.id.amt150:
                betAmount += 200;
                amt.setText(String.valueOf(betAmount));
                amtflag = 1;
                break;

            case R.id.amt200:
                betAmount += 500;
                amt.setText(String.valueOf(betAmount));
                amtflag = 1;
                break;

            default:
                break;
        }

    }

    void placebet() {

        Map<String, Object> bet = new HashMap<>();
        PlaceBetModel ob;
        if(flag == 0) {
            ob = new PlaceBetModel(betAmount, userId, team1.getText().toString());
        }
        else {
            ob = new PlaceBetModel(betAmount, userId, team2.getText().toString());
        }

        db.collection("matches").document(fixture.getMatchId().toString()).update(
                "roulette", FieldValue.arrayUnion(ob)
        );
        db.collection("matches").document(fixture.getMatchId()).get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
            @Override
            public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                DocumentSnapshot documentSnapshot = task.getResult();
                if (flag == 0) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 174-200

int total = Integer.parseInt(documentSnapshot.get("total").toString());
                    int match1 = Integer.parseInt(documentSnapshot.get("team1").toString());
                    match1++;
                    total++;
                    HashMap<String, Object> hashMap = new HashMap<>();
                    hashMap.put("team1", match1);
                    hashMap.put("total", total);
                    db.collection("matches").document(fixture.getMatchId()).set(hashMap, SetOptions.merge());
                }
                if (flag == 1) {
                    int total = Integer.parseInt(documentSnapshot.get("total").toString());
                    int match2 = Integer.parseInt(documentSnapshot.get("team2").toString());
                    match2++;
                    total++;
                    HashMap<String, Object> hashMap = new HashMap<>();
                    hashMap.put("team2", match2);
                    hashMap.put("total", total);
                    db.collection("matches").document(fixture.getMatchId()).set(hashMap, SetOptions.merge());
                }
            }
        });
        Toast.makeText(context, "Bet Placed", Toast.LENGTH_SHORT).show();

        Map<String, Object> userBet = new HashMap<>();

        userBet.put("betAmount", betAmount);
        userBet.put("match_id", fixture.getMatchId());
PATH: app/src/main/java/com/bitspilani/bosmroulette/BetDialog.java
LINES: 201-227

userBet.put("team1", fixture.getCollege1());
        userBet.put("team2", fixture.getCollege2());
        userBet.put("bettedOn", flag);
        userBet.put("game", fixture.getGame());
        userBet.put("score1", -1);
        userBet.put("score2", -1);
        userBet.put("update", false);
        userBet.put("result", -1);

        db.collection("users").document(userId).collection("bets").document(fixture.getMatchId().toString()).set(userBet);

        db.collection("users").document(userId).get().addOnSuccessListener(
                new OnSuccessListener<DocumentSnapshot>() {
                    @Override
                    public void onSuccess(DocumentSnapshot documentSnapshot) {
                        double wallet = Double.parseDouble(documentSnapshot.get("wallet").toString());
                        wallet = wallet - betAmount;
                        Map<String, Double> walletMap = new HashMap<>();
                        walletMap.put("wallet", wallet);

                        db.collection("users").document(userId)
                                .set(walletMap, SetOptions.merge());
                    }
                }
        );
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/LinePageIndicatorDecoration.java
LINES: 1-47

package com.bitspilani.bosmroulette;

import android.content.res.Resources;
import android.graphics.Canvas;
import android.graphics.Paint;
import android.graphics.Rect;
import android.view.View;
import android.view.animation.AccelerateDecelerateInterpolator;
import android.view.animation.Interpolator;

import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

public class LinePageIndicatorDecoration extends RecyclerView.ItemDecoration {

    private int colorActive = 0xFF1B5C44;
    private int colorInactive = 0xFFFFFFFF;

    private static final float DP = Resources.getSystem().getDisplayMetrics().density;

    /**
     * Height of the space the indicator takes up at the bottom of the view.
     */
    private final int mIndicatorHeight = (int) (DP * 16);

    /**
     * Indicator stroke width.
     */
    private final float mIndicatorStrokeWidth = DP * 2;

    /**
     * Indicator width.
     */
    private final float mIndicatorItemLength = DP * 16;
    /**
     * Padding between indicators.
     */
    private final float mIndicatorItemPadding = DP * 4;

    /**
     * Some more natural animation interpolation
     */
    private final Interpolator mInterpolator = new AccelerateDecelerateInterpolator();

    private final Paint mPaint = new Paint();

    public LinePageIndicatorDecoration() {
PATH: app/src/main/java/com/bitspilani/bosmroulette/LinePageIndicatorDecoration.java
LINES: 48-79

mPaint.setStrokeCap(Paint.Cap.ROUND);
        mPaint.setStrokeWidth(mIndicatorStrokeWidth);
        mPaint.setStyle(Paint.Style.STROKE);
        mPaint.setAntiAlias(true);
    }

    @Override
    public void onDrawOver(Canvas c, RecyclerView parent, RecyclerView.State state) {
        super.onDrawOver(c, parent, state);

        int itemCount = parent.getAdapter().getItemCount();

        // center horizontally, calculate width and subtract half from center
        float totalLength = mIndicatorItemLength * itemCount;
        float paddingBetweenItems = Math.max(0, itemCount - 1) * mIndicatorItemPadding;
        float indicatorTotalWidth = totalLength + paddingBetweenItems;
        float indicatorStartX = (parent.getWidth() - indicatorTotalWidth) / 2F;

        // center vertically in the allotted space
        float indicatorPosY = parent.getHeight() - mIndicatorHeight / 2F;

        drawInactiveIndicators(c, indicatorStartX, indicatorPosY, itemCount);


        // find active page (which should be highlighted)
        LinearLayoutManager layoutManager = (LinearLayoutManager) parent.getLayoutManager();
        int activePosition = layoutManager.findFirstVisibleItemPosition();
        if (activePosition == RecyclerView.NO_POSITION) {
            return;
        }

        // find offset of active page (if the user is scrolling)
PATH: app/src/main/java/com/bitspilani/bosmroulette/LinePageIndicatorDecoration.java
LINES: 80-111

final View activeChild = layoutManager.findViewByPosition(activePosition);
        int left = activeChild.getLeft();
        int width = activeChild.getWidth();

        // on swipe the active item will be positioned from [-width, 0]
        // interpolate offset for smooth animation
        float progress = mInterpolator.getInterpolation(left * -1 / (float) width);

        drawHighlights(c, indicatorStartX, indicatorPosY, activePosition, progress, itemCount);
    }

    private void drawInactiveIndicators(Canvas c, float indicatorStartX, float indicatorPosY, int itemCount) {
        mPaint.setColor(colorInactive);

        // width of item indicator including padding
        final float itemWidth = mIndicatorItemLength + mIndicatorItemPadding;

        float start = indicatorStartX;
        for (int i = 0; i < itemCount; i++) {
            // draw the line for every item
            c.drawLine(start, indicatorPosY, start + mIndicatorItemLength, indicatorPosY, mPaint);
            start += itemWidth;
        }
    }

    private void drawHighlights(Canvas c, float indicatorStartX, float indicatorPosY,
                                int highlightPosition, float progress, int itemCount) {
        mPaint.setColor(colorActive);

        // width of item indicator including padding
        final float itemWidth = mIndicatorItemLength + mIndicatorItemPadding;
PATH: app/src/main/java/com/bitspilani/bosmroulette/LinePageIndicatorDecoration.java
LINES: 112-140

if (progress == 0F) {
            // no swipe, draw a normal indicator
            float highlightStart = indicatorStartX + itemWidth * highlightPosition;
            c.drawLine(highlightStart, indicatorPosY,
                    highlightStart + mIndicatorItemLength, indicatorPosY, mPaint);
        } else {
            float highlightStart = indicatorStartX + itemWidth * highlightPosition;
            // calculate partial highlight
            float partialLength = mIndicatorItemLength * progress;

            // draw the cut off highlight
            c.drawLine(highlightStart + partialLength, indicatorPosY,
                    highlightStart + mIndicatorItemLength, indicatorPosY, mPaint);

            // draw the highlight overlapping to the next item as well
            if (highlightPosition < itemCount - 1) {
                highlightStart += itemWidth;
                c.drawLine(highlightStart, indicatorPosY,
                        highlightStart + partialLength, indicatorPosY, mPaint);
            }
        }
    }

    @Override
    public void getItemOffsets(Rect outRect, View view, RecyclerView parent, RecyclerView.State state) {
        super.getItemOffsets(outRect, view, parent, state);
        outRect.bottom = mIndicatorHeight;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/SwipeGestureDetector.java
LINES: 1-42

package com.bitspilani.bosmroulette;

import android.content.Context;
import android.view.GestureDetector;
import android.view.MotionEvent;
import android.view.View;

public class SwipeGestureDetector {
    class SwipeGestureListener extends GestureDetector.SimpleOnGestureListener implements
            View.OnTouchListener {
        Context context;
        GestureDetector gDetector;
        static final int SWIPE_MIN_DISTANCE = 120;
        static final int SWIPE_MAX_OFF_PATH = 250;
        static final int SWIPE_THRESHOLD_VELOCITY = 200;

        public SwipeGestureListener() {
            super();
        }

        public SwipeGestureListener(Context context) {
            this(context, null);
        }

        public SwipeGestureListener(Context context, GestureDetector gDetector) {

            if (gDetector == null)
                gDetector = new GestureDetector(context, this);

            this.context = context;
            this.gDetector = gDetector;
        }

        @Override
        public boolean onFling(MotionEvent e1, MotionEvent e2, float velocityX,
                               float velocityY) {

//            final int position = lvCountry.pointToPosition(
//                    Math.round(e1.getX()), Math.round(e1.getY()));
//
//            String countryName = (String) lvCountry.getItemAtPosition(position);
//
PATH: app/src/main/java/com/bitspilani/bosmroulette/SwipeGestureDetector.java
LINES: 43-69

//            if (Math.abs(e1.getY() - e2.getY()) > SWIPE_MAX_OFF_PATH) {
//                if (Math.abs(e1.getX() - e2.getX()) > SWIPE_MAX_OFF_PATH
//                        || Math.abs(velocityY) < SWIPE_THRESHOLD_VELOCITY) {
//                    return false;
//                }
//                if (e1.getY() - e2.getY() > SWIPE_MIN_DISTANCE) {
//                    Toast.makeText(DemoSwipe.this, "bottomToTop" + countryName,
//                            Toast.LENGTH_SHORT).show();
//                } else if (e2.getY() - e1.getY() > SWIPE_MIN_DISTANCE) {
//                    Toast.makeText(DemoSwipe.this,
//                            "topToBottom  " + countryName, Toast.LENGTH_SHORT)
//                            .show();
//                }
//            } else {
//                if (Math.abs(velocityX) < SWIPE_THRESHOLD_VELOCITY) {
//                    return false;
//                }
//                if (e1.getX() - e2.getX() > SWIPE_MIN_DISTANCE) {
//                    Toast.makeText(DemoSwipe.this,
//                            "swipe RightToLeft " + countryName, 5000).show();
//                } else if (e2.getX() - e1.getX() > SWIPE_MIN_DISTANCE) {
//                    Toast.makeText(DemoSwipe.this,
//                            "swipe LeftToright  " + countryName, 5000).show();
//                }
//            }
PATH: app/src/main/java/com/bitspilani/bosmroulette/SwipeGestureDetector.java
LINES: 70-85

return super.onFling(e1, e2, velocityX, velocityY);

        }

        @Override
        public boolean onTouch(View v, MotionEvent event) {

            return gDetector.onTouchEvent(event);
        }

        public GestureDetector getDetector() {
            return gDetector;
        }

    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/DepthTransformation.java
LINES: 1-38

package com.bitspilani.bosmroulette.activity;

import android.view.View;

import androidx.viewpager.widget.ViewPager;

public class DepthTransformation implements ViewPager.PageTransformer{
    @Override
    public void transformPage(View page, float position) {

        if (position < -1){    // [-Infinity,-1)
            // This page is way off-screen to the left.
            page.setAlpha(0);

        }
        else if (position <= 0){    // [-1,0]
            page.setAlpha(1);
            page.setTranslationX(0);
            page.setScaleX(1);
            page.setScaleY(1);

        }
        else if (position <= 1){    // (0,1]
            page.setTranslationX(-position*page.getWidth());
            page.setAlpha(1-Math.abs(position));
            page.setScaleX(1-Math.abs(position));
            page.setScaleY(1-Math.abs(position));

        }
        else {    // (1,+Infinity]
            // This page is way off-screen to the right.
            page.setAlpha(0);

        }


    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Developers.java
LINES: 1-33

package com.bitspilani.bosmroulette.activity;

import androidx.appcompat.app.AppCompatActivity;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.os.Bundle;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.adapters.DevAdapter;
import com.bitspilani.bosmroulette.models.DevDesc;

import java.util.ArrayList;

public class Developers extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_developers);
        RecyclerView recyclerView=findViewById(R.id.recycler_view);
        ArrayList<DevDesc> list=new ArrayList<>();

        list.add(new DevDesc("Ayush Singh","App Developer","Ayush.jpg"));
        list.add(new DevDesc("Gauransh Sawhney","App Developer","Gauransh.jpg"));
        list.add(new DevDesc("Kalit Inani","App Developer","Kalit.jpg"));
        list.add(new DevDesc("Mukund Paliwal","UI/UX Designer","Mukund.png"));
        list.add(new DevDesc("Ayushi Jain","UI/UX Designer","AYUSHI.jpg"));
        list.add(new DevDesc("Sonal Prasad","UI/UX Designer","Sonal.jpg"));
        DevAdapter devAdapter=new DevAdapter(this,list);

        recyclerView.setLayoutManager(new LinearLayoutManager(this));
        recyclerView.setHasFixedSize(true);
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Developers.java
LINES: 34-40

recyclerView.setAdapter(devAdapter);




    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/HPC.java
LINES: 1-37

package com.bitspilani.bosmroulette.activity;

import androidx.appcompat.app.AppCompatActivity;

import android.os.Bundle;
import android.view.View;
import android.webkit.WebView;
import android.webkit.WebViewClient;
import android.widget.ProgressBar;

import com.bitspilani.bosmroulette.R;

public class HPC extends AppCompatActivity {

    ProgressBar progress;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_hpc);
        final WebView webview = (WebView) findViewById(R.id.webview);
        progress = (ProgressBar) findViewById(R.id.progressBar);
        progress.setVisibility(View.GONE);
        webview.getSettings().setJavaScriptEnabled(true);
        webview.setWebViewClient(new MyWebViewClient());

        webview.loadUrl("https://hindipressclub.wordpress.com");
    }

    private class MyWebViewClient extends WebViewClient {
        @Override
        public boolean shouldOverrideUrlLoading(WebView view, String url) {
            view.loadUrl(url);
            return true;
        }
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Instructions.java
LINES: 1-47

package com.bitspilani.bosmroulette.activity;

import android.content.Context;
import android.content.Intent;
import android.graphics.Color;
import android.os.Build;
import android.os.Bundle;
import android.text.Html;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.view.Window;
import android.view.WindowManager;
import android.widget.Button;
import android.widget.LinearLayout;
import android.widget.TextView;

import androidx.appcompat.app.AppCompatActivity;
import androidx.viewpager.widget.PagerAdapter;
import androidx.viewpager.widget.ViewPager;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.services.PrefManager;

public class Instructions extends AppCompatActivity {
    private ViewPager viewPager;
    private MyViewPagerAdapter myViewPagerAdapter;
    private LinearLayout dotsLayout;
    private TextView[] dots;
    private int[] layouts;
    private Button btnNext;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);





        // Making notification bar transparent
        if (Build.VERSION.SDK_INT >= 21) {
            getWindow().getDecorView().setSystemUiVisibility(View.SYSTEM_UI_FLAG_LAYOUT_STABLE | View.SYSTEM_UI_FLAG_LAYOUT_FULLSCREEN);
        }

        setContentView(R.layout.activity_welcome);
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Instructions.java
LINES: 48-86

viewPager = (ViewPager) findViewById(R.id.view_pager);

        DepthTransformation depthTransformation = new DepthTransformation();
        viewPager.setPageTransformer(true,depthTransformation);
        dotsLayout = (LinearLayout) findViewById(R.id.layoutDots);
        btnNext = (Button) findViewById(R.id.btn_next);


        // layouts of all welcome sliders
        // add few more layouts if you want
        layouts = new int[]{
                R.layout.fragment_slider1,
                R.layout.fragment_slider2,
                R.layout.fragment_slider3,
                R.layout.fragment_slider4};

        // adding bottom dots
        addBottomDots(0);

        // making notification bar transparent
        changeStatusBarColor();

        myViewPagerAdapter = new MyViewPagerAdapter();
        viewPager.setAdapter(myViewPagerAdapter);
        viewPager.addOnPageChangeListener(viewPagerPageChangeListener);




        btnNext.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                // checking for last page
                // if last page home screen will be launched
                int current = getItem(+1);
                if (current < layouts.length) {
                    // move to next screen
                    viewPager.setCurrentItem(current);
                } else {
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Instructions.java
LINES: 87-130

launchHomeScreen();
                }
            }
        });
    }


    private void addBottomDots(int currentPage) {
        dots = new TextView[layouts.length];

        int[] colorsActive = getResources().getIntArray(R.array.array_dot_active);
        int[] colorsInactive = getResources().getIntArray(R.array.array_dot_inactive);

        dotsLayout.removeAllViews();
        for (int i = 0; i < dots.length; i++) {
            dots[i] = new TextView(this);
            dots[i].setText(Html.fromHtml("&#8226;"));
            dots[i].setTextSize(35);
            dots[i].setTextColor(colorsInactive[currentPage]);
            dotsLayout.addView(dots[i]);
        }

        if (dots.length > 0)
            dots[currentPage].setTextColor(colorsActive[currentPage]);
    }

    private int getItem(int i) {
        return viewPager.getCurrentItem() + i;
    }

    private void launchHomeScreen() {
        Intent intent=new Intent(Instructions.this, HomeActivity.class);
        intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
        startActivity(intent);
        finish();
    }

    //  viewpager change listener
    ViewPager.OnPageChangeListener viewPagerPageChangeListener = new ViewPager.OnPageChangeListener() {

        @Override
        public void onPageSelected(int position) {
            addBottomDots(position);
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Instructions.java
LINES: 131-176

// changing the next button text 'NEXT' / 'GOT IT'
            if (position == layouts.length - 1) {
                // last page. make button text to GOT IT
                btnNext.setText("GOT IT");
            } else {
                // still pages are left
                btnNext.setText("NEXT");
            }
        }

        @Override
        public void onPageScrolled(int arg0, float arg1, int arg2) {

        }

        @Override
        public void onPageScrollStateChanged(int arg0) {

        }
    };

    /**
     * Making notification bar transparent
     */
    private void changeStatusBarColor() {
        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {
            Window window = getWindow();
            window.addFlags(WindowManager.LayoutParams.FLAG_DRAWS_SYSTEM_BAR_BACKGROUNDS);
            window.setStatusBarColor(Color.TRANSPARENT);
        }
    }

    /**
     * View pager adapter
     */
    public class MyViewPagerAdapter extends PagerAdapter {
        private LayoutInflater layoutInflater;

        public MyViewPagerAdapter() {
        }

        @Override
        public Object instantiateItem(ViewGroup container, int position) {
            layoutInflater = (LayoutInflater) getSystemService(Context.LAYOUT_INFLATER_SERVICE);

            View view = layoutInflater.inflate(layouts[position], container, false);
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/Instructions.java
LINES: 177-201

container.addView(view);

            return view;
        }

        @Override
        public int getCount() {
            return layouts.length;
        }

        @Override
        public boolean isViewFromObject(View view, Object obj) {
            return view == obj;
        }


        @Override
        public void destroyItem(ViewGroup container, int position, Object object) {
            View view = (View) object;
            container.removeView(view);
        }
    }


}
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 1-33

package com.bitspilani.bosmroulette.activity;

import android.app.ProgressDialog;
import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.models.UserBetModel;
import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInAccount;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.common.api.ApiException;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.AuthCredential;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.auth.GoogleAuthProvider;
import com.google.firebase.firestore.FieldValue;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreSettings;
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 34-70

import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.firestore.SetOptions;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;
import java.util.StringTokenizer;

public class LoginActivity extends AppCompatActivity {
    private static final String TAG = "SignInActivity";
    private static final int RC_SIGN_IN = 9001;
    private FirebaseAuth mAuth;
    private GoogleSignInClient mGoogleSignInClient;
    private TextView mStatusTextView;
    private ProgressDialog mProgressDialog;
    TextView gmail;
    Button button;
    SharedPreferences sharedPref;
    public static ArrayList<UserBetModel> userBetsList;


    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_login);
        userBetsList = new ArrayList<>();
        sharedPref = getSharedPreferences("userInfo", Context.MODE_PRIVATE);

        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestEmail()
                .requestIdToken(getString(R.string.default_web_client_id))
                .build();
        mGoogleSignInClient = GoogleSignIn.getClient(this, gso);
        button = findViewById(R.id.googlelogin);

        mAuth = FirebaseAuth.getInstance();
        FirebaseUser user = mAuth.getCurrentUser();
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 71-106

if (user != null) {
            SharedPreferences.Editor editor = sharedPref.edit();
            editor.putString("username", user.getUid());
            editor.putString("name", user.getDisplayName());
            editor.apply();
            startActivity(new Intent(LoginActivity.this, HomeActivity.class));
            finish();
        }

        button.setOnClickListener(
                new View.OnClickListener() {
                    @Override
                    public void onClick(View v) {
                        signIn();
                    }
                });
    }

    private void signIn() {
        Intent signInIntent = mGoogleSignInClient.getSignInIntent();
        startActivityForResult(signInIntent, RC_SIGN_IN);
    }

    @Override
    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        // Result returned from launching the Intent from GoogleSignInApi.getSignInIntent(...);
        if (requestCode == RC_SIGN_IN) {
            Task<GoogleSignInAccount> task = GoogleSignIn.getSignedInAccountFromIntent(data);
            try {
                // Google Sign In was successful, authenticate with Firebase
                GoogleSignInAccount account = task.getResult(ApiException.class);
                firebaseAuthWithGoogle(account);
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 107-136

} catch (ApiException e) {
                // Google Sign In failed, update UI appropriately
                Log.w(TAG, "Google sign in failed", e);
                // ...
            }
        }
    }

    private void firebaseAuthWithGoogle(GoogleSignInAccount acct) {
        Log.d(TAG, "firebaseAuthWithGoogle:" + acct.getId());

        AuthCredential credential = GoogleAuthProvider.getCredential(acct.getIdToken(), null);
        mAuth.signInWithCredential(credential)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {

                            // Sign in success, update UI with the signed-in user's information
                            Log.d(TAG, "signInWithCredential:success");
                            FirebaseUser user = mAuth.getCurrentUser();
                            updateUI(user);
                            //             startActivity(new Intent(LoginActivity.this, HomeActivity.class));
                            //           finish();

                        } else {

                            // If sign in fails, display a message to the user.
                            Log.w(TAG, "signInWithCredential:failure", task.getException());
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 137-162

Toast.makeText(getApplicationContext(), R.string.app_name, Toast.LENGTH_LONG).show();
                        }
                    }
                });
    }

    private void updateUI(FirebaseUser user) {
        if (user != null) {
            final FirebaseFirestore db = FirebaseFirestore.getInstance();
            FirebaseFirestoreSettings settings = new FirebaseFirestoreSettings.Builder()
                    .setPersistenceEnabled(true)
                    .build();
            db.setFirestoreSettings(settings);

            db.collection("users").whereEqualTo("email", user.getEmail()).get().addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                @Override
                public void onComplete(@NonNull Task<QuerySnapshot> task) {
                    if (task.getResult().getDocuments().isEmpty()) {
                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("username", user.getUid());
                        editor.apply();
                        Map<String, Object> data = new HashMap<>();
                        data.put("email", user.getEmail());
                        data.put("name", user.getDisplayName());
                        data.put("username", user.getUid());
                        data.put("wallet", 1000.0);
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 163-186

data.put("score", 0.0);
                        data.put("bonusTime", "");
                        data.put("lossTime", "");
                        data.put("bonus", false);
                        data.put("loss", false);
                        data.put("slot_time", FieldValue.serverTimestamp());
                        StringTokenizer stringTokenizer = new StringTokenizer(user.getEmail(), "@");
                        String qrcode = stringTokenizer.nextToken();
                        data.put("qr_code", qrcode);

                        Log.d("test2", user.getUid().toString());
                        db.collection("users").document(user.getUid()).set(data, SetOptions.merge()).addOnCompleteListener(new OnCompleteListener<Void>() {
                            @Override
                            public void onComplete(@NonNull Task<Void> task) {
                                if (task.isSuccessful()) {
                                    Intent i = new Intent(LoginActivity.this,  WelcomeActivity.class);
                                    i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                                    startActivity(i);

                                    finish();
                                } else {

                                    Toast.makeText(LoginActivity.this, "Connection error!", Toast.LENGTH_SHORT).show();
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 187-217

}
                            }
                        });
                    }

                    else{
                        Intent i = new Intent(LoginActivity.this, WelcomeActivity.class);
                        i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        startActivity(i);
                        finish();


                    }
                }
            });}}}

//            db.collection("users").whereEqualTo("email", user.getEmail())
//                    .addSnapshotListener(new EventListener<QuerySnapshot>() {
//                        @Override
//                        public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
//                            if (queryDocumentSnapshots.getDocuments() == null) {
//
//
////                                db.collection("users").document(user.getUid()).get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
////                                    @Override
////                                    public void onComplete(@NonNull Task<DocumentSnapshot> task) {
////                                        if (task.isSuccessful()) {
////                                            if (task.getResult().getData() == null) {
////
////
////                                            } else {
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/LoginActivity.java
LINES: 218-228

////                                                Toast.makeText(LoginActivity.this, "Connection Error", Toast.LENGTH_SHORT).show();
////
////                                            }
////                                        }
////                                    }
////                                });
//                            } else {
//                                startActivity(new Intent(getApplicationContext(), MainActivity.class));
//                            }
//                        }
//                    });
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/ScoreActivity.java
LINES: 1-45

package com.bitspilani.bosmroulette.activity;

import android.graphics.Typeface;
import android.os.Bundle;
import android.util.DisplayMetrics;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.TextView;

import androidx.appcompat.app.AppCompatActivity;

import com.bitspilani.bosmroulette.R;

public class ScoreActivity extends AppCompatActivity {


    private TextView tV_score;
    private Button close;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_score);
        tV_score = findViewById(R.id.score);
        close = findViewById(R.id.close_button);
        try {
            Typeface font = Typeface.createFromAsset(getAssets(), "fonts/helvetica.ttf");
            tV_score.setTypeface(font);
        } catch (Exception e) {
            Log.d("test", e.toString());
        }


        DisplayMetrics dm = new DisplayMetrics();
        getWindowManager().getDefaultDisplay().getMetrics(dm);

        int width = dm.widthPixels;
        int height = dm.heightPixels;

        getWindow().setLayout((int) (width * .6), (int) (height * .4));

        int score = getIntent().getIntExtra("score", 0);
        tV_score.setText("You won " + String.valueOf(score) + " points!");
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/ScoreActivity.java
LINES: 46-55

close.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                finish();
            }
        });
    }


}
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/WelcomeActivity.java
LINES: 1-45

package com.bitspilani.bosmroulette.activity;

import android.content.Context;
import android.content.Intent;
import android.graphics.Color;
import android.os.Build;
import android.os.Bundle;
import android.text.Html;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.view.Window;
import android.view.WindowManager;
import android.widget.Button;
import android.widget.LinearLayout;
import android.widget.TextView;

import androidx.appcompat.app.AppCompatActivity;
import androidx.viewpager.widget.PagerAdapter;
import androidx.viewpager.widget.ViewPager;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.services.PrefManager;

public class WelcomeActivity extends AppCompatActivity {
    private PrefManager prefManager;
    private ViewPager viewPager;
    private MyViewPagerAdapter myViewPagerAdapter;
    private LinearLayout dotsLayout;
    private TextView[] dots;
    private int[] layouts;
    private Button btnNext;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);


            // Checking for first time launch - before calling setContentView()
            prefManager = new PrefManager(this);
            if (!prefManager.isFirstTimeLaunch() ) {
                launchHomeScreen();
                finish();
            }
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/WelcomeActivity.java
LINES: 46-82

// Making notification bar transparent
            if (Build.VERSION.SDK_INT >= 21) {
                getWindow().getDecorView().setSystemUiVisibility(View.SYSTEM_UI_FLAG_LAYOUT_STABLE | View.SYSTEM_UI_FLAG_LAYOUT_FULLSCREEN);
            }

            setContentView(R.layout.activity_welcome);

            viewPager = (ViewPager) findViewById(R.id.view_pager);

        DepthTransformation depthTransformation = new DepthTransformation();
            viewPager.setPageTransformer(true,depthTransformation);
            dotsLayout = (LinearLayout) findViewById(R.id.layoutDots);
            btnNext = (Button) findViewById(R.id.btn_next);


            // layouts of all welcome sliders
            // add few more layouts if you want
            layouts = new int[]{
                    R.layout.fragment_slider1,
                    R.layout.fragment_slider2,
                    R.layout.fragment_slider3,
                    R.layout.fragment_slider4};

            // adding bottom dots
            addBottomDots(0);

            // making notification bar transparent
            changeStatusBarColor();

            myViewPagerAdapter = new MyViewPagerAdapter();
            viewPager.setAdapter(myViewPagerAdapter);
            viewPager.addOnPageChangeListener(viewPagerPageChangeListener);




            btnNext.setOnClickListener(new View.OnClickListener() {
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/WelcomeActivity.java
LINES: 83-113

@Override
                public void onClick(View v) {
                    // checking for last page
                    // if last page home screen will be launched
                    int current = getItem(+1);
                    if (current < layouts.length) {
                        // move to next screen
                        viewPager.setCurrentItem(current);
                    } else {
//                        SharedPreferences.Editor sharedPreferencesEditor =
//                                PreferenceManager.getDefaultSharedPreferences(getApplicationContext()).edit();
//                        sharedPreferencesEditor.putBoolean(
//                                COMPLETED_ONBOARDING_PREF_NAME, true);
//                        sharedPreferencesEditor.apply();
                        launchHomeScreen();
                    }
                }
            });
        }


        private void addBottomDots(int currentPage) {
            dots = new TextView[layouts.length];

            int[] colorsActive = getResources().getIntArray(R.array.array_dot_active);
            int[] colorsInactive = getResources().getIntArray(R.array.array_dot_inactive);

            dotsLayout.removeAllViews();
            for (int i = 0; i < dots.length; i++) {
                dots[i] = new TextView(this);
                dots[i].setText(Html.fromHtml("&#8226;"));
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/WelcomeActivity.java
LINES: 114-155

dots[i].setTextSize(35);
                dots[i].setTextColor(colorsInactive[currentPage]);
                dotsLayout.addView(dots[i]);
            }

            if (dots.length > 0)
                dots[currentPage].setTextColor(colorsActive[currentPage]);
        }

        private int getItem(int i) {
            return viewPager.getCurrentItem() + i;
        }

        private void launchHomeScreen() {
            prefManager.setFirstTimeLaunch(false);
            startActivity(new Intent(WelcomeActivity.this, HomeActivity.class));
            finish();
        }

        //  viewpager change listener
        ViewPager.OnPageChangeListener viewPagerPageChangeListener = new ViewPager.OnPageChangeListener() {

            @Override
            public void onPageSelected(int position) {
                addBottomDots(position);

                // changing the next button text 'NEXT' / 'GOT IT'
                if (position == layouts.length - 1) {
                    // last page. make button text to GOT IT
                    btnNext.setText("GOT IT");
                } else {
                    // still pages are left
                    btnNext.setText("NEXT");
                }
            }

            @Override
            public void onPageScrolled(int arg0, float arg1, int arg2) {

            }

            @Override
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/WelcomeActivity.java
LINES: 156-202

public void onPageScrollStateChanged(int arg0) {

            }
        };

        /**
         * Making notification bar transparent
         */
        private void changeStatusBarColor() {
            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {
                Window window = getWindow();
                window.addFlags(WindowManager.LayoutParams.FLAG_DRAWS_SYSTEM_BAR_BACKGROUNDS);
                window.setStatusBarColor(Color.TRANSPARENT);
            }
        }

        /**
         * View pager adapter
         */
        public class MyViewPagerAdapter extends PagerAdapter {
            private LayoutInflater layoutInflater;

            public MyViewPagerAdapter() {
            }

            @Override
            public Object instantiateItem(ViewGroup container, int position) {
                layoutInflater = (LayoutInflater) getSystemService(Context.LAYOUT_INFLATER_SERVICE);

                View view = layoutInflater.inflate(layouts[position], container, false);
                container.addView(view);

                return view;
            }

            @Override
            public int getCount() {
                return layouts.length;
            }

            @Override
            public boolean isViewFromObject(View view, Object obj) {
                return view == obj;
            }


            @Override
PATH: app/src/main/java/com/bitspilani/bosmroulette/activity/WelcomeActivity.java
LINES: 203-210

public void destroyItem(ViewGroup container, int position, Object object) {
                View view = (View) object;
                container.removeView(view);
            }
        }


    }
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/CustomAdapter.java
LINES: 1-45

package com.bitspilani.bosmroulette.adapters;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.models.FixtureModel;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.FirebaseFirestore;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Date;
import java.util.Locale;


public class CustomAdapter extends RecyclerView.Adapter<CustomAdapter.ViewHolder> {

    private ArrayList<FixtureModel> fixtures;
    private Context context;
    private static ClickListener clickListener;
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    private FirebaseAuth mAuth;
    String userId;


    public CustomAdapter(ArrayList<FixtureModel> fixtures, Context context) {
        this.fixtures = fixtures;
        this.context = context;
        mAuth = FirebaseAuth.getInstance();
        userId = mAuth.getCurrentUser().getUid();
    }


    @NonNull
    @Override
    public CustomAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int i) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/CustomAdapter.java
LINES: 46-87

View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.home_item_new,parent,false);
        ViewHolder viewHolder=new ViewHolder(view);

        return viewHolder;
    }

    @Override
    public void onBindViewHolder(@NonNull final CustomAdapter.ViewHolder holder, int position) {
        SimpleDateFormat sdf = new SimpleDateFormat("dd-MM-yyyy HH:mm:ss", Locale.getDefault());

        holder.team1.setText(fixtures.get(position).getCollege1());
        holder.team2.setText(fixtures.get(position).getCollege2());
        String time =  fixtures.get(position).getTimestamp();
        Date date =new Date();
        try {
            date = sdf.parse(time);
        } catch (ParseException e) {
            e.printStackTrace();
        }
        int hours = date.getHours();
        int minutes = date.getMinutes();
        String testMinutes = String.valueOf(minutes);
        if(testMinutes.length() == 1)
            testMinutes = testMinutes + "0";

        int dt = date.getDate();
        String month = " Sept";

        String t = dt + month + "\n"+ hours+" : "+ testMinutes;
        holder.time.setText(t);
        holder.sports.setText(fixtures.get(position).getGame());

        //teams = new String[]{};


    }

    @Override
    public int getItemCount() {
        return fixtures.size();
    }
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/CustomAdapter.java
LINES: 88-123

public class ViewHolder extends RecyclerView.ViewHolder implements View.OnClickListener {
        TextView team1, team2;
        TextView time, sports;
        String game;
        int count, total;
   //     ImageView background;

        public ViewHolder(@NonNull final View itemView) {
            super(itemView);
            team1 = itemView.findViewById(R.id.team1);
            team2 = itemView.findViewById(R.id.team2);
       //     background = itemView.findViewById(R.id.sportsbg);
            time = itemView.findViewById(R.id.time);
            sports = itemView.findViewById(R.id.sports);

            itemView.setOnClickListener(this);

        }

        @Override
        public void onClick(View v) {
            clickListener.onItemClicked(getAdapterPosition(), v);
        }


    }

    public interface ClickListener {
        void onItemClicked(int position, View v);
    }


    public void setOnItemClickListener(ClickListener clickListener) {
        CustomAdapter.clickListener = clickListener;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/DevAdapter.java
LINES: 1-39

package com.bitspilani.bosmroulette.adapters;

import android.content.Context;
import android.net.Uri;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ArrayAdapter;
import android.widget.ImageView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.models.DevDesc;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;
import com.mikhaellopez.circularimageview.CircularImageView;
import com.squareup.picasso.Picasso;

import java.util.ArrayList;

public class DevAdapter extends RecyclerView.Adapter<DevAdapter.ViewHolder> {

    ArrayList<DevDesc> devDesc;
    Context context;
    public DevAdapter(Context context, ArrayList<DevDesc> devDesc){
        this.devDesc=devDesc;
        this.context=context;
    }
    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.developer_item,parent,false);
        DevAdapter.ViewHolder viewHolder=new ViewHolder(view);
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/DevAdapter.java
LINES: 40-81

return viewHolder;
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {
        holder.name.setText(devDesc.get(position).getName());
        holder.desc.setText(devDesc.get(position).getDesc());
        String path=devDesc.get(position).getId();

        FirebaseStorage firebaseStorage=FirebaseStorage.getInstance();
        StorageReference storage=firebaseStorage.getReference().child("Pictures");
        StorageReference childreference=storage.child(path);


        childreference.getDownloadUrl()
                .addOnSuccessListener(new OnSuccessListener<Uri>() {
                    @Override
                    public void onSuccess(Uri uri) {
                        Log.d("TAG","Heyy");
                        Picasso.with(context).load(uri.toString()).into(holder.imageView);
                    }
                });
    }

    @Override
    public int getItemCount() {
        return 6;
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView name,desc;
        CircularImageView imageView;
        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            name=itemView.findViewById(R.id.blaName);
            desc=itemView.findViewById(R.id.bladesc);
            imageView=itemView.findViewById(R.id.blapic);

        }
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/LeaderBoardAdapter.java
LINES: 1-43

package com.bitspilani.bosmroulette.adapters;

import android.content.Context;
import android.graphics.Color;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.constraintlayout.widget.ConstraintLayout;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.models.RankClass;

import java.util.ArrayList;

public class LeaderBoardAdapter extends RecyclerView.Adapter<LeaderBoardAdapter.ViewHolder>
{

    private ArrayList<RankClass> items;
    private Context context;

    public LeaderBoardAdapter(ArrayList<RankClass> items, Context context) {
        this.items = items;
        this.context = context;
    }

    public ViewHolder onCreateViewHolder(@NonNull ViewGroup viewGroup, int i) {
        LayoutInflater inflater = LayoutInflater.from(viewGroup.getContext());
        View view = inflater.inflate(R.layout.leaderboard_item_layout,viewGroup,false);
        ViewHolder vh = new ViewHolder(view);
        return vh;
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {

//        if(position == 0){
//            holder.layout.setBackgroundResource(R.drawable.rank_one);
//        }
//        if(position == 1){
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/LeaderBoardAdapter.java
LINES: 44-83

//            holder.layout.setBackgroundResource(R.drawable.rank_two);
//        }
//        if(position == 2){
//            holder.layout.setBackgroundResource(R.drawable.rank_three);
//        }
//
//        if(position == 0){
//            holder.layout.setBackgroundColor(0xffeea228);
//        }
//        if(position == 1){
//            holder.layout.setBackgroundColor( Color.argb(255,98,159,252));
//        }
//        if(position == 2){
//
//            holder.layout.setBackgroundColor(Color.argb(255,119,216,181));
//        }
//        else
            holder.layout.setAlpha((float) (0.85));

        holder.name.setText(items.get(position).getUsername().toUpperCase());
        holder.rank.setText(String.valueOf(items.get(position).getRank()));

        holder.score.setText(String.valueOf(Math.round(items.get(position).getWallet())));

    }

    @Override
    public int getItemCount() {
        return items.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        TextView rank,name,score;
        ConstraintLayout layout;

        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            layout = itemView.findViewById(R.id.constraint_leaderboard_item);
            rank =itemView.findViewById(R.id.player_rank);
            name= itemView.findViewById(R.id.player_name);
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/LeaderBoardAdapter.java
LINES: 84-87

score = itemView.findViewById(R.id.player_score);
        }
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/MyBetAdapter.java
LINES: 1-43

package com.bitspilani.bosmroulette.adapters;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.models.UserBetModel;

import java.util.ArrayList;
import java.util.StringTokenizer;

public class MyBetAdapter extends RecyclerView.Adapter<MyBetAdapter.ViewHolder>
{
    private ArrayList<UserBetModel> items = new ArrayList<>();
    private Context context;


    public MyBetAdapter(ArrayList<UserBetModel> items, Context context) {
        this.items = items;
        this.context = context;
    }

    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup viewGroup, int i) {
        LayoutInflater inflater = LayoutInflater.from(viewGroup.getContext());
        View view = inflater.inflate(R.layout.mybet_layout,viewGroup,false);
        ViewHolder vh = new ViewHolder(view);
        return vh;
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {
        holder.team1.setText(items.get(position).getTeam1());
        holder.team2.setText(String.valueOf(items.get(position).getTeam2()));

        if(items.get(position).getBettedOn()==0) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/MyBetAdapter.java
LINES: 44-66

holder.team1.setTextColor(context.getResources().getColor(R.color.white));
            holder.team1.setBackground(context.getDrawable(R.drawable.borders_team_selected));
            holder.team2.setTextColor(context.getResources().getColor(R.color.black));
            holder.team2.setBackground(context.getDrawable(R.drawable.borders_team_notselected));
        }
        else {
            holder.team1.setTextColor(context.getResources().getColor(R.color.black));
            holder.team1.setBackground(context.getDrawable(R.drawable.borders_team_notselected));
            holder.team2.setTextColor(context.getResources().getColor(R.color.white));
            holder.team2.setBackground(context.getDrawable(R.drawable.borders_team_selected));
       }

        StringTokenizer strk = new StringTokenizer(String.valueOf(items.get(position).getBetAmount()),".");

        holder.game.setText(items.get(position).getGame());
        if(!((items.get(position).getScore1() == -1) && (items.get(position).getScore2()==-1))) {
            holder.score1.setVisibility(View.VISIBLE);
            holder.score2.setVisibility(View.VISIBLE);
            holder.score1.setText(String.valueOf(items.get(position).getScore1()));
            holder.score2.setText(String.valueOf(items.get(position).getScore2()));
        }
        else{
            holder.score1.setVisibility(View.INVISIBLE);
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/MyBetAdapter.java
LINES: 67-100

holder.score2.setVisibility(View.INVISIBLE);
        }
        if(items.get(position).isUpdate()){
            if(items.get(position).getBettedOn() == items.get(position).getResult()){
                holder.betAmount.setBackground(context.getDrawable(R.drawable.green_block));
                holder.betAmount.setText("+"+strk.nextToken());
                holder.status.setText("Won");
            }else{
                holder.status.setText("Lose");
                holder.betAmount.setText("-"+strk.nextToken());
                holder.betAmount.setBackground(context.getDrawable(R.drawable.brown_block));
            }
        }else{
            holder.betAmount.setBackground(context.getDrawable(R.drawable.yellow_block));
            holder.betAmount.setText(strk.nextToken());
        }
    }

    @Override
    public int getItemCount() {
        return items.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        TextView team1,team2;
        TextView betAmount,game,score1,score2;
        TextView status;

        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            team1 = itemView.findViewById(R.id.team1);
            team2 = itemView.findViewById(R.id.team2);
            betAmount = itemView.findViewById(R.id.bet_amount);
            game = itemView.findViewById(R.id.game);
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/MyBetAdapter.java
LINES: 101-106

score1 = itemView.findViewById(R.id.score1);
            score2 = itemView.findViewById(R.id.score2);
            status = itemView.findViewById(R.id.tV_status);
        }
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/TrendingAdapter.java
LINES: 1-39

package com.bitspilani.bosmroulette.adapters;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.models.TrendingModel;
import com.firebase.ui.firestore.FirestoreRecyclerAdapter;
import com.firebase.ui.firestore.FirestoreRecyclerOptions;

public class TrendingAdapter extends FirestoreRecyclerAdapter<TrendingModel,TrendingAdapter.ViewHolder>

{

    private Context context;

    public TrendingAdapter(@NonNull FirestoreRecyclerOptions<TrendingModel> options, Context context) {
        super(options);
        this.context = context;
    }

    public ViewHolder onCreateViewHolder(@NonNull ViewGroup viewGroup, int i) {
        LayoutInflater inflater = LayoutInflater.from(viewGroup.getContext());
        View view = inflater.inflate(R.layout.trending_layout,viewGroup,false);
        ViewHolder vh = new ViewHolder(view);
        return vh;
    }


    @Override
    protected void onBindViewHolder(@NonNull ViewHolder holder, int i, @NonNull TrendingModel model) {
        holder.team1.setText(model.getCollege1());
        holder.team2.setText(model.getCollege2());
PATH: app/src/main/java/com/bitspilani/bosmroulette/adapters/TrendingAdapter.java
LINES: 40-66

holder.sports_name.setText(model.getSports_name());
        if(!model.getScore1().equals("-1"))
                holder.score1.setText(model.getScore1());
        else
            holder.score1.setText("_");

        if(!model.getScore2().equals("-1"))
            holder.score2.setText(model.getScore2());
        else
            holder.score2.setText("_");
    }


    public class ViewHolder extends RecyclerView.ViewHolder {
        TextView sports_name,team1,team2,score1,score2;


        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            sports_name = itemView.findViewById(R.id.sports);
            team1 = itemView.findViewById(R.id.team1);
            team2 = itemView.findViewById(R.id.team2);
            score1 = itemView.findViewById(R.id.score1);
            score2  = itemView.findViewById(R.id.score2);
        }
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/BlankFragment.java
LINES: 1-33

package com.bitspilani.bosmroulette.fragments;

import android.content.DialogInterface;
import android.content.Intent;
import android.os.Bundle;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.Button;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.Nullable;
import androidx.appcompat.app.AlertDialog;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.PagerSnapHelper;
import androidx.recyclerview.widget.RecyclerView;
import androidx.recyclerview.widget.SnapHelper;

import com.bitspilani.bosmroulette.LinePageIndicatorDecoration;
import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.activity.Instructions;
import com.bitspilani.bosmroulette.activity.LoginActivity;
import com.bitspilani.bosmroulette.activity.WelcomeActivity;
import com.bitspilani.bosmroulette.adapters.TrendingAdapter;
import com.bitspilani.bosmroulette.models.TrendingModel;
import com.firebase.ui.firestore.FirestoreRecyclerOptions;
import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.firebase.auth.FirebaseAuth;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/BlankFragment.java
LINES: 34-72

import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.Query;


public class BlankFragment extends Fragment {
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    private String mParam1;
    private String mParam2;
    private Button signOut;
    private ImageView instruction;
    private GoogleSignInClient mGoogleSignInClient;
    TextView balance, name;
    private FirebaseAuth mAuth;
    private RecyclerView rv;
    private TrendingAdapter adapter;
    private AlertDialog.Builder builder;

    public BlankFragment() {
        // Required empty public constructor
    }

    public static BlankFragment newInstance(String param1, String param2) {
        BlankFragment fragment = new BlankFragment();
        Bundle args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        args.putString(ARG_PARAM2, param2);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        mAuth = FirebaseAuth.getInstance();
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/BlankFragment.java
LINES: 73-101

if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        View v = inflater.inflate(R.layout.fragment_blank, container, false);
        rv = v.findViewById(R.id.trending_rv);
        signOut = v.findViewById(R.id.signOut);
        balance = v.findViewById(R.id.balance);
        name = v.findViewById(R.id.username);
        instruction=v.findViewById(R.id.instruction);
        builder = new AlertDialog.Builder(getContext());

        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestEmail()
                .requestIdToken(getString(R.string.default_web_client_id))
                .build();
        mGoogleSignInClient = GoogleSignIn.getClient(getActivity(), gso);

        String userId = mAuth.getCurrentUser().getUid();
        db.collection("users").document(userId).addSnapshotListener(
                new EventListener<DocumentSnapshot>() {
                    @Override
                    public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/BlankFragment.java
LINES: 102-127

double wallet = Math.round(Double.parseDouble(documentSnapshot.get("wallet").toString()));
                        balance.setText(String.valueOf(wallet));
                        name.setText(documentSnapshot.get("name").toString().toUpperCase());
                    }
                }
        );
        setUpRecyclerView();


        signOut.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                builder.setMessage("Do you want to Sign Out?")
                        .setCancelable(false)
                        .setPositiveButton("Yes", new DialogInterface.OnClickListener() {
                            public void onClick(DialogInterface dialog, int id) {
                                mGoogleSignInClient.signOut();
                                mAuth.signOut();
                                startActivity(new Intent(getActivity(), LoginActivity.class));
                                getActivity().finish();
                                Toast.makeText(getContext(),"Signed Out",
                                        Toast.LENGTH_SHORT).show();
                            }
                        })
                        .setNegativeButton("No", new DialogInterface.OnClickListener() {
                            public void onClick(DialogInterface dialog, int id) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/BlankFragment.java
LINES: 128-164

//  Action for 'NO' Button
                                dialog.cancel();
                            }
                        });
                //Creating dialog box
                AlertDialog alert = builder.create();
                //Setting the title manually
                alert.setTitle("Sign Out");
                alert.show();
            }
        });

        instruction.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent intent=new Intent(getActivity(), Instructions.class);
                startActivity(intent);

            }
        });
        return v;

    }


    private void setUpRecyclerView() {
        Query query = db.collection("matches").orderBy("total", Query.Direction.DESCENDING).limit(5);
        FirestoreRecyclerOptions<TrendingModel> options = new FirestoreRecyclerOptions.Builder<TrendingModel>()
                .setQuery(query,TrendingModel.class)
                .build();
        adapter = new TrendingAdapter(options,getContext());
        rv.setLayoutManager(new LinearLayoutManager(getContext(), RecyclerView.HORIZONTAL,false));
        SnapHelper snapHelper = new PagerSnapHelper();
        snapHelper.attachToRecyclerView(rv);

        rv.addItemDecoration(new LinePageIndicatorDecoration());
        rv.setHasFixedSize(true);
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/BlankFragment.java
LINES: 165-180

rv.setAdapter(adapter);
    }

    @Override
    public void onStart() {
        super.onStart();
        adapter.startListening();
    }

    @Override
    public void onStop() {
        super.onStop();
        adapter.stopListening();
    }

}
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 1-35

package com.bitspilani.bosmroulette.fragments;

import android.content.Context;
import android.content.SharedPreferences;
import android.graphics.Color;
import android.graphics.drawable.ColorDrawable;
import android.net.Uri;
import android.os.Bundle;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.view.Window;
import android.widget.ImageView;
import android.widget.ProgressBar;
import android.widget.TextView;

import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.BetDialog;
import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.adapters.CustomAdapter;
import com.bitspilani.bosmroulette.models.FixtureModel;
import com.github.ybq.android.spinkit.sprite.Sprite;
import com.github.ybq.android.spinkit.style.Wave;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.CollectionReference;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 36-83

import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;
import java.util.Date;
import java.util.Locale;
import java.util.Objects;


public class Home extends Fragment {

    private RecyclerView recyclerView;
    private RecyclerView.LayoutManager layoutManager;
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    private CollectionReference matchRef = db.collection("matches");
    private String TAG = "test1";
    SharedPreferences sharedPreferences;
    private Date d1, d2;
    private CustomAdapter adapter;
    ImageView baxter;
    TextView textView;
    private String userId;
    private FirebaseAuth mAuth;
    private ArrayList<String> matchesBetId;
    private ArrayList<String> matchesId = new ArrayList<>();


    public Home() {
        // Required empty public constructor
    }


    public static Home newInstance(String param1, String param2) {
        Home fragment = new Home();
        Bundle args = new Bundle();
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
        }
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 84-113

}

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View view = inflater.inflate(R.layout.fragment_home, container, false);
        ProgressBar progressBar = view.findViewById(R.id.progressbarmatches);
        Sprite viewloader = new Wave();
        textView = view.findViewById(R.id.textView3);
        baxter = view.findViewById(R.id.imageView);
        textView.setVisibility(View.INVISIBLE);
        baxter.setVisibility(View.INVISIBLE);
        progressBar.setIndeterminateDrawable(viewloader);
        mAuth = FirebaseAuth.getInstance();
        String userId = mAuth.getCurrentUser().getUid();
        matchesBetId = new ArrayList<>();
        ArrayList<FixtureModel> fixtures = new ArrayList<>();
        //   DateFormat dateFormat=new DateFormat("MMM dd HH:mm:ss",Locale) ;
        SimpleDateFormat sdf = new SimpleDateFormat("dd-MM-yyyy HH:mm:ss", Locale.getDefault());
        String currentTime = sdf.format(new Date());
        Log.d("mytime", currentTime.toString());
        try {
            d1 = sdf.parse(currentTime);
        } catch (ParseException e) {
            e.printStackTrace();
        }


        db.collection("users").document(userId).collection("bets")
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 114-133

.addSnapshotListener(new EventListener<QuerySnapshot>() {
                    int i = 0;

                    @Override
                    public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {

                        matchesBetId.clear();
                        for (QueryDocumentSnapshot doc : queryDocumentSnapshots) {
                            matchesBetId.add(doc.getId());
                        }
                        db.collection("matches")
                                .addSnapshotListener(new EventListener<QuerySnapshot>() {
                                    @Override
                                    public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
                                        fixtures.clear();
                                        for (QueryDocumentSnapshot doc : queryDocumentSnapshots) {
                                            FixtureModel ob = new FixtureModel(doc.getData().get("college1").toString(),
                                                    doc.getData().get("college2").toString(),
                                                    doc.getData().get("timestamp").toString(),
                                                    doc.getData().get("matchId").toString(),
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 134-155

doc.getData().get("sports_name").toString(),
                                                    i);
                                            if (!(matchesBetId.contains(Objects.requireNonNull(doc.getData().get("matchId")).toString()))) {
                                                try {
                                                    d2 = sdf.parse(ob.getTimestamp());
                                                    if (d2.getTime() >= d1.getTime())
                                                        fixtures.add(ob);
                                                    i++;
                                                } catch (ParseException e1) {
                                                    e1.printStackTrace();
                                                }

                                            }


                                        }

                                        Comparator<FixtureModel> compareByWallet = (FixtureModel o1, FixtureModel o2) -> {
                                            try {
                                                int i = sdf.parse(o1.getTimestamp()).compareTo(sdf.parse(o2.getTimestamp()));
                                                return i;
                                            } catch (ParseException ex) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 156-177

ex.printStackTrace();
                                            }
                                            return 0;
                                        };

                                        Collections.sort(fixtures, compareByWallet);

                                        recyclerView = view.findViewById(R.id.recycler_view);
                                        recyclerView.setHasFixedSize(false);

                                        // use a linear layout manager
                                        layoutManager = new LinearLayoutManager(getContext());
                                        recyclerView.setLayoutManager(layoutManager);

                                        // specify an adapter (see also next example)
                                        adapter = new CustomAdapter(fixtures, getContext());
                                        recyclerView.setAdapter(adapter);

                                        adapter.setOnItemClickListener(new CustomAdapter.ClickListener() {
                                            @Override
                                            public void onItemClicked(int position, View v) {
                                                Log.d(TAG, "onItemClick position: " + position);
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 178-194

db.collection("users").document(userId).get().addOnSuccessListener(
                                                        new OnSuccessListener<DocumentSnapshot>() {
                                                            @Override
                                                            public void onSuccess(DocumentSnapshot documentSnapshot) {
                                                                double wallet = Double.parseDouble(documentSnapshot.get("wallet").toString());
                                                                BetDialog betDialog = new BetDialog(getContext(), fixtures.get(position), wallet);
                                                                betDialog.show();
                                                                Window window = betDialog.getWindow();
                                                                window.setLayout(ViewGroup.LayoutParams.MATCH_PARENT, 720);
                                                                window.setBackgroundDrawable(new ColorDrawable(Color.TRANSPARENT));

                                                            }
                                                        }
                                                );
                                            }
                                        });
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/Home.java
LINES: 195-233

progressBar.setVisibility(View.INVISIBLE);
                                        if (adapter.getItemCount() == 0) {
                                            textView.setVisibility(View.VISIBLE);
                                            baxter.setVisibility(View.VISIBLE);
                                        } else {
                                            textView.setVisibility(View.INVISIBLE);
                                            baxter.setVisibility(View.INVISIBLE);
                                        }
                                    }
                                });
                    }
                });

        return view;
    }

    @Override
    public void onStart() {
        super.onStart();
    }

    @Override
    public void onStop() {
        super.onStop();
    }

    public void onButtonPressed(Uri uri) {
    }

    @Override
    public void onAttach(Context context) {
        super.onAttach(context);
    }

    @Override
    public void onDetach() {
        super.onDetach();
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/LeaderBoardFrag.java
LINES: 1-37

package com.bitspilani.bosmroulette.fragments;

import android.os.Bundle;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ProgressBar;
import android.widget.TextView;

import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.adapters.LeaderBoardAdapter;
import com.bitspilani.bosmroulette.models.RankClass;
import com.github.ybq.android.spinkit.sprite.Sprite;
import com.github.ybq.android.spinkit.style.Wave;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.firestore.CollectionReference;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;

import java.util.ArrayList;
import java.util.Collections;
import java.util.Comparator;

public class LeaderBoardFrag extends Fragment {

    private RecyclerView leaderlist;
    private LeaderBoardAdapter adapter;
    private TextView yourRank,yourScore,yourName;
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/LeaderBoardFrag.java
LINES: 38-70

private CollectionReference leaderRef;
    private FirebaseAuth mAuth;
    private TextView leaderboard;
    FirebaseUser user;
    private static ArrayList<RankClass> mArrayList = new ArrayList<>();
    private static ArrayList<RankClass> mArrayListTemp = new ArrayList<>();

    public LeaderBoardFrag() {
        // Required empty public constructor
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
         View v = inflater.inflate(R.layout.fragment_leader_board, container, false);
         leaderlist = v.findViewById(R.id.leader_rv);
         yourRank = v.findViewById(R.id.your_rank);
         yourScore = v.findViewById(R.id.your_score);
         leaderboard=v.findViewById(R.id.leaderboardtv);
         yourName = v.findViewById(R.id.your_name);
        mAuth=FirebaseAuth.getInstance();
        ProgressBar progressBar = v.findViewById(R.id.progressbarmatches);
        Sprite viewloader = new Wave();
        user=mAuth.getCurrentUser();
        String name=user.getDisplayName();
        String userId=user.getUid();
        progressBar.setIndeterminateDrawable(viewloader);
        yourName.setText(name.toUpperCase());

        Log.d("name",name);
        if (userId != null) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/LeaderBoardFrag.java
LINES: 71-91

db.collection("users").get().addOnSuccessListener(
                    new OnSuccessListener<QuerySnapshot>() {
                        int i =0;
                        @Override
                        public void onSuccess(QuerySnapshot queryDocumentSnapshots) {
                                mArrayList.clear();
                                mArrayListTemp.clear();
                            for(QueryDocumentSnapshot documentSnapshot:queryDocumentSnapshots)
                            {
                                RankClass ob = new RankClass(Double.parseDouble(String.valueOf(documentSnapshot.get("wallet"))),
                                        documentSnapshot.get("name").toString(),i+1,documentSnapshot.getId());
                                mArrayList.add(ob);
                                Log.d("mytest",ob.toString());
                                i++;
                            }

                            Comparator<RankClass> compareByWallet = (RankClass o1, RankClass o2) -> (int) (o2.getWallet()- o1.getWallet());
                            Collections.sort(mArrayList, compareByWallet);
                            int j=0;
                            for(RankClass rc: mArrayList){
                                mArrayListTemp.add(new RankClass(rc.getWallet(),rc.getUsername(),
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/LeaderBoardFrag.java
LINES: 92-135

j+1,rc.getId()));
                                j++;
                            }


                            for(RankClass rc:mArrayListTemp){
                                if(rc.getId().equals(userId))
                                {
                                    yourRank.setText(String.valueOf(rc.getRank()));
                                    yourScore.setText(String.valueOf(Math.round(rc.getWallet())));
                                }
                           }

                            adapter = new LeaderBoardAdapter(mArrayListTemp,getContext());
                            leaderlist.setLayoutManager(new LinearLayoutManager(getContext()));
                            leaderlist.setHasFixedSize(true);
                            leaderlist.setAdapter(adapter);

                            progressBar.setVisibility(View.INVISIBLE);

                        }
                    }
            );

        }
        return v;
    }

    @Override
    public void onStart() {
        super.onStart();

    }

    @Override
    public void onStop() {
        super.onStop();

    }
}

/*if(i==0)
                                    mArrayList.add(i,new RankClass(Double.parseDouble(documentSnapshot.get("wallet").toString()),
                                            documentSnapshot.get("name").toString(),i));
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/LeaderBoardFrag.java
LINES: 136-149

else{
                                    if(mArrayList.get(i).getWallet() <= Double.parseDouble(documentSnapshot.get("wallet").toString()))
                                        mArrayList.add(i+1,new RankClass(Double.parseDouble(documentSnapshot.get("wallet").toString()),
                                                documentSnapshot.get("name").toString(),i+1));
                                    else {
                                        RankClass ob = new RankClass(mArrayList.get(i).getWallet(),
                                                mArrayList.get(i).getUsername(),i+1);

                                        mArrayList.add(i,new RankClass(Double.parseDouble(documentSnapshot.get("wallet").toString()),
                                                documentSnapshot.get("name").toString(),i));
                                        mArrayList.add(i+1,ob);
                                    }
                                }
                                i++;*/
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/More.java
LINES: 1-49

package com.bitspilani.bosmroulette.fragments;

import android.content.Context;
import android.content.Intent;
import android.net.Uri;
import android.os.Bundle;

import androidx.constraintlayout.widget.ConstraintLayout;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.RecyclerView;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.AdapterView;
import android.widget.ArrayAdapter;
import android.widget.Button;
import android.widget.ListView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.activity.Developers;
import com.bitspilani.bosmroulette.activity.HPC;
import com.bitspilani.bosmroulette.activity.Instructions;

import java.lang.reflect.Array;
import java.util.ArrayList;


public class More extends Fragment {
    Button c1,c2,c3;


    public More() {
        // Required empty public constructor
    }


    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View view= inflater.inflate(R.layout.fragment_more, container, false);
        c1=view.findViewById(R.id.help);
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/More.java
LINES: 50-80

c2=view.findViewById(R.id.developers);
        c3=view.findViewById(R.id.blog);

        c1.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(getActivity(), Instructions.class));
            }
        });

        c2.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(getActivity(),Developers.class));
            }
        });

        c3.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(getActivity(), HPC.class));
            }
        });

        return view;
    }




}
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 1-34

package com.bitspilani.bosmroulette.fragments;


import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.ProgressBar;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.adapters.MyBetAdapter;
import com.bitspilani.bosmroulette.models.UserBetModel;
import com.github.ybq.android.spinkit.sprite.Sprite;
import com.github.ybq.android.spinkit.style.Wave;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.CollectionReference;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 35-72

import com.google.firebase.firestore.SetOptions;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.Map;

public class MyBetsFrag extends Fragment {

    private RecyclerView betlist;
    private MyBetAdapter adapter;
    private ArrayList<UserBetModel> items = new ArrayList<>();
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    private CollectionReference userRef;
    private SharedPreferences sharedPreferences;
    private ArrayList<UserBetModel> it = new ArrayList<>();
    private double wallet;
    private FirebaseAuth mAuth;
    private double betAmount;
    private ProgressBar progressBar;
 TextView textView;
     ImageView baxter;
    public MyBetsFrag() {
        // Required empty public constructor
    }


    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View v = inflater.inflate(R.layout.fragment_my_bets, container, false);
        betlist = v.findViewById(R.id.myBets_rv);
        progressBar = v.findViewById(R.id.progressbar);
        Sprite wave = new Wave();
        textView=v.findViewById(R.id.textView8);
        baxter=v.findViewById(R.id.imageView7);
        progressBar.setIndeterminateDrawable(wave);
        mAuth = FirebaseAuth.getInstance();
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 73-90

String userId = mAuth.getCurrentUser().getUid();
        if (userId != null) {
            db.collection("users").document(userId).collection("bets")
                    .addSnapshotListener(new EventListener<QuerySnapshot>() {
                        @Override
                        public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
                            it.clear();
                            items.clear();
                            for (QueryDocumentSnapshot doc : queryDocumentSnapshots) {
                                UserBetModel ob = new UserBetModel(
                                        doc.getData().get("match_id").toString(),
                                        Double.parseDouble(doc.getData().get("betAmount").toString()),
                                        doc.getData().get("team1").toString(),
                                        doc.getData().get("team2").toString(),
                                        Integer.parseInt(doc.getData().get("bettedOn").toString()),
                                        doc.getData().get("game").toString(),
                                        Boolean.parseBoolean(doc.get("update").toString()),
                                        Integer.parseInt(doc.getData().get("score1").toString()),
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 91-115

Integer.parseInt(doc.getData().get("score2").toString()),
                                        Integer.parseInt(doc.getData().get("result").toString())
                                );
                                if (!(Boolean.parseBoolean(doc.get("update").toString()))) {
                                    it.add(ob);
                                }
                                items.add(ob);

                            }
                            adapter = new MyBetAdapter(items, getContext());
                            betlist.setLayoutManager(new LinearLayoutManager(getContext()));
                            betlist.setHasFixedSize(true);
                            betlist.setAdapter(adapter);

                            progressBar.setVisibility(View.INVISIBLE);
                            if (adapter.getItemCount() == 0) {
                                textView.setVisibility(View.VISIBLE);
                                baxter.setVisibility(View.VISIBLE);
                            } else {
                                textView.setVisibility(View.INVISIBLE);
                                baxter.setVisibility(View.INVISIBLE);
                            }
                            for (UserBetModel item : it) {

                                Log.d("itemid", item.getMatch_id());
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 116-133

db.collection("matches").document(item.getMatch_id())
                                        .get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
                                    @Override
                                    public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                                        if (task.isSuccessful()) {


                                            DocumentSnapshot document = task.getResult();
                                            Log.d("myPrint", "printing here");
                                            boolean is_result = Boolean.parseBoolean(document.get("is_result").toString());
                                            int score1 = Integer.parseInt(document.get("score1").toString());
                                            int score2 = Integer.parseInt(document.get("score2").toString());
                                            int team1 = Integer.parseInt(document.get("team1").toString());
                                            int team2 = Integer.parseInt(document.get("team2").toString());
                                            int win_team, lose_team;

                                            Map<String, Object> scoreMap = new HashMap<>();
                                            scoreMap.put("score1", score1);
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 134-153

scoreMap.put("score2", score2);
                                            db.collection("users").document(userId).collection("bets")
                                                    .document(item.getMatch_id()).set(scoreMap, SetOptions.merge());

                                            if (Integer.parseInt(document.getData().get("winner").toString()) == 0) {
                                                if(team1 == 0 && team2 == 0){
                                                   win_team = 1;
                                                   lose_team = 1;
                                                }else if(team1 == 0) {
                                                    win_team = 1;
                                                    lose_team = team2;
                                                }
                                                else if(team2 == 0){
                                                    win_team = team1;
                                                    lose_team = 1;
                                                }else{
                                                    win_team = team1;
                                                    lose_team = team2;
                                                }
                                            } else {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 154-175

if(team1 == 0 && team2 == 0){
                                                    win_team = 1;
                                                    lose_team = 1;
                                                }else if(team1 == 0) {
                                                    win_team = 1;
                                                    lose_team = team1;
                                                }
                                                else if(team2 == 0){
                                                    win_team = team2;
                                                    lose_team = 1;
                                                }else{
                                                    win_team = team2;
                                                    lose_team = team1;
                                                }

                                            }
                                            betAmount = item.getBetAmount();
                                            if (is_result) {
                                                if (item.getBettedOn() == Integer.parseInt(document.getData().get("winner").toString())) {


                                                    db.collection("users").document(userId)
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 176-188

.get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {

                                                        @Override
                                                        public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                                                            if (task.isSuccessful()) {
                                                                DocumentSnapshot doc = task.getResult();
                                                                wallet = Double.parseDouble(doc.get("wallet").toString());
                                                                double betamt;
                                                                if (Boolean.parseBoolean(doc.get("bonus").toString())){
                                                                    wallet = wallet + (betAmount * 1.25 + betAmount * (1 - (double) win_team / (lose_team * 100))) * 1.1;
                                                                    betamt = (betAmount * 1.25 + betAmount * (1 - (double) win_team / (lose_team * 100))) * 1.1;
                                                                }
                                                                else{
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 189-202

wallet = wallet + betAmount * 1.25 + betAmount * (1 - (double) win_team / (lose_team * 100));
                                                                    betamt = betAmount * 1.25 + betAmount * (1 - (double) win_team / (lose_team * 100));
                                                                }
                                                                Map<String, Double> newWallet = new HashMap<>();
                                                                newWallet.put("wallet", wallet);

                                                                db.collection("users").document(userId).set(newWallet, SetOptions.merge());
                                                                Map<String, Object> map = new HashMap<>();
                                                                map.put("update", true);
                                                                map.put("result", item.getBettedOn());
                                                                map.put("betAmount",betamt);

                                                                db.collection("users").document(userId).collection("bets")
                                                                        .document(item.getMatch_id()).set(map, SetOptions.merge());
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 203-217

}
                                                        }
                                                    });
//                                                    db.collection("users").document(userId).collection("bets").document(item.getMatch_id()).get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
//                                                        @Override
//                                                        public void onComplete(@NonNull Task<DocumentSnapshot> task) {
//                                                            DocumentSnapshot snapshot = task.getResult();
//                                                            Double bet = Double.parseDouble(snapshot.get("betAmount").toString());
//
//                                                            bet = betAmount * .25 + betAmount * (1 - (double) win_team / (lose_team * 100));
//                                                            HashMap<String, Object> hashMap = new HashMap<>();
//                                                            hashMap.put("betAmount", bet);
//                                                            db.collection("users").document(userId).collection("bets").document(item.getMatch_id()).set(hashMap, SetOptions.merge());
//
//
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 218-232

//                                                        }
//                                                    });
                                                } else if (Integer.parseInt(document.getData().get("winner").toString()) == 2) {
                                                    db.collection("users").document(userId)
                                                            .get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {

                                                        @Override
                                                        public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                                                            if (task.isSuccessful()) {
                                                                DocumentSnapshot doc = task.getResult();
                                                                wallet = Double.parseDouble(doc.get("wallet").toString());
                                                                if (Boolean.parseBoolean(doc.get("bonus").toString()))
                                                                    wallet = wallet + betAmount;
                                                                else
                                                                    wallet = wallet + betAmount;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 233-248

Map<String, Double> newWallet = new HashMap<>();
                                                                newWallet.put("wallet", wallet);

                                                                db.collection("users").document(userId).set(newWallet, SetOptions.merge());
                                                                Map<String, Object> map = new HashMap<>();
                                                                map.put("update", true);
                                                                map.put("result", item.getBettedOn());

                                                                db.collection("users").document(userId).collection("bets")
                                                                        .document(item.getMatch_id()).set(map, SetOptions.merge());
                                                            }
                                                        }
                                                    });
                                                } else {
                                                    db.collection("users").document(userId)
                                                            .get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 249-265

@Override
                                                        public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                                                            if (task.isSuccessful()) {
                                                                double betamt;
                                                                DocumentSnapshot doc = task.getResult();

                                                                wallet = Double.parseDouble(doc.get("wallet").toString());
                                                                Log.d("mytest",doc.get("loss").toString());
                                                                if(Boolean.parseBoolean(doc.get("loss").toString())) {
                                                                    wallet = wallet + betAmount;
                                                                    betamt = 0;
                                                                }
                                                                else{
                                                                    wallet = wallet + betAmount * 0.25;
                                                                    betamt = betAmount*0.75;
                                                                }
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 266-284

Map<String, Double> newWallet = new HashMap<>();
                                                                newWallet.put("wallet", wallet);

                                                                db.collection("users").document(userId).set(newWallet, SetOptions.merge());

                                                                Map<String, Object> map = new HashMap<>();
                                                                map.put("update", true);
                                                                map.put("result", Integer.parseInt(document.getData().get("winner").toString()));
                                                                map.put("betAmount",betamt);

                                                                db.collection("users").document(userId).collection("bets")
                                                                        .document(item.getMatch_id()).set(map, SetOptions.merge());
                                                            }
                                                        }
                                                    });
                                                }
                                            }
                                        }
                                    }
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/MyBetsFrag.java
LINES: 285-302

});
                            }
                        }
                    });
        }
        return v;
    }

    @Override
    public void onStart() {
        super.onStart();
    }

    @Override
    public void onStop() {
        super.onStop();
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 1-37

package com.bitspilani.bosmroulette.fragments;


import android.annotation.SuppressLint;
import android.app.Activity;
import android.content.Intent;
import android.content.SharedPreferences;
import android.graphics.Color;
import android.os.Bundle;
import android.os.CountDownTimer;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.MotionEvent;
import android.view.View;
import android.view.ViewGroup;
import android.view.animation.Animation;
import android.view.animation.DecelerateInterpolator;
import android.view.animation.RotateAnimation;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.activity.ScoreActivity;
import com.etebarian.meowbottomnavigation.MeowBottomNavigation;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.QuerySnapshot;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 38-79

import com.google.firebase.firestore.SetOptions;

import java.text.ParseException;
import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Random;
import java.util.Timer;
import java.util.TimerTask;

import butterknife.ButterKnife;
import nl.dionsegijn.konfetti.KonfettiView;
import nl.dionsegijn.konfetti.models.Shape;
import nl.dionsegijn.konfetti.models.Size;

import static android.content.Context.MODE_PRIVATE;

public class RouletteFrag extends Fragment {
    private static final String[] sectors = {"100", "550",
            "extra", "200", "500", "150", "450", "loss", "250",
            "100", "extra", "200", "150", "500", "125", "250",
            "100", "200", "300", "150", "extra", "125",
            "100", "200", "100", "150", "450", "loss",
            "loss", "200", "350", "400", "750", "125",
            "100", "200", "150"
    };
    int total = 0;
    private Date d1;
    private Date d2;
    private Date d3;
    int count = 0;
    private ImageView wheel;
    SimpleDateFormat sdf = new SimpleDateFormat("HH:mm", Locale.getDefault());
    private FirebaseFirestore db = FirebaseFirestore.getInstance();
    private String t1 = "0";

    private static final long START_TIME_IN_MILLIS = 7200000;
    private FirebaseAuth mAuth;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 80-117

private String userId;
    private TextView mTextViewCountDown;
    private double wallet;
    private CountDownTimer mCountDownTimer;
    private boolean mTimerRunning;
    private long mTimeLeftInMillis;
    private long mEndTime;
    private static final Random RANDOM = new Random();
    private int degree = 0, degreeOld = 0;
    private static final float HALF_SECTOR = 360f / 37f / 2f;
    private TextView featureInfo;
    KonfettiView konfettiView;
    public static boolean notallowback;
    MeowBottomNavigation bottomNavigation;
    private ImageView bonus;
    private ImageView loss;

    public RouletteFrag() {

    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
    }

    @SuppressLint("ClickableViewAccessibility")
    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        View view = inflater.inflate(R.layout.fragment_roulette, container, false);

        bottomNavigation = getActivity().findViewById(R.id.bottom_nav);
        mTextViewCountDown = view.findViewById(R.id.text_view_countdown);
        bonus = view.findViewById(R.id.flash);
        loss = view.findViewById(R.id.heart);
        wheel = view.findViewById(R.id.wheel);
        ButterKnife.bind((Activity) getContext());
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 118-152

Calendar calendar = Calendar.getInstance();
        konfettiView = view.findViewById(R.id.viewKonfetti);
        calendar.setTime(new Date());


        loss.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Toast.makeText(getContext(), "You get 100% of the points back on lost bets!  ", Toast.LENGTH_SHORT).show();
            }
        });
        bonus.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Toast.makeText(getContext(), "Extra 10% points on each bet won during the active time period!", Toast.LENGTH_SHORT).show();
            }
        });
        //     featureInfo = view.findViewById(R.id.tV_feature);
        String currentTime = sdf.format(calendar.getTime());
        try {
            d1 = sdf.parse(currentTime);
        } catch (ParseException e) {
            e.printStackTrace();
        }
        wheel.setOnTouchListener(new View.OnTouchListener() {
            @Override
            public boolean onTouch(View view, MotionEvent motionEvent) {
                spin(view);
                wheel.setEnabled(false);
                bottomNavigation.setVisibility(View.INVISIBLE);
                notallowback = true;
                return true;
            }
        });
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 153-181

mAuth = FirebaseAuth.getInstance();
        userId = mAuth.getCurrentUser().getUid();

        db.collection("users").document(userId).addSnapshotListener(new EventListener<DocumentSnapshot>() {
            @Override
            public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {
                Log.d("name", documentSnapshot.get("email").toString());

                if (documentSnapshot.get("bonusTime").toString().length() != 0) {
                    try {
                        d2 = sdf.parse(documentSnapshot.get("bonusTime").toString());

                    } catch (ParseException ex) {
                        ex.printStackTrace();
                    }

                    if(d2 != null){
                        if (d2.getTime() >= d1.getTime()) {
                            bonus.setVisibility(View.INVISIBLE);
                            HashMap<String, Object> disablebonus = new HashMap<>();
                            disablebonus.put("bonus", false);
                            disablebonus.put("bonusTime", "");
                            db.collection("users").document(userId).set(disablebonus, SetOptions.merge());
                        }
                    }

                }
                if (documentSnapshot.get("lossTime").toString().length() != 0) {
                    try {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 182-217

d3 = sdf.parse(documentSnapshot.get("lossTime").toString());
                    } catch (ParseException ex) {

                    }
                    Log.d("current", d1.toString());
                    Log.d("loss", d3.toString());
                    if(d3 != null){
                        if (d1.getTime() >= d3.getTime()) {
                            loss.setVisibility(View.INVISIBLE);
                            HashMap<String, Object> disableloss = new HashMap<>();
                            disableloss.put("loss", false);
                            disableloss.put("lossTime", "");
                            db.collection("users").document(userId).set(disableloss, SetOptions.merge());
                        } else {
                            loss.setVisibility(View.VISIBLE);
                        }
                    }

                }

            }

        });

        return view;
    }

    private void updateCountDownText() {
        int minutes = (int) ((mTimeLeftInMillis / (1000 * 60)) % 60);
        int seconds = (int) (mTimeLeftInMillis / 1000) % 60;
        int hours = (int) ((mTimeLeftInMillis / (1000 * 60 * 60)) % 24);
        String timeLeftFormatted = String.format(Locale.getDefault(), "%02d:%02d:%02d", hours, minutes, seconds);

        mTextViewCountDown.setText(timeLeftFormatted);
    }
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 218-268

private void updateButtons() {
        if (mTimerRunning) {
            wheel.setClickable(false);
            wheel.setEnabled(false);
        } else {
            mTimeLeftInMillis = START_TIME_IN_MILLIS;
            wheel.setClickable(true);
            wheel.setEnabled(true);
        }
    }

    private void startTimer() {
        mEndTime = System.currentTimeMillis() + mTimeLeftInMillis;

        mCountDownTimer = new CountDownTimer(mTimeLeftInMillis, 1000) {
            @Override
            public void onTick(long millisUntilFinished) {
                mTimeLeftInMillis = millisUntilFinished;
                updateCountDownText();
            }

            @Override
            public void onFinish() {
                mTimerRunning = false;
                updateButtons();
            }
        }.start();

        mTimerRunning = true;
        updateButtons();
    }


    public void spin(View v) {
        wheel.setEnabled(false);
        if (mTimerRunning) {
            // gestureListener.gDetector=null;
            wheel.setClickable(false);
            wheel.setEnabled(false);
        } else {
            wheel.setClickable(true);
            wheel.setEnabled(true);
            startTimer();
        }

        Timer buttonTimer = new Timer();
        buttonTimer.schedule(new TimerTask() {

            @Override
            public void run() {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 269-307

if (getActivity() == null)
                    return;

                getActivity().runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        wheel.setEnabled(true);
                    }
                });
            }
        }, 20000);

        count++;
        degreeOld = degree % 360;
        // we calculate random angle for rotation of our wheel
        degree = RANDOM.nextInt(360) + 720;

        RotateAnimation rotateAnim = new RotateAnimation(degreeOld, degree,
                RotateAnimation.RELATIVE_TO_SELF, 0.5f, RotateAnimation.RELATIVE_TO_SELF, 0.5f);
        rotateAnim.setDuration(3600);
        rotateAnim.setFillAfter(true);
        rotateAnim.setInterpolator(new DecelerateInterpolator());
        rotateAnim.setAnimationListener(new Animation.AnimationListener() {
            @Override
            public void onAnimationStart(Animation animation) {

            }

            @Override
            public void onAnimationEnd(Animation animation) {


                Calendar calendar = Calendar.getInstance();
                calendar.setTime(new Date());
                calendar.add(Calendar.HOUR_OF_DAY, 3);

                bottomNavigation.setVisibility(View.VISIBLE);
                notallowback = false;
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 308-329

SimpleDateFormat sdf = new SimpleDateFormat("HH:mm", Locale.getDefault());
                String currentTime = sdf.format(calendar.getTime());
                Log.d("mytime", currentTime);


                if (getSector(360 - (degree % 360)).charAt(0) == 'e') {
                    Map<String, Object> bonusMap = new HashMap<>();
                    bonusMap.put("bonus", true);
                    bonusMap.put("bonusTime", currentTime);
                    bonus.setVisibility(View.VISIBLE);
                    konfettiView.build().
                            addColors(Color.YELLOW, Color.GREEN, Color.MAGENTA)
                            .setDirection(0.0, 359.0)
                            .setSpeed(1f, 5f)
                            .setFadeOutEnabled(true)
                            .setTimeToLive(2000L)
                            .addShapes(Shape.RECT, Shape.CIRCLE)
                            .addSizes(new Size(12, 5))
                            .setPosition(-50f, konfettiView.getWidth() + 50f, -50f, -50f)
                            .streamFor(300, 5000L);
                    Toast.makeText(getContext(), "Extra 10% points on each bet won during the active time period! So try your luck", Toast.LENGTH_LONG).show();
                    db.collection("users").document(userId).set(bonusMap, SetOptions.merge());
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 330-349

//         featureInfo.setText("Bonus Activated for 3 hours!!");
                } else if (getSector(360 - (degree % 360)).charAt(0) == 'l') {
                    Map<String, Object> lossMap = new HashMap<>();
                    lossMap.put("loss", true);
                    loss.setVisibility(View.VISIBLE);
                    lossMap.put("lossTime", currentTime);
                    konfettiView.build().
                            addColors(Color.YELLOW, Color.GREEN, Color.MAGENTA)
                            .setDirection(0.0, 359.0)
                            .setSpeed(1f, 5f)
                            .setFadeOutEnabled(true)
                            .setTimeToLive(2000L)
                            .addShapes(Shape.RECT, Shape.CIRCLE)
                            .addSizes(new Size(12, 5))
                            .setPosition(-50f, konfettiView.getWidth() + 50f, -50f, -50f)
                            .streamFor(300, 5000L);
                    Toast.makeText(getContext(), "You get 100% of the points back on lost bets! So bet away  ", Toast.LENGTH_LONG).show();
                    db.collection("users").document(userId).set(lossMap, SetOptions.merge());
                    //       featureInfo.setText("Loss Forgiveness activated for 3 hours !!");
                } else {
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 350-366

total = Integer.parseInt(getSector(360 - (degree % 360)).substring(0, 3));
                    db.collection("users").whereEqualTo("email", mAuth.getCurrentUser().getEmail()).get()
                            .addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                                @Override
                                public void onComplete(@NonNull Task<QuerySnapshot> task) {
                                    if (task.isSuccessful()) {
                                        List<DocumentSnapshot> documents = task.getResult().getDocuments();
                                        for (DocumentSnapshot document : documents) {
                                            userId = document.get("username").toString();
                                            Log.d("user", userId.toString());
                                            wallet = Double.parseDouble(document.get("wallet").toString());
                                        }
                                        wallet = wallet + total;
                                        Map<String, Object> myWallet = new HashMap<>();
                                        myWallet.put("wallet", wallet);
                                        db.collection("users").document(userId).set(myWallet, SetOptions.merge());
                                    }
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 367-417

}
                            });
                    Intent scoreIntent = new Intent(getContext(), ScoreActivity.class);
                    scoreIntent.putExtra("score", total);
                    startActivity(scoreIntent);
                }

            }

            @Override
            public void onAnimationRepeat(Animation animation) {

            }
        });

        wheel.startAnimation(rotateAnim);

    }


    @Override
    public void onStop() {
        super.onStop();

        SharedPreferences prefs = getContext().getSharedPreferences("prefs", MODE_PRIVATE);
        SharedPreferences.Editor editor = prefs.edit();

        editor.putLong("millisLeft", mTimeLeftInMillis);
        editor.putBoolean("timerRunning", mTimerRunning);
        editor.putLong("endTime", mEndTime);
        editor.apply();

        if (mCountDownTimer != null) {
            mCountDownTimer.cancel();
        }
    }

    @Override
    public void onStart() {
        super.onStart();

        SharedPreferences prefs = getContext().getSharedPreferences("prefs", MODE_PRIVATE);

        mTimeLeftInMillis = prefs.getLong("millisLeft", START_TIME_IN_MILLIS);
        mTimerRunning = prefs.getBoolean("timerRunning", false);

        updateCountDownText();
        updateButtons();

        if (mTimerRunning) {
            mEndTime = prefs.getLong("endTime", 0);
PATH: app/src/main/java/com/bitspilani/bosmroulette/fragments/RouletteFrag.java
LINES: 418-453

mTimeLeftInMillis = mEndTime - System.currentTimeMillis();

            if (mTimeLeftInMillis < 0) {
                mTimeLeftInMillis = 0;
                mTimerRunning = false;
                updateCountDownText();
                updateButtons();
            } else {
                startTimer();
            }
        }
    }

    private String getSector(int degrees) {
        int i = 0;
        String text = null;

        do {
            float start = HALF_SECTOR * (i * 2 + 1);
            float end = HALF_SECTOR * (i * 2 + 3);

            if (degrees >= start && degrees < end) {
                text = sectors[i];
            }
            i++;
        } while (text == null && i < sectors.length);

        return text;
    }

    @Override
    public void onResume() {
        super.onResume();

    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/DevDesc.java
LINES: 1-23

package com.bitspilani.bosmroulette.models;

public class DevDesc {
    String name,desc,Id;

    public DevDesc(String name, String desc, String Id){
        this.desc=desc;
        this.name=name;
        this.Id=Id;
    }

    public String getName() {
        return name;
    }

    public String getDesc() {
        return desc;
    }

    public String getId() {
        return Id;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/Fixture.java
LINES: 1-60

package com.bitspilani.bosmroulette.models;

public class Fixture {
    private String college1,college2;
    private String timestamp;
    private String game;
    private String matchId;

    public Fixture() {
    }

    public Fixture(String college1, String college2, String timestamp, String matchId,String game) {
        this.college1 = college1;
        this.college2 = college2;
        this.timestamp = timestamp;
        this.matchId = matchId;
        this.game = game;

    }

    public String getGame() {
        return game;
    }

    public void setGame(String game) {
        this.game = game;
    }

    public String getCollege1() {
        return college1;
    }

    public void setCollege1(String college1) {
        this.college1 = college1;
    }

    public String getCollege2() {
        return college2;
    }

    public void setCollege2(String college2) {
        this.college2 = college2;
    }

    public String getTimestamp() {
        return timestamp;
    }

    public void setTimestamp(String timestamp) {
        this.timestamp = timestamp;
    }

    public String getMatchId() {
        return matchId;
    }

    public void setMatchId(String matchId) {
        this.matchId = matchId;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/FixtureModel.java
LINES: 1-67

package com.bitspilani.bosmroulette.models;

public class FixtureModel {
    private String college1,college2;
    private String timestamp;
    private String game;
    private String matchId;
    private int rank;

    public FixtureModel() {
    }

    public FixtureModel(String college1, String college2, String timestamp, String matchId,String game,int rank) {

        this.college1 = college1;
        this.college2 = college2;
        this.timestamp = timestamp;
        this.matchId = matchId;
        this.game = game;
        this.rank = rank;
    }

    public int getRank() {
        return rank;
    }

    public void setRank(int rank) {
        this.rank = rank;
    }

    public String getGame() {
        return game;
    }

    public void setGame(String game) {
        this.game = game;
    }

    public String getCollege1() {
        return college1;
    }

    public void setCollege1(String college1) {
        this.college1 = college1;
    }

    public String getCollege2() {
        return college2;
    }

    public void setCollege2(String college2) {
        this.college2 = college2;
    }

    public String getTimestamp() {
        return timestamp;
    }

    public void setTimestamp(String timestamp) {
        this.timestamp = timestamp;
    }

    public String getMatchId() {
        return matchId;
    }

    public void setMatchId(String matchId) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/FixtureModel.java
LINES: 68-70

this.matchId = matchId;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/MyBetsModel.java
LINES: 1-61

package com.bitspilani.bosmroulette.models;

public class MyBetsModel
{
    String event;
    int betAmount;
    String type;
    boolean result;
    boolean win;

    public MyBetsModel() {
    }

    public MyBetsModel(String event, int betAmount,String type,boolean result,boolean win) {
        this.event = event;
        this.betAmount = betAmount;
        this.type = type;
        this.result = result;
        this.win = win;
    }

    public boolean isWin() {
        return win;
    }

    public void setWin(boolean win) {
        this.win = win;
    }

    public String getType() {
        return type;
    }

    public void setType(String type) {
        this.type = type;
    }

    public boolean isResult() {
        return result;
    }

    public void setResult(boolean result) {
        this.result = result;
    }

    public String getEvent() {
        return event;
    }

    public void setEvent(String event) {
        this.event = event;
    }

    public int getBetAmount() {
        return betAmount;
    }

    public void setBetAmount(int betAmount) {
        this.betAmount = betAmount;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/PlaceBetModel.java
LINES: 1-41

package com.bitspilani.bosmroulette.models;

public class PlaceBetModel {

    private int betAmount;
    private String userId;
    private String college;

    public PlaceBetModel() {
    }

    public PlaceBetModel(int betAmount, String userId, String college) {
        this.betAmount = betAmount;
        this.userId = userId;
        this.college = college;
    }

    public int getBetAmount() {
        return betAmount;
    }

    public void setBetAmount(int betAmount) {
        this.betAmount = betAmount;
    }

    public String getUserId() {
        return userId;
    }

    public void setUserId(String userId) {
        this.userId = userId;
    }

    public String getCollege() {
        return college;
    }

    public void setCollege(String college) {
        this.college = college;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/RankClass.java
LINES: 1-50

package com.bitspilani.bosmroulette.models;

public class RankClass {
    private String username;
    private String id;
    private double wallet;
    private int rank;

    public RankClass() {
    }

    public RankClass(double wallet, String username,int rank,String id) {
        this.wallet = wallet;
        this.username = username;
        this.rank = rank;
        this.id=id;
    }

    public String getId() {
        return id;
    }

    public void setId(String id) {
        this.id = id;
    }

    public double getWallet() {
        return wallet;
    }

    public int getRank() {
        return rank;
    }

    public void setRank(int rank) {
        this.rank = rank;
    }

    public void setWallet(double wallet) {
        this.wallet = wallet;
    }

    public String getUsername() {
        return username;
    }

    public void setUsername(String username) {
        this.username = username;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/TrendingModel.java
LINES: 1-59

package com.bitspilani.bosmroulette.models;

public class TrendingModel {
    private String college1,college2;
    private String timestamp;
    private String sports_name;
    private String matchId;
    private String score1,score2;

    public TrendingModel() {
    }

    public TrendingModel(String college1, String college2, String timestamp, String sports_name, String matchId, String score1, String score2) {
        this.college1 = college1;
        this.college2 = college2;
        this.timestamp = timestamp;
        this.sports_name = sports_name;
        this.matchId = matchId;
        this.score1 = score1;
        this.score2 = score2;
    }

    public String getCollege1() {
        return college1;
    }

    public void setCollege1(String college1) {
        this.college1 = college1;
    }

    public String getCollege2() {
        return college2;
    }

    public void setCollege2(String college2) {
        this.college2 = college2;
    }

    public String getTimestamp() {
        return timestamp;
    }

    public void setTimestamp(String timestamp) {
        this.timestamp = timestamp;
    }

    public String getSports_name() {
        return sports_name;
    }

    public void setGame(String game) {
        this.sports_name = game;
    }

    public String getMatchId() {
        return matchId;
    }

    public void setMatchId(String matchId) {
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/TrendingModel.java
LINES: 60-78

this.matchId = matchId;
    }

    public String getScore1() {
        return score1;
    }

    public void setScore1(String score1) {
        this.score1 = score1;
    }

    public String getScore2() {
        return score2;
    }

    public void setScore2(String score2) {
        this.score2 = score2;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/UserBetModel.java
LINES: 1-62

package com.bitspilani.bosmroulette.models;

public class UserBetModel
{
    private String match_id;
    private double betAmount;
    private String team1;
    private String team2;
    private int bettedOn;
    private String game;
    private boolean update;
    private int score1,score2,result;

    public UserBetModel() {
    }

    public UserBetModel(String match_id, double betAmount, String team1, String team2, int bettedOn, String game, boolean update
    ,int score1,int score2,int result) {
        this.score1 = score1;
        this.score2 = score2;
        this.match_id = match_id;
        this.betAmount = betAmount;
        this.team1 = team1;
        this.team2 = team2;
        this.bettedOn = bettedOn;
        this.game = game;
        this.update = update;
        this.result = result;
    }

    public int getResult() {
        return result;
    }

    public void setResult(int result) {
        this.result = result;
    }

    public int getScore1() {
        return score1;
    }

    public void setScore1(int score1) {
        this.score1 = score1;
    }

    public int getScore2() {
        return score2;
    }

    public void setScore2(int score2) {
        this.score2 = score2;
    }

    public String getMatch_id() {
        return match_id;
    }

    public void setMatch_id(String match_id) {
        this.match_id = match_id;
    }
PATH: app/src/main/java/com/bitspilani/bosmroulette/models/UserBetModel.java
LINES: 63-110

public double getBetAmount() {
        return betAmount;
    }

    public void setBetAmount(double betAmount) {
        this.betAmount = betAmount;
    }

    public String getTeam1() {
        return team1;
    }

    public void setTeam1(String team1) {
        this.team1 = team1;
    }

    public String getTeam2() {
        return team2;
    }

    public void setTeam2(String team2) {
        this.team2 = team2;
    }

    public int getBettedOn() {
        return bettedOn;
    }

    public void setBettedOn(int bettedOn) {
        this.bettedOn = bettedOn;
    }

    public String getGame() {
        return game;
    }

    public void setGame(String game) {
        this.game = game;
    }

    public boolean isUpdate() {
        return update;
    }

    public void setUpdate(boolean update) {
        this.update = update;
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/services/MyFirebaseMessagingService.java
LINES: 1-13

package com.bitspilani.bosmroulette.services;

import androidx.annotation.NonNull;

import com.google.firebase.messaging.FirebaseMessagingService;
import com.google.firebase.messaging.RemoteMessage;

public class MyFirebaseMessagingService extends FirebaseMessagingService {
    @Override
    public void onMessageReceived(@NonNull RemoteMessage remoteMessage) {
        super.onMessageReceived(remoteMessage);
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/services/PrefManager.java
LINES: 1-33

package com.bitspilani.bosmroulette.services;

import android.content.Context;
import android.content.SharedPreferences;

public class PrefManager {
    SharedPreferences pref;
    SharedPreferences.Editor editor;
    Context _context;

    // shared pref mode
    int PRIVATE_MODE = 0;

    // Shared preferences file name
    private static final String PREF_NAME = "androidhive-welcome";

    private static final String IS_FIRST_TIME_LAUNCH = "IsFirstTimeLaunch";

    public PrefManager(Context context) {
        this._context = context;
        pref = _context.getSharedPreferences(PREF_NAME, PRIVATE_MODE);
        editor = pref.edit();
    }

    public void setFirstTimeLaunch(boolean isFirstTime) {
        editor.putBoolean(IS_FIRST_TIME_LAUNCH, isFirstTime);
        editor.commit();
    }

    public boolean isFirstTimeLaunch() {
        return pref.getBoolean(IS_FIRST_TIME_LAUNCH, true);
    }
}
PATH: app/src/main/java/com/bitspilani/bosmroulette/services/SplashScreen.java
LINES: 1-42

package com.bitspilani.bosmroulette.services;

import androidx.appcompat.app.AppCompatActivity;

import android.content.Context;
import android.content.Intent;
import android.os.Bundle;
import android.os.Handler;
import android.view.animation.Animation;
import android.view.animation.AnimationUtils;
import android.view.animation.RotateAnimation;
import android.widget.ImageView;

import com.bitspilani.bosmroulette.R;
import com.bitspilani.bosmroulette.activity.LoginActivity;
import com.bitspilani.bosmroulette.activity.WelcomeActivity;

public class SplashScreen extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_splash_screen);


        ImageView cards = findViewById(R.id.cards);
        Context context;
        Animation rotate = AnimationUtils.loadAnimation(getApplicationContext(), R.anim.rotate);
        cards.startAnimation(rotate);
        rotate.setAnimationListener(new Animation.AnimationListener() {
            @Override
            public void onAnimationStart(Animation animation) {

            }

            @Override
            public void onAnimationEnd(Animation animation) {
                startActivity(new Intent(getApplicationContext(), LoginActivity.class));
                finish();
            }

            @Override
PATH: app/src/main/java/com/bitspilani/bosmroulette/services/SplashScreen.java
LINES: 43-50

public void onAnimationRepeat(Animation animation) {

            }
        });

    }

}
PATH: app/src/test/java/com/bitspilani/bosmroulette/ExampleUnitTest.java
LINES: 1-17

package com.bitspilani.bosmroulette;

import org.junit.Test;

import static org.junit.Assert.*;

/**
 * Example local unit test, which will execute on the development machine (host).
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
public class ExampleUnitTest {
    @Test
    public void addition_isCorrect() {
        assertEquals(4, 2 + 2);
    }
}
React Typewriter App

This is a React application that fetches a hidden text string from a URL and displays it with a typewriter animation. The app is built using only native browser APIs and React hooks, without any external libraries.

## Features

* **Data Fetching:** Makes a network request to a specified URL using the `fetch` API.

* **Loading State:** Displays a "Loading..." message while waiting for the data to be fetched.

* **Typewriter Animation:** Reveals the fetched text character by character with a half-second delay between each.

* **Dynamic Rendering:** Renders the animated text as a list of characters (`
- ` elements).

***

## How it Works

The application is structured into a single `App` component that manages its state using React hooks.

1. **Fetching Data (`useEffect`):** On component mount, a `useEffect` hook triggers an asynchronous `fetch` request to the URL. It uses `DOMParser` to parse the incoming HTML and extract the text from the `` element.

2. **Animation (`useEffect`):** A second `useEffect` hook watches for changes in the `fullText` state. Once the text is available, it starts a recursive `setTimeout` loop that adds one character to the `charArray` state every 500 milliseconds.
The application is structured into a single `App` component that manages its state using React hooks.

1. **Fetching Data (`useEffect`):** On component mount, a `useEffect` hook triggers an asynchronous `fetch` request to the URL. It uses `DOMParser` to parse the incoming HTML and extract the text from the `` element.

2. **Animation (`useEffect`):** A second `useEffect` hook watches for changes in the `fullText` state. Once the text is available, it starts a recursive `setTimeout` loop that adds one character to the `charArray` state every 500 milliseconds.

3. **Rendering:** The component's render function conditionally displays a loading message or an unordered list (``). It uses the `.map()` function to iterate over the `charArray` and render each character as a list item, creating the typewriter effect in real-time.

***
PATH: eslint.config.js
LINES: 1-29

import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{js,jsx}'],
    extends: [
      js.configs.recommended,
      reactHooks.configs['recommended-latest'],
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
      parserOptions: {
        ecmaVersion: 'latest',
        ecmaFeatures: { jsx: true },
        sourceType: 'module',
      },
    },
    rules: {
      'no-unused-vars': ['error', { varsIgnorePattern: '^[A-Z_]' }],
    },
  },
])
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/image.png" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>CodeSandbox</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.jsx"></script>
  </body>
</html>
PATH: src/App.jsx
LINES: 1-43

import { useState, useEffect } from 'react';

/*
Python script used to get the URL in step 2:
import requests
from bs4 import BeautifulSoup

# The URL to fetch the HTML content from.
url = "https://tns4lpgmziiypnxxzel5ss5nyu0nftol.lambda-url.us-east-1.on.aws/challenge"

# Use a try-except block to handle potential network errors
try:
    # Fetch the HTML content from the specified URL
    response = requests.get(url)
    response.raise_for_status()  # Raise an exception for bad status codes (4xx or 5xx)
    html_content = response.text
except requests.exceptions.RequestException as e:
    # Print an error message and exit if fetching the URL fails
    print(f"Error fetching URL: {e}")
    html_content = None

soup = BeautifulSoup(html_content, 'html.parser')

# A list to store the extracted characters
url_characters = []

# Find all <b> tags with the class "ref"
b_tags = soup.find_all('b', class_='ref')

# Iterate through each matching <b> tag and get its value
for b_tag in b_tags:
    char = b_tag.get('value')
    if char:
        url_characters.append(char)

# Join the characters to form the complete URL
final_url = "".join(url_characters)

# Print the result
print(f"The extracted URL is: {final_url}")

*/
PATH: src/App.jsx
LINES: 44-94

// Main React component for the application.
const App = () => {
  // State to hold the loading status of the fetch request.
  const [isLoading, setIsLoading] = useState(true);
  // State to hold the full, fetched text.
  const [fullText, setFullText] = useState('');
  // State to hold the text as it is being revealed character by character.
  const [charArray, setCharArray] = useState([]);

  // useEffect hook to handle the initial data fetching from the URL.
  // The empty dependency array [] ensures this runs only once when the component mounts.
  useEffect(() => {
    // The URL to fetch data from.
    const url = "https://wgg522pwivhvi5gqsn675gth3q0otdja.lambda-url.us-east-1.on.aws/636172";

    // Asynchronous function to fetch and process the data.
    const fetchData = async () => {
      try {
        const response = await fetch(url);
        // Ensure the HTTP request was successful.
        if (!response.ok) {
          throw new Error(`HTTP error! Status: ${response.status}`);
        }
        
        // Get the HTML content as text.
        const html = await response.text();

        // Parse the HTML to extract text content.
        const parser = new DOMParser();
        const doc = parser.parseFromString(html, 'text/html');
        const extractedText = doc.body.textContent.trim();

        // Set the state with the extracted text and update loading status.
        setFullText(extractedText);
        setIsLoading(false);

      } catch (error) {
        // Log any errors that occur during the fetch process.
        console.error("Failed to fetch or parse the URL:", error);
        setIsLoading(false);
      }
    };

    // Call the fetchData function.
    fetchData();
  }, []); // Empty dependency array ensures this effect runs once.

  // This effect runs whenever the fullText state changes (i.e., after the fetch is complete).
  useEffect(() => {
    // Only proceed if the fullText is available and there's content to animate.
    if (fullText.length > 0) {
PATH: src/App.jsx
LINES: 95-133

// Define a function for the animation loop that takes an index as a parameter.
      const animateText = (index) => {
        // Use a timeout for each character to add the delay.
        setTimeout(() => {
          setCharArray(prev => [...prev, fullText[index]]);
          // Continue the animation if there are more characters.
          if (index + 1 < fullText.length) {
            animateText(index + 1);
          }
        }, 500); // The delay is set to 500ms.
      };

      // Start the animation with the first character at index 0.
      animateText(0);
    }
  }, [fullText]); // This effect depends on the fullText state.

  // The component's render function.
  return (
    <div>
      {/* Conditionally render "Loading..." text while the data is being fetched. */}
      {isLoading && <p>Loading...</p>}

      {/* Conditionally render the flag once it's available and being animated. */}
      {!isLoading && fullText.length > 0 && (
        <ul>
          {/*
            Map over the charArray, which is a stable array of characters, to render each as a list item.
          */}
          {charArray.map((char, index) => (
            <li key={index}>{char}</li>
          ))}
        </ul>
      )}
    </div>
  );
};

export default App;
PATH: src/main.jsx
LINES: 1-10

import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './index.css'
import App from './App.jsx'

createRoot(document.getElementById('root')).render(
  <StrictMode>
    <App />
  </StrictMode>,
)
PATH: vite.config.js
LINES: 1-7

import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react()],
})
# DaanCorona App
- Vendor app for the MSMEs to register with the platform
- Uses phone authentication to verify users of the app
- The app uses Google Maps API for location input and a Django backend
- Built using Android Studio
PATH: app/src/androidTest/java/com/codingclub/daancorona/ExampleInstrumentedTest.java
LINES: 1-27

package com.lendeasy.daancorona;

import android.content.Context;

import androidx.test.platform.app.InstrumentationRegistry;
import androidx.test.ext.junit.runners.AndroidJUnit4;

import org.junit.Test;
import org.junit.runner.RunWith;

import static org.junit.Assert.*;

/**
 * Instrumented test, which will execute on an Android device.
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
@RunWith(AndroidJUnit4.class)
public class ExampleInstrumentedTest {
    @Test
    public void useAppContext() {
        // Context of the app under test.
        Context appContext = InstrumentationRegistry.getInstrumentation().getTargetContext();

        assertEquals("com.lendeasy.daancorona", appContext.getPackageName());
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/BankDetailsActvity.java
LINES: 1-49

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;



import org.jetbrains.annotations.NotNull;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.IOException;

import okhttp3.Call;
import okhttp3.Callback;
import okhttp3.FormBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

public class BankDetailsActvity extends AppCompatActivity {

    Button btnproceedBnk;
    EditText acc,ifsc;
    String acc_no,ifsc_no;
    LoadingDialog dialog;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_bank_details_actvity);
        dialog=new LoadingDialog(this);

        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        String token=sharedPref.getString("Token","");


        final OkHttpClient httpClient = new OkHttpClient();

        Request request1 = new Request.Builder()
                .url("https://daancorona.tech/api/recipient_profile/")
                .addHeader("Authorization", "JWT " + token)
PATH: app/src/main/java/com/codingclub/daancorona/BankDetailsActvity.java
LINES: 50-80

.build();
        dialog.startloadingDialog();

        httpClient.newCall(request1).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {
                dialog.dismissDialog();
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                BankDetailsActvity.this.runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        try {
                            dialog.dismissDialog();
                            JSONObject jsonObject=new JSONObject(response.body().string());
                            acc.setText(jsonObject.getString("account_no"));
                            ifsc.setText(jsonObject.getString("ifsc_code"));

                        } catch (JSONException | IOException e) {

                            BankDetailsActvity.this.runOnUiThread(new Runnable() {
                                @Override
                                public void run() {
                                    dialog.dismissDialog();
                                    Toast.makeText(BankDetailsActvity.this, "Check your Internet Connection", Toast.LENGTH_SHORT).show();
                                }
                            });
PATH: app/src/main/java/com/codingclub/daancorona/BankDetailsActvity.java
LINES: 81-116

e.printStackTrace();
                        }
                    }
                });


            }
        });

        acc=findViewById(R.id.acc_no);
        ifsc=findViewById(R.id.ifsc);

        btnproceedBnk = findViewById(R.id.proceed_bank_details);


        if(sharedPref.getString("Lang","").equals("hin")){
            acc.setHint(getResources().getString(R.string.enteracc));
            ifsc.setHint(getResources().getString(R.string.ifsc));
            btnproceedBnk.setText(getResources().getString(R.string.proceed));
        }
        btnproceedBnk.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                dialog.startloadingDialog();
                SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
                String token=sharedPref.getString("Token","");

                acc_no=acc.getText().toString().trim();
                ifsc_no=ifsc.getText().toString().trim();

                if(acc_no.isEmpty() | ifsc_no.isEmpty())
                    Toast.makeText(BankDetailsActvity.this,"Enter Details",Toast.LENGTH_SHORT).show();
                else{
                    final OkHttpClient client = new OkHttpClient();

                    RequestBody formBody = new FormBody.Builder()
PATH: app/src/main/java/com/codingclub/daancorona/BankDetailsActvity.java
LINES: 117-144

.addEncoded("account_no", acc_no)
                            .addEncoded("ifsc_code",ifsc_no)
                            .build();


                    Request request = new Request.Builder()
                            .url("https://daancorona.tech/api/recipient_profile/")
                            .addHeader("Authorization", "JWT " + token)
                            .post(formBody)
                            .build();

                    client.newCall(request).enqueue(new Callback() {
                        @Override
                        public void onFailure(@NotNull Call call, @NotNull IOException e) {
                            BankDetailsActvity.this.runOnUiThread(new Runnable() {
                                @Override
                                public void run() {
                                    dialog.dismissDialog();
                                    Toast.makeText(BankDetailsActvity.this,"Error!",Toast.LENGTH_SHORT).show();
                                }
                            });
                        }

                        @Override
                        public void onResponse(@NotNull Call call, @NotNull Response response) {

                            BankDetailsActvity.this.runOnUiThread(new Runnable() {
                                @Override
PATH: app/src/main/java/com/codingclub/daancorona/BankDetailsActvity.java
LINES: 145-167

public void run() {
                                    dialog.dismissDialog();
                                    Toast.makeText(BankDetailsActvity.this,"Success!",Toast.LENGTH_SHORT).show();
                                }
                            });

                            SharedPreferences.Editor editor=sharedPref.edit();
                            editor.putBoolean("Page3",true);
                            editor.apply();

                            Intent i = new Intent(BankDetailsActvity.this, MOUActivity.class);
                            i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_NEW_TASK);
                            startActivity(i);

                        }
                    });

                }

            }
        });
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/Contact.java
LINES: 1-23

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.content.SharedPreferences;
import android.os.Bundle;
import android.widget.TextView;


public class Contact extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_contact);
        TextView contact;
        contact=findViewById(R.id.contact);
        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        if(sharedPref.getString("Lang","").equals("hin")){
            contact.setText(" ");
        }
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/DepthTransformation.java
LINES: 1-38

package com.codingclub.daancorona;

import android.view.View;

import androidx.viewpager.widget.ViewPager;

public class DepthTransformation implements ViewPager.PageTransformer{
    @Override
    public void transformPage(View page, float position) {

        if (position < -1){    // [-Infinity,-1)
            // This page is way off-screen to the left.
            page.setAlpha(0);

        }
        else if (position <= 0){    // [-1,0]
            page.setAlpha(1);
            page.setTranslationX(0);
            page.setScaleX(1);
            page.setScaleY(1);

        }
        else if (position <= 1){    // (0,1]
            page.setTranslationX(-position*page.getWidth());
            page.setAlpha(1- Math.abs(position));
            page.setScaleX(1- Math.abs(position));
            page.setScaleY(1- Math.abs(position));

        }
        else {    // (1,+Infinity]
            // This page is way off-screen to the right.
            page.setAlpha(0);

        }


    }
}
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 1-47

package com.codingclub.daancorona;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.annotation.RequiresApi;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;

import android.Manifest;
import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.database.Cursor;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.provider.MediaStore;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import com.bumptech.glide.Glide;
import com.bumptech.glide.load.engine.GlideException;
import com.google.android.material.textfield.TextInputLayout;

import org.json.JSONException;
import org.json.JSONObject;

import java.io.File;
import java.io.IOException;
import java.util.concurrent.TimeUnit;

import de.hdodenhof.circleimageview.CircleImageView;
import okhttp3.MediaType;
import okhttp3.MultipartBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

public class EditProfile extends AppCompatActivity {

    private static final int MY_GALLERY_REQUEST_CODE =102 ;
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 48-75

private static final int LOCATION_PERMISSION_REQUEST_CODE =1234;
    private static final int MY_GALLERY_REQUEST_CODE1 = 1235;
    private EditText shop_name,first_name,last_name,shop_type,address,maxcredit,buss_address;
    private TextInputLayout shop_name1,first_name1,last_name1,shop_type1,address1,maxcredit1,buss_address1;
    private Button proceed, location;
    private CircleImageView userImageView, shopImage;
    private static final int USER_IMAGE = 100;
    private static final int SHOP_IMAGE = 101;
    String shopName,firstName,lastName,shopType,latitude="",longitude="",shopAddress,MaxCredit,BussAddress,profile_img,shop_img;
    double lat=-1.0,lng=-1.0,dwnldlat=-1.0,dwnldlng=-1.0;
    Uri userImageURI, shopImageURI,dwnloaduser,dwnldshop;
    String token,shopUrl,userUrl;
    LoadingDialog dialog;
    SharedPreferences sharedPref;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_edit_profile);
        sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        initializeItems();
        if(sharedPref.getString("Lang","").equals("hin")){
            location.setText(getResources().getString(R.string.bussloc));
            proceed.setText(getResources().getString(R.string.change));
        }
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 76-109

userImageView.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE1);
            }
        });

        shopImage.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE);
            }
        });

        location.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                getLocationPermission(LOCATION_PERMISSION_REQUEST_CODE);
            }
        });




        proceed.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                declaration();

                if(firstName.isEmpty() || lastName.isEmpty() || shopName.isEmpty() || shopType.isEmpty() ||
                        /*latitude.isEmpty() || longitude.isEmpty() ||*/ shopAddress.isEmpty() || MaxCredit.isEmpty()
                        || BussAddress.isEmpty() || userImageURI==null || shopImageURI==null)
                    Toast.makeText(EditProfile.this,"Enter all details",Toast.LENGTH_SHORT).show();
                else {
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 110-141

dialog.startloadingDialog();
                    new sendDataTask().execute(firstName, lastName, shopName, shopType, latitude, longitude, shopAddress, MaxCredit, BussAddress);
                }
            }
        });
    }

    private void initializeItems() {
        location = findViewById(R.id.shopLocation);
        first_name = findViewById(R.id.firstname);
        last_name = findViewById(R.id.lastname);
        proceed = findViewById(R.id.signin);
        userImageView = findViewById(R.id.user_image);
        shopImage = findViewById(R.id.shop_image);
        shop_name = findViewById(R.id.shopName);
        shop_type = findViewById(R.id.shopType);
        address = findViewById(R.id.address);
        maxcredit=findViewById(R.id.maxcredit);
        buss_address=findViewById(R.id.businessaddress);

        first_name1 = findViewById(R.id.firstname1);
        last_name1 = findViewById(R.id.lastname1);
        shop_name1 = findViewById(R.id.shopName1);
        shop_type1 = findViewById(R.id.shopType1);
        address1 = findViewById(R.id.address1);
        maxcredit1=findViewById(R.id.maxcredit1);
        buss_address1=findViewById(R.id.businessaddress1);


        userImageView.setImageResource(R.drawable.ic_launcher_background);
        shopImage.setImageResource(R.drawable.ic_launcher_background);
        dialog=new LoadingDialog(this);
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 142-164

dialog.startloadingDialog();

        new getTask().execute();
    }

    private void setData(String shopName, String firstName, String lastName, String shopType, String shopAddress, String maxCredit, String bussAddress,String  profile_img,String shop_img) throws GlideException {
        shop_name.setText(shopName);
        first_name.setText(firstName);
        last_name.setText(lastName);
        shop_type.setText(shopType);
        address.setText(shopAddress);
        maxcredit.setText(maxCredit);
        buss_address.setText(bussAddress);


//        if(sharedPref.getString("Lang","").equals(("hin"))){
//            shop_name.setText(TranslateTo.getTranslation(shop_name.getText().toString(),EditProfile.this));
//            first_name.setText(TranslateTo.getTranslation(first_name.getText().toString(),EditProfile.this));
//            last_name.setText(TranslateTo.getTranslation(last_name.getText().toString(),EditProfile.this));
//            shop_type.setText(TranslateTo.getTranslation(shop_type.getText().toString(),EditProfile.this));
//            address.setText(TranslateTo.getTranslation(address.getText().toString(),EditProfile.this));
//            maxcredit.setText(TranslateTo.getTranslation(maxcredit.getText().toString(),EditProfile.this));
//            buss_address.setText(TranslateTo.getTranslation(buss_address.getText().toString(),EditProfile.this));
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 165-198

//        }
        if(sharedPref.getString("Lang","").equals(("hin"))){
            first_name1.setHint(getResources().getString(R.string.firstname));
            last_name1.setHint(getResources().getString(R.string.lastname));
            shop_name1.setHint(getResources().getString(R.string.bussname));
            shop_type1.setHint(getResources().getString(R.string.busstype));
            address1.setHint(getResources().getString(R.string.address));
            maxcredit1.setHint(getResources().getString(R.string.maxcredit));
            buss_address1.setHint(getResources().getString(R.string.bussaddr));
        }
            Intent intent1 = getIntent();
            lat = intent1.getDoubleExtra("lat", -1.0);
            lng = intent1.getDoubleExtra("lng", -1.0);
            Log.d("lat",lat+"");
            if(lat!=-1.0) {
                latitude = Double.toString(lat);
                longitude = Double.toString(lng);
            }
            else{

                latitude = Double.toString(dwnldlat);
                longitude = Double.toString(dwnldlng);
            }


        Log.d("Img",profile_img);

        Glide.with(this).load(userImageURI).into(userImageView);
        Glide.with(this).load(shopImageURI).into(shopImage);
    }

    private void declaration() {

        firstName = first_name.getText().toString();
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 199-231

lastName = last_name.getText().toString();
        shopName = shop_name.getText().toString();
        shopType = shop_type.getText().toString();
        shopAddress = address.getText().toString();
        MaxCredit=maxcredit.getText().toString();
        BussAddress=buss_address.getText().toString();
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (resultCode == RESULT_OK && requestCode == USER_IMAGE) {
            userImageURI = data.getData();
            Log.d("scjsdk","cnsdcbsdh");
            userImageView.setImageURI(userImageURI);
        }
        if (resultCode == RESULT_OK && requestCode == SHOP_IMAGE) {
            shopImageURI = data.getData();
            shopImage.setImageURI(shopImageURI);
        }
    }

    class sendDataTask extends AsyncTask<String,Void,String> {

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected String doInBackground(String... strings) {

            final OkHttpClient httpClient = new OkHttpClient().newBuilder().writeTimeout(1, TimeUnit.MINUTES).build();

            final MediaType MEDIA_TYPE_JPEG = MediaType.parse("image/jpeg");
            File user,shop;
            RequestBody formBody;
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 232-256

//File path = Environment.getExternalStoragePublicDirectory(
            //      Environment.DIRECTORY_PICTURES);

            Log.d("TAG",""+userImageURI);
            
            if(dwnloaduser!=userImageURI && dwnldshop!=shopImageURI) {
                String userpath=getPath(userImageURI),shoppath=getPath(shopImageURI);
                user = new File(userpath);
                shop = new File(shoppath);

                Log.d("TAG", "" + user.getName());


                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("first_name", strings[0])
                        .addFormDataPart("last_name", strings[1])
                        .addFormDataPart("business_name", strings[2])
                        .addFormDataPart("business_type", strings[3])
                        .addFormDataPart("lat", strings[4])
                        .addFormDataPart("long", strings[5])
                        .addFormDataPart("address", strings[6])
                        .addFormDataPart("max_credit", strings[7])
                        .addFormDataPart("business_address", strings[8])
                        .addFormDataPart("recipient_photo", user.getName(), RequestBody.create(MEDIA_TYPE_JPEG, user))
                        .addFormDataPart("business_photo", shop.getName(), RequestBody.create(MEDIA_TYPE_JPEG, shop))
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 257-283

.build();
            }
            else if(dwnloaduser!=userImageURI){

                String userpath=getPath(userImageURI);
                user = new File(userpath);

                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("first_name", strings[0])
                        .addFormDataPart("last_name", strings[1])
                        .addFormDataPart("business_name", strings[2])
                        .addFormDataPart("business_type", strings[3])
                        .addFormDataPart("lat", strings[4])
                        .addFormDataPart("long", strings[5])
                        .addFormDataPart("address", strings[6])
                        .addFormDataPart("max_credit", strings[7])
                        .addFormDataPart("business_address", strings[8])
                        .addFormDataPart("recipient_photo", user.getName(), RequestBody.create(MEDIA_TYPE_JPEG, user))
                        .build();
            }
            else if(dwnldshop!=shopImageURI){

                String shoppath=getPath(shopImageURI);
                shop = new File(shoppath);

                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("first_name", strings[0])
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 284-306

.addFormDataPart("last_name", strings[1])
                        .addFormDataPart("business_name", strings[2])
                        .addFormDataPart("business_type", strings[3])
                        .addFormDataPart("lat", strings[4])
                        .addFormDataPart("long", strings[5])
                        .addFormDataPart("address", strings[6])
                        .addFormDataPart("max_credit", strings[7])
                        .addFormDataPart("business_address", strings[8])
                        .addFormDataPart("business_photo", shop.getName(), RequestBody.create(MEDIA_TYPE_JPEG, shop))
                        .build();
            }
            else{

                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("first_name", strings[0])
                        .addFormDataPart("last_name", strings[1])
                        .addFormDataPart("business_name", strings[2])
                        .addFormDataPart("business_type", strings[3])
                        .addFormDataPart("lat", strings[4])
                        .addFormDataPart("long", strings[5])
                        .addFormDataPart("address", strings[6])
                        .addFormDataPart("max_credit", strings[7])
                        .addFormDataPart("business_address", strings[8])
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 307-346

.build();
            }

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_profile/")
                    .addHeader("Authorization","JWT "+token)
                    .post(formBody)
                    .build();

            Log.d("Tag",strings[4]);

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                Log.d("Tag",response.body()+"");
                return "Done";

            } catch (IOException e ) {
                e.printStackTrace();
            } catch (NullPointerException e){
                e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String s) {
            super.onPostExecute(s);
            dialog.dismissDialog();
            if(s!=null) {

                Toast.makeText(EditProfile.this,s,Toast.LENGTH_LONG).show();
                Intent intent = new Intent(EditProfile.this, MainActivity.class);
                intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                startActivity(intent);
                finish();
            }
            else
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 347-378

Toast.makeText(EditProfile.this,"Error",Toast.LENGTH_LONG).show();
        }
    }

    class getTask extends AsyncTask<Void, Void, String> {

        @Override
        protected String doInBackground(Void... voids) {

            OkHttpClient httpClient = new OkHttpClient().newBuilder().readTimeout(1,TimeUnit.MINUTES).build();

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_profile/")
                    .addHeader("Authorization","JWT "+token)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                JSONObject jsonObject=new JSONObject(response.body().string());

                shopName=jsonObject.getString("business_name");
                firstName=jsonObject.getString("first_name");
                lastName=jsonObject.getString("last_name");
                shopType=jsonObject.getString("business_type");
                latitude=jsonObject.getString("lat");
                longitude=jsonObject.getString("long");
                shopAddress=jsonObject.getString("address");
                MaxCredit=jsonObject.getString("max_credit");
                BussAddress=jsonObject.getString("business_address");
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 379-413

profile_img=(String)jsonObject.get("recipient_photo");
                Log.d("Taggg",jsonObject.get("recipient_photo")+"");

                shop_img=(String)jsonObject.get("business_photo");

                userUrl="https://daancorona.tech"+profile_img;
                userImageURI=Uri.parse(userUrl);
                shopUrl="https://daancorona.tech"+shop_img;
                shopImageURI=Uri.parse(shopUrl);

                dwnldshop=shopImageURI;
                dwnloaduser=userImageURI;
                dwnldlat=Double.parseDouble(latitude);
                dwnldlng=Double.parseDouble(longitude);
//
//                SaveImageFromUrl.saveImage("http://daancorona.pythonanywhere.com"+profile_img , profile_img.substring(profile_img.lastIndexOf('/')));
//                SaveImageFromUrl.saveImage("http://daancorona.pythonanywhere.com"+shop_img, shop_img.substring(shop_img.lastIndexOf('/')));

                return "Done";

            } catch (IOException | JSONException e) {
                e.printStackTrace();
            }

            return "";
        }

        @Override
        protected void onPostExecute(String s) {

            super.onPostExecute(s);
            dialog.dismissDialog();

            if(s.equals("Done")){
                try {
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 414-454

setData(shopName,firstName,lastName,shopType,shopAddress,MaxCredit,BussAddress,profile_img,shop_img);
                } catch (GlideException e) {
                    e.printStackTrace();

                }
            }
            else
                Toast.makeText(getApplicationContext(),"Error!",Toast.LENGTH_SHORT).show();
        }
    }


    public String getPath(Uri uri) {
        String[] projection = {MediaStore.MediaColumns.DATA};
        Cursor cursor = managedQuery(uri, projection, null, null, null);
        int column_index = cursor
                .getColumnIndexOrThrow(MediaStore.MediaColumns.DATA);
        cursor.moveToFirst();
        String imagePath = cursor.getString(column_index);

        return cursor.getString(column_index);
    }



    public void checkPermission(String permission, int requestCode)
    {
        // Checking if permission is not granted
        if (ContextCompat.checkSelfPermission(
                this,
                permission)
                == PackageManager.PERMISSION_DENIED) {
            ActivityCompat
                    .requestPermissions(
                            this,
                            new String[] { permission },
                            requestCode);
        }
        else{
            if(requestCode== MY_GALLERY_REQUEST_CODE){
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 455-491

Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, SHOP_IMAGE);
            }
            if(requestCode==MY_GALLERY_REQUEST_CODE1){
                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, USER_IMAGE);
            }
        }

    }

    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults)
    {
        super.onRequestPermissionsResult(requestCode,
                permissions,
                grantResults);

        if (requestCode ==  MY_GALLERY_REQUEST_CODE) {
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Storage Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();

                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, SHOP_IMAGE);

            }
            else {
                Toast.makeText(this,
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 492-523

"Storage Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
        else{
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Location Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();
                Intent intent = new Intent(EditProfile.this, MapActivity.class);
                intent.putExtra("edit",true);
                startActivity(intent);
                finish();
            }
            else {
                Toast.makeText(this,
                        "Location Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
    }

    private void getLocationPermission(int requestCode){
        Log.d("isnull","Null");

        String[] permissions = {Manifest.permission.ACCESS_FINE_LOCATION, android.Manifest.permission.ACCESS_COARSE_LOCATION};
        if (ContextCompat.checkSelfPermission(this, android.Manifest.permission.ACCESS_FINE_LOCATION) != PackageManager.PERMISSION_GRANTED &&
                ActivityCompat.checkSelfPermission(this, android.Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) {
PATH: app/src/main/java/com/codingclub/daancorona/EditProfile.java
LINES: 524-536

ActivityCompat.requestPermissions(this,permissions, LOCATION_PERMISSION_REQUEST_CODE);
        }else{
            if(requestCode== LOCATION_PERMISSION_REQUEST_CODE){
                Intent intent = new Intent(EditProfile.this, MapActivity.class);
                intent.putExtra("edit",true);
                startActivity(intent);
                finish();
            }
        }
    }


}
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 1-49

package com.codingclub.daancorona;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.annotation.RequiresApi;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;

import android.Manifest;
import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.database.Cursor;
import android.icu.text.IDNA;
import android.media.Image;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.os.Environment;
import android.provider.MediaStore;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Toast;


import org.json.JSONException;
import org.json.JSONObject;

import java.io.File;
import java.io.IOException;

import de.hdodenhof.circleimageview.CircleImageView;
import okhttp3.FormBody;
import okhttp3.MediaType;
import okhttp3.MultipartBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

public class InfoActivity extends AppCompatActivity {

    private static final int MY_GALLERY_REQUEST_CODE =102 ;
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 50-83

private static final int STORAGE_PERMISSION_CODE = 103;
    private EditText shop_name,first_name,last_name,shop_type,address,maxcredit,buss_address,upi;
    private Button proceed, location;
    private CircleImageView userImageView, shopImage;
    private static final int USER_IMAGE = 100;
    private static final int SHOP_IMAGE = 101;
    String shopName,firstName,lastName,shopType,latitude,longitude,shopAddress,MaxCredit,BussAddress,Upi;
    double lat,lng;
    Uri userImageURI, shopImageURI;
    String token;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_info);

        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        initializeItems();

        userImageView.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {

                checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE);
                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, USER_IMAGE);
            }
        });

        shopImage.setOnClickListener(new View.OnClickListener() {
            @Override
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 84-113

public void onClick(View v) {
                checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE);
                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, SHOP_IMAGE);
            }
        });

        location.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent intent = new Intent(InfoActivity.this, MapActivity.class);
                startActivity(intent);
                finish();
            }
        });

        Intent intent1 = getIntent();
        lat=intent1.getDoubleExtra("lat",0.0);
        lng=intent1.getDoubleExtra("lng",0.0);
        latitude = Double.toString(lat);
        longitude = Double.toString(lng);

        proceed.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                declaration();
                if(firstName.isEmpty() || lastName.isEmpty() || shopName.isEmpty() || shopType.isEmpty() ||
                        latitude.isEmpty() || longitude.isEmpty() || shopAddress.isEmpty() || MaxCredit.isEmpty()
                        || BussAddress.isEmpty() || Upi.isEmpty() || userImageURI==null || shopImageURI==null)
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 114-146

Toast.makeText(InfoActivity.this,"Enter all details",Toast.LENGTH_SHORT).show();
                else
                    new sendDataTask().execute(firstName,lastName,shopName,shopType,latitude,
                            longitude,shopAddress,MaxCredit,BussAddress,Upi);
            }
        });

    }

    private void initializeItems() {

        location = findViewById(R.id.shopLocation);
        first_name = findViewById(R.id.firstname);
        last_name = findViewById(R.id.lastname);
        proceed = findViewById(R.id.signin);
        userImageView = findViewById(R.id.user_image);
        shopImage = findViewById(R.id.shop_image);
        shop_name = findViewById(R.id.shopName);
        shop_type = findViewById(R.id.shopType);
        address = findViewById(R.id.address);
        maxcredit=findViewById(R.id.maxcredit);
        buss_address=findViewById(R.id.businessaddress);

        userImageView.setImageResource(R.drawable.ic_launcher_background);
        shopImage.setImageResource(R.drawable.ic_launcher_background);
    }

    private void declaration() {
        firstName = first_name.getText().toString();
        lastName = last_name.getText().toString();
        shopName = shop_name.getText().toString();
        shopType = shop_type.getText().toString();
        shopAddress = address.getText().toString();
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 147-183

MaxCredit=maxcredit.getText().toString();
        BussAddress=buss_address.getText().toString();
        Upi=upi.getText().toString();
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (resultCode == RESULT_OK && requestCode == USER_IMAGE) {
            userImageURI = data.getData();
            userImageView.setImageURI(userImageURI);
        }
        if (resultCode == RESULT_OK && requestCode == SHOP_IMAGE) {
            shopImageURI = data.getData();
            shopImage.setImageURI(shopImageURI);
        }
    }

    class sendDataTask extends AsyncTask<String,Void,String>{

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected String doInBackground(String... strings) {

            final OkHttpClient httpClient = new OkHttpClient();

            final MediaType MEDIA_TYPE_PNG = MediaType.parse("image/jpeg");
            //File path = Environment.getExternalStoragePublicDirectory(
              //      Environment.DIRECTORY_PICTURES);

            Log.d("TAG",""+userImageURI);

            String userpath=getPath(userImageURI),shoppath=getPath(shopImageURI);
            Log.d("TAG",""+userpath);
            Log.d("TAG",""+shoppath);

            File user = new File(userpath);
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 184-210

File shop = new File(shoppath);

            Log.d("TAG",""+user.getName());

            RequestBody formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                    .addFormDataPart("first_name",strings[0])
                    .addFormDataPart("last_name",strings[1])
                    .addFormDataPart("business_name",strings[2])
                    .addFormDataPart("business_type",strings[3])
                    .addFormDataPart("lat",strings[4])
                    .addFormDataPart("long",strings[5])
                    .addFormDataPart("address",strings[6])
                    .addFormDataPart("max_credit",strings[7])
                    .addFormDataPart("business_address",strings[8])
                    .addFormDataPart("recipient_photo",user.getName(),RequestBody.create(MEDIA_TYPE_PNG,user))
                    .addFormDataPart("business_photo",shop.getName(),RequestBody.create(MEDIA_TYPE_PNG,shop))
                    .build();

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_profile/")
                    .addHeader("Authorization","JWT "+token)
                    .post(formBody)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 211-251

throw new IOException("Unexpected code " + response);

                Log.d("Tag",response.body()+"");
                return "Done";

            } catch (IOException e ) {
                e.printStackTrace();
            } catch (NullPointerException e){
                e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String s) {
            super.onPostExecute(s);
            if(s!=null) {
                Toast.makeText(InfoActivity.this,s,Toast.LENGTH_LONG).show();
                Intent intent = new Intent(InfoActivity.this, MainActivity.class);
                startActivity(intent);
                finish();
            }
            else
                Toast.makeText(InfoActivity.this,"Error",Toast.LENGTH_LONG).show();
        }
    }
    public String getPath(Uri uri) {
        String[] projection = {MediaStore.MediaColumns.DATA};
        Cursor cursor = managedQuery(uri, projection, null, null, null);
        int column_index = cursor
                .getColumnIndexOrThrow(MediaStore.MediaColumns.DATA);
        cursor.moveToFirst();
        String imagePath = cursor.getString(column_index);

        return cursor.getString(column_index);
    }

    public void checkPermission(String permission, int requestCode)
    {

        // Checking if permission is not granted
PATH: app/src/main/java/com/codingclub/daancorona/InfoActivity.java
LINES: 252-292

if (ContextCompat.checkSelfPermission(
                this,
                permission)
                == PackageManager.PERMISSION_DENIED) {
            ActivityCompat
                    .requestPermissions(
                            this,
                            new String[] { permission },
                            requestCode);
        }

    }

    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults)
    {
        super.onRequestPermissionsResult(requestCode,
                        permissions,
                        grantResults);


        if (requestCode ==  STORAGE_PERMISSION_CODE) {
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Storage Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();
            }
            else {
                Toast.makeText(this,
                        "Storage Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/InfoNew.java
LINES: 1-40

package com.codingclub.daancorona;

        import androidx.appcompat.app.AppCompatActivity;
        import androidx.viewpager.widget.PagerAdapter;
        import androidx.viewpager.widget.ViewPager;

        import android.content.Context;
        import android.content.Intent;
        import android.content.SharedPreferences;
        import android.graphics.Color;
        import android.os.Build;
        import android.os.Bundle;
        import android.text.Html;
        import android.view.LayoutInflater;
        import android.view.View;
        import android.view.ViewGroup;
        import android.view.Window;
        import android.view.WindowManager;
        import android.widget.Button;
        import android.widget.LinearLayout;
        import android.widget.TextView;


public class InfoNew extends AppCompatActivity {

    private ViewPager viewPager;
    private MyViewPagerAdapter myViewPagerAdapter;
    private LinearLayout dotsLayout;
    private TextView[] dots;
    private int[] layouts;
    private Button btnNext;
    SharedPreferences sharedPref;
    String token;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        // Making notification bar transparent
        if (Build.VERSION.SDK_INT >= 21) {
PATH: app/src/main/java/com/codingclub/daancorona/InfoNew.java
LINES: 41-76

getWindow().getDecorView().setSystemUiVisibility(View.SYSTEM_UI_FLAG_LAYOUT_STABLE | View.SYSTEM_UI_FLAG_LAYOUT_FULLSCREEN);
        }

        setContentView(R.layout.activity_info_new);

        viewPager = (ViewPager) findViewById(R.id.view_pager);

        DepthTransformation depthTransformation = new DepthTransformation();
        viewPager.setPageTransformer(true,depthTransformation);
        dotsLayout = (LinearLayout) findViewById(R.id.layoutDots);
        btnNext = (Button) findViewById(R.id.btn_next);


        // layouts of all welcome sliders
        // add few more layouts if you want

        sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        if(sharedPref.getString("Lang","").equals("hin")){
            layouts = new int[]{
                    R.layout.fragment_sliderhn1,
                    R.layout.fragment_sliderhn2,
                    R.layout.fragment_sliderhn3,
                    R.layout.fragment_sliderhn4,
                    R.layout.fragment_sliderhn5};
        }
        else{
            layouts = new int[]{
                    R.layout.fragment_slider1,
                    R.layout.fragment_slider2,
                    R.layout.fragment_slider3,
                    R.layout.fragment_slider4,
                    R.layout.fragment_slider5};
        }
PATH: app/src/main/java/com/codingclub/daancorona/InfoNew.java
LINES: 77-114

// adding bottom dots
        addBottomDots(0);

        // making notification bar transparent
        changeStatusBarColor();

        myViewPagerAdapter = new MyViewPagerAdapter();
        viewPager.setAdapter(myViewPagerAdapter);
        viewPager.addOnPageChangeListener(viewPagerPageChangeListener);

        btnNext.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                // checking for last page
                // if last page home screen will be launched
                int current = getItem(+1);
                if (current < layouts.length) {
                    // move to next screen
                    viewPager.setCurrentItem(current);
                } else {
                    launchHomeScreen();
                }
            }
        });

    }

    private void addBottomDots(int currentPage) {
        dots = new TextView[layouts.length];

        int[] colorsActive = getResources().getIntArray(R.array.array_dot_active);
        int[] colorsInactive = getResources().getIntArray(R.array.array_dot_inactive);

        dotsLayout.removeAllViews();
        for (int i = 0; i < dots.length; i++) {
            dots[i] = new TextView(this);
            dots[i].setText(Html.fromHtml("&#8226;"));
            dots[i].setTextSize(35);
PATH: app/src/main/java/com/codingclub/daancorona/InfoNew.java
LINES: 115-164

dots[i].setTextColor(colorsInactive[currentPage]);
            dotsLayout.addView(dots[i]);
        }

        if (dots.length > 0)
            dots[currentPage].setTextColor(colorsActive[currentPage]);
    }

    private int getItem(int i) {
        return viewPager.getCurrentItem() + i;
    }

    private void launchHomeScreen() {
        Intent intent=new Intent(InfoNew.this, MainActivity.class);
        intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
        startActivity(intent);
        finish();
    }

    //  viewpager change listener
    ViewPager.OnPageChangeListener viewPagerPageChangeListener = new ViewPager.OnPageChangeListener() {

        @Override
        public void onPageSelected(int position) {
            addBottomDots(position);

            // changing the next button text 'NEXT' / 'GOT IT'
            if (position == layouts.length - 1) {
                // last page. make button text to GOT IT
                btnNext.setText("GOT IT");
            } else {
                // still pages are left
                btnNext.setText("NEXT");
            }
        }

        @Override
        public void onPageScrolled(int arg0, float arg1, int arg2) {

        }

        @Override
        public void onPageScrollStateChanged(int arg0) {

        }
    };

    /**
     * Making notification bar transparent
     */
PATH: app/src/main/java/com/codingclub/daancorona/InfoNew.java
LINES: 165-210

private void changeStatusBarColor() {
        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {
            Window window = getWindow();
            window.addFlags(WindowManager.LayoutParams.FLAG_DRAWS_SYSTEM_BAR_BACKGROUNDS);
            window.setStatusBarColor(Color.TRANSPARENT);
        }
    }

    /**
     * View pager adapter
     */
    public class MyViewPagerAdapter extends PagerAdapter {
        private LayoutInflater layoutInflater;

        public MyViewPagerAdapter() {
        }

        @Override
        public Object instantiateItem(ViewGroup container, int position) {
            layoutInflater = (LayoutInflater) getSystemService(Context.LAYOUT_INFLATER_SERVICE);

            View view = layoutInflater.inflate(layouts[position], container, false);
            container.addView(view);

            return view;
        }

        @Override
        public int getCount() {
            return layouts.length;
        }

        @Override
        public boolean isViewFromObject(View view, Object obj) {
            return view == obj;
        }


        @Override
        public void destroyItem(ViewGroup container, int position, Object object) {
            View view = (View) object;
            container.removeView(view);
        }
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/InstructionsSlider.java
LINES: 1-44

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;
import androidx.viewpager.widget.PagerAdapter;
import androidx.viewpager.widget.ViewPager;

import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.graphics.Color;
import android.os.Build;
import android.os.Bundle;
import android.text.Html;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.view.Window;
import android.view.WindowManager;
import android.widget.Button;
import android.widget.LinearLayout;
import android.widget.TextView;


public class InstructionsSlider extends AppCompatActivity {

    private ViewPager viewPager;
    private MyViewPagerAdapter myViewPagerAdapter;
    private LinearLayout dotsLayout;
    private TextView[] dots;
    private int[] layouts;
    private Button btnNext;
    private PrefManager prefManager;
    SharedPreferences sharedPref;
    String token;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

        // Making notification bar transparent
        if (Build.VERSION.SDK_INT >= 21) {
            getWindow().getDecorView().setSystemUiVisibility(View.SYSTEM_UI_FLAG_LAYOUT_STABLE | View.SYSTEM_UI_FLAG_LAYOUT_FULLSCREEN);
        }
PATH: app/src/main/java/com/codingclub/daancorona/InstructionsSlider.java
LINES: 45-84

setContentView(R.layout.activity_instructions_slider);

        prefManager = new PrefManager(this);
        if (!prefManager.isFirstTimeLaunch() ) {
            prefManager.setFirstTimeLaunch(false);
            launchHomeScreen();
            finish();
        }

        sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        if(sharedPref.getString("Lang","").equals("hin")){
            layouts = new int[]{
                    R.layout.fragment_sliderhn1,
                    R.layout.fragment_sliderhn2,
                    R.layout.fragment_sliderhn3,
                    R.layout.fragment_sliderhn4,
                    R.layout.fragment_sliderhn5};
        }
        else{
            layouts = new int[]{
                    R.layout.fragment_slider1,
                    R.layout.fragment_slider2,
                    R.layout.fragment_slider3,
                    R.layout.fragment_slider4,
                    R.layout.fragment_slider5};
        }




        viewPager = (ViewPager) findViewById(R.id.view_pager);

        DepthTransformation depthTransformation = new DepthTransformation();
        viewPager.setPageTransformer(true,depthTransformation);
        dotsLayout = (LinearLayout) findViewById(R.id.layoutDots);
        btnNext = (Button) findViewById(R.id.btn_next);
PATH: app/src/main/java/com/codingclub/daancorona/InstructionsSlider.java
LINES: 85-129

// layouts of all welcome sliders
        // add few more layouts if you want






        // adding bottom dots
        addBottomDots(0);

        // making notification bar transparent
        changeStatusBarColor();

        myViewPagerAdapter = new MyViewPagerAdapter();
        viewPager.setAdapter(myViewPagerAdapter);
        viewPager.addOnPageChangeListener(viewPagerPageChangeListener);

        btnNext.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                // checking for last page
                // if last page home screen will be launched
                int current = getItem(+1);
                if (current < layouts.length) {
                    // move to next screen
                    viewPager.setCurrentItem(current);
                } else {
                    launchHomeScreen();
                }
            }
        });

    }

    private void addBottomDots(int currentPage) {
        dots = new TextView[layouts.length];

        int[] colorsActive = getResources().getIntArray(R.array.array_dot_active);
        int[] colorsInactive = getResources().getIntArray(R.array.array_dot_inactive);

        dotsLayout.removeAllViews();
        for (int i = 0; i < dots.length; i++) {
            dots[i] = new TextView(this);
            dots[i].setText(Html.fromHtml("&#8226;"));
PATH: app/src/main/java/com/codingclub/daancorona/InstructionsSlider.java
LINES: 130-178

dots[i].setTextSize(35);
            dots[i].setTextColor(colorsInactive[currentPage]);
            dotsLayout.addView(dots[i]);
        }

        if (dots.length > 0)
            dots[currentPage].setTextColor(colorsActive[currentPage]);
    }

    private int getItem(int i) {
        return viewPager.getCurrentItem() + i;
    }

    private void launchHomeScreen() {
        Intent intent=new Intent(InstructionsSlider.this, WelcomePageActivity.class);
        intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
        startActivity(intent);
        finish();
    }

    //  viewpager change listener
    ViewPager.OnPageChangeListener viewPagerPageChangeListener = new ViewPager.OnPageChangeListener() {

        @Override
        public void onPageSelected(int position) {
            addBottomDots(position);

            // changing the next button text 'NEXT' / 'GOT IT'
            if (position == layouts.length - 1) {
                // last page. make button text to GOT IT
                btnNext.setText("GOT IT");
            } else {
                // still pages are left
                btnNext.setText("NEXT");
            }
        }

        @Override
        public void onPageScrolled(int arg0, float arg1, int arg2) {

        }

        @Override
        public void onPageScrollStateChanged(int arg0) {

        }
    };

    /**
PATH: app/src/main/java/com/codingclub/daancorona/InstructionsSlider.java
LINES: 179-226

* Making notification bar transparent
     */
    private void changeStatusBarColor() {
        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP) {
            Window window = getWindow();
            window.addFlags(WindowManager.LayoutParams.FLAG_DRAWS_SYSTEM_BAR_BACKGROUNDS);
            window.setStatusBarColor(Color.TRANSPARENT);
        }
    }

    /**
     * View pager adapter
     */
    public class MyViewPagerAdapter extends PagerAdapter {
        private LayoutInflater layoutInflater;

        public MyViewPagerAdapter() {
        }

        @Override
        public Object instantiateItem(ViewGroup container, int position) {
            layoutInflater = (LayoutInflater) getSystemService(Context.LAYOUT_INFLATER_SERVICE);

            View view = layoutInflater.inflate(layouts[position], container, false);
            container.addView(view);

            return view;
        }

        @Override
        public int getCount() {
            return layouts.length;
        }

        @Override
        public boolean isViewFromObject(View view, Object obj) {
            return view == obj;
        }


        @Override
        public void destroyItem(ViewGroup container, int position, Object object) {
            View view = (View) object;
            container.removeView(view);
        }
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/Item.java
LINES: 1-26

package com.codingclub.daancorona;

import java.sql.Timestamp;

public class Item {
    private String name,amount,uid;

    public Item(String name, String amount,String uid) {
        this.name = name;
        this.amount = amount;
        this.uid=uid;

    }

    public String getName() {
        return name;
    }

    public String getAmount() {
        return amount;
    }

    public String getUid() {
        return uid;
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/ItemAdapter.java
LINES: 1-49

package com.codingclub.daancorona;

import android.content.Context;
import android.content.SharedPreferences;
import android.os.AsyncTask;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.Button;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.google.android.gms.common.util.Strings;

import org.json.JSONException;
import org.json.JSONObject;

import java.io.IOException;
import java.util.ArrayList;

import okhttp3.FormBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

import static android.content.Context.MODE_PRIVATE;

public class ItemAdapter extends RecyclerView.Adapter<ItemAdapter.ViewHolder> {
    private View view;
    ArrayList<Item> list;
    Context context;
    public ItemAdapter(ArrayList<Item> list, Context context){
        this.list=list;
        this.context=context;
    }

    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        view= LayoutInflater.from(parent.getContext()).inflate(R.layout.list_item,parent,false);
        return new ViewHolder(view);
    }

    @Override
PATH: app/src/main/java/com/codingclub/daancorona/ItemAdapter.java
LINES: 50-88

public void onBindViewHolder(@NonNull ItemAdapter.ViewHolder holder, int position) {

        SharedPreferences sharedPref=context.getSharedPreferences("User",MODE_PRIVATE);
        String token=sharedPref.getString("Token","");
        holder.name.setText(list.get(position).getName());
        holder.amount.setText( ""+list.get(position).getAmount());

//        if(sharedPref.getString("Lang","").equals("hin")){
//            holder.name.setText(TranslateTo.getTranslation(list.get(position).getName(),context));
//            holder.amount.setText(" "+TranslateTo.getTranslation(list.get(position).getAmount(),context));
//        }


        holder.thanks.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                //new Populate().execute(list.get(position).getUid(),token);
                //Toast.makeText(context,"Sent!!!",Toast.LENGTH_LONG).show();

            }
        });
    }

    @Override
    public int getItemCount() {
        return list.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView name,UiD,amount;
        Button thanks;

        public ViewHolder(@NonNull View itemView) {

            super(itemView);
            name=itemView.findViewById(R.id.name);
            amount=itemView.findViewById(R.id.amount);
PATH: app/src/main/java/com/codingclub/daancorona/ItemAdapter.java
LINES: 89-137

thanks=itemView.findViewById(R.id.thanks);
        }
    }

    class Populate extends AsyncTask<String,Void,String>{

        @Override
        protected String doInBackground(String... strings) {


            final OkHttpClient httpClient = new OkHttpClient();

            RequestBody formbody=new FormBody.Builder()
                    .addEncoded("donor_id",strings[0])
                    .build();

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/send_thanks/")
                    .addHeader("Authorization","JWT "+strings[1])
                    .post(formbody)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                Log.d("Tag",response.body()+"");

                return "Done";

            } catch (IOException e) {
                e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String s) {

            super.onPostExecute(s);
            if(s!=null)
                Toast.makeText(context,"Sent!!!",Toast.LENGTH_LONG).show();
            else
                Toast.makeText(context,"Error",Toast.LENGTH_LONG).show();

        }
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/LanguageSelectActivity.java
LINES: 1-43

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.app.Activity;
import android.content.Intent;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;
import android.widget.Toast;
import android.view.View;
import android.widget.RelativeLayout;


public class LanguageSelectActivity extends AppCompatActivity {

    Button langEng,langHindi;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_language_select);

        langEng = findViewById(R.id.laguage_english);
        langHindi = findViewById(R.id.laguage_hindi);


        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        if(!sharedPref.getString("Lang","").equals("")){
            startActivity(new Intent(LanguageSelectActivity.this,LoginActivity.class));
            finish();
        }

        SharedPreferences.Editor editor=sharedPref.edit();

        langEng.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                editor.putString("Lang","eng");
                editor.apply();
                Intent i = new Intent(LanguageSelectActivity.this, InstructionsSlider.class);
PATH: app/src/main/java/com/codingclub/daancorona/LanguageSelectActivity.java
LINES: 44-62

startActivity(i);
                finish();
            }
        });
        langHindi.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                editor.putString("Lang","hin");
                editor.apply();
              //  Toast.makeText(LanguageSelectActivity.this, "Feature yet to be added", Toast.LENGTH_SHORT).show();
                startActivity(new Intent(LanguageSelectActivity.this,InstructionsSlider.class));
                finish();

            }
        });

    }

}
PATH: app/src/main/java/com/codingclub/daancorona/LoadingDialog.java
LINES: 1-30

package com.codingclub.daancorona;

import android.app.Activity;
import android.app.AlertDialog;
import android.view.LayoutInflater;


class LoadingDialog {

    private Activity activity;
    private AlertDialog dialog;
    LoadingDialog(Activity activity){
        this.activity=activity;
    }

    void startloadingDialog(){
        AlertDialog.Builder builder=new AlertDialog.Builder(activity);

        LayoutInflater layoutInflater=activity.getLayoutInflater();
        builder.setView(layoutInflater.inflate(R.layout.loading,null));
        builder.setCancelable(false);

        dialog=builder.create();
        dialog.show();
    }

    void dismissDialog(){
        dialog.dismiss();
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 1-49

package com.codingclub.daancorona;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.os.CountDownTimer;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.RequiresApi;
import androidx.appcompat.app.AppCompatActivity;

import com.google.android.material.textfield.TextInputEditText;
import com.google.android.material.textfield.TextInputLayout;

import org.json.JSONException;
import org.json.JSONObject;

import java.io.IOException;

import okhttp3.FormBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

public class LoginActivity extends AppCompatActivity {

    EditText editTxtPhone, editTxtOtp;
    TextInputLayout editTxtPhone1,editTxtOtp1;
    Button btnSendotp, btnVerifyOtp,resend;
    LoadingDialog dialog;
    TextView timer;
    private int totalTimeCountInMilliseconds;
    OTPDialog otpDialog;
    private CountDownTimer countDownTimer;
  //  TextView textOtp,textPhone;
    String codeSent,code,phoneNumber;
    boolean newuser;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 50-83

setContentView(R.layout.activity_login);

       // textPhone = findViewById(R.id.txt_phone);
        editTxtPhone =findViewById(R.id.editTxt_phone);
        btnSendotp = findViewById(R.id.btn_send_otp);

        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);

      //  textOtp = findViewById(R.id.txt_otp);
        editTxtOtp = findViewById(R.id.edit_txt_otp);
        btnVerifyOtp = findViewById(R.id.btn_verify_otp);
        editTxtOtp1=findViewById(R.id.edit_txt_otp1);
        editTxtPhone1=findViewById(R.id.editTxt_phone1);
        timer=findViewById(R.id.timer);
        resend=findViewById(R.id.resend);

        editTxtOtp.setVisibility(View.GONE);
        btnVerifyOtp.setVisibility(View.GONE);
       // textOtp.setVisibility(View.GONE);

        dialog=new LoadingDialog(this);
        if(sharedPref.getString("Lang","").equals("hin")){
            btnSendotp.setText(getResources().getString(R.string.getotp));
            btnVerifyOtp.setText(getResources().getString(R.string.verifyotp));
            editTxtPhone1.setHint(getResources().getString(R.string.enterphn));
            editTxtOtp1.setHint(getResources().getString(R.string.enterotp));
            resend.setText(" ");

        }

        resend.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 84-122

btnSendotp.setVisibility(View.VISIBLE);
                editTxtPhone.setVisibility(View.VISIBLE);
                resend.setVisibility(View.GONE);
                editTxtOtp.setVisibility(View.GONE);
                btnVerifyOtp.setVisibility(View.GONE);
            }
        });

        btnSendotp.setOnClickListener(new View.OnClickListener() {

            @Override
            public void onClick(View view) {



                phoneNumber = editTxtPhone.getText().toString().trim();
                if(phoneNumber.length()==10) {
                    dialog.startloadingDialog();
                    new GetOtpTask().execute(phoneNumber);
                }
                else
                    Toast.makeText(getApplicationContext(),"Invalid phone number",Toast.LENGTH_SHORT).show();
            }
        });

        btnVerifyOtp.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                dialog.startloadingDialog();
                code= editTxtOtp.getText().toString();
                    new VerifyOtpTask().execute(phoneNumber,code);
                //verifySignIn();
            }
        });
    }
    class GetOtpTask extends AsyncTask<String,Void,String>{

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 123-160

protected String doInBackground(String... strings) {

             final OkHttpClient httpClient = new OkHttpClient();
             Log.d("Ph No.",strings[0]);
                RequestBody formbody=new FormBody.Builder()
                        .addEncoded("mobile",strings[0])
                        .build();

                Request request = new Request.Builder()
                        .url("https://daancorona.tech/api/mobile/")
                        .post(formbody)
                        .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);
                JSONObject jsonObject=new JSONObject(response.body().string());

//                codeSent=jsonObject.getString("otp");

                Log.d("Tag",response.body()+"");

//                JSONObject jsonObject=new JSONObject(response.body().string());
//                codeSent= jsonObject.getString("otp");

                return "Done";

            } catch (IOException | JSONException e) {
                e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String s) {

            //Toast.makeText(getApplicationContext(),"code:"+s,Toast.LENGTH_LONG).show();
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 161-200

super.onPostExecute(s);
            dialog.dismissDialog();

            if(s==null)
                Toast.makeText(LoginActivity.this,"Error",Toast.LENGTH_SHORT).show();

            else {
                editTxtOtp.setVisibility(View.VISIBLE);
                btnVerifyOtp.setVisibility(View.VISIBLE);
                // textOtp.setVisibility(View.VISIBLE);
                editTxtPhone.setText("");
                btnSendotp.setVisibility(View.GONE);
                editTxtPhone.setVisibility(View.GONE);
                setTimer();
                // textPhone.setVisibility(View.GONE);
            }
        }
    }


    class VerifyOtpTask extends AsyncTask<String,Void,String[]>{

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected String[] doInBackground(String... strings) {

            String access="",refresh="";

            final OkHttpClient httpClient = new OkHttpClient();

            RequestBody formbody=new FormBody.Builder()
                    .addEncoded("mobile",strings[0])
                    .addEncoded("token",strings[1])
                    .build();

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/otp/")
                    .post(formbody)
                    .build();
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 201-240

try (okhttp3.Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                Log.d("Tag",response.body()+"");

                JSONObject jsonObject=new JSONObject(response.body().string());
                JSONObject jsonObject1=jsonObject.getJSONObject("token");

                access=jsonObject1.getString("access");
                refresh=jsonObject1.getString("refresh");

                newuser=jsonObject.getBoolean("newUser");
                Log.d("NewUser",newuser+"");

            } catch (IOException | JSONException e) {
                access=null;
                e.printStackTrace();
                return null;
            }

            return new String[]{access, refresh};
        }

        @Override
        protected void onPostExecute(String... s) {

            super.onPostExecute(s);
            dialog.dismissDialog();

            if(s==null ) {
                Toast.makeText(LoginActivity.this, "Error!!!", Toast.LENGTH_SHORT).show();
                return;
            }

            SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
            SharedPreferences.Editor editor=sharedPref.edit();
            editor.putString("Token",s[0]);
            editor.putString("Token1",s[1]);
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 241-277

editor.apply();
//            Toast.makeText(getApplicationContext(), "Token: "+s, Toast.LENGTH_SHORT).show();

            if(!newuser) {

                Intent i;

                if(!sharedPref.getBoolean("Page1",false))
                    i=new Intent(LoginActivity.this,PersonalInfoActivity.class);
                else if(!sharedPref.getBoolean("Page2",false))
                    i=new Intent(LoginActivity.this,ShopInfoActivity.class);
                else if(!sharedPref.getBoolean("Page3",false))
                    i=new Intent(LoginActivity.this,PaymentModeActivity.class);
                else
                    i = new Intent(LoginActivity.this, MainActivity.class);

                i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                startActivity(i);
                finish();
            }
            else{

                editor.putBoolean("Page1",false);
                editor.putBoolean("Page2",false);
                editor.putBoolean("Page3",false);
                editor.apply();

                Intent i = new Intent(LoginActivity.this, PersonalInfoActivity.class);
                i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                startActivity(i);
                finish();
            }

        }
    }

    private void setTimer() {
PATH: app/src/main/java/com/codingclub/daancorona/LoginActivity.java
LINES: 278-311

totalTimeCountInMilliseconds=20;
        //Toast.makeText(getContext(), "Please Enter Minutes...",
        //      Toast.LENGTH_LONG).show();
        totalTimeCountInMilliseconds = totalTimeCountInMilliseconds * 1000;
        //totalTimeCountInMilliseconds = 60 * time * 1000;

        startTimer();
    }

    private void startTimer() {
        timer.setVisibility(View.VISIBLE);
        countDownTimer = new CountDownTimer(totalTimeCountInMilliseconds, 1000) {
            // 500 means, onTick function will be called at every 500
            // milliseconds
            @Override
            public void onFinish() {

                resend.setVisibility(View.VISIBLE);
                timer.setVisibility(View.GONE);
            }
            @Override
            public void onTick(long leftTimeInMilliseconds) {

                SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
                if(sharedPref.getString("Lang","").equals("hin"))
                    timer.setText(" " +leftTimeInMilliseconds/1000+ "   otp     ");
                else
                    timer.setText("You can resend otp in "+leftTimeInMilliseconds/1000+"s");
            }

        }.start();
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 1-52

package com.codingclub.daancorona;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;
import androidx.core.content.FileProvider;

import android.Manifest;
import android.app.DownloadManager;
import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;
import android.content.IntentFilter;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.Environment;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.Toast;


import org.jetbrains.annotations.NotNull;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.File;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.InputStream;
import java.net.HttpURLConnection;
import java.net.URL;

import okhttp3.Call;
import okhttp3.Callback;
import okhttp3.FormBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;
import okhttp3.ResponseBody;
import okio.Buffer;
import okio.BufferedSink;
import okio.BufferedSource;
import okio.Okio;

import static java.security.AccessController.getContext;
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 53-91

public class LogoActivity extends AppCompatActivity {

    private static final int MY_GALLERY_REQUEST_CODE = 1;
    Button btnStart;
    String access,refresh,VERSION_APP="1.1";
    LoadingDialog dialog;
    SharedPreferences sharedPref;
    Uri uri;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_logo);
//
//        AppUpdater appUpdater = new AppUpdater(this)
//                .setUpdateFrom();
//        appUpdater.start();

        sharedPref = getSharedPreferences("User",MODE_PRIVATE);
        access=sharedPref.getString("Token","");
        refresh=sharedPref.getString("Token1","");//Update this for every commit

        btnStart = findViewById(R.id.btn_start);
        dialog=new LoadingDialog(this);

        checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE);
        //updateapk();

        //checkToken();
        btnStart.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {

                dialog.startloadingDialog();

                OkHttpClient client = new OkHttpClient();

                Request request = new Request.Builder()
                        .header("Authorization","JWT "+access)
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 92-117

.url("https://daancorona.tech/api/auth/")
                        .build();

                client.newCall(request).enqueue(new Callback() {
                    @Override
                    public void onFailure(@NotNull Call call, @NotNull IOException e) {

                        if(access.equals("")){
                            Intent i = new Intent(LogoActivity.this, LanguageSelectActivity.class);
                            startActivity(i);
                            finish();
                        }
                    }

                    @Override
                    public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                        if(response.code()==200 || response.code()==201) {
                            dialog.dismissDialog();
                            Log.d("Ekdum sahi","yayyy");
                            Intent i;
                            if(sharedPref.getString("Lang","").equals(""))
                                i=new Intent(LogoActivity.this,LanguageSelectActivity.class);
                            else if(!sharedPref.getBoolean("Page1",false))
                                i=new Intent(LogoActivity.this,LoginActivity.class);
                            else if(!sharedPref.getBoolean("Page2",false))
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 118-144

i=new Intent(LogoActivity.this,ShopInfoActivity.class);
                            else if(!sharedPref.getBoolean("Page3",false))
                                i=new Intent(LogoActivity.this,PaymentModeActivity.class);
                            else
                                i = new Intent(LogoActivity.this, MainActivity.class);
                            startActivity(i);
                            finish();
                        }
                        else {
                            OkHttpClient client1 = new OkHttpClient();

                            RequestBody formbody = new FormBody.Builder()
                                    .addEncoded("refresh", refresh)
                                    .build();

                            Request request = new Request.Builder()
                                    .url("https://daancorona.tech/api/refresh/")
                                    .post(formbody)
                                    .build();

                            client1.newCall(request).enqueue(new Callback() {
                                @Override
                                public void onFailure(@NotNull Call call, @NotNull IOException e) {
                                    dialog.dismissDialog();
                                }

                                @Override
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 145-163

public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                                    if(response.code()==200 || response.code()==201) {
                                        try {

                                            JSONObject jsonObject = new JSONObject(response.body().string());
                                            access = jsonObject.getString("access");
                                            SharedPreferences.Editor editor = sharedPref.edit();

                                            if (access != null) {
                                                editor.putString("Token", access);
                                                editor.apply();

                                                Intent i;
                                                Log.d("Ekdum sahi", "kinda");
                                                if(!sharedPref.getBoolean("Page1",false))
                                                    i=new Intent(LogoActivity.this,LanguageSelectActivity.class);
                                                else if(!sharedPref.getBoolean("Page2",false))
                                                    i=new Intent(LogoActivity.this,ShopInfoActivity.class);
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 164-187

else if(!sharedPref.getBoolean("Page3",false))
                                                    i=new Intent(LogoActivity.this,PaymentModeActivity.class);
                                                else
                                                    i = new Intent(LogoActivity.this, MainActivity.class);

                                                dialog.dismissDialog();
                                                startActivity(i);
                                                finish();
                                            }

                                        } catch (JSONException ex) {

                                            LogoActivity.this.runOnUiThread(new Runnable() {
                                                @Override
                                                public void run() {
                                                    dialog.dismissDialog();
                                                    Toast.makeText(LogoActivity.this, "Error", Toast.LENGTH_SHORT).show();
                                                }
                                            });
                                            ex.printStackTrace();

                                        }
                                    }
                                    else{
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 188-227

Log.d("Ekdum sahi","galat");
                                        dialog.dismissDialog();

                                        Intent i = new Intent(LogoActivity.this, LanguageSelectActivity.class);
                                        startActivity(i);
                                        finish();
                                    }
                                }
                            });
                        }
                    }
                });

            }
        });
    }

    private void checkToken(){

        OkHttpClient client = new OkHttpClient();

        Request request = new Request.Builder()
                .header("Authorization","JWT "+access)
                .url("https://daancorona.tech/api/auth/")
                .build();

        client.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {

                if(access.equals("")){
                    Intent i = new Intent(LogoActivity.this, LanguageSelectActivity.class);
                    startActivity(i);
                    finish();
                }
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 228-253

if(response.code()==200 || response.code()==201) {
                    dialog.dismissDialog();
                    Log.d("Ekdum sahi","yayyy");
                    Intent i;
                    if(sharedPref.getString("Lang","").equals(""))
                        i=new Intent(LogoActivity.this,LanguageSelectActivity.class);
                    else if(!sharedPref.getBoolean("Page1",false))
                        i=new Intent(LogoActivity.this,LoginActivity.class);
                    else if(!sharedPref.getBoolean("Page2",false))
                        i=new Intent(LogoActivity.this,ShopInfoActivity.class);
                    else if(!sharedPref.getBoolean("Page3",false))
                        i=new Intent(LogoActivity.this,PaymentModeActivity.class);
                    else
                        i = new Intent(LogoActivity.this, MainActivity.class);
                    startActivity(i);
                    finish();
                }
                else {
                    OkHttpClient client1 = new OkHttpClient();

                    RequestBody formbody = new FormBody.Builder()
                            .addEncoded("refresh", refresh)
                            .build();

                    Request request = new Request.Builder()
                            .url("https://daancorona.tech/api/refresh/")
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 254-280

.post(formbody)
                            .build();

                    client1.newCall(request).enqueue(new Callback() {
                        @Override
                        public void onFailure(@NotNull Call call, @NotNull IOException e) {
                            dialog.dismissDialog();
                        }

                        @Override
                        public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                            if(response.code()==200 || response.code()==201) {
                                try {

                                    JSONObject jsonObject = new JSONObject(response.body().string());
                                    access = jsonObject.getString("access");
                                    SharedPreferences.Editor editor = sharedPref.edit();

                                    if (access != null) {
                                        editor.putString("Token", access);
                                        editor.apply();

                                        Intent i;
                                        Log.d("Ekdum sahi", "kinda");
                                        if(!sharedPref.getBoolean("Page1",false))
                                            i=new Intent(LogoActivity.this,LanguageSelectActivity.class);
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 281-305

else if(!sharedPref.getBoolean("Page2",false))
                                            i=new Intent(LogoActivity.this,ShopInfoActivity.class);
                                        else if(!sharedPref.getBoolean("Page3",false))
                                            i=new Intent(LogoActivity.this,PaymentModeActivity.class);
                                        else
                                            i = new Intent(LogoActivity.this, MainActivity.class);

                                        dialog.dismissDialog();
                                        startActivity(i);
                                        finish();
                                    }

                                } catch (JSONException ex) {

                                    LogoActivity.this.runOnUiThread(new Runnable() {
                                        @Override
                                        public void run() {
                                            dialog.dismissDialog();
                                            Toast.makeText(LogoActivity.this, "Error", Toast.LENGTH_SHORT).show();
                                        }
                                    });
                                    ex.printStackTrace();

                                }
                            }
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 306-342

else{
                                Log.d("Ekdum sahi","galat");
                                dialog.dismissDialog();

                            }
                        }
                    });
                }
            }
        });
    }

    private void updateapk(){
        OkHttpClient client = new OkHttpClient();
        dialog.startloadingDialog();
        Request request = new Request.Builder()
                .url("https://daancorona.tech/api/app_update/")
                .build();

        client.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {
                LogoActivity.this.runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        dialog.dismissDialog();
                        Toast.makeText(LogoActivity.this, "Error", Toast.LENGTH_SHORT).show();
                    }
                });
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {
                try {
                    JSONObject jsonObject=new JSONObject(response.body().string());
                    if(VERSION_APP.equals(jsonObject.getString("version")))
                        checkToken();
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 343-382

else {
                        downloadapk();
                    }

                } catch (JSONException e) {
                    e.printStackTrace();
                    LogoActivity.this.runOnUiThread(new Runnable() {
                        @Override
                        public void run() {
                            dialog.dismissDialog();
                            Toast.makeText(LogoActivity.this, "Error", Toast.LENGTH_SHORT).show();
                        }
                    });
                }
            }
        });
    }

    private void downloadapk(){

        LogoActivity.this.runOnUiThread(new Runnable() {
            @Override
            public void run() {


                String destination = Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_DOWNLOADS) + "/";
                String fileName = "DaanCorona.apk";
                destination += fileName;

                Log.d("file",destination);

                //Delete update file if exists
                File file = new File(destination);
                if (file.exists())
                    file.delete(); //- test this, I think sometimes it doesnt work

                uri=Uri.fromFile(file);

                String url="https://daancorona.tech/download/DaanCorona.apk";
                //get url of app on server
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 383-407

final String dest=destination;

                //set downloadmanager
                DownloadManager.Request request = new DownloadManager.Request(Uri.parse(url));
                request.setDescription("Update the app");
                request.setTitle("App Update");

                if(android.os.Build.VERSION.SDK_INT >= Build.VERSION_CODES.N){
                    uri=FileProvider.getUriForFile(LogoActivity.this,BuildConfig.APPLICATION_ID +".provider",file);
                }
                //set destination
                Log.d("Uri",uri.toString());
                //request.setDestinationUri(uri);
                request.setDestinationInExternalPublicDir(Environment.DIRECTORY_DOWNLOADS,"DaanCorona.apk");

                // get download service and enqueue file
                final DownloadManager manager = (DownloadManager) getSystemService(Context.DOWNLOAD_SERVICE);
                final long downloadId = manager.enqueue(request);

                //set BroadcastReceiver to install app when .apk is downloaded
                BroadcastReceiver onComplete = new BroadcastReceiver() {
                    public void onReceive(Context ctxt, Intent intent) {
                        dialog.dismissDialog();
                        Intent install = new Intent(Intent.ACTION_VIEW);
                        install.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 408-446

if(android.os.Build.VERSION.SDK_INT >= Build.VERSION_CODES.N)
                            install.addFlags(Intent.FLAG_GRANT_READ_URI_PERMISSION);
                        else
                            install.setDataAndType(uri,manager.getMimeTypeForDownloadedFile(downloadId));

                        install.setData(uri);
                        startActivity(install);
                        Log.d("Yaay","Mofos!!!");
                        unregisterReceiver(this);
                        finish();

                    }
                };
                //register receiver for when .apk download is compete
                registerReceiver(onComplete, new IntentFilter(DownloadManager.ACTION_DOWNLOAD_COMPLETE));
            }
        });

    }

    public void checkPermission(String permission, int requestCode)
    {
        // Checking if permission is not granted
        if (ContextCompat.checkSelfPermission(
                this,
                permission)
                == PackageManager.PERMISSION_DENIED) {
            ActivityCompat
                    .requestPermissions(
                            this,
                            new String[] { permission },
                            requestCode);
        }
        else{
            updateapk();
        }

    }
    @Override
PATH: app/src/main/java/com/codingclub/daancorona/LogoActivity.java
LINES: 447-473

public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults)
    {
        super.onRequestPermissionsResult(requestCode,
                permissions,
                grantResults);

            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Location Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();
                updateapk();

            }
            else {
                Toast.makeText(this,
                        "Location Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
                finishAffinity();
            }

    }
}
PATH: app/src/main/java/com/codingclub/daancorona/MOUActivity.java
LINES: 1-38

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;
import android.widget.TextView;


import java.text.SimpleDateFormat;
import java.util.Calendar;
import java.util.Date;

public class MOUActivity extends AppCompatActivity {
    Button btnMou;
    TextView MOU;
    SharedPreferences sharedPref;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_mou);
        btnMou = findViewById(R.id.btn_mou_accepted);
        MOU=findViewById(R.id.MOU);
        Date c = Calendar.getInstance().getTime();
        sharedPref = getSharedPreferences("User", MODE_PRIVATE);

        SimpleDateFormat df = new SimpleDateFormat("dd-MMM-yyyy");
        String formattedDate = df.format(c);

        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);

        if(sharedPref.getString("Lang","").equals("hin")){
            btnMou.setText(getResources().getString(R.string.agree));
            MOU.setText(getResources().getString(R.string.HinMou1)+" "+sharedPref.getString("shopName","")+" "+
                    getResources().getString(R.string.HinMou2)+" "+formattedDate+" "+
PATH: app/src/main/java/com/codingclub/daancorona/MOUActivity.java
LINES: 39-62

getResources().getString(R.string.HinMou3)+" "+formattedDate+"\n"
                    +" :\n"+sharedPref.getString("Name","")+"\n"
                    +sharedPref.getString("shopName","")+"\n\n"+
                    "DaanCorona");
        }
        else{
            MOU.setText(getResources().getString(R.string.mouDetails1)+" "+sharedPref.getString("shopName","")+" "+
                            getResources().getString(R.string.mouDetails2)+" "+formattedDate+" "+
                    getResources().getString(R.string.mouDetails3)+" "+formattedDate+"\n"
            +"Signed by:\n"+sharedPref.getString("Name","")+"\n"
            +sharedPref.getString("shopName","")+"\n\n"+
                    "DaanCorona");
        }

        btnMou.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                Intent i = new Intent(MOUActivity.this, MainActivity.class);
                startActivity(i);
                finish();
            }
        });
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 1-46

package com.codingclub.daancorona;

import android.app.AlertDialog;
import android.content.DialogInterface;
import android.content.Intent;
import android.content.SharedPreferences;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.util.Log;
import android.view.MenuItem;
import android.view.View;
import android.widget.Button;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.annotation.RequiresApi;
import androidx.appcompat.app.ActionBar;
import androidx.appcompat.app.ActionBarDrawerToggle;
import androidx.appcompat.app.AppCompatActivity;
import androidx.appcompat.widget.Toolbar;
import androidx.core.view.GravityCompat;
import androidx.drawerlayout.widget.DrawerLayout;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;
import androidx.swiperefreshlayout.widget.SwipeRefreshLayout;

import com.google.android.material.navigation.NavigationView;

import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.IOException;
import java.util.ArrayList;

import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.Response;

public class MainActivity extends AppCompatActivity {
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 47-84

private Toolbar toolbar;

    RecyclerView recyclerView;
    ItemAdapter itemAdapter;
    String nametxt, net, maxcred;
    TextView name, maxcredit, netamt, maxcredittxt, netamttxt, donation,nodonation;
    String token;
    Button transaction;
    SharedPreferences sharedPref;
    private SwipeRefreshLayout mSwipeRefreshLayout;

    private DrawerLayout dl;
    private NavigationView nv;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);

        toolbar = findViewById(R.id.toolbar);
        dl = (DrawerLayout)findViewById(R.id.dl);
        setSupportActionBar(toolbar);


        final ActionBar actionBar = getSupportActionBar();
        actionBar.setDisplayHomeAsUpEnabled(true);
        actionBar.setHomeAsUpIndicator(R.drawable.ic_menu_black_24dp);

        nv = (NavigationView)findViewById(R.id.nv);
        nv.setNavigationItemSelectedListener(new NavigationView.OnNavigationItemSelectedListener() {
            @Override
            public boolean onNavigationItemSelected(@NonNull MenuItem item) {
                int id = item.getItemId();
                switch(id)
                {
                    case R.id.edit_profile:
                        startActivity(new Intent(MainActivity.this,EditProfile.class));
                        dl.closeDrawers();
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 85-121

break;
                    case R.id.contact_us:
                        startActivity(new Intent(MainActivity.this,Contact.class));
                        dl.closeDrawers();
                        break;
                    case R.id.info:
                        startActivity(new Intent(MainActivity.this,InfoNew.class));
                        dl.closeDrawers();
                        break;
                    case R.id.terms:
                        startActivity(new Intent(MainActivity.this,PDFActivity.class));
                        dl.closeDrawers();
                        break;
                    default:
                        return true;
                }
                return true;

            }
        });


        sharedPref = getSharedPreferences("User", MODE_PRIVATE);
        token = sharedPref.getString("Token", "");

        name = findViewById(R.id.name);
        maxcredittxt = findViewById(R.id.tgttext);
        netamttxt = findViewById(R.id.blctext);
        donation = findViewById(R.id.text);
        transaction = findViewById(R.id.transc);
        nodonation=findViewById(R.id.nodon);

        maxcredit = findViewById(R.id.target);
        netamt = findViewById(R.id.balance);

        if (sharedPref.getString("Lang", "").equals("hin")) {
            maxcredittxt.setText(" ");
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 122-164

netamttxt.setText(getResources().getString(R.string.netamt));
            donation.setText(getResources().getString(R.string.donations));
            transaction.setText(getResources().getString(R.string.transactions));
        }

        mSwipeRefreshLayout = findViewById(R.id.swiperefresh_items);
        mSwipeRefreshLayout.setOnRefreshListener(new SwipeRefreshLayout.OnRefreshListener() {
            @Override
            public void onRefresh() {
                new SetProfile().execute();
                new SetRecyclerView().execute();
            }
        });

        recyclerView = findViewById(R.id.recyclerview);
        transaction.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                startActivity(new Intent(MainActivity.this, Transactions.class));
            }
        });

        new SetProfile().execute();
        new SetRecyclerView().execute();

    }

    @Override
    public boolean onOptionsItemSelected(MenuItem item) {

        switch (item.getItemId()){
            case android.R.id.home:
                dl.openDrawer(GravityCompat.START);
                return true;
        }
        return super.onOptionsItemSelected(item);
    }


    @Override
    public void onBackPressed() {
        new AlertDialog.Builder(this)
                .setIcon(R.drawable.alert)
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 165-201

.setTitle("Alert")
                .setMessage("Sure to exit DaanCorona ???")
                .setPositiveButton("Yes", new DialogInterface.OnClickListener()
                {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        Intent a = new Intent(Intent.ACTION_MAIN);
                        a.addCategory(Intent.CATEGORY_HOME);
                        a.setFlags(Intent.FLAG_ACTIVITY_NEW_TASK);
                        startActivity(a);
                        finishAffinity();
                    }

                })
                .setNegativeButton("No", null)
                .show();
    }


    class SetProfile extends AsyncTask<Void, Void, String[]> {

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected String[] doInBackground(Void... voids) {

            final OkHttpClient httpClient = new OkHttpClient();

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_details/")
                    .addHeader("Authorization", "JWT " + token)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 202-242

JSONObject jsonObject = new JSONObject(response.body().string());
                nametxt = jsonObject.getString("name");
                net = jsonObject.getString("total_amt");
                maxcred = jsonObject.getString("max_credit");

                Log.d("Values", nametxt + net + maxcred + "");
                return new String[]{nametxt, net, maxcred};
                // Get response body

            } catch (IOException | JSONException e) {
                e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String... s) {

            super.onPostExecute(s);
            if (s != null) {
                name.setText(s[0]);
                netamt.setText(s[1]);
                maxcredit.setText(s[2]);
                if (sharedPref.getString("Lang", "").equals("hin")) {
                    name.setText(name.getText().toString());
                }
            }
            if (mSwipeRefreshLayout.isRefreshing()) {
                mSwipeRefreshLayout.setRefreshing(false);
            }
        }
    }

    class SetRecyclerView extends AsyncTask<Void, Void, JSONArray> {
        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected JSONArray doInBackground(Void... voids) {


            final OkHttpClient httpClient = new OkHttpClient();
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 243-278

Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_details/")
                    .addHeader("Authorization", "JWT " + token)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                JSONObject jsonObject = new JSONObject(response.body().string());

                return jsonObject.getJSONArray("donors");

            } catch (IOException | JSONException e) {
                e.printStackTrace();
            }

            return null;
        }

        @Override
        protected void onPostExecute(JSONArray jsonArray) {
            super.onPostExecute(jsonArray);

            try {
                ArrayList<Item> list = new ArrayList<>();
                if (jsonArray != null) {

                    for (int i = 0; i < jsonArray.length(); i++) {
                        JSONObject jsonObject = jsonArray.getJSONObject(i);
                        list.add(new Item(jsonObject.getString("name"), jsonObject.getString("amount"), jsonObject.getString("donor_id")));
                    }

                    if(jsonArray.length()==0){
                        if(sharedPref.getString("Lang","").equals("hin"))
PATH: app/src/main/java/com/codingclub/daancorona/MainActivity.java
LINES: 279-306

nodonation.setText(getResources().getString(R.string.nodon));
                        recyclerView.setVisibility(View.GONE);
                        nodonation.setVisibility(View.VISIBLE);
                    }
                    else {
                        recyclerView.setVisibility(View.VISIBLE);
                        nodonation.setVisibility(View.GONE);
                        recyclerView.setLayoutManager(new LinearLayoutManager(MainActivity.this));
                        itemAdapter = new ItemAdapter(list, MainActivity.this);

                        recyclerView.setAdapter(itemAdapter);
                    }
                }
            } catch (JSONException e) {
                e.printStackTrace();
            }
            if (mSwipeRefreshLayout.isRefreshing()) {
                mSwipeRefreshLayout.setRefreshing(false);
            }
        }
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

    }
}
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 1-36

package com.codingclub.daancorona;

import androidx.annotation.NonNull;
import androidx.annotation.RequiresApi;
import androidx.appcompat.app.AlertDialog;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;

import android.Manifest;
import android.app.Activity;
import android.content.DialogInterface;
import android.content.Intent;
import android.content.pm.PackageManager;
import android.content.res.Resources;
import android.location.Location;
import android.net.Uri;
import android.os.Build;
import android.os.Bundle;
import android.os.Handler;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.Toast;

import com.google.android.gms.location.FusedLocationProviderClient;
import com.google.android.gms.location.LocationServices;
import com.google.android.gms.maps.CameraUpdateFactory;
import com.google.android.gms.maps.GoogleMap;
import com.google.android.gms.maps.OnMapReadyCallback;
import com.google.android.gms.maps.SupportMapFragment;
import com.google.android.gms.maps.model.CameraPosition;
import com.google.android.gms.maps.model.LatLng;
import com.google.android.gms.maps.model.MapStyleOptions;
import com.google.android.gms.maps.model.Marker;
import com.google.android.gms.maps.model.MarkerOptions;
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 37-73

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;

import java.util.Objects;

public class MapActivity extends AppCompatActivity implements OnMapReadyCallback{
    private GoogleMap map;
    private Location currentLocation;
    private MarkerOptions markerOptions1;
    private Marker m;
    private Button ok;
    // sab badiya hai
    private static final int LOCATION_PERMISSION_REQUEST_CODE =1234;
    private static final float DEFAULT_ZOOM = 17.5f;
    private Boolean mLocationPermissionsGranted = false,edit;
    private static final String TAG = "MapFragment";
    Intent intent1;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_map);
        intent1=getIntent();
        edit=intent1.getBooleanExtra("edit",false);

        ok = findViewById(R.id.ok);
        getLocationPermission();
    }

    private void initMap(){
        // Obtain the SupportMapFragment and get notified when map is ready to be used
        assert getFragmentManager() != null;
        SupportMapFragment mapFragment = (SupportMapFragment) getSupportFragmentManager().findFragmentById(R.id.map);
        mapFragment.getMapAsync(this);
    }

    private void getDeviceLocation(){
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 74-92

FusedLocationProviderClient fusedLocationProviderClient = LocationServices.getFusedLocationProviderClient(this);
        try{
            if (mLocationPermissionsGranted){
                final Task location = fusedLocationProviderClient.getLastLocation();
                location.addOnCompleteListener(new OnCompleteListener() {
                    @RequiresApi(api = Build.VERSION_CODES.KITKAT)
                    @Override
                    public void onComplete(@NonNull Task task) {
                        if (task.isSuccessful()){
                            Log.d(TAG,"onComplete: found location");
                            currentLocation = (Location)task.getResult();
                            if (currentLocation != null) {
                                CameraPosition position = new CameraPosition.Builder()
                                        .target(new LatLng(currentLocation.getLatitude(), currentLocation.getLongitude())) // Sets the new camera position
                                        .zoom(DEFAULT_ZOOM) // Sets the zoom
                                        .bearing(0) // Rotate the camera
                                        .tilt(70) // Set the camera tilt
                                        .build(); // Creates a CameraPosition from the builder
                                map.animateCamera(CameraUpdateFactory
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 93-120

.newCameraPosition(position), new GoogleMap.CancelableCallback() {
                                    @Override
                                    public void onFinish() {
                                        // Code to execute when the animateCamera task has finished
                                    }
                                    @Override
                                    public void onCancel() {
                                        // Code to execute when the user has canceled the animateCamera task
                                    }
                                });
                            }else{
                                showGPSDisabledAlertToUser();
                            }
                        }else{
                            Log.d(TAG,"onComplete: current Location is null");
                            Toast.makeText(MapActivity.this, "unable to get current location", Toast.LENGTH_SHORT).show();
                        }
                    }
                });
            }
        }catch (SecurityException e){
            Log.e(TAG,"getDeviceLocation: SecurityException: "+ e.getMessage());
        }
    }

    private void getLocationPermission(){
        Log.d("isnull","Null");
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 121-147

String[] permissions = {Manifest.permission.ACCESS_FINE_LOCATION, android.Manifest.permission.ACCESS_COARSE_LOCATION};
        if (ContextCompat.checkSelfPermission(this, android.Manifest.permission.ACCESS_FINE_LOCATION) != PackageManager.PERMISSION_GRANTED &&
                ActivityCompat.checkSelfPermission(this, android.Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) {
            ActivityCompat.requestPermissions(this,permissions, LOCATION_PERMISSION_REQUEST_CODE);
        }else{
            mLocationPermissionsGranted = true;
            initMap();
        }
    }

    @Override
    public void onMapReady(GoogleMap googleMap) {
        map = googleMap;
        // Theme customization
        try { // Customise the styling of the base map using a JSON object defined
            // in a raw resource file.
            boolean success = googleMap.setMapStyle(
                    MapStyleOptions.loadRawResourceStyle(MapActivity.this, R.raw.night_json));
            if (!success) {
                Log.e(TAG, "Style parsing failed.");
            }
        } catch (Resources.NotFoundException e) {
            Log.e(TAG, "Can't find style. Error: ", e);
        }
        if (mLocationPermissionsGranted){
            getDeviceLocation();
            map.setMyLocationEnabled(true);
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 148-181

map.getUiSettings().setMyLocationButtonEnabled(true);
            map.getUiSettings().setCompassEnabled(false);
        }else{
            showGPSDisabledAlertToUser();
        }
        map.setOnMapClickListener(new GoogleMap.OnMapClickListener() {
            @Override
            public void onMapClick(final LatLng latLng) {
                markerOptions1 = new MarkerOptions();
                markerOptions1.position(latLng);
                map.clear();
                m = map.addMarker(markerOptions1);
                m.setVisible(true);
                ok.setVisibility(View.VISIBLE);

                ok.setOnClickListener(new View.OnClickListener() {
                    @Override
                    public void onClick(View v) {
                        Intent intent;
                        if(edit)
                            intent = new Intent(MapActivity.this, EditProfile.class);
                        else
                            intent = new Intent(MapActivity.this, ShopInfoActivity.class);
                        intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        intent.putExtra("lat",latLng.latitude);
                        intent.putExtra("lng",latLng.longitude);
                        startActivity(intent);
                        finish();
                    }
                });
            }
        });
    }
PATH: app/src/main/java/com/codingclub/daancorona/MapActivity.java
LINES: 182-211

// gps dialog box
    private void showGPSDisabledAlertToUser() {
        AlertDialog.Builder alertDialogBuilder = new AlertDialog.Builder(this);
        alertDialogBuilder.setMessage("GPS is disabled in your device. Would you like to enable it?")
                .setCancelable(false)
                .setPositiveButton("Goto Settings Page To Enable GPS",
                        new DialogInterface.OnClickListener() {
                            public void onClick(DialogInterface dialog, int id) {
                                Intent callGPSSettingIntent = new Intent(android.provider.Settings.ACTION_LOCATION_SOURCE_SETTINGS);
                                startActivity(callGPSSettingIntent);
                            }
                        });
        alertDialogBuilder.setNegativeButton("Cancel",
                new DialogInterface.OnClickListener() {
                    public void onClick(DialogInterface dialog, int id) {
                        dialog.cancel();
                        Intent intent = new Intent(MapActivity.this, ShopInfoActivity.class);
                        startActivity(intent);
                        finish();
                    }
                });
        AlertDialog alert = alertDialogBuilder.create();
        alert.show();
    }

    public MapActivity() {
        // Required empty public constructor
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/OTPDialog.java
LINES: 1-36

package com.codingclub.daancorona;

import android.app.Activity;
import android.app.AlertDialog;
import android.app.Dialog;
import android.content.Context;
import android.os.Bundle;
import android.view.LayoutInflater;
import android.view.View;
import android.widget.TextView;


public class OTPDialog extends Dialog {



    private Context context;
    private AlertDialog dialog;
    private String otptxt;
    OTPDialog(String otptxt, Context context){
        super(context);

        this.otptxt=otptxt;
        this.context=context;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.otp_dialog);

        TextView otp=findViewById(R.id.otp);
        otp.setText(otptxt);
        setCancelable(true);
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/PDFActivity.java
LINES: 1-45

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.os.Bundle;
import android.util.Log;
import android.webkit.WebView;
import android.webkit.WebViewClient;
import android.widget.Toast;


import es.voghdev.pdfviewpager.library.RemotePDFViewPager;
import es.voghdev.pdfviewpager.library.adapter.PDFPagerAdapter;
import es.voghdev.pdfviewpager.library.remote.DownloadFile;

public class PDFActivity extends AppCompatActivity implements DownloadFile.Listener{

    PDFPagerAdapter pdfPagerAdapter;
    RemotePDFViewPager remotePDFViewPager;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_p_d_f);
        Toast.makeText(this,"Swipe left to browse",Toast.LENGTH_LONG).show();

        String url="https://daancorona.tech/media/tnc.pdf";

        remotePDFViewPager = new RemotePDFViewPager(PDFActivity.this, url, this);
    }

    @Override
    public void onSuccess(String url, String destinationPath) {

        Log.d("sdncks","Done");
        pdfPagerAdapter = new PDFPagerAdapter(this, "tnc.pdf");
        remotePDFViewPager.setAdapter(pdfPagerAdapter);
        setContentView(remotePDFViewPager);
    }

    @Override
    public void onFailure(Exception e) {
        Log.d("sdncks","Fail");
    }

    @Override
PATH: app/src/main/java/com/codingclub/daancorona/PDFActivity.java
LINES: 46-54

public void onProgressUpdate(int progress, int total) {
    }
    @Override
    protected void onDestroy() {
        super.onDestroy();

        pdfPagerAdapter.close();
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/PaymentModeActivity.java
LINES: 1-43

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;


public class PaymentModeActivity extends AppCompatActivity {

    private Button upi,bank;
    SharedPreferences sharedPref;
    String token;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_payment_mode);

        upi = findViewById(R.id.upi);
        bank = findViewById(R.id.bank);
        sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        if(sharedPref.getString("Lang","").equals("hin")){
            upi.setText(getResources().getString(R.string.accept_UPI));
            bank.setText(getResources().getString(R.string.accept_bank));
        }

        upi.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent intent = new Intent(PaymentModeActivity.this, UPIDetailsActivity.class);
                startActivity(intent);
            }
        });

        bank.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
PATH: app/src/main/java/com/codingclub/daancorona/PaymentModeActivity.java
LINES: 44-50

Intent intent = new Intent(PaymentModeActivity.this, BankDetailsActvity.class);
                startActivity(intent);
            }
        });

    }
}
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 1-47

package com.codingclub.daancorona;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.annotation.RequiresApi;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;

import android.Manifest;
import android.app.Activity;
import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.database.Cursor;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.provider.MediaStore;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.CheckBox;
import android.widget.EditText;
import android.widget.Gallery;
import android.widget.TextView;
import android.widget.Toast;

import com.bumptech.glide.Glide;
import com.google.android.material.textfield.TextInputLayout;

import org.jetbrains.annotations.NotNull;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.File;
import java.io.IOException;
import java.net.SocketException;
import java.net.SocketTimeoutException;
import java.util.concurrent.TimeUnit;

import de.hdodenhof.circleimageview.CircleImageView;
import okhttp3.Call;
import okhttp3.Callback;
import okhttp3.MediaType;
import okhttp3.MultipartBody;
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 48-90

import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

public class PersonalInfoActivity extends AppCompatActivity {

    private static final int MY_GALLERY_REQUEST_CODE =102 ;
    private static final int STORAGE_PERMISSION_CODE = 103;
    TextView pInfo,pImage;
    private EditText first_name,last_name,address;
    private TextInputLayout first_name1,last_name1,address1;
    CheckBox checkBox;
    private Button proceed;
    private CircleImageView userImageView;
    private static final int USER_IMAGE = 100;
    String firstName,lastName,shopAddress;
    Uri userImageURI,dwnldimageUri;
    String token;
    LoadingDialog dialog;
    SharedPreferences sharedPref;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_personal_info);

        sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        initializeItems();


        userImageView.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {

                checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE);

            }
        });

        proceed.setOnClickListener(new View.OnClickListener() {
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 91-122

@Override
            public void onClick(View v) {
                declaration();
                if(firstName.isEmpty() || lastName.isEmpty() || userImageURI==null || shopAddress.isEmpty())
                    Toast.makeText(PersonalInfoActivity.this,"Enter all details",Toast.LENGTH_SHORT).show();
                else{
                    dialog.startloadingDialog();
                  new PersonalInfoActivity.sendDataTask().execute(firstName,lastName,shopAddress);
//                    Intent i = new Intent(PersonalInfoActivity.this, ShopInfoActivity.class);
//                    startActivity(i);
                }
            }
        });
    }

    private void initializeItems() {

        first_name = findViewById(R.id.firstname);
        last_name = findViewById(R.id.lastname);
        address = findViewById(R.id.address);
        proceed = findViewById(R.id.signin);
        userImageView = findViewById(R.id.user_image);
        pInfo=findViewById(R.id.pinfo);
        pImage=findViewById(R.id.pimg);
        first_name1=findViewById(R.id.firstname1);
        last_name1=findViewById(R.id.lastname1);
        address1=findViewById(R.id.address1);
        checkBox=findViewById(R.id.checkbox);

        dialog=new LoadingDialog(this);

        if(sharedPref.getString("Lang","").equals("hin")){
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 123-151

pInfo.setText(getResources().getString(R.string.personal_info));
            pImage.setText(getResources().getString(R.string.select_profile));
            proceed.setText(getResources().getString(R.string.proceed));
            first_name1.setHint(getResources().getString(R.string.firstname));
            last_name1.setHint(getResources().getString(R.string.lastname));
            address1.setHint(getResources().getString(R.string.address));
        }

        OkHttpClient httpClient = new OkHttpClient().newBuilder().readTimeout(1,TimeUnit.MINUTES).build();

        Request request = new Request.Builder()
                .url("https://daancorona.tech/api/recipient_profile/")
                .addHeader("Authorization","JWT "+token)
                .build();

        dialog.startloadingDialog();
        httpClient.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {
                dialog.dismissDialog();
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                    PersonalInfoActivity.this.runOnUiThread(new Runnable() {
                        @Override
                        public void run() {
                            try {
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 152-181

dialog.dismissDialog();
                            JSONObject jsonObject=new JSONObject(response.body().string());
                            first_name.setText(jsonObject.getString("first_name"));
                            last_name.setText(jsonObject.getString("last_name"));
                            address.setText(jsonObject.getString("address"));
                            userImageURI=Uri.parse("https://daancorona.tech"+(String) jsonObject.get("recipient_photo"));
                            dwnldimageUri=userImageURI;
                            Glide.with(PersonalInfoActivity.this).load(userImageURI).into(userImageView);
                            } catch (JSONException | IOException e) {

                            PersonalInfoActivity.this.runOnUiThread(new Runnable() {
                                @Override
                                public void run() {
                                    dialog.dismissDialog();
                                }
                            });
                            e.printStackTrace();
                        }
                        }
                    });



            }
        });
    }

    private void declaration() {
        firstName = first_name.getText().toString().trim();
        lastName = last_name.getText().toString().trim();
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 182-218

shopAddress = address.getText().toString().trim();
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (resultCode == RESULT_OK && requestCode == USER_IMAGE) {
            userImageURI = data.getData();
            userImageView.setImageURI(userImageURI);
        }
    }

    class sendDataTask extends AsyncTask<String,Void,String> {

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected String doInBackground(String... strings) {

            RequestBody formBody;
            final OkHttpClient httpClient = new OkHttpClient()
                    .newBuilder().writeTimeout(1, TimeUnit.MINUTES).build();

            if(dwnldimageUri!=userImageURI) {
                final MediaType MEDIA_TYPE_PNG = MediaType.parse("image/jpeg");

                Log.d("TAG", "" + userImageURI);

                String userPath = getPath(userImageURI);
                Log.d("TAG", "" + userPath);

                File user = new File(userPath);

                Log.d("TAG", "" + user.getName());

                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("first_name", strings[0])
                        .addFormDataPart("last_name", strings[1])
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 219-252

.addFormDataPart("address", strings[2])
                        .addFormDataPart("recipient_photo", user.getName(), RequestBody.create(MEDIA_TYPE_PNG, user))
                        .build();
            }

            else{
                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("first_name", strings[0])
                        .addFormDataPart("last_name", strings[1])
                        .addFormDataPart("address", strings[2])
                        .build();
            }

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_profile/")
                    .addHeader("Authorization","JWT "+token)
                    .post(formBody)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                Log.d("Tag",response.body()+"");
                return "Done";

            }catch (SocketTimeoutException | SocketException e){
                e.printStackTrace();
                return "timeout";
            }
            catch (IOException e ) {
                e.printStackTrace();
            } catch (NullPointerException e){
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 253-288

e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String s) {
            super.onPostExecute(s);
            dialog.dismissDialog();
            if(s!=null) {

                if(s.equals("timeout")) {
                    Toast.makeText(PersonalInfoActivity.this, "Please check your internet connection", Toast.LENGTH_SHORT).show();
                    return;
                }
                Toast.makeText(PersonalInfoActivity.this,s,Toast.LENGTH_SHORT).show();

                SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
                SharedPreferences.Editor editor=sharedPref.edit();
                editor.putBoolean("Page1",true);
                editor.putString("Name",first_name.getText().toString()+" "+last_name.getText().toString());
                editor.apply();

                Intent intent = new Intent(PersonalInfoActivity.this, ShopInfoActivity.class);
                startActivity(intent);
                //Toast.makeText(PersonalInfoActivity.this,"Enter Shop location first",Toast.LENGTH_LONG).show();


            }
            else
                Toast.makeText(PersonalInfoActivity.this,"Error",Toast.LENGTH_LONG).show();
        }
    }

    public String getPath(Uri uri) {
        String[] projection = {MediaStore.MediaColumns.DATA};
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 289-329

Cursor cursor = managedQuery(uri, projection, null, null, null);
        int column_index = cursor
                .getColumnIndexOrThrow(MediaStore.MediaColumns.DATA);
        cursor.moveToFirst();


        return cursor.getString(column_index);
    }

    public void checkPermission(String permission, int requestCode)
    {
        // Checking if permission is not granted
        if (ContextCompat.checkSelfPermission(
                this,
                permission)
                == PackageManager.PERMISSION_DENIED) {
            ActivityCompat
                    .requestPermissions(
                            this,
                            new String[] { permission },
                            requestCode);
        }
        else{
            if(requestCode== MY_GALLERY_REQUEST_CODE){
                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, USER_IMAGE);
            }
        }

    }

    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults)
    {
        super.onRequestPermissionsResult(requestCode,
                permissions,
                grantResults);
PATH: app/src/main/java/com/codingclub/daancorona/PersonalInfoActivity.java
LINES: 330-350

if (requestCode ==  MY_GALLERY_REQUEST_CODE) {
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Storage Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();
                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, USER_IMAGE);
            }
            else {
                Toast.makeText(this,
                        "Storage Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/PrefManager.java
LINES: 1-33

package com.codingclub.daancorona;

import android.content.Context;
import android.content.SharedPreferences;

public class PrefManager {
    SharedPreferences pref;
    SharedPreferences.Editor editor;
    Context _context;

    // shared pref mode
    int PRIVATE_MODE = 0;

    // Shared preferences file name
    private static final String PREF_NAME = "welcome";

    private static final String IS_FIRST_TIME_LAUNCH = "IsFirstTimeLaunch";

    public PrefManager(Context context) {
        this._context = context;
        pref = _context.getSharedPreferences(PREF_NAME, PRIVATE_MODE);
        editor = pref.edit();
    }

    public void setFirstTimeLaunch(boolean isFirstTime) {
        editor.putBoolean(IS_FIRST_TIME_LAUNCH, isFirstTime);
        editor.commit();
    }

    public boolean isFirstTimeLaunch() {
        return pref.getBoolean(IS_FIRST_TIME_LAUNCH, true);
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 1-48

package com.codingclub.daancorona;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.annotation.RequiresApi;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;

import android.Manifest;
import android.app.Activity;
import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.database.Cursor;
import android.net.Uri;
import android.os.AsyncTask;
import android.os.Build;
import android.os.Bundle;
import android.provider.MediaStore;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.TextView;
import android.widget.Toast;

import com.bumptech.glide.Glide;
import com.google.android.material.textfield.TextInputLayout;

import org.jetbrains.annotations.NotNull;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.File;
import java.io.IOException;
import java.net.SocketException;
import java.net.SocketTimeoutException;
import java.util.concurrent.TimeUnit;

import de.hdodenhof.circleimageview.CircleImageView;
import okhttp3.Call;
import okhttp3.Callback;
import okhttp3.MediaType;
import okhttp3.MultipartBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 49-82

import okhttp3.Response;

public class ShopInfoActivity extends AppCompatActivity {

    private static final int MY_GALLERY_REQUEST_CODE =102 ;
    private static final int STORAGE_PERMISSION_CODE = 103;
    private static final int LOCATION_PERMISSION_REQUEST_CODE =1234;
    private EditText shop_name,shop_type,maxCredit,buss_address;
    private TextInputLayout shop_name1,shop_type1,maxCredit1,buss_address1;
    private Button proceed, location;
    private TextView buss_info,pImage;
    private CircleImageView shopImage;
    private static final int SHOP_IMAGE = 101;
    String shopName,shopType,latitude="",longitude="",MaxCredit,BussAddress;
    double lat=-1.0,lng=-1.0,dwnldlat=-1.0,dwnldlng=-1.0;
    Uri shopImageURI,dwnldimageUri;
    String token;
    LoadingDialog dialog;
    SharedPreferences sharedPref;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_shop_info);
//        Toast.makeText(ShopInfoActivity.this,"Enter Shop location first",Toast.LENGTH_LONG).show();

        sharedPref = getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        initializeItems();


        if(sharedPref.getString("Lang","").equals("hin")){
            location.setText(getResources().getString(R.string.shop_location));
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 83-115

pImage.setText(getResources().getString(R.string.select_buss));
            proceed.setText(getResources().getString(R.string.proceed));
            buss_info.setText(getResources().getString(R.string.buss_info));
            shop_name1.setHint(getResources().getString(R.string.bussname));
            shop_type1.setHint(getResources().getString(R.string.busstype));
            buss_address1.setHint(getResources().getString(R.string.bussaddr));
            maxCredit1.setHint(getResources().getString(R.string.maxcredit));
        }

        shopImage.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                checkPermission(new String[]{Manifest.permission.READ_EXTERNAL_STORAGE}[0], MY_GALLERY_REQUEST_CODE);

            }
        });

        location.setOnClickListener(new View.OnClickListener() {

            @Override
            public void onClick(View v) {

                declaration();
                getLocationPermission(LOCATION_PERMISSION_REQUEST_CODE);
            }
        });


        proceed.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                declaration();
                if(shopName.isEmpty() || shopType.isEmpty() ||
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 116-148

/*latitude.isEmpty() || longitude.isEmpty() ||*/ MaxCredit.isEmpty()
                        || BussAddress.isEmpty() || shopImageURI==null)
                    Toast.makeText(ShopInfoActivity.this,"Enter all details",Toast.LENGTH_SHORT).show();
                else{
                    dialog.startloadingDialog();
                    new ShopInfoActivity.sendDataTask().execute(shopName,shopType,latitude,
                            longitude,MaxCredit,BussAddress);

//                    Intent i = new Intent(ShopInfoActivity.this, PaymentModeActivity.class);
//                    startActivity(i);

                }

            }
        });
    }

    private void initializeItems() {

        location = findViewById(R.id.shopLocation);
        proceed = findViewById(R.id.signin);
        shopImage = findViewById(R.id.shop_image);
        shop_name1=findViewById(R.id.shopName1);
        shop_name = findViewById(R.id.shopName);
        shop_type1=findViewById(R.id.shopType1);
        shop_type = findViewById(R.id.shopType);
        maxCredit=findViewById(R.id.maxcredit);
        maxCredit1=findViewById(R.id.maxcredit1);
        buss_address=findViewById(R.id.businessaddress);
        buss_address1=findViewById(R.id.businessaddress1);
        buss_info=findViewById(R.id.bussinfo);
        pImage=findViewById(R.id.pimg);
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 149-182

dialog=new LoadingDialog(this);

        shop_name.setText(sharedPref.getString("shopName",""));
        shop_type.setText(sharedPref.getString("shopType",""));
        maxCredit.setText(sharedPref.getString("MaxCredit",""));
        buss_address.setText(sharedPref.getString("BussAddress",""));

        if(!sharedPref.getString("Uri","").equals("")) {
            shopImageURI = Uri.parse(sharedPref.getString("Uri", ""));
            shopImage.setImageURI(shopImageURI);
        }

        dialog.startloadingDialog();
        OkHttpClient httpClient = new OkHttpClient().newBuilder().readTimeout(1, TimeUnit.MINUTES).build();

        Request request = new Request.Builder()
                .url("https://daancorona.tech/api/recipient_profile/")
                .addHeader("Authorization","JWT "+token)
                .build();

        httpClient.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {

                dialog.dismissDialog();
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                    ShopInfoActivity.this.runOnUiThread(new Runnable() {
                        @Override
                        public void run() {
                            try {
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 183-201

dialog.dismissDialog();
                            JSONObject jsonObject=new JSONObject(response.body().string());
                            Log.d("dcjsdc",""+jsonObject);
                            shop_name.setText(jsonObject.getString("business_name"));
                            shop_type.setText(jsonObject.getString("business_type"));
                            buss_address.setText(jsonObject.getString("business_address"));
                            maxCredit.setText(String.valueOf(jsonObject.getDouble("max_credit")));

                                shopImageURI=Uri.parse("https://daancorona.tech"+(String) jsonObject.get("business_photo"));

                            if(jsonObject.getString("business_photo").equals("/media/default.jpg"))
                                shopImageURI=null;
                            dwnldimageUri=shopImageURI;

                                if(!sharedPref.getString("shopName","").equals(""))
                                    shop_name.setText(sharedPref.getString("shopName",""));
                                if(!sharedPref.getString("shopType","").equals(""))
                                    shop_type.setText(sharedPref.getString("shopType",""));
                                if(!sharedPref.getString("MaxCredit","").equals(""))
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 202-225

maxCredit.setText(sharedPref.getString("MaxCredit",""));
                                if(!sharedPref.getString("BussAddress","").equals(""))
                                    buss_address.setText(sharedPref.getString("BussAddress",""));

                                dwnldlat=jsonObject.getDouble("lat");
                                dwnldlng=jsonObject.getDouble("long");
                                latitude = Double.toString(dwnldlat);
                                longitude = Double.toString(dwnldlng);
                                if(shopImageURI!=null)
                                    Glide.with(ShopInfoActivity.this).load(shopImageURI).into(shopImage);


                                Intent intent1 = getIntent();
                                lat=intent1.getDoubleExtra("lat",-1.0);
                                lng=intent1.getDoubleExtra("lng",-1.0);

                                if(lat==-1.0){
                                    latitude = Double.toString(dwnldlat);
                                    longitude = Double.toString(dwnldlng);
                                }
                                else if(dwnldlat!=lat ){
                                    latitude = Double.toString(lat);
                                    longitude = Double.toString(lng);
                                }
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 226-247

Log.d("Tag",latitude+longitude);

                        } catch (JSONException | IOException e) {

                                ShopInfoActivity.this.runOnUiThread(new Runnable() {
                                    @Override
                                    public void run() {
                                        if(!sharedPref.getString("Uri","").equals("")) {
                                            shopImageURI = Uri.parse(sharedPref.getString("Uri", ""));
                                            shopImage.setImageURI(shopImageURI);
                                        }
                                        else
                                            shopImage.setImageDrawable(getResources().getDrawable(R.drawable.shop));
                                        Intent intent1 = getIntent();
                                        lat=intent1.getDoubleExtra("lat",-1.0);
                                        lng=intent1.getDoubleExtra("lng",-1.0);
                                        latitude = Double.toString(lat);
                                        longitude = Double.toString(lng);
                                        if(lat==-1.0){
                                            latitude="";
                                            longitude="";
                                        }
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 248-287

Log.d("Tag",latitude+longitude);
                                        dialog.dismissDialog();
                                    }
                                });

                            e.printStackTrace();
                        }
                        }
                    });


            }
        });
    }

    private void declaration() {
        shopName = shop_name.getText().toString().trim();
        shopType = shop_type.getText().toString().trim();
        MaxCredit=maxCredit.getText().toString().trim();
        BussAddress=buss_address.getText().toString().trim();

        SharedPreferences.Editor editor=sharedPref.edit();
        editor.putString("shopName",shopName);
        editor.putString("shopType",shopType);
        editor.putString("MaxCredit",MaxCredit);
        editor.putString("BussAddress",BussAddress);
        if(shopImageURI!=null)
            editor.putString("Uri",shopImageURI.toString());
        editor.apply();
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, @Nullable Intent data) {
        super.onActivityResult(requestCode, resultCode, data);
        if (resultCode == RESULT_OK && requestCode == SHOP_IMAGE) {
            shopImageURI = data.getData();
            shopImage.setImageURI(shopImageURI);
        }
    }
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 288-316

class sendDataTask extends AsyncTask<String,Void,String> {

        @RequiresApi(api = Build.VERSION_CODES.KITKAT)
        @Override
        protected String doInBackground(String... strings) {

            final OkHttpClient httpClient = new OkHttpClient().newBuilder().writeTimeout(1,TimeUnit.MINUTES).build();

            RequestBody formBody;

            if(dwnldimageUri!=shopImageURI) {

                final MediaType MEDIA_TYPE_PNG = MediaType.parse("image/jpeg");
                //File path = Environment.getExternalStoragePublicDirectory(
                //      Environment.DIRECTORY_PICTURES);

                String shopPath = getPath(shopImageURI);
                Log.d("TAG", "" + shopPath);

                File shop = new File(shopPath);

                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("business_name", strings[0])
                        .addFormDataPart("business_type", strings[1])
                        .addFormDataPart("lat", strings[2])
                        .addFormDataPart("long", strings[3])
                        .addFormDataPart("max_credit", strings[4])
                        .addFormDataPart("business_address", strings[5])
                        .addFormDataPart("business_photo", shop.getName(), RequestBody.create(MEDIA_TYPE_PNG, shop))
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 317-349

.build();
            }
            else {
                formBody = new MultipartBody.Builder().setType(MultipartBody.FORM)
                        .addFormDataPart("business_name", strings[0])
                        .addFormDataPart("business_type", strings[1])
                        .addFormDataPart("lat", strings[2])
                        .addFormDataPart("long", strings[3])
                        .addFormDataPart("max_credit", strings[4])
                        .addFormDataPart("business_address", strings[5])
                        .build();
            }

            Request request = new Request.Builder()
                    .url("https://daancorona.tech/api/recipient_profile/")
                    .addHeader("Authorization","JWT "+token)
                    .post(formBody)
                    .build();

            try (Response response = httpClient.newCall(request).execute()) {

                if (!response.isSuccessful())
                    throw new IOException("Unexpected code " + response);

                Log.d("Tag",response.body()+"");
                return "Done";

            }catch (SocketTimeoutException | SocketException e){
                e.printStackTrace();
                return "timeout";
            }
            catch (IOException e ) {
                e.printStackTrace();
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 350-383

} catch (NullPointerException e){
                e.printStackTrace();
            }
            return null;
        }

        @Override
        protected void onPostExecute(String s) {
            super.onPostExecute(s);
            dialog.dismissDialog();
            if(s!=null) {

                if(s.equals("timeout")) {
                    Toast.makeText(ShopInfoActivity.this, "Please check your internet connection", Toast.LENGTH_SHORT).show();
                    return;
                }
                Toast.makeText(ShopInfoActivity.this,s,Toast.LENGTH_LONG).show();
                SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
                SharedPreferences.Editor editor=sharedPref.edit();
                editor.putBoolean("Page2",true);
                editor.apply();

                Intent intent = new Intent(ShopInfoActivity.this, PaymentModeActivity.class);
                startActivity(intent);
            }
            else
                Toast.makeText(ShopInfoActivity.this,"Error",Toast.LENGTH_LONG).show();
        }
    }
    public String getPath(Uri uri) {
        String[] projection = {MediaStore.MediaColumns.DATA};
        Cursor cursor = managedQuery(uri, projection, null, null, null);
        int column_index = cursor
                .getColumnIndexOrThrow(MediaStore.MediaColumns.DATA);
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 384-424

cursor.moveToFirst();
        String imagePath = cursor.getString(column_index);

        return cursor.getString(column_index);
    }

    public void checkPermission(String permission, int requestCode)
    {
        // Checking if permission is not granted
        if (ContextCompat.checkSelfPermission(
                this,
                permission)
                == PackageManager.PERMISSION_DENIED) {
            ActivityCompat
                    .requestPermissions(
                            this,
                            new String[] { permission },
                            requestCode);
        }
        else{
            if(requestCode== MY_GALLERY_REQUEST_CODE){

                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, SHOP_IMAGE);
            }
        }

    }

    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults)
    {
        super.onRequestPermissionsResult(requestCode,
                permissions,
                grantResults);

        if (requestCode ==  MY_GALLERY_REQUEST_CODE) {
            if (grantResults.length > 0
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 425-463

&& grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Storage Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();

                Intent gallery = new Intent(Intent.ACTION_PICK);
                gallery.setType("image/*");
                startActivityForResult(gallery, SHOP_IMAGE);

            }
            else {
                Toast.makeText(this,
                        "Storage Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
        else{
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(this,
                        "Location Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();
                Intent intent = new Intent(ShopInfoActivity.this, MapActivity.class);
                startActivity(intent);
                finish();
            }
            else {
                Toast.makeText(this,
                        "Location Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
    }

    private void getLocationPermission(int requestCode){
PATH: app/src/main/java/com/codingclub/daancorona/ShopInfoActivity.java
LINES: 464-479

Log.d("isnull","Null");

        String[] permissions = {Manifest.permission.ACCESS_FINE_LOCATION, android.Manifest.permission.ACCESS_COARSE_LOCATION};
        if (ContextCompat.checkSelfPermission(this, android.Manifest.permission.ACCESS_FINE_LOCATION) != PackageManager.PERMISSION_GRANTED &&
                ActivityCompat.checkSelfPermission(this, android.Manifest.permission.ACCESS_COARSE_LOCATION) != PackageManager.PERMISSION_GRANTED) {
            ActivityCompat.requestPermissions(this,permissions, LOCATION_PERMISSION_REQUEST_CODE);
        }else{
            if(requestCode== LOCATION_PERMISSION_REQUEST_CODE){
                Intent intent = new Intent(ShopInfoActivity.this, MapActivity.class);
                startActivity(intent);
                finish();
            }
        }
    }

}
PATH: app/src/main/java/com/codingclub/daancorona/TransactionAdapter.java
LINES: 1-43

package com.codingclub.daancorona;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;


import java.util.ArrayList;

public class TransactionAdapter extends RecyclerView.Adapter<TransactionAdapter.ViewHolder>{

    ArrayList<TransactionModel> list;
    Context context;

    public TransactionAdapter(ArrayList<TransactionModel> list, Context context) {
        this.list = list;
        this.context = context;
    }

    @NonNull
    @Override
    public TransactionAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view= LayoutInflater.from(parent.getContext()).inflate(R.layout.transaction_item,parent,false);
        return new TransactionAdapter.ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull TransactionAdapter.ViewHolder holder, int position) {
        if(list.get(position).getType()==0){
            holder.text.setText("You received "+list.get(position).getAmount()+" from "+list.get(position).getName());
        }
        else {
            holder.text.setText("Sold goods worth "+list.get(position).getAmount()+" to "+list.get(position).getName());
        }

    }

    @Override
PATH: app/src/main/java/com/codingclub/daancorona/TransactionAdapter.java
LINES: 44-56

public int getItemCount() {
        return list.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView text;
        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            text=itemView.findViewById(R.id.trnsctxt);
        }
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/TransactionModel.java
LINES: 1-25

package com.codingclub.daancorona;

public class TransactionModel {
    private String name;
    private String amount;
    private int type;

    public TransactionModel(String name, String amount, int type) {
        this.name = name;
        this.amount = amount;
        this.type = type;
    }

    public String getName() {
        return name;
    }

    public String getAmount() {
        return amount;
    }

    public int getType() {
        return type;
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/Transactions.java
LINES: 1-49

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.TextView;
import android.widget.Toast;


import org.jetbrains.annotations.NotNull;
import org.json.JSONArray;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.IOException;
import java.util.ArrayList;

import okhttp3.Call;
import okhttp3.Callback;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.Response;

public class Transactions extends AppCompatActivity {

    RecyclerView recyclerView;
    TransactionAdapter transactionAdapter;
    String access;
    ArrayList<TransactionModel> list;
    TextView notransactions;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_transactions);

        recyclerView=findViewById(R.id.recyclerview);
        notransactions=findViewById(R.id.notrans);

        SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        access=sharedPref.getString("Token","");

        OkHttpClient client = new OkHttpClient();
PATH: app/src/main/java/com/codingclub/daancorona/Transactions.java
LINES: 50-79

Request request = new Request.Builder()
                .header("Authorization","JWT "+access)
                .url("https://daancorona.tech/api/recipient_transactions/")
                .build();

        client.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {
                Transactions.this.runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        Toast.makeText(Transactions.this,"Error",Toast.LENGTH_SHORT).show();
                    }
                });
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                JSONObject jsonObject;
                list=new ArrayList<>();
                try {
                    jsonObject = new JSONObject(response.body().string());
                    Log.d("resp",response.body()+"");
                    JSONArray jsonArray=jsonObject.getJSONArray("transactions");

                    if (jsonArray != null) {

                        for (int i = 0; i < jsonArray.length(); i++) {
                            JSONObject jsonObject1 = jsonArray.getJSONObject(i);
PATH: app/src/main/java/com/codingclub/daancorona/Transactions.java
LINES: 80-107

list.add(new TransactionModel(jsonObject1.getString("name"), jsonObject1.getString("amount"),jsonObject1.getInt("type")));
                        }

                        Transactions.this.runOnUiThread(new Runnable() {
                            @Override
                            public void run() {

                                if(jsonArray.length()==0){

                                    if(sharedPref.getString("Lang","").equals("hin"))
                                        notransactions.setText(getResources().getString(R.string.transaction));
                                    recyclerView.setVisibility(View.GONE);
                                    notransactions.setVisibility(View.VISIBLE);
                                }
                                else {

                                    recyclerView.setVisibility(View.VISIBLE);
                                    notransactions.setVisibility(View.GONE);
                                    recyclerView.setLayoutManager(new LinearLayoutManager(Transactions.this));
                                    transactionAdapter = new TransactionAdapter(list, Transactions.this);

                                    recyclerView.setAdapter(transactionAdapter);
                                }
                            }
                        });

                    }
PATH: app/src/main/java/com/codingclub/daancorona/Transactions.java
LINES: 108-116

}catch (JSONException e) {
                    e.printStackTrace();
                }

            }
        });

    }
}
PATH: app/src/main/java/com/codingclub/daancorona/UPIDetailsActivity.java
LINES: 1-49

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.app.Dialog;
import android.content.Intent;
import android.content.SharedPreferences;
import android.net.Uri;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.Toast;

import com.bumptech.glide.Glide;
import com.google.android.material.textfield.TextInputEditText;
import com.google.android.material.textfield.TextInputLayout;

import org.jetbrains.annotations.NotNull;
import org.json.JSONException;
import org.json.JSONObject;

import java.io.IOException;

import okhttp3.Call;
import okhttp3.Callback;
import okhttp3.FormBody;
import okhttp3.MultipartBody;
import okhttp3.OkHttpClient;
import okhttp3.Request;
import okhttp3.RequestBody;
import okhttp3.Response;

public class UPIDetailsActivity extends AppCompatActivity {
    Button btnproceedUpi;
    LoadingDialog dialog;
    @Override
    protected void onCreate(Bundle savedInstanceState) {

        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_u_p_i_details);

        SharedPreferences sharedPref;
        sharedPref=getSharedPreferences("User",MODE_PRIVATE);

        TextInputEditText upivpa=findViewById(R.id.upi_vpa);
        TextInputLayout upivpa1=findViewById(R.id.upi_vpa1);
        dialog=new LoadingDialog(this);
PATH: app/src/main/java/com/codingclub/daancorona/UPIDetailsActivity.java
LINES: 50-81

btnproceedUpi = findViewById(R.id.proceed_upi_details);
        if(sharedPref.getString("Lang","").equals("hin")){
            btnproceedUpi.setText(getResources().getString(R.string.proceed));
            upivpa1.setHint(getResources().getString(R.string.entervpa));
        }

        btnproceedUpi.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {

                SharedPreferences sharedPref=getSharedPreferences("User",MODE_PRIVATE);

                String token=sharedPref.getString("Token","");
                String upi=upivpa.getText().toString();

                if(upi.isEmpty())
                    Toast.makeText(UPIDetailsActivity.this,"Enter Upi",Toast.LENGTH_SHORT).show();
                else {

                    dialog.startloadingDialog();
                    final OkHttpClient client = new OkHttpClient();

                    RequestBody formBody = new FormBody.Builder()
                            .addEncoded("upi", upi)
                            .build();

                    Request request = new Request.Builder()
                            .url("https://daancorona.tech/api/recipient_profile/")
                            .addHeader("Authorization", "JWT " + token)
                            .post(formBody)
                            .build();
PATH: app/src/main/java/com/codingclub/daancorona/UPIDetailsActivity.java
LINES: 82-106

client.newCall(request).enqueue(new Callback() {
                        @Override
                        public void onFailure(@NotNull Call call, @NotNull IOException e) {
                            UPIDetailsActivity.this.runOnUiThread(new Runnable() {
                                @Override
                                public void run() {
                                    dialog.dismissDialog();
                                    Toast.makeText(UPIDetailsActivity.this, "Check your Internet Connection", Toast.LENGTH_SHORT).show();
                                }
                            });

                        }

                        @Override
                        public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                            UPIDetailsActivity.this.runOnUiThread(new Runnable() {
                                @Override
                                public void run() {
                                    dialog.dismissDialog();
                                    Toast.makeText(UPIDetailsActivity.this, "Success", Toast.LENGTH_SHORT).show();
                                }
                            });
                            Log.d("Resp",""+response);
                            SharedPreferences.Editor editor=sharedPref.edit();
PATH: app/src/main/java/com/codingclub/daancorona/UPIDetailsActivity.java
LINES: 107-141

editor.putBoolean("Page3",true);
                            editor.apply();
                            Intent i = new Intent(UPIDetailsActivity.this, MOUActivity.class);
                            i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_NEW_TASK);
                            startActivity(i);
                        }
                    });
                }

            }
        });

        dialog.startloadingDialog();

        final OkHttpClient httpClient = new OkHttpClient();
        String token=sharedPref.getString("Token","");

        Request request = new Request.Builder()
                .url("https://daancorona.tech/api/recipient_profile/")
                .addHeader("Authorization", "JWT " + token)
                .build();
        httpClient.newCall(request).enqueue(new Callback() {
            @Override
            public void onFailure(@NotNull Call call, @NotNull IOException e) {
                dialog.dismissDialog();
            }

            @Override
            public void onResponse(@NotNull Call call, @NotNull Response response) throws IOException {

                UPIDetailsActivity.this.runOnUiThread(new Runnable() {
                    @Override
                    public void run() {
                        try {
                            dialog.dismissDialog();
PATH: app/src/main/java/com/codingclub/daancorona/UPIDetailsActivity.java
LINES: 142-163

JSONObject jsonObject=new JSONObject(response.body().string());
                            upivpa.setText(jsonObject.getString("upi"));

                        } catch (JSONException | IOException e) {

                            UPIDetailsActivity.this.runOnUiThread(new Runnable() {
                                @Override
                                public void run() {
                                    dialog.dismissDialog();
                                }
                            });

                            e.printStackTrace();
                        }
                    }
                });


            }
        });
    }
}
PATH: app/src/main/java/com/codingclub/daancorona/WelcomePageActivity.java
LINES: 1-46

package com.codingclub.daancorona;

import androidx.appcompat.app.AppCompatActivity;

import android.app.Activity;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;
import android.widget.TextView;
import android.widget.Toast;


public class WelcomePageActivity extends AppCompatActivity {

    Button register;
    TextView txtQuote;
    SharedPreferences sharedPref;
    String token;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_welcome_page);

       register = findViewById(R.id.register);
       txtQuote = findViewById(R.id.quote);
        sharedPref=getSharedPreferences("User",MODE_PRIVATE);
        token=sharedPref.getString("Token","");

        if(sharedPref.getString("Lang","").equals("hin")){
            register.setText(getResources().getString(R.string.register_new_user));
            txtQuote.setText(getResources().getString(R.string.quote));
        }

        register.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                Intent i = new Intent(WelcomePageActivity.this, LoginActivity.class);
                startActivity(i);
            }
        });

    }
}
PATH: app/src/test/java/com/codingclub/daancorona/ExampleUnitTest.java
LINES: 1-17

package com.lendeasy.daancorona;

import org.junit.Test;

import static org.junit.Assert.*;

/**
 * Example local unit test, which will execute on the development machine (host).
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
public class ExampleUnitTest {
    @Test
    public void addition_isCorrect() {
        assertEquals(4, 2 + 2);
    }
}
# Facial-Expresson
Repository for Facial Expression Recognition using FER-2013 dataset. The architecture used consists of 4 CNN layers (all having size 3x3 except the second which is 5x5) followed by 2 Fully Connected Layers.

Val accuracy - 61%

Train Accuracy - 76%
# Forehead_Biometric
Repository for Class Project for the course Machine Learning (BITS 464).
## Image Matching

Image matching is an important concept in computer vision and object recognition. The images of the same subject can be acquired from different angles, lighting and scales. Therefore, it is best to find features that are descriptive and invariant to the aforementioned variations in order to categorize the images correctly.

SIFT helps locate the local features in an image, commonly known as the keypoints of the image. These keypoints are scale & rotation invariant that can be used for various computer vision applications, like image matching, object detection, scene detection, etc.

We can also use the keypoints generated using SIFT as features for the image during model training. The major advantage of SIFT features, over edge features or hog features, is that they are not affected by the size or orientation of the image.

```python
!pip install opencv-python==3.4.2.17
!pip install opencv-contrib-python==3.4.2.17
```

```python
import numpy as np
import cv2
from matplotlib import pyplot as plt
import os
from natsort import natsorted
import random
import seaborn as sns
```

```python
from google.colab import drive
drive.mount('/content/drive')
```
```python
from google.colab import drive
drive.mount('/content/drive')
```

### Now, for each image, we are going to generate the SIFT features. First, we have to construct a SIFT object and then use the function detectAndCompute to get the keypoints. It will return two values  the keypoints and the descriptors.

Next, match the features of each possible pair of images using match() from the BFmatcher (brute force match) module.

```python
file1 = open("/content/drive/MyDrive/temp_edge_8", "w")
path1 = '/content/drive/MyDrive/Images'


sift = cv2.xfeatures2d.SIFT_create(edgeThreshold = 8 )
bf = cv2.BFMatcher()
for images in natsorted(os.listdir(path1)):
    str1 = str(images)
    img1 = os.path.join(path1, images)
    #print(img1)
    img1 = cv2.imread(img1)
    kp1, des1 = sift.detectAndCompute(img1,None)
    if len(kp1)>0:
       for image in natsorted(os.listdir(path1)):
           img2 = os.path.join(path1, image)
        #print(img2)
           str2 = str(image)
           if (img1 != img2): # queryImage
               img2 = cv2.imread(img2) # trainImage
               kp2, des2 = sift.detectAndCompute(img2,None)
            # Initiate SIFT detector
            #sift = cv2.SIFT()            
            # find the keypoints and descriptors with SIFT
            # BFMatcher with default params
               if len(kp1)==0 or len(kp2)==0 or len(kp1)==1 or len(kp2)==1:
kp2, des2 = sift.detectAndCompute(img2,None)
            # Initiate SIFT detector
            #sift = cv2.SIFT()            
            # find the keypoints and descriptors with SIFT
            # BFMatcher with default params
               if len(kp1)==0 or len(kp2)==0 or len(kp1)==1 or len(kp2)==1:
                 continue
               matches1 = bf.knnMatch(des1,des2, k=2) 
               matches2 = bf.knnMatch(des2,des1, k=2)
               dis = bf.match(des1,des2)
            #----------------------------------------------
               number_keypoints = 0
               if len(kp1) >= len(kp2):
                   number_keypoints = len(kp1)
               else:
                   number_keypoints = len(kp2)
               good1 = []
               good2 = []
               for m,n in matches1:
                   if m.distance < 0.8*n.distance:
                      good1.append([m])
               for m,n in matches2:
                   if m.distance < 0.8*n.distance:
                      good2.append([m])
            #print("Keypoints 1ST Image: " + str(len(kp1)))
            #print("Keypoints 2ND Image: " + str(len(kp2)))
            #print("GOOD Matches:", len(good))
               score1 = len(good1) / number_keypoints
               score2 = len(good2) / number_keypoints
               score = min(score1, score2)
#print("Keypoints 1ST Image: " + str(len(kp1)))
            #print("Keypoints 2ND Image: " + str(len(kp2)))
            #print("GOOD Matches:", len(good))
               score1 = len(good1) / number_keypoints
               score2 = len(good2) / number_keypoints
               score = min(score1, score2)
            #print("How good it's the match: ", score) #* 100, "%")
               file1.write( str1 + " " + str2 + " " + str(score)+"\n")
               print(str1 + " " + str2 + " " + str(score)) # replace with 'cos' if using cosine similarity
file1.close()
```

```python
scores_file = open("/content/drive/MyDrive/temp_edge_8", "r")
content = scores_file.readlines()
for i in range(len(content)):
  content[i]=content[i].split(' ')
  content[i][2] = float(content[i][2][:-1])
content[:10]
```

```python
#Seperating genuine and imposter scores in different lists
genuine = [ x[2] for x in content if x[0].split('_')[0]==x[1].split('_')[0] and x[0]!=x[1]]
imposter = [ x[2] for x in content if x[0].split('_')[0]!=x[1].split('_')[0]]
```

```python
#Code to plot probability of imposter and genuine scores 
imposter_count = [0] * 101
imposter_prob = [0] * 101

genuine_count = [0]*101
genuine_prob = [0]*101
len1 = len(imposter)
len2 = len(genuine)
for x in imposter :
  temp1 = int (x/0.01)
  imposter_count[temp1]+=1
for y in genuine :
  temp2 = int (y/0.01)
len2 = len(genuine)
for x in imposter :
  temp1 = int (x/0.01)
  imposter_count[temp1]+=1
for y in genuine :
  temp2 = int (y/0.01)
  genuine_count[temp2]+=1
imposter_prob[:] = [x/ len1 for x in imposter_count]
genuine_prob[:] = [x/ len2 for x in genuine_count]
```

```python
vals = [ 0.001*i for i in range(0, 1010, 10)]
plt.plot(vals,imposter_prob, label = 'imposter')
plt.plot(vals,genuine_prob, label = 'genuine')
plt.xlabel('Matching Score')
plt.ylabel('Probability')
plt.legend()
plt.show()
```

```python
def gen_thersholds(n):
    """ Generate a list of  n thresholds between 0.0 and 1.0"""
    thersholds = []
    for x in range(1,n+1):
        thersholds.append(float("{:.9f}".format(random.uniform(0.0,1))))

    return sorted(thersholds)
```

```python
def calculate_scores(num_of_thresholds, imposter, genuine):
    """ Calculates a confusion matrix and then calculates far, frr, tpr, tnr per threshold """

    # generate n number of thresholds
    thresholds = gen_thersholds(num_of_thresholds)

    far = []
    frr = []
    tpr = []
    tnr = []
    
    # for each threshold, calculate confusion matrix.
    for t in thresholds:

        FP = 0
        FN = 0
        TP = 0
        TN = 0

        # go through imposters
        for score in imposter:

            if score >= t:
                # imposter passes as a genuine user
                FP += 1
            else:
for score in imposter:

            if score >= t:
                # imposter passes as a genuine user
                FP += 1
            else:
                # imposter correctly rejected
                TN += 1

        for score in genuine:
            if score >= t:
                # genuine user correctly identified
                TP += 1
            else:
                # genuine user incorrectly rejected
                FN += 1

        far_current = float(FP) / float(len(imposter))
        frr_current = float(FN) / float(len(genuine))

        tpr_current = float(TP) / float(len(genuine))
        tnr_current = float(TN) / float(len(imposter))

        # calculate our false accept rate(FAR) and add to list
        far.append(far_current)

        # calculate our false reject rate(FRR) and add to list
        frr.append(frr_current)

        tpr.append(tpr_current)
        tnr.append(tnr_current)

    return far, frr, tpr, tnr, thresholds
```

```python
far, frr, tpr, tnr, thresholds = calculate_scores(1000, imposter, genuine)
```

```python
def find_EER(far, frr, tpr, tnr, thresholds):
    """ Returns the most optimal FAR and FRR values """

    # The lower the equal error rate value,
    # the higher the accuracy of the biometric system.

    t = []
    far_optimum = 0
    frr_optimum = 0
    tpr_optimum = 0
    tnr_optimum = 0
    threshold_optimum = 0
t = []
    far_optimum = 0
    frr_optimum = 0
    tpr_optimum = 0
    tnr_optimum = 0
    threshold_optimum = 0
    num_thresholds = len(thresholds)
    # go through each value for FAR and FRR, calculate
    for i in range(num_thresholds):
        t.append(abs(far[i] + frr[i]))

    # smallest value is most accurate
    smallest = min(t)

    for i in range(num_thresholds):
        if smallest == abs(far[i] + frr[i]):

            # Found EER
            far_optimum = far[i]
            frr_optimum = frr[i]
            tpr_optimum = tpr[i]
            tnr_optimum = tnr[i]
            threshold_optimum = thresholds[i]
            break

    return far_optimum, frr_optimum, tpr_optimum, tnr_optimum, threshold_optimum
```

```python
far_optimum, frr_optimum, tpr_optimum, tnr_optimum, threshold_optimum = find_EER(far, frr, tpr, tnr, thresholds)
far_optimum, frr_optimum, tpr_optimum, tnr_optimum, threshold_optimum
accuracy = 1-(far_optimum+frr_optimum)/2
accuracy*100
```

```python
#Plotting RoC curve
FAR = [1] + far
FRR = [0] + frr
plt.plot(FAR, FRR)
plt.title('RoC')
plt.xlabel('FAR')
plt.ylabel('FRR')
plt.show()
```

```python
file_gen = open("/content/drive/MyDrive/genuine_edge_8.txt", "w")
file_imp = open("/content/drive/MyDrive/imposter_edge_8.txt", "w")

for g in genuine:
  file_gen.write(str(g)+"\n")
for i in imposter:
  file_imp.write(str(i)+"\n")

file_gen.close()
for g in genuine:
  file_gen.write(str(g)+"\n")
for i in imposter:
  file_imp.write(str(i)+"\n")

file_gen.close()
file_imp.close()
```

```python
!pip install pyeer
```

```python
#Using PyEER library to generate comprehensive report for given imposter and genuine scores 
!geteerinf -p "/content/drive/MyDrive/" -i "imposter_edge_8.txt" -g "genuine_edge_8.txt" -e "SIFT_Modified" -pf "pdf" -rf "tex"
```

```python
#Calculating CRR assuming matching within sessions
Dict = {}
for ele in content:
  if not ele[0] in Dict:
    if ele[0]!=ele[1]:
      Dict[ele[0]] = [ele[1], ele[2]]
  elif ele[1]!=ele[0] and Dict[ele[0]][1]<ele[2]:
    Dict[ele[0]] = [ele[1], ele[2]]
count = 0
for key, value in Dict.items():
  if key.split('_')[0]==value[0].split('_')[0]:
    count+=1
crr = count/len(Dict)
crr
```

```python
#Calculating CRR assuming matching only between sessions
Dict1 = {}
for ele in content:
  if not ele[0] in Dict1:
    if ele[0]!=ele[1] and ele[0].split('_')[2]=='S2' and ele[1].split('_')[2]=='S1':
      Dict1[ele[0]] = [ele[1], ele[2]]
  elif ele[1]!=ele[0] and Dict1[ele[0]][1]<ele[2] and ele[0].split('_')[2]=='S2' and ele[1].split('_')[2]=='S1':
    Dict1[ele[0]] = [ele[1], ele[2]]
count = 0
for key, value in Dict1.items():
  if key.split('_')[0]==value[0].split('_')[0]:
    count+=1
crr = count/len(Dict1)
crr
```
# Graph Random Neural Networks for Semi-Supervised Learning on Graphs
Implementation of the paper "Graph Random Neural Networks for Semi-Supervised Learning on Graphs" for the Term Assignment for the course Machine Learning (BITS F464).
# Image-Colouration
Image colouration for CV Project
<!DOCTYPE html>
<html lang="en"><head>  
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <title>Computer Vision Course Project
  | ECE, Virginia Tech | Fall 2021: ECE 4554/5554</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="author" content="">

<!-- Le styles -->  
<link href="css/bootstrap.css" rel="stylesheet">
<style>

.custom-img {
    width: 3000; /* or set a specific value like 500px */
    height: 3000; /* Maintains the aspect ratio */
    object-fit: contain; /* Ensures the image fits inside the container without distortion */
}

body {
padding-top: 60px; /* 60px to make the container go all the way to the bottom of the topbar */
}
.vis {
color: #3366CC;
}
.data {
color: #FF9900;
}
</style>
  
<link href="css/bootstrap-responsive.min.css" rel="stylesheet">

<!-- HTML5 shim, for IE6-8 support of HTML5 elements --><!--[if lt IE 9]>
<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
</head>

<body>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Gray to Glory: Performance Analysis of Image Coloration Models</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Gauransh Sawhney, Daksh Dave, Tushar Deshpande</strong></span><br>
<div class="container">
<div class="page-header">

<!-- Title and Name --> 
<h1>Gray to Glory: Performance Analysis of Image Coloration Models</h1> 
<span style="font-size: 20px; line-height: 1.5em;"><strong>Gauransh Sawhney, Daksh Dave, Tushar Deshpande</strong></span><br>
<span style="font-size: 18px; line-height: 1.5em;">Fall 2024 ECE 4554/5554 Computer Vision: Course Project</span><br>
<span style="font-size: 18px; line-height: 1.5em;">Virginia Tech</span>
<hr>

<!-- Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, because you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained. -->

<!-- Goal -->
<h2>Abstract</h2>
<hr>

<!-- Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, because you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained. -->

<!-- Goal -->
<h2>Abstract</h2>
<!-- Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, because you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained. -->

<!-- Goal -->
<h2>Abstract</h2>
<!-- Please see <a href="http://vision.cs.utexas.edu/projects/adapted_attributes/">this</a> for an example of how to lay out the various details of your project. You may need to provide more details than this, because you will not be submitting an associated paper to accompany the webpage. So the page should be self-contained. -->

<!-- Goal -->
<h2>Abstract</h2>
<!-- Goal -->
<h2>Abstract</h2>
Automatic image coloration is a challenging problem in computer vision, motivated by its potential applications in image restoration, enhancement, and creative tasks. This project explores the performance of various deep learning architectures in transforming grayscale images into realistic color versions, aiming to understand the theoretical underpinnings and practical performance of these models. The approach involves experimenting with a progression of architectures: a simple CNN-based encoder-decoder, a U-Net with a pre-trained ResNet backbone, and a conditional GAN with the aforementioned U-Net as the generator, incorporating adversarial loss. Preliminary results indicate that as model complexity increases, the generated images exhibit improved color fidelity and realism, highlighting the potential of GANs in achieving context-aware and high-quality colorization. We observe an improvement in Mean Absolute Error (MAE) from <strong>0.0848</strong> for the CNN-based Encoder-Decoder to <strong>0.0807</strong> for cGAN. Similarly we also observe an improvement Learned Perceptual Image Patch Similarity (LPIPS) score (using VGG) from <strong>0.148</strong> for the CNN-based Encoder-Decoder to <strong>0.132</strong> for cGAN.   
<!-- Plans for experiments. Describe the experimental setup you will follow. Describe the datasets you
<!-- Goal -->
<h2>Abstract</h2>
Automatic image coloration is a challenging problem in computer vision, motivated by its potential applications in image restoration, enhancement, and creative tasks. This project explores the performance of various deep learning architectures in transforming grayscale images into realistic color versions, aiming to understand the theoretical underpinnings and practical performance of these models. The approach involves experimenting with a progression of architectures: a simple CNN-based encoder-decoder, a U-Net with a pre-trained ResNet backbone, and a conditional GAN with the aforementioned U-Net as the generator, incorporating adversarial loss. Preliminary results indicate that as model complexity increases, the generated images exhibit improved color fidelity and realism, highlighting the potential of GANs in achieving context-aware and high-quality colorization. We observe an improvement in Mean Absolute Error (MAE) from <strong>0.0848</strong> for the CNN-based Encoder-Decoder to <strong>0.0807</strong> for cGAN. Similarly we also observe an improvement Learned Perceptual Image Patch Similarity (LPIPS) score (using VGG) from <strong>0.148</strong> for the CNN-based Encoder-Decoder to <strong>0.132</strong> for cGAN.   
<!-- Plans for experiments. Describe the experimental setup you will follow. Describe the datasets you
<h2>Abstract</h2>
Automatic image coloration is a challenging problem in computer vision, motivated by its potential applications in image restoration, enhancement, and creative tasks. This project explores the performance of various deep learning architectures in transforming grayscale images into realistic color versions, aiming to understand the theoretical underpinnings and practical performance of these models. The approach involves experimenting with a progression of architectures: a simple CNN-based encoder-decoder, a U-Net with a pre-trained ResNet backbone, and a conditional GAN with the aforementioned U-Net as the generator, incorporating adversarial loss. Preliminary results indicate that as model complexity increases, the generated images exhibit improved color fidelity and realism, highlighting the potential of GANs in achieving context-aware and high-quality colorization. We observe an improvement in Mean Absolute Error (MAE) from <strong>0.0848</strong> for the CNN-based Encoder-Decoder to <strong>0.0807</strong> for cGAN. Similarly we also observe an improvement Learned Perceptual Image Patch Similarity (LPIPS) score (using VGG) from <strong>0.148</strong> for the CNN-based Encoder-Decoder to <strong>0.132</strong> for cGAN.   
<!-- Plans for experiments. Describe the experimental setup you will follow. Describe the datasets you
Automatic image coloration is a challenging problem in computer vision, motivated by its potential applications in image restoration, enhancement, and creative tasks. This project explores the performance of various deep learning architectures in transforming grayscale images into realistic color versions, aiming to understand the theoretical underpinnings and practical performance of these models. The approach involves experimenting with a progression of architectures: a simple CNN-based encoder-decoder, a U-Net with a pre-trained ResNet backbone, and a conditional GAN with the aforementioned U-Net as the generator, incorporating adversarial loss. Preliminary results indicate that as model complexity increases, the generated images exhibit improved color fidelity and realism, highlighting the potential of GANs in achieving context-aware and high-quality colorization. We observe an improvement in Mean Absolute Error (MAE) from <strong>0.0848</strong> for the CNN-based Encoder-Decoder to <strong>0.0807</strong> for cGAN. Similarly we also observe an improvement Learned Perceptual Image Patch Similarity (LPIPS) score (using VGG) from <strong>0.148</strong> for the CNN-based Encoder-Decoder to <strong>0.132</strong> for cGAN.   
<!-- Plans for experiments. Describe the experimental setup you will follow. Describe the datasets you
<!-- Plans for experiments. Describe the experimental setup you will follow. Describe the datasets you
plan to use, what code you will implement yourself, what existing code you will borrow (if any), and
what you would define as a success for the project. If you intend to collect your own data, provide a
description of the procedure that you will follow. Provide a list of experiments that you will perform.
Describe what you expect the experiments to reveal, or what is uncertain about the potential
outcomes -->

<!-- <br><br> -->
<!-- figure -->
<!-- <h3>Teaser figure</h3> -->
<!-- A figure that conveys the main idea behind the project or the main application being addressed. -->
<!-- <br><br> -->
<!-- Main Illustrative Figure --> 
<!-- <div style="text-align: center;"> -->
<!-- <img style="height: 200px;" alt="" src="mainfig.png"> -->
<!-- </div> -->
<br><br><br>
<img src="Images/Diagram.png" alt="Trulli" class="custom-img"/>

<!-- <br><br> -->
<!-- Introduction -->
<!-- <h3>Introduction</h3>
Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new. -->

<!-- <br><br> -->
<!-- Approach -->
<h2>Introduction</h2>
<!-- <h3>Introduction</h3>
Motivation behind the problem you are solving, what applications it has, any brief background on the particular domain you are working in (if not regular RBG photographs), etc. If you are using a new way to solve an existing problem, briefly mention and describe the existing approaches and tell us how your approach is new. -->

<!-- <br><br> -->
<!-- Approach -->
<h2>Introduction</h2>
Image colorizationthe process of converting grayscale images into plausible color versionshas garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.
<!-- <br><br> -->
<!-- Approach -->
<h2>Introduction</h2>
Image colorizationthe process of converting grayscale images into plausible color versionshas garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.
<!-- <br><br> -->
<!-- Approach -->
<h2>Introduction</h2>
Image colorizationthe process of converting grayscale images into plausible color versionshas garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.
<!-- Approach -->
<h2>Introduction</h2>
Image colorizationthe process of converting grayscale images into plausible color versionshas garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.
<h2>Introduction</h2>
Image colorizationthe process of converting grayscale images into plausible color versionshas garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.
Image colorizationthe process of converting grayscale images into plausible color versionshas garnered significant interest due to its wide-ranging applications, including the restoration of historical photographs, enhancement of medical imaging, and the generation of visually enriched content for creative industries. This task is inherently challenging, as it requires a model to infer appropriate colors based on contextual and semantic information present in the grayscale input.
Traditional methods for image colorization often relied on manual intervention or heuristic algorithms, which were labor-intensive and lacked generalizability across diverse image datasets. The advent of deep learning has revolutionized this field, enabling the development of models that can learn complex mappings from grayscale to color images directly from data. Notable approaches include convolutional neural networks (CNNs) and generative adversarial networks (GANs), which have demonstrated remarkable success in producing realistic colorizations. For instance, Zhang et al. proposed a deep learning approach that combines a CNN with high-level features extracted from a pre-trained model, achieving impressive results in automatic colorization tasks [1]. Similarly, Deshpande et al. introduced a variational approach to produce diverse and contextually appropriate colorizations, highlighting the potential of probabilistic models in this domain [3].

Despite these advancements, challenges remain, particularly in generating contextually accurate and diverse colorizations. Recent research has focused on leveraging generative models to address these issues. Wu et al. introduced a method that utilizes a pretrained GAN to provide rich and diverse color priors, enabling the production of vivid and varied colorizations [2].
Traditional methods for image colorization often relied on manual intervention or heuristic algorithms, which were labor-intensive and lacked generalizability across diverse image datasets. The advent of deep learning has revolutionized this field, enabling the development of models that can learn complex mappings from grayscale to color images directly from data. Notable approaches include convolutional neural networks (CNNs) and generative adversarial networks (GANs), which have demonstrated remarkable success in producing realistic colorizations. For instance, Zhang et al. proposed a deep learning approach that combines a CNN with high-level features extracted from a pre-trained model, achieving impressive results in automatic colorization tasks [1]. Similarly, Deshpande et al. introduced a variational approach to produce diverse and contextually appropriate colorizations, highlighting the potential of probabilistic models in this domain [3].

Despite these advancements, challenges remain, particularly in generating contextually accurate and diverse colorizations. Recent research has focused on leveraging generative models to address these issues. Wu et al. introduced a method that utilizes a pretrained GAN to provide rich and diverse color priors, enabling the production of vivid and varied colorizations [2].
Despite these advancements, challenges remain, particularly in generating contextually accurate and diverse colorizations. Recent research has focused on leveraging generative models to address these issues. Wu et al. introduced a method that utilizes a pretrained GAN to provide rich and diverse color priors, enabling the production of vivid and varied colorizations [2].

In this project, we aim to conduct a comparative study of various deep learning architectures for image colorization, including simple CNN-based encoder-decoders, U-Net architectures with pre-trained backbones, and GAN-based frameworks. By systematically evaluating these models across multiple datasets, we seek to identify the most effective architecture for producing realistic and high-quality colorizations, thereby advancing the current state of the art in automatic image colorization.




<section id="approach">
  <h2>Approach</h2>
<section id="approach">
  <h2>Approach</h2>
  To solve the problem of automatic image coloration, we employed a progressive approach involving multiple architectures. This approach was designed to analyze and compare the effectiveness of various models in transforming grayscale images into realistic colorized versions. The process included data preprocessing and three main modeling phases: baseline CNN encoder-decoder, U-Net architecture with a ResNet-18 backbone, and a Conditional GAN (cGAN) with a PatchGAN discriminator [4].

  <h3>Data Preprocessing</h3>
  <ul>
    <li>The images from the COCO dataset were converted into the Lab color space, where the L* channel (lightness) represents grayscale information, and the A* and B* channels encode color information.</li>
    <li>The L* channel was used as the input to the model, while the A* and B* channels served as the ground truth outputs for supervised learning.</li>
    <li>To ensure consistency, all images were resized to 256x256 pixels and normalized to a range between -1 and 1 for both input L channel and output AB channel.</li>
    <li>Random horizontal flipping was applied to the images before breakdown into L and AB channel to enhance model generalization and robustness.</li>
  </ul>
  <br>
  <img src="Images/LAB.png" alt="Trulli" class="custom-img"/>
<li>The L* channel was used as the input to the model, while the A* and B* channels served as the ground truth outputs for supervised learning.</li>
    <li>To ensure consistency, all images were resized to 256x256 pixels and normalized to a range between -1 and 1 for both input L channel and output AB channel.</li>
    <li>Random horizontal flipping was applied to the images before breakdown into L and AB channel to enhance model generalization and robustness.</li>
  </ul>
  <br>
  <img src="Images/LAB.png" alt="Trulli" class="custom-img"/>
<!-- add the three architecture diagram form borrowing or draw them on your own or tell tushar to put them   -->
  <h3>Model Architectures</h3>
  <ul>
    <li>
      <strong>CNN Encoder-Decoder:</strong> We started with a simple encoder-decoder CNN architecture [8] as our baseline. The encoder extracted latent grayscale features, while the decoder reconstructed the A* and B* color channels. This model served as a foundational benchmark for further comparisons.
      <ul>
        <li>
          <strong>Encoder:</strong> This module captures the spatial features by applying multiple convolution layers followed by downsampling like max pooling. This helps in reducing the dimensions and extracts important features.</li>
<ul>
    <li>
      <strong>CNN Encoder-Decoder:</strong> We started with a simple encoder-decoder CNN architecture [8] as our baseline. The encoder extracted latent grayscale features, while the decoder reconstructed the A* and B* color channels. This model served as a foundational benchmark for further comparisons.
      <ul>
        <li>
          <strong>Encoder:</strong> This module captures the spatial features by applying multiple convolution layers followed by downsampling like max pooling. This helps in reducing the dimensions and extracts important features.</li>
          <li><strong>Lantent Representation:</strong> The encoder outputs a feature representation which is compact in nature and gives the most vital spatial and semantic information
          </li><li><strong>Decoder:</strong> The decoder takes the input from the encoder and performs upsampling with operations like transposed convolutions or interpolation. It thereby increases the spatial dimensions, performs convolutions to refine the features to output the finaloutput.
          </li><li><strong>Output layer:</strong> We have a final convolution layer that outputs the desired output which is in the form of a 2-dimensional AB channel. This output is concatenated with the L channel to produce a LAB image which is then converted to an RGB image.</strong>
        </li>
      </ul>
    </li>

    <br>
</li><li><strong>Output layer:</strong> We have a final convolution layer that outputs the desired output which is in the form of a 2-dimensional AB channel. This output is concatenated with the L channel to produce a LAB image which is then converted to an RGB image.</strong>
        </li>
      </ul>
    </li>

    <br>
    <li>
      <strong>U-Net with ResNet-18 Backbone:</strong> The U-Net architecture [6], with a ResNet-18 backbone in the encoder, was the second phase of our approach. Pre-trained weights from ImageNet were loaded into the encoder to leverage transfer learning, enhancing the model's ability to extract meaningful features. The pre-trained U-Net was fine-tuned on our dataset for 20 epochs to adapt to the specific task of image colorization. The U-Net model was implemented with the DynamicUnet class in fastai [9].
      <ul>
        <li>
          <strong>Encoder: </strong> Similar to the Encoder-Decoder module, the Encoder in UNet captures importatant spatial features, reducing the image size while extracting important features. Here we replace the traditional encoder with a ResNet-18 architecture. ResNet-18 consists of convolutional layers and skip connections (in the form of residual blocks) that help to learn deeper representations without having the vanishing gradient problem.
<br>
    <li>
      <strong>U-Net with ResNet-18 Backbone:</strong> The U-Net architecture [6], with a ResNet-18 backbone in the encoder, was the second phase of our approach. Pre-trained weights from ImageNet were loaded into the encoder to leverage transfer learning, enhancing the model's ability to extract meaningful features. The pre-trained U-Net was fine-tuned on our dataset for 20 epochs to adapt to the specific task of image colorization. The U-Net model was implemented with the DynamicUnet class in fastai [9].
      <ul>
        <li>
          <strong>Encoder: </strong> Similar to the Encoder-Decoder module, the Encoder in UNet captures importatant spatial features, reducing the image size while extracting important features. Here we replace the traditional encoder with a ResNet-18 architecture. ResNet-18 consists of convolutional layers and skip connections (in the form of residual blocks) that help to learn deeper representations without having the vanishing gradient problem.  
        </li><li><strong>Connection: </strong> The deepest part of the UNet from the encoder gets connected to the decoder, which has captured the most important spatial features
        </li><li><strong>Decoder: </strong> This model performs upsamling and recaptures the image resolution by concantenating them with corresponding feature maps from encoder.
<strong>U-Net with ResNet-18 Backbone:</strong> The U-Net architecture [6], with a ResNet-18 backbone in the encoder, was the second phase of our approach. Pre-trained weights from ImageNet were loaded into the encoder to leverage transfer learning, enhancing the model's ability to extract meaningful features. The pre-trained U-Net was fine-tuned on our dataset for 20 epochs to adapt to the specific task of image colorization. The U-Net model was implemented with the DynamicUnet class in fastai [9].
      <ul>
        <li>
          <strong>Encoder: </strong> Similar to the Encoder-Decoder module, the Encoder in UNet captures importatant spatial features, reducing the image size while extracting important features. Here we replace the traditional encoder with a ResNet-18 architecture. ResNet-18 consists of convolutional layers and skip connections (in the form of residual blocks) that help to learn deeper representations without having the vanishing gradient problem.  
        </li><li><strong>Connection: </strong> The deepest part of the UNet from the encoder gets connected to the decoder, which has captured the most important spatial features
        </li><li><strong>Decoder: </strong> This model performs upsamling and recaptures the image resolution by concantenating them with corresponding feature maps from encoder.
<ul>
        <li>
          <strong>Encoder: </strong> Similar to the Encoder-Decoder module, the Encoder in UNet captures importatant spatial features, reducing the image size while extracting important features. Here we replace the traditional encoder with a ResNet-18 architecture. ResNet-18 consists of convolutional layers and skip connections (in the form of residual blocks) that help to learn deeper representations without having the vanishing gradient problem.  
        </li><li><strong>Connection: </strong> The deepest part of the UNet from the encoder gets connected to the decoder, which has captured the most important spatial features
        </li><li><strong>Decoder: </strong> This model performs upsamling and recaptures the image resolution by concantenating them with corresponding feature maps from encoder.
      </li><strong>Skip Connections: </strong> The skips connections link the encoder layer to the corresponding decoder layers, which preserves fine-grained spatial features which get lost during the downsampling operation. 
    </li><strong>Training: </strong> The UNet was trained using the Adam optimizer for 20 epochs with L1 loss as the criterion, since it - preserves spatial structure, it is robust to any outliers and has a better gradient flow. 
    </li>
      </ul>
    </li>
</li><strong>Skip Connections: </strong> The skips connections link the encoder layer to the corresponding decoder layers, which preserves fine-grained spatial features which get lost during the downsampling operation. 
    </li><strong>Training: </strong> The UNet was trained using the Adam optimizer for 20 epochs with L1 loss as the criterion, since it - preserves spatial structure, it is robust to any outliers and has a better gradient flow. 
    </li>
      </ul>
    </li>

    <center><img src="Images/UNet Arch.png" alt="UNet Architecture" class="custom-img"></center>
    
    <br>
    <li>
      <strong>Conditional GAN:</strong> The pre-trained U-Net was integrated as the generator in a Conditional GAN (cGAN) framework. cGANs were chosen because they condition the output (colorized image) on the input (grayscale image), enabling the generator to produce outputs that are contextually consistent with the input image. Our implementation is inspired by the pix2pix implementation [4].
      <ul>
<center><img src="Images/UNet Arch.png" alt="UNet Architecture" class="custom-img"></center>
    
    <br>
    <li>
      <strong>Conditional GAN:</strong> The pre-trained U-Net was integrated as the generator in a Conditional GAN (cGAN) framework. cGANs were chosen because they condition the output (colorized image) on the input (grayscale image), enabling the generator to produce outputs that are contextually consistent with the input image. Our implementation is inspired by the pix2pix implementation [4].
      <ul>
        <li><strong>PatchGAN Discriminator:</strong> We used a PatchGAN discriminator, which evaluates the realism of small image patches rather than the entire image. This approach ensures local coherence, as the discriminator classifies each patch of the generated image as real or fake. PatchGAN was chosen because it is effective in preserving fine details and textures [5], which are critical for realistic image colorization.</li>
        <li><strong>Generator:</strong> The cGAN uses the UNet module as it's generator output. It creates images using the UNet module by effectively capturing spatial dimensions with skip layers. This enables accurate mappings from grayscale to colorized outputs.
<br>
    <li>
      <strong>Conditional GAN:</strong> The pre-trained U-Net was integrated as the generator in a Conditional GAN (cGAN) framework. cGANs were chosen because they condition the output (colorized image) on the input (grayscale image), enabling the generator to produce outputs that are contextually consistent with the input image. Our implementation is inspired by the pix2pix implementation [4].
      <ul>
        <li><strong>PatchGAN Discriminator:</strong> We used a PatchGAN discriminator, which evaluates the realism of small image patches rather than the entire image. This approach ensures local coherence, as the discriminator classifies each patch of the generated image as real or fake. PatchGAN was chosen because it is effective in preserving fine details and textures [5], which are critical for realistic image colorization.</li>
        <li><strong>Generator:</strong> The cGAN uses the UNet module as it's generator output. It creates images using the UNet module by effectively capturing spatial dimensions with skip layers. This enables accurate mappings from grayscale to colorized outputs.
<li>
      <strong>Conditional GAN:</strong> The pre-trained U-Net was integrated as the generator in a Conditional GAN (cGAN) framework. cGANs were chosen because they condition the output (colorized image) on the input (grayscale image), enabling the generator to produce outputs that are contextually consistent with the input image. Our implementation is inspired by the pix2pix implementation [4].
      <ul>
        <li><strong>PatchGAN Discriminator:</strong> We used a PatchGAN discriminator, which evaluates the realism of small image patches rather than the entire image. This approach ensures local coherence, as the discriminator classifies each patch of the generated image as real or fake. PatchGAN was chosen because it is effective in preserving fine details and textures [5], which are critical for realistic image colorization.</li>
        <li><strong>Generator:</strong> The cGAN uses the UNet module as it's generator output. It creates images using the UNet module by effectively capturing spatial dimensions with skip layers. This enables accurate mappings from grayscale to colorized outputs.
<strong>Conditional GAN:</strong> The pre-trained U-Net was integrated as the generator in a Conditional GAN (cGAN) framework. cGANs were chosen because they condition the output (colorized image) on the input (grayscale image), enabling the generator to produce outputs that are contextually consistent with the input image. Our implementation is inspired by the pix2pix implementation [4].
      <ul>
        <li><strong>PatchGAN Discriminator:</strong> We used a PatchGAN discriminator, which evaluates the realism of small image patches rather than the entire image. This approach ensures local coherence, as the discriminator classifies each patch of the generated image as real or fake. PatchGAN was chosen because it is effective in preserving fine details and textures [5], which are critical for realistic image colorization.</li>
        <li><strong>Generator:</strong> The cGAN uses the UNet module as it's generator output. It creates images using the UNet module by effectively capturing spatial dimensions with skip layers. This enables accurate mappings from grayscale to colorized outputs.
        </li><li> <strong>Discriminator: </strong> This is a custom CNN module which differentiates between the real images or generated ones. This assesses the authenticity, which makes use of convolutions to differentiate between real or generated real and generated colorizations.
<ul>
        <li><strong>PatchGAN Discriminator:</strong> We used a PatchGAN discriminator, which evaluates the realism of small image patches rather than the entire image. This approach ensures local coherence, as the discriminator classifies each patch of the generated image as real or fake. PatchGAN was chosen because it is effective in preserving fine details and textures [5], which are critical for realistic image colorization.</li>
        <li><strong>Generator:</strong> The cGAN uses the UNet module as it's generator output. It creates images using the UNet module by effectively capturing spatial dimensions with skip layers. This enables accurate mappings from grayscale to colorized outputs.
        </li><li> <strong>Discriminator: </strong> This is a custom CNN module which differentiates between the real images or generated ones. This assesses the authenticity, which makes use of convolutions to differentiate between real or generated real and generated colorizations.
        </li><li><strong>Training: </strong> We trained the cGAN using BCE Loss for 20 epochs. We used the adam optimizer again for training the discriminator with decay rate taken as 0.9.</li> 
      </ul>
    </li>

    <center><img src="Images/GAN Arch.png" alt="GAN Architecture" class="custom-img"></center>


  </ul>

  <h3>Implementation Details</h3>
  <ul>
</ul>

  <h3>Implementation Details</h3>
  <ul>
    <li><strong>Dataset:</strong> A subset of COCO dataset was used, consisting of 10,000 images. The dataset was split into 8,000 images for training and 2,000 for testing.</li>
    <li><strong>Training Setup:</strong> The CNN encoder-decoder was trained for 100 epochs as a baseline. The U-Net architecture, pre-trained on ImageNet, was fine-tuned on our dataset for 20 epochs. The Conditional GAN was trained by integrating the this U-Net as a generator with the PatchGAN discriminator.</li>
    <li><strong>Pre-trained Models:</strong> The ImageNet pre-trained weights for the ResNet-18 backbone was utilized for U-net generator pre-trainig to expedite the training process and to get better performance compared to a model trained from scratch.</li>
  </ul>
  <table>
    <tr>
        <td><img src="Images/CNNTraining.png" alt="Trulli" class="custom-img" width="600" height="600"/></td>
        <td><img src="Images/UnetTraining.png" alt="Trulli" class="custom-img" width="600" height="600" /></td>
    </tr>
</table>
  <!-- <img src="Images/CNNTraining.png" alt="Trulli" class="custom-img" width="600" height="600"/> <img src="Images/UnetTraining.png" alt="Trulli" class="custom-img" width="600" height="600"/> -->
  <h3>Design Choices</h3>
  <ul>
    Our GAN architecture is inspired by the pix2pix architecture [4].
</tr>
</table>
  <!-- <img src="Images/CNNTraining.png" alt="Trulli" class="custom-img" width="600" height="600"/> <img src="Images/UnetTraining.png" alt="Trulli" class="custom-img" width="600" height="600"/> -->
  <h3>Design Choices</h3>
  <ul>
    Our GAN architecture is inspired by the pix2pix architecture [4].
    <li><strong>Why Conditional GAN:</strong> cGANs condition the output on the input, ensuring that the generated image aligns with the grayscale input's semantic content. Conditional GANs (cGANs) learn a conditional generative model
      [10]. This makes cGANs suitable for image-to-image translation tasks, where we condition on an input image and generate a corresponding output image.[4]</li>
    <li><strong>Why PatchGAN Discriminator:</strong> PatchGAN discriminates at the level of image patches, which is effective for ensuring local coherence in generated images. This is particularly useful for image colorization, where local color consistency is critical.</li>
  </ul>

</section>

<section id="experiments-results">
  <h2>Experiments and Results</h2>

  <h3>Experimental Setup</h3>
  <ul>
    <li><strong>Datasets:</strong> A subset of the COCO dataset with 10,000 images. The training set consisted of 8,000 images, and the test set included 2,000 images.</li>
<section id="experiments-results">
  <h2>Experiments and Results</h2>

  <h3>Experimental Setup</h3>
  <ul>
    <li><strong>Datasets:</strong> A subset of the COCO dataset with 10,000 images. The training set consisted of 8,000 images, and the test set included 2,000 images.</li>
    <li><strong>Metrics:</strong> L1 Error was used to evaluate pixel-wise accuracy during training. Mean Absolute Error (MAE) and Learned Perceptual Image Patch Similarity (LPIPS) score (using latent features from pretrained VGG) [7] are used to quantitatively compare the model performances.</li>
    <li><strong>Hardware:</strong> The experiments were performed on Google Colab with the L4 GPU.</li>
  </ul>

  <h3>Results</h3>
  <ul>
    <li><strong>CNN Encoder-Decoder:</strong> Achieved baseline results with limited ability to capture complex features, showing noticeable artifacts in the colorized outputs. We got a validation loss of 0.0848 after training it for 100 epochs. We trained the module using the L1 loss criterion with learning rate set as 0.001</li>
    <li><strong>U-Net with ResNet-18 Backbone:</strong> Improved performance compared to the baseline, with enhanced feature extraction and better contextual colorization. The UNet was trained for 20 epochs and acheieved a L1 loss of 0.07586. We set the learning rate to 0.0001</li>
</ul>

  <h3>Results</h3>
  <ul>
    <li><strong>CNN Encoder-Decoder:</strong> Achieved baseline results with limited ability to capture complex features, showing noticeable artifacts in the colorized outputs. We got a validation loss of 0.0848 after training it for 100 epochs. We trained the module using the L1 loss criterion with learning rate set as 0.001</li>
    <li><strong>U-Net with ResNet-18 Backbone:</strong> Improved performance compared to the baseline, with enhanced feature extraction and better contextual colorization. The UNet was trained for 20 epochs and acheieved a L1 loss of 0.07586. We set the learning rate to 0.0001</li>
    <li><strong>Conditional GAN:</strong> Produced the most realistic and visually consistent outputs. The incorporation of adversarial loss with the PatchGAN discriminator significantly reduced artifacts and improved local color coherence. We achieved a discriminator loss of 0.59404 and discriminator loss of 11.37 after training for 20 epochs. </li>
    <br>
  
    The MAE and LPIPS value on the test dataset for each model is listed in Table 1.

    <h5>Table 1: Results</h5>
    <table border="1" style="border-collapse: collapse; width: 50%;"> 
      <thead>
          <tr>
              <th>Model</th>
              <th>MAE</th>
              <th>LPIPS</th>
          </tr>
      </thead>
      <tbody>
          <tr align="center">
<th>MAE</th>
              <th>LPIPS</th>
          </tr>
      </thead>
      <tbody>
          <tr align="center">
              <td>CNN Encoder-Decoder</td>
              <td>0.0848</td>
              <td>0.148</td>
          </tr> 
          <tr align="center">
              <td>U-Net (ResNet-18 backbone)</td>
              <td>0.0846</td>
              <td>0.141</td>
          </tr>
          <tr align="center">
            <td><strong>cGAN with U-Net</strong></td>
            <td><strong>0.0807</strong></td>
            <td><strong>0.132</strong></td>
        </tr></center>
      </tbody>
  </table>
  </ul> <br><br>

  <h3>Qualitative Results</h3>
  Outputs by each model on 5 sample images from the test dataset are listed below:

  <img src="Images/result_1.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_2.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_3.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_4.png" alt="Trulli" class="custom-img"/>
  <img src="Images/result_5.png" alt="Trulli" class="custom-img"/>

  <h3>Parameter Analysis</h3>
    Adjusting the patch size in the PatchGAN discriminator affected the realism of local features.
    Larger patches reduced sensitivity to finer details, while smaller patches improved local
    consistency at the expense of global structure.
    <ul>
<h3>Parameter Analysis</h3>
    Adjusting the patch size in the PatchGAN discriminator affected the realism of local features.
    Larger patches reduced sensitivity to finer details, while smaller patches improved local
    consistency at the expense of global structure.
    <ul>
      <li><strong>Epoch: </strong> We have tried to use the number of epochs for getting better accuracy but also making sure we do not overfit any model. Hence for training the UNet we used 20 epochs, after which we were seeing results getting converged with no frther reductions in losses. Similarly for cGAN, we kept the epoch numbers to 20 so that the generator does not overflow and discriminator does not overfit.</li>
      <li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
Larger patches reduced sensitivity to finer details, while smaller patches improved local
    consistency at the expense of global structure.
    <ul>
      <li><strong>Epoch: </strong> We have tried to use the number of epochs for getting better accuracy but also making sure we do not overfit any model. Hence for training the UNet we used 20 epochs, after which we were seeing results getting converged with no frther reductions in losses. Similarly for cGAN, we kept the epoch numbers to 20 so that the generator does not overflow and discriminator does not overfit.</li>
      <li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
consistency at the expense of global structure.
    <ul>
      <li><strong>Epoch: </strong> We have tried to use the number of epochs for getting better accuracy but also making sure we do not overfit any model. Hence for training the UNet we used 20 epochs, after which we were seeing results getting converged with no frther reductions in losses. Similarly for cGAN, we kept the epoch numbers to 20 so that the generator does not overflow and discriminator does not overfit.</li>
      <li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
<ul>
      <li><strong>Epoch: </strong> We have tried to use the number of epochs for getting better accuracy but also making sure we do not overfit any model. Hence for training the UNet we used 20 epochs, after which we were seeing results getting converged with no frther reductions in losses. Similarly for cGAN, we kept the epoch numbers to 20 so that the generator does not overflow and discriminator does not overfit.</li>
      <li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
<li><strong>Epoch: </strong> We have tried to use the number of epochs for getting better accuracy but also making sure we do not overfit any model. Hence for training the UNet we used 20 epochs, after which we were seeing results getting converged with no frther reductions in losses. Similarly for cGAN, we kept the epoch numbers to 20 so that the generator does not overflow and discriminator does not overfit.</li>
      <li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
<li><strong>Learning Rate: </strong> Since learning rate is a vital parameter in training neural networks since they control by how much the weights should be adjusted, we kept it as 0.01 so as to avoid any irregularities and unstable updates that would have destabilized the training.</li>
      <li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
      <li><strong>Criterion: </strong> We used L1 loss as the criterion for training because it allows for pixelwise accuracy which helps in generating realistic images. We used the BCE loss for training the GAN which is a common method, the discriminator learns to differentiate between real and fake images and generator is given the taks to fool the discriminator into classfying the generated images as real ones. </li>
      <li><strong>Batch Size: </strong> The batch size allows the neural network to update the weights in batches (controls the frequency as to when weights are updated). Since larger batch size required more memory and smaller ones lead to noiser gradients, we set the batch size as 32. </li>
<li><strong>Beta1: </strong> Beta1 used in training the GAN is also a important parameter for the ADAM optimizer to take into account past weight updates for the current weights. We kept it as 0.5 to prevent any rapid updates, which allowed for stable training especially in earlier stages.</li>
      <li><strong>Criterion: </strong> We used L1 loss as the criterion for training because it allows for pixelwise accuracy which helps in generating realistic images. We used the BCE loss for training the GAN which is a common method, the discriminator learns to differentiate between real and fake images and generator is given the taks to fool the discriminator into classfying the generated images as real ones. </li>
      <li><strong>Batch Size: </strong> The batch size allows the neural network to update the weights in batches (controls the frequency as to when weights are updated). Since larger batch size required more memory and smaller ones lead to noiser gradients, we set the batch size as 32. </li>
      <!-- <li><strong>Noise Dimension: </strong> The noise dimension provides the size of random noise fed to the generator, larger ones lead to overfitting but help generate more creative and realistic images. We kept it as 100, which is usually used in GANs to provide more diversity.  </li> -->
    </ul>

  <h3>Trends</h3>
<li><strong>Criterion: </strong> We used L1 loss as the criterion for training because it allows for pixelwise accuracy which helps in generating realistic images. We used the BCE loss for training the GAN which is a common method, the discriminator learns to differentiate between real and fake images and generator is given the taks to fool the discriminator into classfying the generated images as real ones. </li>
      <li><strong>Batch Size: </strong> The batch size allows the neural network to update the weights in batches (controls the frequency as to when weights are updated). Since larger batch size required more memory and smaller ones lead to noiser gradients, we set the batch size as 32. </li>
      <!-- <li><strong>Noise Dimension: </strong> The noise dimension provides the size of random noise fed to the generator, larger ones lead to overfitting but help generate more creative and realistic images. We kept it as 100, which is usually used in GANs to provide more diversity.  </li> -->
    </ul>

  <h3>Trends</h3>
  The progression from traditional CNN-based encoder-decoder architectures to more advanced models, such as U-Nets and conditional GANs (cGANs), demonstrates notable advancements in image colorization quality.
<li><strong>Batch Size: </strong> The batch size allows the neural network to update the weights in batches (controls the frequency as to when weights are updated). Since larger batch size required more memory and smaller ones lead to noiser gradients, we set the batch size as 32. </li>
      <!-- <li><strong>Noise Dimension: </strong> The noise dimension provides the size of random noise fed to the generator, larger ones lead to overfitting but help generate more creative and realistic images. We kept it as 100, which is usually used in GANs to provide more diversity.  </li> -->
    </ul>

  <h3>Trends</h3>
  The progression from traditional CNN-based encoder-decoder architectures to more advanced models, such as U-Nets and conditional GANs (cGANs), demonstrates notable advancements in image colorization quality. 
  While CNN-based encoder-decoders often produce desaturated, brownish tones due to L1 loss minimization during training, U-Nets, equipped with a ResNet-18 backbone and skip connections, show improved colorization with greater semantic fidelity but still lean toward grayish hues. 
  The integration of U-Nets as generators within a cGAN framework, coupled with a PatchGAN discriminator, significantly enhances colorization quality by leveraging adversarial training. The paradigm provides the U-Net the necessary feedback to produce more realistic images.
</ul>

  <h3>Trends</h3>
  The progression from traditional CNN-based encoder-decoder architectures to more advanced models, such as U-Nets and conditional GANs (cGANs), demonstrates notable advancements in image colorization quality. 
  While CNN-based encoder-decoders often produce desaturated, brownish tones due to L1 loss minimization during training, U-Nets, equipped with a ResNet-18 backbone and skip connections, show improved colorization with greater semantic fidelity but still lean toward grayish hues. 
  The integration of U-Nets as generators within a cGAN framework, coupled with a PatchGAN discriminator, significantly enhances colorization quality by leveraging adversarial training. The paradigm provides the U-Net the necessary feedback to produce more realistic images.
  As expected, cGAN performs better than the baseline encoder-decoder and U-Net methods due to the adversarial training which gives the generator better feedback for generating colorized images.
</section>

<br>
<!-- Results -->
<section id="conclusion">
  <h2>Conclusion</h2>
    This report has described the design, implementation, and evaluation of various deep learning
    architectures for automatic image coloration. Starting with a baseline CNN encoder-decoder, 
    progressing to a U-Net with a ResNet-18 backbone, and culminating with a Conditional GAN
<!-- Results -->
<section id="conclusion">
  <h2>Conclusion</h2>
    This report has described the design, implementation, and evaluation of various deep learning
    architectures for automatic image coloration. Starting with a baseline CNN encoder-decoder, 
    progressing to a U-Net with a ResNet-18 backbone, and culminating with a Conditional GAN 
    incorporating a PatchGAN discriminator, the study highlighted the strengths and limitations of 
    each approach. The results demonstrated that increasing model complexity, along with the 
    integration of adversarial loss, significantly improved the realism and coherence of the colorized outputs.
    The U-Net architecture with pre-trained ResNet-18 weights provided a strong foundation for 
    feature extraction and contextual understanding, while the Conditional GAN produced the 
    most visually realistic results by leveraging adversarial training and patch-level discrimination. 
    These findings validated the effectiveness of combining local and global feature learning to achieve 
    high-quality colorizations.
  <h4>Future Improvements</h4>
  To further enhance the approach, several improvements could be considered:
  <ul>
most visually realistic results by leveraging adversarial training and patch-level discrimination. 
    These findings validated the effectiveness of combining local and global feature learning to achieve 
    high-quality colorizations.
  <h4>Future Improvements</h4>
  To further enhance the approach, several improvements could be considered:
  <ul>
    <li><strong>Data Augmentation:</strong> Expanding the dataset with more diverse images and applying advanced augmentation techniques could help the model generalize better to unseen images.</li>
    <li><strong>Loss Functions:</strong> Experimenting with perceptual loss or combining MSE with adversarial loss might improve the models ability to generate outputs that are both quantitatively and qualitatively superior.</li>
    <li><strong>Higher-Resolution Models:</strong> Incorporating super-resolution techniques or adapting the model for higher-resolution inputs could enhance the fine details in the colorized images.</li>
    <li><strong>Diffusion Models:</strong> Diffusion models are state of the art image generation models which iteratively take noisy or partially incomplete images and refine them to produce a high-quality output. 
      <ul>
<ul>
    <li><strong>Data Augmentation:</strong> Expanding the dataset with more diverse images and applying advanced augmentation techniques could help the model generalize better to unseen images.</li>
    <li><strong>Loss Functions:</strong> Experimenting with perceptual loss or combining MSE with adversarial loss might improve the models ability to generate outputs that are both quantitatively and qualitatively superior.</li>
    <li><strong>Higher-Resolution Models:</strong> Incorporating super-resolution techniques or adapting the model for higher-resolution inputs could enhance the fine details in the colorized images.</li>
    <li><strong>Diffusion Models:</strong> Diffusion models are state of the art image generation models which iteratively take noisy or partially incomplete images and refine them to produce a high-quality output. 
      <ul>
        <li>Models like Denoising Diffusion Probabilistic Models (DDPMs) [11] can be used, which add noise to images in the feed forward process and learns to reverse this during the training period. DDPM and Guided Diffusion [12] are some framework that can be explored in future.</li>
<li><strong>Data Augmentation:</strong> Expanding the dataset with more diverse images and applying advanced augmentation techniques could help the model generalize better to unseen images.</li>
    <li><strong>Loss Functions:</strong> Experimenting with perceptual loss or combining MSE with adversarial loss might improve the models ability to generate outputs that are both quantitatively and qualitatively superior.</li>
    <li><strong>Higher-Resolution Models:</strong> Incorporating super-resolution techniques or adapting the model for higher-resolution inputs could enhance the fine details in the colorized images.</li>
    <li><strong>Diffusion Models:</strong> Diffusion models are state of the art image generation models which iteratively take noisy or partially incomplete images and refine them to produce a high-quality output. 
      <ul>
        <li>Models like Denoising Diffusion Probabilistic Models (DDPMs) [11] can be used, which add noise to images in the feed forward process and learns to reverse this during the training period. DDPM and Guided Diffusion [12] are some framework that can be explored in future.</li>
<li><strong>Loss Functions:</strong> Experimenting with perceptual loss or combining MSE with adversarial loss might improve the models ability to generate outputs that are both quantitatively and qualitatively superior.</li>
    <li><strong>Higher-Resolution Models:</strong> Incorporating super-resolution techniques or adapting the model for higher-resolution inputs could enhance the fine details in the colorized images.</li>
    <li><strong>Diffusion Models:</strong> Diffusion models are state of the art image generation models which iteratively take noisy or partially incomplete images and refine them to produce a high-quality output. 
      <ul>
        <li>Models like Denoising Diffusion Probabilistic Models (DDPMs) [11] can be used, which add noise to images in the feed forward process and learns to reverse this during the training period. DDPM and Guided Diffusion [12] are some framework that can be explored in future.</li>
        <li>Another set of these models are Latent Diffusion Models (LDM) [13], which compress and compute the predictions in a compressed data space making them more efficient during computation. Stable Diffusion which are state of the art (SOTA) models nowadays, can be used to generate more realistic images.</li>
      </ul>
    </li>
  </ul>
    Overall, this study serves as a foundation for understanding and improving automatic image
<li>Models like Denoising Diffusion Probabilistic Models (DDPMs) [11] can be used, which add noise to images in the feed forward process and learns to reverse this during the training period. DDPM and Guided Diffusion [12] are some framework that can be explored in future.</li>
        <li>Another set of these models are Latent Diffusion Models (LDM) [13], which compress and compute the predictions in a compressed data space making them more efficient during computation. Stable Diffusion which are state of the art (SOTA) models nowadays, can be used to generate more realistic images.</li>
      </ul>
    </li>
  </ul>
    Overall, this study serves as a foundation for understanding and improving automatic image 
    coloration, paving the way for more robust and application-specific advancements in the field. We believe that going forward, we can make build a SOTA model that would take any grayscale image and produce it's colorized version, which has huge applications like in image compression. Since grayscale image stores data in reduced data space (only 1 channel), this can give huge potential for data storage techniques going forward.
</section>

<h2>References</h2>
<ol>
<li>
coloration, paving the way for more robust and application-specific advancements in the field. We believe that going forward, we can make build a SOTA model that would take any grayscale image and produce it's colorized version, which has huge applications like in image compression. Since grayscale image stores data in reduced data space (only 1 channel), this can give huge potential for data storage techniques going forward.
</section>

<h2>References</h2>
<ol>
<li>
R. Zhang, P. Isola, and A. A. Efros, "Colorful Image Colorization," Proceedings of the European Conference on Computer Vision (ECCV), pp. 649666, 2016. [Online]. Available: https://arxiv.org/abs/1603.08511
</li>
<li>
Y. Wu, P. Zhang, and C. Zhang, "Generative Colorization for Diverse Images," arXiv preprint arXiv:2108.08826, 2021. [Online]. Available: https://arxiv.org/abs/2108.08826
</li>
<li>
A. Deshpande, J. Lu, M. Yeh, M. Jin, and D. Forsyth, "Learning Diverse Image Colorization," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 68376845, 2017. [Online]. Available: https://arxiv.org/abs/1612.01958
</li>
<li>
Isola, Phillip, et al. "Image-to-image translation with conditional adversarial networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
</li>
<li>
A. Deshpande, J. Lu, M. Yeh, M. Jin, and D. Forsyth, "Learning Diverse Image Colorization," Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 68376845, 2017. [Online]. Available: https://arxiv.org/abs/1612.01958
</li>
<li>
Isola, Phillip, et al. "Image-to-image translation with conditional adversarial networks." Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.
</li>
<li>
C. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. ECCV, 2016
</li>
<li>
O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015
</li>
<li>
Zhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,
  O. The unreasonable effectiveness of deep features as a
  perceptual metric. In Proceedings of the IEEE conference
  on computer vision and pattern recognition, 2018.
</li>
<li>
basu369victor. Image Colorization Basic Implementation with CNN. Kaggle, 28 Mar. 2020, www.kaggle.com/code/basu369victor/image-colorization-basic-implementation-with-cnn. Accessed 15 Nov. 2024. 
</li>
<li>
Howard J, Gugger S. Fastai: A Layered API for Deep Learning. Information. 2020; 11(2):108. https://doi.org/10.3390/info11020108
</li>
<li>
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
</li>
<li>
Howard J, Gugger S. Fastai: A Layered API for Deep Learning. Information. 2020; 11(2):108. https://doi.org/10.3390/info11020108
</li>
<li>
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,
  D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In NIPS, 2014.
</li>
<li>
Ho, Jonathan, Ajay Jain, and Pieter Abbeel. "Denoising diffusion probabilistic models." Advances in neural information processing systems 33 (2020): 6840-6851.
</li>
<li>
Dhariwal, Prafulla, and Alexander Nichol. "Diffusion models beat gans on image synthesis." Advances in neural information processing systems 34 (2021): 8780-8794.
</li>
<li>
Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.
</li>
</ol>
<!-- <br><br> -->

<!-- Main Results Figure --> 
<!-- <div style="text-align: center;">

</div>
<br><br> -->

<!-- Results -->
<!-- <h3>Qualitative results</h3>
Show several visual examples of inputs/outputs of your system (success cases and failures) that help us better understand your approach. -->

<!-- Main Results Figure --> 
<!-- <div style="text-align: center;">
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br> -->

<!-- Conclusion -->
<!-- <h3>Conclusion</h3>
<img style="height: 300px;" alt="" src="qual_results.png">
</div>
<br><br> -->

<!-- Conclusion -->
<!-- <h3>Conclusion</h3>
This report has described .... Briefly summarize what you have done. 
<br><br> -->

<!-- References -->
<!-- <h3>References</h3>
Provide a list of references to other work that supported your project.
<br><br> -->


  <hr>
  <footer> 
  <p> Gauransh Sawhney, Tushar Deshpande and Daksh Dave </p>
  </footer>
</div>
</div>

<br><br>

</body></html>
```python
import numpy as np
import pandas as pd
```

```python
data = pd.read_csv('movie_metadata.csv')
data.head()
```

```python
data.dropna(axis=0,how='any')
```
# IMDB-MovieRating-Prediction
A machine learning application to predict imdb moview rating based on some features.

## About the data set
The data set along with its description is defined [here](https://www.kaggle.com/deepmatrix/imdb-5000-movie-dataset).

## Steps
* After preprocessing/cleaning the data there were around 2000 data points.
* The main task was to predict the IMDB rating of a movie.
* This was considered as a classification problem by taking 10 classes 1-10 i.e the rating.
* There were initially many features which was then reduced using the domain knowledge finally only 9 features was taken into consideration, the filtered and processed data is saved in the after csv.csv file.
* All the models are pickled in the models folder.

## Usage
To view the predictions run the main.py just change the filename of your data

## Results

Results are summarized in the table
* First, the accuracy was calculated by letting the model only predict the exact rating.

| Model | Accuracy (%) |
|-------------------- |-------------- |
| K Nearest Neighbours| 37.52 |
| Logitic Regression | 40.9 |
| SVC | 36.35 |
| Naive Bayes(Bernoulli)| 33.67 |

* Secondly, the accuracy was calculated by letting the model only predict the range of rating i.e with the error of +-1, so for e.g if the rating predicted was 8 then the accuracy was tested if the actual label was between 8-1 to 8+1.
| Logitic Regression | 40.9 |
| SVC | 36.35 |
| Naive Bayes(Bernoulli)| 33.67 |

* Secondly, the accuracy was calculated by letting the model only predict the range of rating i.e with the error of +-1, so for e.g if the rating predicted was 8 then the accuracy was tested if the actual label was between 8-1 to 8+1.

| Model | Accuracy (%) |
|-------------------- |-------------- |
| K Nearest Neighbours| 80.90 |
| Logitic Regression | 85.09 |
| SVC | 83.91 |
| Naive Bayes(Bernoulli)| 80.23 |

## Conclusion
Finally, the Conclusion that I made from the results
* Predicting the exact rating is quiet difficult also any predictions wouldn't be that accurate because the main contribution towards the rating is the story of the movie.
* There seems very less relation between the stats and the rating since some actors may be good but the movie's facebook likes wouldn't be that high because of less publicity of the movie.
* L2 regularized logistic Regression gave the best results as compared to other classification algorithms.
PATH: main.py
LINES: 1-22

import pickle
import pandas as pd
from preprocess import process
from sklearn.linear_model import LogisticRegression

FILENAME = 'movie_metadata_filtered_aftercsv.csv'

def main():
    model = pickle.loads(open('models/LogRegression_thre1'))
    # provide your filename here
    process(filename='your_file_name.csv')
    datadf = pd.read_csv(FILENAME)
    datadf = datadf.drop(datadf.columns[[0]],axis=1)
    datadf = (datadf-datadf.mean())/(datadf.max()-datadf.min())
    X = np.array(datadf)
    predictions = model.predict(X)
    # consider the predicted rating to be in the range of +.- 1
    # for example of predicted is 7 then it may be between 6-8
    print predictions

if __name__ == '__main__':
    main()
PATH: ml_model.py
LINES: 1-45

import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
# from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.naive_bayes import GaussianNB,BernoulliNB
import pickle

np.random.seed(0)

FILENAME = 'movie_metadata_filtered_aftercsv.csv'
THRESHOLD_PREDICTION = 1

def _make_in_format(filename):
    datadf = pd.read_csv(filename)
    #separate classes and stuffs
    y = np.array(datadf['imdb_score'])
    datadf = datadf.drop(datadf.columns[[0,9]],axis=1)
    #normalize
    datadf = (datadf-datadf.mean())/(datadf.max()-datadf.min())
    X = np.array(datadf)

    return X,y

def _pickle_it(model,filename):
    a = pickle.dumps(model)
    write_file = open('models/'+filename,'w')
    write_file.write(a)

def accuracy_score(y_test,predictions):
        correct = []
        for i in range(len(y_test)):
            if y_test[i]>=predictions[i]-THRESHOLD_PREDICTION and y_test[i]<=predictions[i]+THRESHOLD_PREDICTION:
                correct.append(1)
            else:
                correct.append(0)

        accuracy = sum(map(int,correct))*1.0/len(correct)
        return accuracy

def Knn():
    X,y = _make_in_format(FILENAME)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)
PATH: ml_model.py
LINES: 46-80

model = KNeighborsClassifier(algorithm='ball_tree')
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    # print y_test
    _pickle_it(model,"Knn_thre1")
    print "knn score ",accuracy_score(y_test,predictions)*100

def LogRegression():
    X,y = _make_in_format(FILENAME)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)
    model = LogisticRegression(solver='newton-cg',multi_class='ovr',max_iter=200,penalty='l2')
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    _pickle_it(model,"LogRegression_thre1")
    print "LogRegression ",accuracy_score(y_test,predictions)*100

def Svm():
    X,y = _make_in_format(FILENAME)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)
    model = svm.SVC()
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    _pickle_it(model,"svm_thre1")
    print "SVM ",accuracy_score(y_test,predictions)*100

def naiveBayes():
    X,y = _make_in_format(FILENAME)
    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=1)
    model_guass = GaussianNB()
    model = BernoulliNB()
    model_guass.fit(X_train,y_train)
    model.fit(X_train,y_train)
    predictions_gauss = model_guass.predict(X_test)
    predictions = model.predict(X_test)
    _pickle_it(model,"Bernoulli_thre1")
PATH: ml_model.py
LINES: 81-95

_pickle_it(model_guass,"guass_thre1")
    print "naive bayes using gaussian ",accuracy_score(y_test,predictions_gauss)*100
    print "naive bayes using Bernoulli ",accuracy_score(y_test,predictions)*100




def main():
    Knn()
    LogRegression()
    Svm()
    naiveBayes()

if __name__ == '__main__':
    main()
```python
import numpy as np
import pandas as pd
```

```python
data = pd.read_csv('movie_metadata.csv')
data.head()
```

```python
data.dropna(axis=0,how='any')
```

```python
datadf = pd.read_csv('movie_metadata.csv')   
#color, duration, actor_3_fb_likes
datadf = datadf.drop(datadf.columns[[0,1,3,5,6,8,10,11,14,15,16,17,19,20,21,23,26]],axis=1)
```

```python
datadf.head()
```

```python
datadf = datadf.replace(0,float("NaN"))
```

```python
datadf.head()
print len(datadf)
```

```python
datadf = datadf.dropna(axis=0,how='any')
print len(datadf)
```

```python
datadf.head()
```

```python
datadf = datadf.drop(datadf.columns[[3]],axis=1)
```

```python
datadf = datadf.round(0)
```

```python
datadf = datadf.astype(int)
```

```python
datadf.head()
```

```python
np.array(datadf)
```

```python
type(datadf['budget'])
```

```python
type(datadf.iloc[:,6])
```

```python
datadf = pd.read_csv('movie_metadata_filtered_aftercsv.csv')
#separate classes and stuffs
y = np.array(datadf['imdb_score'])
X = np.array(datadf.drop(datadf.columns[[0,8]],axis=1))
```

```python
X
```

```python
len(X[0])
```

```python
y[np.newaxis].T
```

```python
datadf
```

```python
datadf.iloc[:,1:]
```

```python
def _make_in_format():
    datadf = pd.read_csv('movie_metadata_filtered_aftercsv.csv')
    #separate classes and stuffs
    y = np.array(datadf['imdb_score'])
```python
def _make_in_format():
    datadf = pd.read_csv('movie_metadata_filtered_aftercsv.csv')
    #separate classes and stuffs
    y = np.array(datadf['imdb_score'])
    datadf = datadf.drop(datadf.columns[[8,0]],axis=1)
    #normalize
    datadf = (datadf-datadf.mean())/(datadf.max()-datadf.min())
    X = np.array(datadf)

    return X,y
```

```python
from sklearn.cross_validation import train_test_split
X,y = _make_in_format()
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)
```

```python
X_train
```

```python
y_train
```
PATH: preprocess.py
LINES: 1-4

import numpy as np
import pandas as pd
PATH: preprocess.py
LINES: 5-16

def process(filename):
    datadf = pd.read_csv(filename)
    #color, duration, actor_3_fb_likes etc.
    datadf = datadf.drop(datadf.columns[[0,1,3,5,6,8,10,11,14,15,16,17,19,20,21,23,26]],axis=1)
    datadf = datadf.replace(0,float("NaN"))
    datadf = datadf.dropna(axis=0,how='any')
    # datadf.to_csv('movie_metadata_filtered.csv')
    #remove genres
    datadf = datadf.drop(datadf.columns[[3]],axis=1)
    #label classes
    datadf = datadf.astype(int)
    datadf.to_csv('movie_metadata_filtered_aftercsv.csv')
PATH: preprocess.py
LINES: 19-20

def main():
    process('movie_metadata.csv')
```python
import numpy as np
import pandas as pd
```

```python
iris=pd.read_csv('Iris.csv')
```

```python
iris.loc[iris['Species']=='Iris-setosa','Species']=0
iris.loc[iris['Species']=='Iris-versicolor','Species']=1
iris.loc[iris['Species']=='Iris-virginica','Species']=2
```

```python
X = iris[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']].values
Y = iris[['Species']].values.astype('uint8')
```

```python
def sigmoid(z):
    return 1 / (1.0 + np.exp(-z))

def sigmoidGradient(z):
    """
    computes the gradient of the sigmoid function
    """
    return sigmoid(z) *(1.0-sigmoid(z))
```

```python
def randInitializeWeights(L_in, L_out):
    """
    randomly initializes the weights of a layer with L_in incoming connections and L_out outgoing connections.
    """
    
    epi = (6**1/2) / (L_in + L_out)**1/2
    
    W = np.random.rand(L_out,L_in +1) *(2*epi) -epi
    
    return W
```

```python
input_layer_size  = 4
hidden_layer_size = 4
num_labels = 3
alpha,num_iters,Lambda=0.8,800,1
```

```python
initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size)

initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels)

initial_nn_params = np.append(initial_Theta1.flatten(),initial_Theta2.flatten())
```

```python
def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda=0.0):
    """
initial_nn_params = np.append(initial_Theta1.flatten(),initial_Theta2.flatten())
```

```python
def nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda=0.0):
    """
    nn_params contains the parameters unrolled into a vector
    
    compute the cost and gradient of the neural network
    """
    # Reshape nn_params back into the parameters Theta1 and Theta2
    Theta1 = nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)
    Theta2 = nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)
    
    m = X.shape[0]
    J=0
    X = np.hstack((np.ones((m,1)),X))
    y10 = np.zeros((m,num_labels))
    
    a1 = sigmoid(X @ Theta1.T)
    a1 = np.hstack((np.ones((m,1)), a1)) # hidden layer
    a2 = sigmoid(a1 @ Theta2.T) # output layer
    
    for i in range(num_labels):
        y10[:,i][:,np.newaxis] = np.where(y==i,1,0)
    for j in range(num_labels):
        J = J + sum(-y10[:,j] * np.log(a2[:,j]) - (1-y10[:,j])*np.log(1-a2[:,j]))
    
    cost = 1/m * J
    
    reg_J = cost + Lambda/(2*m) * (np.sum(Theta1[:,1:]**2) + np.sum(Theta2[:,1:]**2))
    
    # Implement the backpropagation algorithm to compute the gradients
    
    grad1 = np.zeros((Theta1.shape))
    grad2 = np.zeros((Theta2.shape))
    
    for i in range(m):
        xi= X[i,:] # 1 X 151
grad1 = np.zeros((Theta1.shape))
    grad2 = np.zeros((Theta2.shape))
    
    for i in range(m):
        xi= X[i,:] # 1 X 151
        a1i = a1[i,:] # 1 X 5
        a2i =a2[i,:] # 1 X 3
        d2 = a2i - y10[i,:]
        d1 = Theta2.T @ d2.T * sigmoidGradient(np.hstack((1,xi @ Theta1.T)))
        grad1= grad1 + d1[1:][:,np.newaxis] @ xi[:,np.newaxis].T
        grad2 = grad2 + d2.T[:,np.newaxis] @ a1i[:,np.newaxis].T
        
    grad1 = 1/m * grad1
    grad2 = 1/m*grad2
    
    grad1_reg = grad1 + (Lambda/m) * np.hstack((np.zeros((Theta1.shape[0],1)),Theta1[:,1:]))
    grad2_reg = grad2 + (Lambda/m) * np.hstack((np.zeros((Theta2.shape[0],1)),Theta2[:,1:]))
    
    return cost, grad1_reg, grad2_reg,reg_J, grad1,grad2
```

```python
def gradientDescentnn(X,y,initial_nn_params,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, num_labels):
    """
    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps
    with learning rate of alpha
    
    return theta and the list of the cost of theta during each iteration
    """
    Theta1 = initial_nn_params[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)
    Theta2 = initial_nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)
    
    m=len(y)
    J_history =[]
    
    for i in range(num_iters):
Theta2 = initial_nn_params[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)
    
    m=len(y)
    J_history =[]
    
    for i in range(num_iters):
        nn_params = np.append(Theta1.flatten(),Theta2.flatten())
        cost, grad1, grad2 = nnCostFunction(nn_params,input_layer_size, hidden_layer_size, num_labels,X, y,Lambda)[3:]
        #print(cost)
        Theta1 = Theta1 - (alpha * grad1)
        Theta2 = Theta2 - (alpha * grad2)
        J_history.append(cost)
    
    nn_paramsFinal = np.append(Theta1.flatten(),Theta2.flatten())
    return nn_paramsFinal , J_history

nnTheta, nnJ_history = gradientDescentnn(X,Y,initial_nn_params,alpha,num_iters,Lambda,input_layer_size, hidden_layer_size, num_labels)

Theta1 = nnTheta[:((input_layer_size+1) * hidden_layer_size)].reshape(hidden_layer_size,input_layer_size+1)

Theta2 = nnTheta[((input_layer_size +1)* hidden_layer_size ):].reshape(num_labels,hidden_layer_size+1)
```

```python
def predict(Theta1, Theta2, X):
    """
    Predict the label of an input given a trained neural network
    Outputs the predicted label of X given the trained weights of a neural
    network(Theta1, Theta2)
    """
    # Useful values
    m = X.shape[0]
    num_labels = Theta2.shape[0]

    p = np.zeros(m)
    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))
# Useful values
    m = X.shape[0]
    num_labels = Theta2.shape[0]

    p = np.zeros(m)
    h1 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), X], axis=1), Theta1.T))
    h2 = sigmoid(np.dot(np.concatenate([np.ones((m, 1)), h1], axis=1), Theta2.T))
#     print(h2)
    p = np.argmax(h2, axis=1)
    return p
```

```python
pred = predict(Theta1, Theta2, X)
print("Training Set Accuracy:",sum(pred[:,np.newaxis]==Y)[0]/150*100,"%")
```
# Iris
# Kharagpur-DS-Hackathon
This is the repository for submissions of Team BITS for both rounds of Kharagpur Data Science Hackathon, 2021, organized by Kharagpur Data Analytics Group, IIT Kharagpur.

## Round 1: Mengary Revenue Prediction
The link to the problem statement can be found [here](https://www.kaggle.com/c/mengary-revenue-prediction/overview).

Best R2 score: 0.92321

Rank: 28

## Round 2: Semantic Image Segmentation Challenge
The link to the problem statement can be found [here](https://www.kaggle.com/c/sematic-image-segmentation-challenge/overview).

Best IoU score: 0.404

Rank: 2
```python
%cd 
from google.colab import files
uploaded = files.upload()
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
!kaggle competitions download -c mengary-revenue-prediction
!ls
```

```python
import pandas as pd
import numpy as np
import seaborn as sns
```

```python
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
train_df.head()
```

```python
train_df.info(), test_df.info()
```

```python
len(train_df['departure city'].unique()), len(train_df['sub-class'].unique()), len(train_df['RID'].unique()), len(train_df['address code'].unique())
```

```python
train_df.isna().any()
```

```python
train_df = train_df.fillna('Standard Class') #Replacing NaN with most frequent label
```

```python
train_df.isna().any()
```

```python
train_df = train_df.drop(['id', 'RID', 'delivery date', 'placement date'], axis=1)
test_df = test_df.drop(['id', 'RID', 'delivery date', 'placement date'], axis=1)
train_df.head()
```

```python
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 7))
plt.plot(train_df.profit)
```

```python
from sklearn import preprocessing

def transform_labels(labels:list, df):
  for label in labels:
    le = preprocessing.LabelEncoder()
    df[label] = le.fit_transform(df[label])
```

```python
for label in labels:
    le = preprocessing.LabelEncoder()
    df[label] = le.fit_transform(df[label])
```

```python
transform_labels(['departure city', 'location', 'class', 'segment', 'sub-class', 'delivery type', 'departure state'], train_df)
```

```python
train_df.head()
```

```python
transform_labels(['departure city', 'location', 'class', 'segment', 'sub-class', 'delivery type', 'departure state'], test_df)
```

```python
corrmat = train_df.corr()
plt.subplots(figsize=(12,9))
sns.heatmap(corrmat, vmax=0.9, square=True)
```

```python
plt.scatter(train_df['departure city'], train_df.profit)
```

```python
plt.scatter(train_df['departure state'], train_df.profit)
```

```python
plt.scatter(train_df['location'], train_df.profit)
```

```python
plt.scatter(train_df['price'], train_df.profit)
```

```python
plt.scatter(train_df['sub-class'], train_df.profit)
```

```python
plt.scatter(train_df['class'], train_df.profit)
```

```python
plt.scatter(train_df['delivery type'], train_df.profit)
```

```python
plt.scatter(train_df['discount'], train_df.profit)
```

```python
plt.scatter(train_df['address code'], train_df.profit)
```

```python
from scipy.stats import norm, skew
sns.distplot(test_df['price'] , fit=norm)
```

```python
sns.distplot(train_df['profit'] , fit=norm)
```

```python
train_df['price'] = np.log1p(train_df['price'])
```

```python
```python
train_df['price'] = np.log1p(train_df['price'])
```

```python
test_df['price'] = np.log1p(test_df['price'])
```

```python
test_df.head()
```

```python
train_df.head()
```

```python
train = train_df[:int(0.8*len(train_df))]
valid = train_df[int(0.8*len(train_df)):]
X_train = train[['discount', 'price', 'sub-class', 'location', 'departure state']]
y_train = train['profit']
X_valid = valid[['discount', 'price', 'sub-class', 'location', 'departure state']]
y_valid = valid['profit']
```

```python
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor
```

```python
!pip install catboost
```

```python
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from mlxtend.regressor import StackingRegressor
from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor
from lightgbm import LGBMRegressor
```

```python
stack_gen = StackingRegressor(regressors=(GradientBoostingRegressor(n_estimators=500, max_depth=4),
                                          XGBRegressor(n_estimators=500), RandomForestRegressor(n_estimators=500,max_depth=4)),
                              meta_regressor = GradientBoostingRegressor(n_estimators=500, max_depth=4),
                              use_features_in_secondary = True)
stack_gen.fit(X_train, y_train)
```

```python
meta_regressor = GradientBoostingRegressor(n_estimators=500, max_depth=4),
                              use_features_in_secondary = True)
stack_gen.fit(X_train, y_train)
```

```python
stack_gen.score(X_train, y_train), stack_gen.score(X_valid, y_valid)
```

```python
stack_gen.fit(train_df[['discount', 'price', 'sub-class', 'location', 'departure state']], train_df['profit'])
```

```python
test_df = test_df[['discount', 'price', 'sub-class', 'location', 'departure state']]
```

```python
result = stack_gen.predict(test_df)
```

```python
result
```

```python
sample = pd.read_csv('sampleSolution.csv', index_col=False)
sample
```

```python
sample['profit']=result
sample
```

```python
sample.to_csv('sampleSolution.csv', index=False)
```
PATH: app/src/androidTest/java/com/lendeasy/lendeasy/ExampleInstrumentedTest.java
LINES: 1-27

package com.lendeasy.lendeasy;

import android.content.Context;

import androidx.test.platform.app.InstrumentationRegistry;
import androidx.test.ext.junit.runners.AndroidJUnit4;

import org.junit.Test;
import org.junit.runner.RunWith;

import static org.junit.Assert.*;

/**
 * Instrumented test, which will execute on an Android device.
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
@RunWith(AndroidJUnit4.class)
public class ExampleInstrumentedTest {
    @Test
    public void useAppContext() {
        // Context of the app under test.
        Context appContext = InstrumentationRegistry.getInstrumentation().getTargetContext();

        assertEquals("com.lendeasy.lendeasy", appContext.getPackageName());
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/AlertReciever.java
LINES: 1-31

package com.lendeasy.lendeasy;

import android.content.BroadcastReceiver;
import android.content.Context;
import android.content.Intent;

import androidx.core.app.NotificationCompat;
import androidx.core.app.NotificationManagerCompat;

public class AlertReciever extends BroadcastReceiver {
    Context context;

    @Override
    public void onReceive(Context context, Intent intent) {
        this.context = context;
        setUpNotifications();
    }

    public void setUpNotifications() {
        NotificationManagerCompat notificationManagerCompat = NotificationManagerCompat.from(context);

        android.app.Notification notification = new NotificationCompat.Builder(context, com.lendeasy.lendeasy.Notification.CHANNEL_ID)
                .setContentTitle("Lend Easy")
                .setContentText("Item due")
                .setSmallIcon(R.drawable.lend)
                .setPriority(NotificationCompat.PRIORITY_HIGH).setVibrate(new long[]{1000, 1000, 1000, 1000, 1000})
                .setCategory(NotificationCompat.CATEGORY_REMINDER)
                .build();
        notificationManagerCompat.notify(1, notification);
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/Borrow.java
LINES: 1-44

package com.lendeasy.lendeasy;

import android.os.Bundle;

import androidx.fragment.app.Fragment;
import androidx.viewpager.widget.ViewPager;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import com.ogaclejapan.smarttablayout.SmartTabLayout;
import com.ogaclejapan.smarttablayout.utils.v4.FragmentPagerItemAdapter;
import com.ogaclejapan.smarttablayout.utils.v4.FragmentPagerItems;


/**
 * A simple {@link Fragment} subclass.
 * Use the {@link Borrow#newInstance} factory method to
 * create an instance of this fragment.
 */
public class Borrow extends Fragment {
    // TODO: Rename parameter arguments, choose names that match
    // the fragment initialization parameters, e.g. ARG_ITEM_NUMBER
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";

    // TODO: Rename and change types of parameters
    private String mParam1;
    private String mParam2;

    public Borrow() {
        // Required empty public constructor
    }

    /**
     * Use this factory method to create a new instance of
     * this fragment using the provided parameters.
     *
     * @param param1 Parameter 1.
     * @param param2 Parameter 2.
     * @return A new instance of fragment Borrow.
     */
    // TODO: Rename and change types and number of parameters
PATH: app/src/main/java/com/lendeasy/lendeasy/Borrow.java
LINES: 45-77

public static Borrow newInstance(String param1, String param2) {
        Borrow fragment = new Borrow();
        Bundle args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        args.putString(ARG_PARAM2, param2);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        View view=inflater.inflate(R.layout.fragment_borrow, container, false);
        ViewPager viewPager=view.findViewById(R.id.viewpager);
        //ViewPagerAdapter viewPagerAdapter = new ViewPagerAdapter(getActivity().getSupportFragmentManager());
        FragmentPagerItemAdapter adapter=new FragmentPagerItemAdapter(getChildFragmentManager(),
                FragmentPagerItems.with(getContext())
                        .add("Money",Top10.class)
                        .add("Other Items",Friends.class)
                        .create());

        viewPager.setAdapter(adapter);
        SmartTabLayout viewPagerTab=view.findViewById(R.id.viewpagertab);
PATH: app/src/main/java/com/lendeasy/lendeasy/Borrow.java
LINES: 78-83

viewPagerTab.setViewPager(viewPager);
        // Inflate the layout for this fragment
        return view;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowActivity.java
LINES: 1-43

package com.lendeasy.lendeasy;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.app.ProgressDialog;
import android.content.Intent;
import android.net.Uri;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentReference;
import com.google.firebase.firestore.FieldValue;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.SetOptions;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;
import com.google.firebase.storage.UploadTask;

import java.util.HashMap;
import java.util.Map;

public class BorrowActivity extends AppCompatActivity {

    EditText title,description,time;
    Button submit;
    Map<String, Object> data;
    Uri uri;
    FirebaseAuth mAuth=FirebaseAuth.getInstance();
    FirebaseFirestore db=FirebaseFirestore.getInstance();
    StorageReference storageReference;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_borrow);
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowActivity.java
LINES: 44-82

Intent intent=getIntent();
        uri=(Uri)intent.getExtras().get("Image");

        title=findViewById(R.id.title);
        description=findViewById(R.id.description);
        time=findViewById(R.id.time);

        submit=findViewById(R.id.submit);

        submit.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                sendData();
            }
        });
    }

    private void sendData(){
        if(title.getText().toString().isEmpty()){
            Toast.makeText(this,"Enter Title",Toast.LENGTH_SHORT).show();
            return;
        }
        if(time.getText().toString().isEmpty()){
            Toast.makeText(this,"Enter Time",Toast.LENGTH_SHORT).show();
            return;
        }

        storageReference= FirebaseStorage.getInstance().getReference().child("Images");
        final String path=mAuth.getCurrentUser().getUid()+""+uri.getLastPathSegment();

        data = new HashMap<>();

        data.put("Title",title.getText().toString());
        data.put("Type","Item");
        data.put("Time",time.getText().toString());
        data.put("Description",description.getText().toString().isEmpty()?"":description.getText().toString());
        data.put("Image",path);
        data.put("Borrower",mAuth.getCurrentUser().getDisplayName());
        data.put("Lender","");
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowActivity.java
LINES: 83-107

data.put("isDone",-1);
        data.put("Timestamp", FieldValue.serverTimestamp());
        data.put("LenderId",mAuth.getCurrentUser().getUid());
        data.put("BorrowerId","");

        final ProgressDialog progressDialog=new ProgressDialog(BorrowActivity.this,R.style.Theme_AppCompat_NoActionBar);
        progressDialog.setProgressStyle(ProgressDialog.STYLE_SPINNER);
        progressDialog.setMessage("Loading.....");
        progressDialog.show();

        db.collection("borrow").add(data).addOnSuccessListener(new OnSuccessListener<DocumentReference>() {
            @Override
            public void onSuccess(DocumentReference documentReference) {

                data.put("TransactionId",documentReference.getId());
                db.collection("users").document(mAuth.getCurrentUser().getUid())
                        .set(data, SetOptions.merge());

                storageReference.child(path).putFile(uri)
                        .addOnSuccessListener(new OnSuccessListener<UploadTask.TaskSnapshot>() {
                            @Override
                            public void onSuccess(UploadTask.TaskSnapshot taskSnapshot) {

                                progressDialog.dismiss();
                                Toast.makeText(BorrowActivity.this,"Upload Successful!!",Toast.LENGTH_SHORT).show();
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowActivity.java
LINES: 108-126

Intent i = new Intent(BorrowActivity.this, MainActivity.class);
                                i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                                startActivity(i);
                                finish();
                            }
                        })
                        .addOnFailureListener(new OnFailureListener() {
                    @Override
                    public void onFailure(@NonNull Exception e) {
                        Toast.makeText(BorrowActivity.this,"Upload Failed!!",Toast.LENGTH_SHORT).show();
                    }
                });
            }
        });



    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowAdapter.java
LINES: 1-47

package com.lendeasy.lendeasy;

import android.content.Context;
import android.net.Uri;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.view.Window;
import android.widget.ImageButton;
import android.widget.ImageView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.bumptech.glide.Glide;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;

import java.util.ArrayList;

public class BorrowAdapter extends RecyclerView.Adapter<BorrowAdapter.ViewHolder> {

    ArrayList<BorrowModel> list;
    Context context;


    public BorrowAdapter(ArrayList<BorrowModel> list,Context context){
        this.list=list;
        this.context=context;
    }

    @NonNull
    @Override
    public BorrowAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view;
        view= LayoutInflater.from(parent.getContext()).inflate(R.layout.borrow_item ,parent,false);
        return new BorrowAdapter.ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull final BorrowAdapter.ViewHolder holder, int position) {
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowAdapter.java
LINES: 48-82

holder.Name.setText(list.get(position).getName());
        holder.Title.setText(list.get(position).getTitle());
        holder.Description.setText(list.get(position).getDescription());
        holder.Time.setText("Time: "+list.get(position).getTime());

        FirebaseStorage firebaseStorage=FirebaseStorage.getInstance();
        StorageReference storage=firebaseStorage.getReference().child("Images");

        StorageReference childreference=storage.child(list.get(position).getImageUrl());
        childreference.getDownloadUrl()
                .addOnSuccessListener(new OnSuccessListener<Uri>() {
                    @Override
                    public void onSuccess(Uri uri) {
                        Log.d("TAG","Heyy");
                        Glide.with(context).load(uri.toString()).override(900,900).into(holder.imageView);
                    }
                });
    }

    @Override
    public int getItemCount() {
        return list.size();
    }

    public class ViewHolder extends RecyclerView.ViewHolder {

        TextView Name,Title,Description,Time;
        ImageView imageView;
        ImageButton lend,chat;
        public ViewHolder(@NonNull View itemView) {
            super(itemView);

            Name=itemView.findViewById(R.id.name);
            Title=itemView.findViewById(R.id.title);
            Description=itemView.findViewById(R.id.description);
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowAdapter.java
LINES: 83-90

imageView=itemView.findViewById(R.id.image);
            Time=itemView.findViewById(R.id.time);

            lend=itemView.findViewById(R.id.lend);
            chat=itemView.findViewById(R.id.chat);
        }
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/BorrowModel.java
LINES: 1-34

package com.lendeasy.lendeasy;

public class BorrowModel {

    private String Name,Title,Description,ImageUrl,Time;

    public BorrowModel(String name, String title, String description, String imageUrl,String time) {
        Name = name;
        Title = title;
        Description = description;
        ImageUrl = imageUrl;
        Time=time;
    }

    public String getName() {
        return Name;
    }

    public String getTitle() {
        return Title;
    }

    public String getDescription() {
        return Description;
    }

    public String getImageUrl() {
        return ImageUrl;
    }

    public String getTime() {
        return Time;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatActivity.java
LINES: 1-39

package com.lendeasy.lendeasy.ChatFunctionality;

import android.content.Context;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.preference.PreferenceManager;
import android.util.Log;
import android.view.View;
import android.widget.EditText;
import android.widget.ImageButton;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.appcompat.app.AppCompatActivity;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.CollectionReference;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FieldValue;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.Query;
import com.google.firebase.firestore.QuerySnapshot;
import com.lendeasy.lendeasy.R;

import java.util.ArrayList;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import static com.google.firebase.firestore.DocumentSnapshot.ServerTimestampBehavior.ESTIMATE;

public class ChatActivity extends AppCompatActivity {
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatActivity.java
LINES: 40-72

public static final String USER_ID = "UserId";
    ChatAdapter chatAdapter;
    FirebaseFirestore database;
    Map<String, Object> data;
    CollectionReference chatRoomsCollectionReference;
    CollectionReference usersCollectionReference;
    ImageButton imageButton;
    EditText editText;
    List<ChatModel> arrayList = new ArrayList<>();
    RecyclerView recyclerView;

    public static String getDataFromSharedPref(String key, Context context) {
        SharedPreferences preferences = PreferenceManager.getDefaultSharedPreferences(context);
        return preferences.getString(key, null);
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_chat);

        imageButton = findViewById(R.id.imageButton);
        editText = findViewById(R.id.editText);
        database = FirebaseFirestore.getInstance();
        chatRoomsCollectionReference = database.collection("chatRooms");
        usersCollectionReference = database.collection("users2");


        initRecyclerView();
        setTitle(getIntent().getStringExtra("recieverName"));

//      Query getuserIdDocumentReference =usersCollectionReference.whereEqualTo("userId",getDataFromSharedPref(USER_ID,getApplicationContext()));
//getuserIdDocumentReference.addSnapshotListener(this, new EventListener<QuerySnapshot>() {
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatActivity.java
LINES: 73-99

//    @Override
//    public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
//        if (queryDocumentSnapshots!=null){
//            for(DocumentSnapshot doc:queryDocumentSnapshots) {
//userIdDocumentReference=doc.getId();
//            }
//        }
//    }
//});


        Query getMessages = chatRoomsCollectionReference.document(setOneToOneChat(getDataFromSharedPref(USER_ID, getApplicationContext()), getIntent().getStringExtra("recieverId")))
                .collection("messages").orderBy("timeStamp");


        getMessages.addSnapshotListener(this, new EventListener<QuerySnapshot>() {
            @Override
            public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
                if (queryDocumentSnapshots != null) {
                    arrayList.clear();
                    for (DocumentSnapshot doc : queryDocumentSnapshots) {
                        DocumentSnapshot.ServerTimestampBehavior behavior = ESTIMATE;
                        arrayList.add(new ChatModel(doc.getString("message"), doc.getTimestamp("timeStamp", behavior), doc.getString("senderId"), doc.getString("recieverId")));

                    }
                    chatAdapter.updateRecylerView(arrayList);
                    recyclerView.scrollToPosition(arrayList.size() - 1);
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatActivity.java
LINES: 100-124

}
            }
        });


        imageButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                if (isEmpty(editText)) {
                    Toast.makeText(ChatActivity.this, "Enter a text", Toast.LENGTH_SHORT).show();
                } else {

                    data = new HashMap<>();
                    data.put("timeStamp", FieldValue.serverTimestamp());
                    data.put("senderId", getDataFromSharedPref(USER_ID, getApplicationContext()));
                    data.put("recieverId", getIntent().getStringExtra("recieverId"));
                    data.put("message", editText.getText().toString().trim());
                    editText.setText("");
                    chatRoomsCollectionReference.document(setOneToOneChat(getDataFromSharedPref(USER_ID, getApplicationContext()), getIntent().getStringExtra("recieverId"))).collection("messages").document().set(data).addOnSuccessListener(new OnSuccessListener<Void>() {
                        @Override
                        public void onSuccess(Void aVoid) {
                            Toast.makeText(ChatActivity.this, "Message Sent", Toast.LENGTH_SHORT).show();
                        }
                    }).addOnFailureListener(new OnFailureListener() {
                        @Override
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatActivity.java
LINES: 125-160

public void onFailure(@NonNull Exception e) {
                            Log.i("Error Sending Message", e.getMessage());
                        }
                    });
//chatRoomsCollectionReference.document(getIntent().getStringExtra("chatRoomDocumentId")).collection("Messages").add(data);


                }
            }
        });
    }

    public void initRecyclerView() {

        recyclerView = findViewById(R.id.recylerView);
        chatAdapter = new ChatAdapter(arrayList, getDataFromSharedPref(USER_ID, getApplicationContext()), getApplicationContext());
        recyclerView.setAdapter(chatAdapter);
        recyclerView.setLayoutManager(new LinearLayoutManager(this));
        recyclerView.hasFixedSize();

    }

    private boolean isEmpty(EditText editText) {
        return editText.getText().toString().trim().length() == 0;
    }

    private String setOneToOneChat(String uid1, String uid2) {
//Check if user1s id is less than user2's
        if (uid1.compareTo(uid2) > 0) {
            return uid1 + uid2;
        } else {
            return uid2 + uid1;
        }

    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatAdapter.java
LINES: 1-46

package com.lendeasy.lendeasy.ChatFunctionality;

import android.content.Context;
import android.text.format.DateFormat;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.google.firebase.Timestamp;
import com.lendeasy.lendeasy.R;

import java.util.ArrayList;
import java.util.Calendar;
import java.util.List;

public class ChatAdapter extends RecyclerView.Adapter {
    List<ChatModel> arrayList = new ArrayList<>();
    String senderId;
    Context context;


    public ChatAdapter(List<ChatModel> arrayList, String senderId, Context context) {
        this.arrayList = arrayList;
        this.context = context;
        this.senderId = senderId;
    }

    public void updateRecylerView(List<ChatModel> arrayList) {
        this.arrayList = arrayList;
        notifyDataSetChanged();
    }

    public String getTime(Timestamp time) {
        Calendar calendar = Calendar.getInstance();
        calendar.setTimeInMillis(time.getSeconds() * 1000);
        String date = DateFormat.format("hh:mm a", calendar).toString();
        return date;
    }

    //    public String getDate(Timestamp time) {
//        Calendar calendar = Calendar.getInstance();
//        calendar.setTimeInMillis(time.getSeconds() * 1000);
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatAdapter.java
LINES: 47-86

//        String date = DateFormat.format("hh:mm a", calendar).toString();
//        return date;
//    }
    @Override
    public int getItemViewType(int position) {
//if(hasDateChanged(arrayList.get(position-1),arrayList.get(position))){
//return 0;
//}

        if (arrayList.get(position).getSenderId().equals(senderId))
            return 1;
        else
            return 2;
    }

    @NonNull
    @Override
    public RecyclerView.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        LayoutInflater layoutInflater = LayoutInflater.from(parent.getContext());
        View view;
//        if(viewType==0){
//
//        }
        if (viewType == 1) {
            view = layoutInflater.inflate(R.layout.row_activity_chat, parent, false);
            return new ViewHolder1(view);
        } else {
            view = layoutInflater.inflate(R.layout.row_activity_chat2, parent, false);
            return new ViewHolder2(view);
        }
    }

    @Override
    public void onBindViewHolder(@NonNull RecyclerView.ViewHolder holder, int position) {

        if (arrayList.get(position).getSenderId().equals(senderId)) {
            ViewHolder1 viewHolder1 = (ViewHolder1) holder;
            viewHolder1.chatTV.setText(arrayList.get(position).getMessage());
            viewHolder1.timeTV.setText(getTime(arrayList.get(position).getTimeStamp()));
        } else {
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatAdapter.java
LINES: 87-131

ViewHolder2 viewHolder2 = (ViewHolder2) holder;
            viewHolder2.chatTV2.setText(arrayList.get(position).getMessage());
            viewHolder2.timeTV2.setText(getTime(arrayList.get(position).getTimeStamp()));
        }
    }

    @Override
    public int getItemCount() {
        return arrayList.size();
    }

    public class ViewHolder1 extends RecyclerView.ViewHolder {
        TextView chatTV;
        TextView timeTV;

        public ViewHolder1(@NonNull View itemView) {
            super(itemView);
            chatTV = itemView.findViewById(R.id.chatTV);
            timeTV = itemView.findViewById(R.id.timeTV);
        }
    }

    public class ViewHolder2 extends RecyclerView.ViewHolder {
        TextView chatTV2;
        TextView timeTV2;

        public ViewHolder2(@NonNull View itemView) {
            super(itemView);
            chatTV2 = itemView.findViewById(R.id.chatTV2);
            timeTV2 = itemView.findViewById(R.id.timeTV2);
        }
    }

//    public class DateViewHolder extends RecyclerView.ViewHolder {
//        TextView dateTV;
//
//        public DateViewHolder(@NonNull View itemView) {
//            super(itemView);
//            dateTV = itemView.findViewById(R.id.dateTV);
//
//        }
//    }


//    public boolean hasDateChanged(ChatModel previousChat,ChatModel currentChat){
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatAdapter.java
LINES: 132-141

//        Date previousDay=previousChat.getTimeStamp().toDate();
//        Date currentDay=currentChat.getTimeStamp().toDate();
//
//        Long differenceInDays=(currentDay.getTime()-previousDay.getTime())/(1000*3600*24);
//        if(differenceInDays>1)
//            return true;
//        else
//return false;
//    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ChatModel.java
LINES: 1-37

package com.lendeasy.lendeasy.ChatFunctionality;

import com.google.firebase.Timestamp;

public class ChatModel {
    private String message;
    private Timestamp timeStamp;
    private String senderId;
    private String recieverId;

    public ChatModel() {
    }

    public ChatModel(String message, Timestamp timeStamp, String senderId, String recieverId) {
        this.message = message;
        this.timeStamp = timeStamp;
        this.senderId = senderId;
        this.recieverId = recieverId;
    }

    public String getMessage() {
        return message;
    }

    public String getSenderId() {
        return senderId;
    }

    public String getRecieverId() {
        return recieverId;
    }

    public Timestamp getTimeStamp() {
        return timeStamp;
    }

}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/Contact.java
LINES: 1-29

package com.lendeasy.lendeasy.ChatFunctionality;

public class Contact {
    private String name;
    private String eMail;
    private String userId;

    public Contact(String name, String eMail, String userId) {
        this.name = name;
        this.eMail = eMail;
        this.userId = userId;
    }

    public Contact() {

    }

    public String getName() {
        return name;
    }

    public String getEmail() {
        return eMail;
    }

    public String getUserId() {
        return userId;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactAdapter.java
LINES: 1-47

package com.lendeasy.lendeasy.ChatFunctionality;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import com.lendeasy.lendeasy.R;

import java.util.ArrayList;
import java.util.List;

public class ContactAdapter extends RecyclerView.Adapter<ContactAdapter.ViewHolder> {
    CustomClickListener customClickListener;
    List<Contact> arrayList = new ArrayList<>();


    public ContactAdapter(List<Contact> arrayList) {
        this.arrayList = arrayList;
    }

    public void updateRecylerView(List<Contact> list) {
        this.arrayList = list;
        notifyDataSetChanged();
    }

    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        View view = LayoutInflater.from(parent.getContext()).inflate(R.layout.row_contact, parent, false);
        return new ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull ViewHolder holder, int position) {
        holder.nameTV.setText(arrayList.get(position).getName());
    }

    @Override
    public int getItemCount() {
        return arrayList.size();
    }

    public void setCustomClickListener(CustomClickListener customClickListener) {
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactAdapter.java
LINES: 48-76

this.customClickListener = customClickListener;
    }


    public interface CustomClickListener {
        void onClick(Contact contact);

    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        TextView nameTV;

        public ViewHolder(@NonNull View itemView) {
            super(itemView);
            nameTV = itemView.findViewById(R.id.nameTV);

            itemView.setOnClickListener(new View.OnClickListener() {
                @Override
                public void onClick(View v) {

                    int position = getAdapterPosition();
                    if (position != RecyclerView.NO_POSITION && customClickListener != null) {
                        customClickListener.onClick(arrayList.get(getAdapterPosition()));
                    }
                }
            });
        }
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactsActivity.java
LINES: 1-38

package com.lendeasy.lendeasy.ChatFunctionality;

import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.preference.PreferenceManager;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.appcompat.app.AppCompatActivity;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.CollectionReference;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.Query;
import com.google.firebase.firestore.QuerySnapshot;
import com.lendeasy.lendeasy.R;

import java.util.ArrayList;
import java.util.List;

public class ContactsActivity extends AppCompatActivity {
    public static final String USER_ID = "UserId";
    public static final String USER_NAME = "UserName";
    Button logOutButton;
    FirebaseAuth auth;
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactsActivity.java
LINES: 39-77

FirebaseFirestore database;
    CollectionReference usersCollectionReference;
    ContactAdapter contactAdapter;
    TextView userNameTextView;
    CollectionReference chatRoomsCollectionReference;
    List<Contact> arrayList = new ArrayList<>();

    public static String getDataFromSharedPref(String key, Context context) {
        SharedPreferences preferences = PreferenceManager.getDefaultSharedPreferences(context);
        return preferences.getString(key, null);
    }

    public static void storeDataIntoSharedPref(String key, String value, Context context) {
        SharedPreferences preferences = PreferenceManager.getDefaultSharedPreferences(context);
        SharedPreferences.Editor editor = preferences.edit();
        editor.putString(key, value);
        editor.commit();
    }

    @Override
    protected void onStart() {
        super.onStart();
        auth.addAuthStateListener(new FirebaseAuth.AuthStateListener() {
            @Override
            public void onAuthStateChanged(@NonNull FirebaseAuth firebaseAuth) {
                if (firebaseAuth.getCurrentUser() == null) {
                    startActivity(new Intent(ContactsActivity.this, LoginActivity2.class));
                }
            }
        });
    }

    @Override
    public void onBackPressed() {
        return;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactsActivity.java
LINES: 78-105

super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_contacts);


        logOutButton = findViewById(R.id.logOutButton);
        userNameTextView = findViewById(R.id.userNameTV);
        auth = FirebaseAuth.getInstance();
        database = FirebaseFirestore.getInstance();
        usersCollectionReference = database.collection("users2");
        chatRoomsCollectionReference = database.collection("chatRooms");

        userNameTextView.setText(getDataFromSharedPref(USER_NAME, getApplicationContext()));
        initRecyclerView();

        Query getContacts = usersCollectionReference.orderBy("userId");
        getContacts.addSnapshotListener(this, new EventListener<QuerySnapshot>() {
            @Override
            public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
                arrayList.clear();
                for (DocumentSnapshot doc : queryDocumentSnapshots) {
                    if (!doc.getString("userId").equals(getDataFromSharedPref(USER_ID, getApplicationContext()))) {
                        arrayList.add(new Contact(doc.getString("name"), doc.getString("eMail"), doc.getString("userId")));
                    }
                }
                contactAdapter.updateRecylerView(arrayList);
            }
        });
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactsActivity.java
LINES: 106-132

logOutButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Query getCurrentUser = usersCollectionReference.whereEqualTo("userId", getDataFromSharedPref(USER_ID, getApplicationContext()));
                getCurrentUser.get().addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                    @Override
                    public void onComplete(@NonNull Task<QuerySnapshot> task) {
                        if (task.isSuccessful()) {
                            for (DocumentSnapshot document : task.getResult()) {
                                usersCollectionReference.document(document.getId()).delete();
                            }
                        } else {
                            Log.d("TAG", "Error getting documents: ", task.getException());
                        }
                    }
                });
                auth.signOut();
            }
        });


        contactAdapter.setCustomClickListener(new ContactAdapter.CustomClickListener() {
            @Override
            public void onClick(Contact contact) {
                Intent intent = new Intent(ContactsActivity.this, ChatActivity.class);
                intent.putExtra("recieverName", contact.getName());
                intent.putExtra("recieverId", contact.getUserId());
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/ContactsActivity.java
LINES: 133-156

startActivity(intent);
            }
        });
    }

    public void initRecyclerView() {
        RecyclerView recyclerView = findViewById(R.id.recylerView);
        contactAdapter = new ContactAdapter(arrayList);
        recyclerView.setAdapter(contactAdapter);
        recyclerView.setLayoutManager(new LinearLayoutManager(this));
    }

    private String setOneToOneChat(String uid1, String uid2) {
//Check if user1s id is less than user2's
        if (uid1.compareTo(uid2) > 0) {
            return uid1 + uid2;
        } else {
            return uid2 + uid1;
        }

    }


}
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/LoginActivity2.java
LINES: 1-36

package com.lendeasy.lendeasy.ChatFunctionality;

import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.preference.PreferenceManager;
import android.util.Log;
import android.view.View;
import android.widget.Toast;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInAccount;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.common.SignInButton;
import com.google.android.gms.common.api.ApiException;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.AuthCredential;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.auth.GoogleAuthProvider;
import com.google.firebase.firestore.CollectionReference;
import com.google.firebase.firestore.DocumentReference;
import com.google.firebase.firestore.FirebaseFirestore;
import com.lendeasy.lendeasy.R;

import java.util.HashMap;
import java.util.Map;
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/LoginActivity2.java
LINES: 37-71

public class LoginActivity2 extends AppCompatActivity {

    public static final String USER_ID = "UserId";
    public static final String USER_NAME = "UserName";
    private static final int RC_SIGN_IN = 1;
    SignInButton signInButton;
    GoogleSignInClient mGoogleSignInClient;
    FirebaseAuth mAuth;
    FirebaseAuth.AuthStateListener authStateListener;

    public static void storeDataIntoSharedPref(String key, String value, Context context) {
        SharedPreferences preferences = PreferenceManager.getDefaultSharedPreferences(context);
        SharedPreferences.Editor editor = preferences.edit();
        editor.putString(key, value);
        editor.commit();
    }

    @Override
    protected void onStart() {
        super.onStart();
        mAuth.addAuthStateListener(authStateListener);
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_login);
        signInButton = findViewById(R.id.login);
        mAuth = FirebaseAuth.getInstance();
        authStateListener = new FirebaseAuth.AuthStateListener() {
            @Override
            public void onAuthStateChanged(@NonNull FirebaseAuth firebaseAuth) {
                if (firebaseAuth.getCurrentUser() != null) {
                    Intent intent = new Intent(LoginActivity2.this, ContactsActivity.class);
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/LoginActivity2.java
LINES: 72-107

startActivity(intent);
                }
            }
        };
        // Configure Google Sign In
        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestIdToken(getString(R.string.default_web_client_id))
                .requestEmail()
                .build();

        mGoogleSignInClient = GoogleSignIn.getClient(this, gso);

        signInButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                signIn();
            }
        });
    }

    private void signIn() {
        Intent signInIntent = mGoogleSignInClient.getSignInIntent();
        startActivityForResult(signInIntent, RC_SIGN_IN);
    }

    @Override
    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        // Result returned from launching the Intent from GoogleSignInApi.getSignInIntent(...);
        if (requestCode == RC_SIGN_IN) {
            Task<GoogleSignInAccount> task = GoogleSignIn.getSignedInAccountFromIntent(data);
            try {
                // Google Sign In was successful, authenticate with Firebase
                GoogleSignInAccount account = task.getResult(ApiException.class);
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/LoginActivity2.java
LINES: 108-133

Toast.makeText(this, "Logged In", Toast.LENGTH_SHORT).show();
                firebaseAuthWithGoogle(account);
            } catch (ApiException e) {
                // Google Sign In failed, update UI appropriately
                Log.w("TAG", "Google sign in failed", e);
                // ...
            }
        }
    }

    private void firebaseAuthWithGoogle(GoogleSignInAccount acct) {
        Log.d("TAG", "firebaseAuthWithGoogle:" + acct.getId());

        AuthCredential credential = GoogleAuthProvider.getCredential(acct.getIdToken(), null);
        mAuth.signInWithCredential(credential)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {
                            // Sign in success, update UI with the signed-in user's information
                            Log.d("TAG", "signInWithCredential:success");
                            FirebaseUser user = mAuth.getCurrentUser();
                            storeDataIntoSharedPref(USER_ID, user.getUid(), getApplicationContext());
                            storeDataIntoSharedPref(USER_NAME, user.getDisplayName(), getApplicationContext());
                            updateUI(user);
                        } else {
PATH: app/src/main/java/com/lendeasy/lendeasy/ChatFunctionality/LoginActivity2.java
LINES: 134-166

// If sign in fails, display a message to the user.
                            Log.w("TAG", "signInWithCredential:failure", task.getException());
                            Toast.makeText(LoginActivity2.this, "Authentication Failed", Toast.LENGTH_SHORT).show();
                            updateUI(null);
                        }

                        // ...
                    }
                });
    }

    private void updateUI(final FirebaseUser user) {
        if (user != null) {

            FirebaseFirestore database = FirebaseFirestore.getInstance();
            CollectionReference collectionReference = database.collection("users2");
            Map<String, Object> data = new HashMap<>();
            data.put("userId", user.getUid());
            data.put("name", user.getDisplayName());
            data.put("eMail", user.getEmail());


            collectionReference.add(data).addOnSuccessListener(new OnSuccessListener<DocumentReference>() {
                @Override
                public void onSuccess(DocumentReference documentReference) {
                    Log.i("TAG", "User added");

                }
            });
        }

    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/EditDialog.java
LINES: 1-50

package com.lendeasy.lendeasy;

import android.app.Dialog;
import android.content.Context;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import androidx.annotation.NonNull;

import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.SetOptions;

import java.util.HashMap;
import java.util.Map;

public class EditDialog extends Dialog {

    String key;
    Context context;
    FirebaseFirestore db;
    EditText edit;
    FirebaseUser user= FirebaseAuth.getInstance().getCurrentUser();
    Map<String, Object> data = new HashMap<>();

    public EditDialog(@NonNull Context context,String key) {
        super(context);
        this.context=context;
        this.key=key;
    }

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.editdialog);

        edit=findViewById(R.id.edit);
        db=FirebaseFirestore.getInstance();

        Button ok,cancel;
        ok=findViewById(R.id.ok);
        cancel=findViewById(R.id.cancel);
PATH: app/src/main/java/com/lendeasy/lendeasy/EditDialog.java
LINES: 51-82

ok.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {

                String editstr=edit.getText().toString();
                Double detail= Double.parseDouble(editstr);
                if(!editstr.isEmpty()) {
                    data.put(key, detail);

                    db.collection("users").document(user.getUid())
                            .set(data, SetOptions.merge()).addOnSuccessListener(new OnSuccessListener<Void>() {
                        @Override
                        public void onSuccess(Void aVoid) {
                            Toast.makeText(context,"Successfully Updated!!!",Toast.LENGTH_SHORT).show();
                        }
                    }).addOnFailureListener(new OnFailureListener() {
                        @Override
                        public void onFailure(@NonNull Exception e) {
                            Toast.makeText(context,"Update Failed",Toast.LENGTH_SHORT).show();
                        }
                    });
                    dismiss();
                }
                else
                    Toast.makeText(context,"Enter Valid Value",Toast.LENGTH_SHORT).show();
            }
        });

        cancel.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                dismiss();
PATH: app/src/main/java/com/lendeasy/lendeasy/EditDialog.java
LINES: 83-86

}
        });
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/Friends.java
LINES: 1-45

package com.lendeasy.lendeasy;

import android.os.Bundle;

import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
import com.google.firebase.firestore.QuerySnapshot;

import java.util.ArrayList;


/**
 * A simple {@link Fragment} subclass.
 * Use the {@link Friends#newInstance} factory method to
 * create an instance of this fragment.
 */
public class Friends extends Fragment {
    // TODO: Rename parameter arguments, choose names that match
    // the fragment initialization parameters, e.g. ARG_ITEM_NUMBER
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";

    // TODO: Rename and change types of parameters
    private String mParam1;
    private String mParam2;
    ArrayList<BorrowModel> list;

    public Friends() {
        // Required empty public constructor
    }

    /**
     * Use this factory method to create a new instance of
PATH: app/src/main/java/com/lendeasy/lendeasy/Friends.java
LINES: 46-82

* this fragment using the provided parameters.
     *
     * @param param1 Parameter 1.
     * @param param2 Parameter 2.
     * @return A new instance of fragment Friends.
     */
    // TODO: Rename and change types and number of parameters
    public static Friends newInstance(String param1, String param2) {
        Friends fragment = new Friends();
        Bundle args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        args.putString(ARG_PARAM2, param2);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        final View view=inflater.inflate(R.layout.fragment_friends, container, false);


        FirebaseFirestore db=FirebaseFirestore.getInstance();

        db.collection("borrow").
                addSnapshotListener(new EventListener<QuerySnapshot>() {
                    @Override
PATH: app/src/main/java/com/lendeasy/lendeasy/Friends.java
LINES: 83-110

public void onEvent(@Nullable QuerySnapshot queryDocumentSnapshots, @Nullable FirebaseFirestoreException e) {
                        list=new ArrayList<>();

                        if(queryDocumentSnapshots!=null)
                        for(DocumentSnapshot documentSnapshot:queryDocumentSnapshots) {

                            String Name,Title,Description,ImageUrl,Time;
                            Name=documentSnapshot.getString("Borrower");
                            Title=documentSnapshot.getString("Title");
                            Description=documentSnapshot.getString("Description");
                            ImageUrl=documentSnapshot.getString("Image");
                            Time=documentSnapshot.getString("Time");
                            Log.d("Wht",Name);
                            list.add(new BorrowModel(Name,Title,Description,ImageUrl,Time));

                        }

                        RecyclerView recyclerView=view.findViewById(R.id.recyclerview);
                        BorrowAdapter adapter=new BorrowAdapter(list,getContext());
                        recyclerView.setLayoutManager(new LinearLayoutManager(getContext()));
                        recyclerView.setAdapter(adapter);
                    }

                });
        // Inflate the layout for this fragment
        return view;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/HistoryAdapter.java
LINES: 1-35

package com.lendeasy.lendeasy;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

public class HistoryAdapter extends RecyclerView.Adapter<HistoryAdapter.ViewHolder> {

    View view;
    @NonNull
    @Override
    public HistoryAdapter.ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {
        view= LayoutInflater.from(parent.getContext()).inflate(R.layout.lent_item ,parent,false);
        return new HistoryAdapter.ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull HistoryAdapter.ViewHolder holder, int position) {

    }

    @Override
    public int getItemCount() {
        return 10;
    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        public ViewHolder(@NonNull View itemView) {
            super(itemView);
        }
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/Lend.java
LINES: 1-45

package com.lendeasy.lendeasy;

import android.os.Bundle;

import androidx.annotation.NonNull;
import androidx.fragment.app.Fragment;
import androidx.fragment.app.FragmentPagerAdapter;
import androidx.viewpager.widget.ViewPager;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import com.ogaclejapan.smarttablayout.SmartTabLayout;
import com.ogaclejapan.smarttablayout.utils.v4.FragmentPagerItemAdapter;
import com.ogaclejapan.smarttablayout.utils.v4.FragmentPagerItems;


/**
 * A simple {@link Fragment} subclass.
 * Use the {@link Lend#newInstance} factory method to
 * create an instance of this fragment.
 */
public class Lend extends Fragment {
    // TODO: Rename parameter arguments, choose names that match
    // the fragment initialization parameters, e.g. ARG_ITEM_NUMBER
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";

    // TODO: Rename and change types of parameters
    private String mParam1;
    private String mParam2;

    public Lend() {
        // Required empty public constructor
    }

    /**
     * Use this factory method to create a new instance of
     * this fragment using the provided parameters.
     *
     * @param param1 Parameter 1.
     * @param param2 Parameter 2.
     * @return A new instance of fragment Lend.
     */
PATH: app/src/main/java/com/lendeasy/lendeasy/Lend.java
LINES: 46-81

// TODO: Rename and change types and number of parameters
    public static Lend newInstance(String param1, String param2) {
        Lend fragment = new Lend();
        Bundle args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        args.putString(ARG_PARAM2, param2);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        View view=inflater.inflate(R.layout.fragment_lend, container, false);



        ViewPager viewPager=view.findViewById(R.id.viewpager);
        //ViewPagerAdapter viewPagerAdapter = new ViewPagerAdapter(getActivity().getSupportFragmentManager());
        FragmentPagerItemAdapter adapter=new FragmentPagerItemAdapter(getChildFragmentManager(),
                FragmentPagerItems.with(getContext())
        .add("Top 10",Top10.class)
        .add("Friends",Friends.class)
        .create());

        viewPager.setAdapter(adapter);
PATH: app/src/main/java/com/lendeasy/lendeasy/Lend.java
LINES: 82-89

SmartTabLayout viewPagerTab=view.findViewById(R.id.viewpagertab);

        viewPagerTab.setViewPager(viewPager);

        // Inflate the layout for this fragment
        return view;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/LentAdapter.java
LINES: 1-44

package com.lendeasy.lendeasy;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import androidx.annotation.NonNull;
import androidx.recyclerview.widget.RecyclerView;

import java.util.ArrayList;

public class LentAdapter extends RecyclerView.Adapter<LentAdapter.ViewHolder> {

    private View view;
    private ArrayList<String> list;

    public LentAdapter(ArrayList<String> list){
        this.list=list;
    }
    @NonNull
    @Override
    public ViewHolder onCreateViewHolder(@NonNull ViewGroup parent, int viewType) {

        view= LayoutInflater.from(parent.getContext()).inflate(R.layout.lent_item ,parent,false);
        return new ViewHolder(view);
    }

    @Override
    public void onBindViewHolder(@NonNull LentAdapter.ViewHolder holder, int position) {

    }


    @Override
    public int getItemCount() {
        return 10;
    }

    public class ViewHolder extends RecyclerView.ViewHolder {
        public ViewHolder(@NonNull View itemView) {
            super(itemView);
        }
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 1-33

package com.lendeasy.lendeasy;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInAccount;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.common.SignInButton;
import com.google.android.gms.common.api.ApiException;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.FirebaseException;
import com.google.firebase.auth.AuthCredential;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.auth.GoogleAuthProvider;
import com.google.firebase.auth.PhoneAuthCredential;
import com.google.firebase.auth.PhoneAuthProvider;
import com.google.firebase.firestore.FieldValue;
import com.google.firebase.firestore.FirebaseFirestore;
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 34-72

import com.google.firebase.firestore.FirebaseFirestoreSettings;
import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.firestore.SetOptions;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.TimeUnit;

public class LoginActivity extends AppCompatActivity {

    private static final String TAG = "SignInActivity";
    private static final int RC_SIGN_IN = 9001;
    private FirebaseAuth mAuth;
    private GoogleSignInClient mGoogleSignInClient;
    SignInButton login;
    SharedPreferences sharedPref;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_login);

        login=findViewById(R.id.login);

        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestEmail()
                .requestIdToken(getString(R.string.default_web_client_id))
                .build();
        mGoogleSignInClient = GoogleSignIn.getClient(this, gso);


        mAuth = FirebaseAuth.getInstance();
        FirebaseUser user = mAuth.getCurrentUser();
        sharedPref=getSharedPreferences("Phone", Context.MODE_PRIVATE);

        if (user != null) {

            FirebaseFirestore db = FirebaseFirestore.getInstance();
            Map<String, Object> data = new HashMap<>();
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 73-110

data.put("PhoneNumber", sharedPref.getString("PhoneNum",""));
            db.collection("users").document(user.getUid())
                    .set(data,SetOptions.merge());

            if(sharedPref.getString("PhoneNum","").equals("")) {
                startActivity(new Intent(LoginActivity.this, PhoneAuth.class));
                finish();
            }
            else{
                startActivity(new Intent(LoginActivity.this, MainActivity.class));
                finish();
            }
        }

        login.setOnClickListener(
                new View.OnClickListener() {
                    @Override
                    public void onClick(View v) {
                        signIn();
                    }
                });

    }

    private void signIn() {
        Intent signInIntent = mGoogleSignInClient.getSignInIntent();
        startActivityForResult(signInIntent, RC_SIGN_IN);
        Log.d("here", "here");
    }

    @Override
    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        // Result returned from launching the Intent from GoogleSignInApi.getSignInIntent(...);
        if (requestCode == RC_SIGN_IN) {
            Task<GoogleSignInAccount> task = GoogleSignIn.getSignedInAccountFromIntent(data);
            try {
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 111-140

// Google Sign In was successful, authenticate with Firebase
                GoogleSignInAccount account = task.getResult(ApiException.class);
                    firebaseAuthWithGoogle(account);

            } catch (ApiException e) {
                // Google Sign In failed, update UI appropriately
                Log.w(TAG, "Google sign in failed", e);
                // ...
            }
        }
    }

    private void firebaseAuthWithGoogle(GoogleSignInAccount acct) {
        Log.d(TAG, "firebaseAuthWithGoogle:" + acct.getId());

        AuthCredential credential = GoogleAuthProvider.getCredential(acct.getIdToken(), null);
        mAuth.signInWithCredential(credential)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {

                            // Sign in success, update UI with the signed-in user's information
                            Log.d(TAG, "signInWithCredential:success");
                            FirebaseUser user = mAuth.getCurrentUser();
                            updateUI(user);


                        } else {
                            // If sign in fails, display a message to the user.
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 141-174

Log.w(TAG, "signInWithCredential:failure", task.getException());
                            Toast.makeText(getApplicationContext(), R.string.app_name, Toast.LENGTH_LONG).show();
                        }
                    }
                });
    }






    private void updateUI(final FirebaseUser user) {
        if (user != null) {
            final FirebaseFirestore db = FirebaseFirestore.getInstance();
            FirebaseFirestoreSettings settings = new FirebaseFirestoreSettings.Builder()
                    .setPersistenceEnabled(true)
                    .build();
            db.setFirestoreSettings(settings);

            db.collection("users").whereEqualTo("UserId", user.getUid()).get().addOnCompleteListener(
                    new OnCompleteListener<QuerySnapshot>() {
                @Override
                public void onComplete(@NonNull Task<QuerySnapshot> task) {

                    if (task.getResult().getDocuments().isEmpty()) {

                        SharedPreferences.Editor editor = sharedPref.edit();
                        editor.putString("PhoneNum", "");
                        editor.apply();

                        Map<String, Object> data = new HashMap<>();
                        data.put("Email", user.getEmail());
                        data.put("Name", user.getDisplayName());
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 175-203

data.put("UserId", user.getUid());
                        data.put("PhoneNumber", "");
                        data.put("Target", 0.0);
                        data.put("Balance", 0.0);
                        data.put("Interest", 0.0);
                        data.put("Investment", 0.0);
                        data.put("Score", 0.0);
                        data.put("Slot_time", FieldValue.serverTimestamp());

                        Log.d("test2", user.getUid());

                        db.collection("users").document(user.getUid()).set(data, SetOptions.merge()).addOnCompleteListener(new OnCompleteListener<Void>() {
                            @Override
                            public void onComplete(@NonNull Task<Void> task) {
                                if (task.isSuccessful()) {

                                    Intent i = new Intent(LoginActivity.this,  PhoneAuth.class);
                                    i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                                    startActivity(i);
                                    finish();
                                } else {

                                    Toast.makeText(LoginActivity.this, "Connection error!", Toast.LENGTH_SHORT).show();

                                }
                            }
                        });
                    }
PATH: app/src/main/java/com/lendeasy/lendeasy/LoginActivity.java
LINES: 204-216

else{
                        Intent i = new Intent(LoginActivity.this, MainActivity.class);
                        i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        startActivity(i);
                        finish();


                    }
                }
            });
        }
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/MainActivity.java
LINES: 1-41

package com.lendeasy.lendeasy;

import android.app.AlarmManager;
import android.app.PendingIntent;
import android.content.Context;
import android.content.Intent;
import android.os.Build;
import android.os.Bundle;
import android.view.MenuItem;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;
import androidx.fragment.app.Fragment;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.android.material.bottomnavigation.BottomNavigationView;
import com.google.firebase.Timestamp;
import com.google.firebase.firestore.DocumentReference;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FirebaseFirestore;

import java.util.Calendar;
import java.util.Date;

import static com.google.firebase.firestore.DocumentSnapshot.ServerTimestampBehavior.ESTIMATE;


public class MainActivity extends AppCompatActivity {
    FirebaseFirestore database;
    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);


        loadFragment(new Lend());

        BottomNavigationView bottomNavigationView = findViewById(R.id.bottom_navigation);

        bottomNavigationView.getMenu().findItem(R.id.lend).setChecked(true);
PATH: app/src/main/java/com/lendeasy/lendeasy/MainActivity.java
LINES: 42-77

bottomNavigationView.setOnNavigationItemSelectedListener(new BottomNavigationView.OnNavigationItemSelectedListener() {
            @Override
            public boolean onNavigationItemSelected(@NonNull MenuItem item) {

                switch (item.getItemId()){
                    case R.id.profile:
                        loadFragment(new Profile());
                        item.setChecked(true);
                        break;
                    case R.id.lend:
                        loadFragment(new Lend());
                        item.setChecked(true);
                        break;
                    case R.id.borrow:
                        loadFragment(new Borrow());
                        item.setChecked(true);
                        break;
                    case R.id.target:
                        loadFragment(new Target());
                        item.setChecked(true);
                        break;
                }
                return false;
            }
        });
        setUpNotifications();

    }

    private void setUpNotifications() {
        database = FirebaseFirestore.getInstance();
        DocumentReference borrowItem = database.collection("borrow")
                .document("6vV6grSQxkfi6LE0eJyb");

        borrowItem.get().addOnCompleteListener(new OnCompleteListener<DocumentSnapshot>() {
            @Override
PATH: app/src/main/java/com/lendeasy/lendeasy/MainActivity.java
LINES: 78-103

public void onComplete(@NonNull Task<DocumentSnapshot> task) {
                DocumentSnapshot.ServerTimestampBehavior behavior = ESTIMATE;
                Date borrowDate = task.getResult().getTimestamp("Timestamp", behavior).toDate();

                Calendar calendar = Calendar.getInstance();
                Timestamp timestamp = new Timestamp(new Date(System.currentTimeMillis()));
                Date today = timestamp.toDate();
                long differenceInDays = (today.getTime() - borrowDate.getTime()) / (1000 * 3600 * 24);
                if (differenceInDays <= 3) {
                    AlarmManager alarmManager = (AlarmManager) getSystemService(Context.ALARM_SERVICE);
                    Intent intent = new Intent(getApplicationContext(), AlertReciever.class);
                    PendingIntent pendingIntent = PendingIntent.getBroadcast(getApplicationContext(), 1, intent, 0);
                    if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.KITKAT) {
                        if (alarmManager != null) {
                            alarmManager.setRepeating(AlarmManager.RTC_WAKEUP, calendar.getTimeInMillis(), AlarmManager.INTERVAL_DAY, pendingIntent);
                        }
                    }
                }
            }
        });


    }

    private boolean loadFragment(Fragment fragment) {
        //switching fragment
PATH: app/src/main/java/com/lendeasy/lendeasy/MainActivity.java
LINES: 104-115

if (fragment != null) {
            getSupportFragmentManager()
                    .beginTransaction()
                    .replace(R.id.fragment_container, fragment)
                    .commit();
            return true;
        }
        return false;
    }


}
PATH: app/src/main/java/com/lendeasy/lendeasy/Notification.java
LINES: 1-31

package com.lendeasy.lendeasy;

import android.app.Application;
import android.app.NotificationChannel;
import android.app.NotificationManager;
import android.os.Build;

public class Notification extends Application {
    public static final String CHANNEL_ID = "channel";

    @Override
    public void onCreate() {
        super.onCreate();

        createNotificationChannel();
    }

    private void createNotificationChannel() {
        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.O) {
            NotificationChannel notificationChannel = new NotificationChannel(CHANNEL_ID, "Reminder", NotificationManager.IMPORTANCE_HIGH);
            notificationChannel.setDescription("Notifications to remind user to return borrowed items");

            NotificationManager notificationManager = getSystemService(NotificationManager.class);
            if (notificationManager != null) {
                notificationManager.createNotificationChannel(notificationChannel);
            }
        }


    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/PhoneAuth.java
LINES: 1-44

package com.lendeasy.lendeasy;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Context;
import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.FirebaseException;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.auth.PhoneAuthCredential;
import com.google.firebase.auth.PhoneAuthProvider;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.SetOptions;

import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.TimeUnit;

public class PhoneAuth extends AppCompatActivity {

    EditText phone,otp;
    Button sendotp,verifyotp;
    String codeSent,phoneNumber;
    FirebaseFirestore db=FirebaseFirestore.getInstance();

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_phone_auth);

        phone=findViewById(R.id.phone);
        otp=findViewById(R.id.otp);

        sendotp=findViewById(R.id.sendotp);
PATH: app/src/main/java/com/lendeasy/lendeasy/PhoneAuth.java
LINES: 45-80

verifyotp=findViewById(R.id.verifyotp);

        sendotp.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                sendVerificationCode();
            }
        });

        verifyotp.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                verifySignIn();
            }
        });
    }

    private void verifySignIn(){
        String code=otp.getText().toString();

        if(code.equals(codeSent)){
            Map<String, Object> data = new HashMap<>();
            data.put("PhoneNumber",phoneNumber);

            FirebaseUser user= FirebaseAuth.getInstance().getCurrentUser();
            db.collection("users").document(user.getUid())
                    .set(data, SetOptions.merge()).addOnSuccessListener(new OnSuccessListener<Void>() {
                @Override
                public void onSuccess(Void aVoid) {

                    SharedPreferences sharedPref=getSharedPreferences("Phone", Context.MODE_PRIVATE);
                    SharedPreferences.Editor editor = sharedPref.edit();
                    editor.putString("PhoneNum", phoneNumber);
                    editor.apply();

                    Intent i = new Intent(PhoneAuth.this, MainActivity.class);
PATH: app/src/main/java/com/lendeasy/lendeasy/PhoneAuth.java
LINES: 81-115

i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                    startActivity(i);
                    finish();
                }
            });

        }
    }

    private void sendVerificationCode(){
        phoneNumber=phone.getText().toString();

        if(phoneNumber.isEmpty()){
            Toast.makeText(this,"Invalid",Toast.LENGTH_SHORT).show();
            return;
        }

        PhoneAuthProvider.getInstance().verifyPhoneNumber(
                phoneNumber,        // Phone number to verify
                60,                 // Timeout duration
                TimeUnit.SECONDS,   // Unit of timeout
                this,               // Activity (for callback binding)
                mCallbacks);        // OnVerificationStateChangedCallbacks
    }

    PhoneAuthProvider.OnVerificationStateChangedCallbacks mCallbacks=new PhoneAuthProvider.OnVerificationStateChangedCallbacks() {
        @Override
        public void onVerificationCompleted(@NonNull PhoneAuthCredential phoneAuthCredential) {
            Log.d("TAG","No way");

            Map<String, Object> data = new HashMap<>();
            data.put("PhoneNumber",phoneNumber);

            FirebaseUser user= FirebaseAuth.getInstance().getCurrentUser();
            db.collection("users").document(user.getUid())
PATH: app/src/main/java/com/lendeasy/lendeasy/PhoneAuth.java
LINES: 116-147

.set(data, SetOptions.merge()).addOnSuccessListener(new OnSuccessListener<Void>() {
                @Override
                public void onSuccess(Void aVoid) {

                    SharedPreferences sharedPref=getSharedPreferences("Phone", Context.MODE_PRIVATE);
                    SharedPreferences.Editor editor = sharedPref.edit();
                    editor.putString("PhoneNum", phoneNumber);
                    editor.apply();

                    Intent i = new Intent(PhoneAuth.this, MainActivity.class);
                    i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                    startActivity(i);
                    finish();
                }
            });

        }

        @Override
        public void onVerificationFailed(@NonNull FirebaseException e) {
            Log.d("TAG","Nahi");
        }

        @Override
        public void onCodeSent(@NonNull String s, @NonNull PhoneAuthProvider.ForceResendingToken forceResendingToken) {
            super.onCodeSent(s, forceResendingToken);

            Log.d("TAG","Gaya");
            codeSent=s;
        }
    };
}
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 1-38

package com.lendeasy.lendeasy;

import android.Manifest;
import android.content.ContentValues;
import android.content.Intent;
import android.content.pm.PackageManager;
import android.graphics.Bitmap;
import android.graphics.drawable.BitmapDrawable;
import android.net.Uri;
import android.os.Bundle;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;
import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.provider.MediaStore;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.Button;
import android.widget.ImageView;
import android.widget.TextView;
import android.widget.Toast;

import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 39-81

import com.google.firebase.firestore.QuerySnapshot;
import com.theartofdev.edmodo.cropper.CropImage;
import com.theartofdev.edmodo.cropper.CropImageActivity;
import com.theartofdev.edmodo.cropper.CropImageView;

import java.util.ArrayList;

import static android.app.Activity.RESULT_OK;


/**
 * A simple {@link Fragment} subclass.
 */
public class Profile extends Fragment {
    private static final int STORAGE_PERMISSION_CODE =2 ;
    private static final int CAMERA_PERMISSION_CODE = 3;
    private static final int MY_CAMERA_REQUEST_CODE = 4;
    private ArrayList<String> list;
    private RecyclerView recyclerView;
    private HistoryAdapter lentAdapter;
    private ImageView exit;
    private GoogleSignInClient googleSignInClient;
    private Uri imageuri;
    final int REQ_IMAGE_CAPTURE = 1;
    Uri resulturi;

    private FirebaseAuth mAuth = FirebaseAuth.getInstance();
    private FirebaseFirestore db = FirebaseFirestore.getInstance();

    public Profile() {
        // Required empty public constructor
    }


    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        View view = inflater.inflate(R.layout.fragment_profile, container, false);


        exit = view.findViewById(R.id.logout);
        final TextView target, score, balance, name;
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 82-116

Button borrow = view.findViewById(R.id.borrow);

        borrow.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                checkPermission(new String[]{Manifest.permission.CAMERA}[0], MY_CAMERA_REQUEST_CODE);
            }
        });

        target = view.findViewById(R.id.target);
        score = view.findViewById(R.id.time);
        balance = view.findViewById(R.id.balance);
        name = view.findViewById(R.id.name);

        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestEmail()
                .requestIdToken(getString(R.string.default_web_client_id))
                .build();
        googleSignInClient = GoogleSignIn.getClient(getActivity(), gso);

        exit.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {

                googleSignInClient.signOut();
                mAuth.signOut();
                Intent i = new Intent(getActivity(), LoginActivity.class);
                i.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                startActivity(i);
                getActivity().finish();
                Toast.makeText(getContext(), "Signed Out",
                        Toast.LENGTH_SHORT).show();
            }
        });
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 117-148

String userid = mAuth.getCurrentUser().getUid();


        db.collection("users").document(userid)
                .addSnapshotListener(new EventListener<DocumentSnapshot>() {
                    @Override
                    public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {
                        if (documentSnapshot != null) {

                            Log.d("docsnap", documentSnapshot + "");
                            Log.d("docdouble", documentSnapshot.getDouble("Target") + "");

                            target.setText(documentSnapshot.getDouble("Target") + "");
                            score.setText(documentSnapshot.getDouble("Score") + "");
                            balance.setText(documentSnapshot.getDouble("Balance") + "");
                            name.setText(documentSnapshot.getString("Name"));

                        }
                    }
                });

        list = new ArrayList<>();
        list.add("Hello");
        recyclerView = view.findViewById(R.id.history);
        lentAdapter = new HistoryAdapter();
        recyclerView.setLayoutManager(new LinearLayoutManager(getContext()));
        recyclerView.setAdapter(lentAdapter);
        // Inflate the layout for this fragment
        return view;
    }

    @Override
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 149-188

public void onActivityResult(int requestCode, int resultCode, Intent data) {

        super.onActivityResult(requestCode, resultCode, data);

        if(requestCode==REQ_IMAGE_CAPTURE)
            if(resultCode==RESULT_OK ){
                Log.d("TAg",imageuri.toString());
                CropImage.activity(imageuri)
                        .setGuidelines(CropImageView.Guidelines.ON)
                        .start(getContext(),this);
            }


        if (requestCode == CropImage.CROP_IMAGE_ACTIVITY_REQUEST_CODE) {

            CropImage.ActivityResult result= CropImage.getActivityResult(data);
            if(resultCode == RESULT_OK){

                resulturi = result.getUri();
                Log.d("Img",resulturi.toString());

                Intent intent=new Intent(getActivity(),BorrowActivity.class);
                intent.putExtra("Image",resulturi);
                startActivity(intent);
//                image.setImageURI(resulturi);
//                BitmapDrawable bitmapDrawable=(BitmapDrawable)image.getDrawable();
//                bitmap=bitmapDrawable.getBitmap();

            }
            else
                Log.d("Img","jbjbhjv");
        }

    }

    // Function to check and request permission
    public void checkPermission(String permission, int requestCode)
    {

        // Checking if permission is not granted
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 189-229

if (ContextCompat.checkSelfPermission(
                getContext(),
                permission)
                == PackageManager.PERMISSION_DENIED) {
            ActivityCompat
                    .requestPermissions(
                            getActivity(),
                            new String[] { permission },
                            requestCode);
        }
        else {
            takepicture();
        }
    }

    @Override
    public void onRequestPermissionsResult(int requestCode,
                                           @NonNull String[] permissions,
                                           @NonNull int[] grantResults)
    {
        super
                .onRequestPermissionsResult(requestCode,
                        permissions,
                        grantResults);

        if (requestCode == CAMERA_PERMISSION_CODE) {

            // Checking whether user granted the permission or not.
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {

                // Showing the toast message
                takepicture();
            }
            else {
                Toast.makeText(getContext(),
                        "Camera Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 230-260

else if (requestCode == STORAGE_PERMISSION_CODE) {
            if (grantResults.length > 0
                    && grantResults[0] == PackageManager.PERMISSION_GRANTED) {
                Toast.makeText(getContext(),
                        "Storage Permission Granted",
                        Toast.LENGTH_SHORT)
                        .show();
            }
            else {
                Toast.makeText(getContext(),
                        "Storage Permission Denied",
                        Toast.LENGTH_SHORT)
                        .show();
            }
        }
    }

    private void takepicture(){
        PackageManager pm = getContext().getPackageManager();

        if (pm.hasSystemFeature(PackageManager.FEATURE_CAMERA_ANY)) {
            ContentValues values = new ContentValues();

            values.put(MediaStore.Images.Media.TITLE, "New pic");
            values.put(MediaStore.Images.Media.DESCRIPTION, "Image to text");

            imageuri = getContext().getContentResolver().insert(MediaStore.Images.Media.EXTERNAL_CONTENT_URI, values);
            Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
            takePictureIntent.putExtra(MediaStore.EXTRA_OUTPUT, imageuri);

            if (takePictureIntent.resolveActivity(getActivity().getPackageManager()) != null) {
PATH: app/src/main/java/com/lendeasy/lendeasy/Profile.java
LINES: 261-267

//checking if phone has camera start the activity for getting picture
                startActivityForResult(takePictureIntent, REQ_IMAGE_CAPTURE);
            }
        }
    }

}
PATH: app/src/main/java/com/lendeasy/lendeasy/Target.java
LINES: 1-45

package com.lendeasy.lendeasy;

import android.os.Bundle;

import androidx.annotation.Nullable;
import androidx.fragment.app.Fragment;

import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;

import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.EventListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.FirebaseFirestoreException;


/**
 * A simple {@link Fragment} subclass.
 * create an instance of this fragment.
 */
public class Target extends Fragment {
    ImageView tgtedit,blcedit,timedit;
    TextView target,balance,time;
    FirebaseUser user= FirebaseAuth.getInstance().getCurrentUser();
    FirebaseFirestore db=FirebaseFirestore.getInstance();

    public Target() {
        // Required empty public constructor
    }


    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {

        View view=inflater.inflate(R.layout.fragment_target, container, false);

        tgtedit=view.findViewById(R.id.tgtedit);
        blcedit=view.findViewById(R.id.blcedit);
PATH: app/src/main/java/com/lendeasy/lendeasy/Target.java
LINES: 46-80

timedit=view.findViewById(R.id.timeedit);
        target=view.findViewById(R.id.target);
        balance=view.findViewById(R.id.balance);

        tgtedit.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                EditDialog editDialog=new EditDialog(getContext(),"Target");
                editDialog.show();
            }
        });

        blcedit.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
                EditDialog editDialog=new EditDialog(getContext(),"Balance");
                editDialog.show();
            }
        });

        timedit.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View view) {
//                EditDialog editDialog=new EditDialog(getContext(),"");
//                editDialog.show();
            }
        });

        db.collection("users").document(user.getUid())
                .addSnapshotListener(new EventListener<DocumentSnapshot>() {
                    @Override
                    public void onEvent(@Nullable DocumentSnapshot documentSnapshot, @Nullable FirebaseFirestoreException e) {
                        if(documentSnapshot!=null){

                            Log.d("docsnap",documentSnapshot+"");
PATH: app/src/main/java/com/lendeasy/lendeasy/Target.java
LINES: 81-92

Log.d("docdouble",documentSnapshot.getDouble("Target")+"");

                            target.setText(documentSnapshot.getDouble("Target")+"");
                            balance.setText(documentSnapshot.getDouble("Balance")+"");

                        }
                    }
                });
        // Inflate the layout for this fragment
        return view;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/Top10.java
LINES: 1-50

package com.lendeasy.lendeasy;

import android.os.Bundle;

import androidx.fragment.app.Fragment;
import androidx.recyclerview.widget.LinearLayoutManager;
import androidx.recyclerview.widget.RecyclerView;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;

import java.util.ArrayList;


/**
 * A simple {@link Fragment} subclass.
 * create an instance of this fragment.
 */
public class Top10 extends Fragment {
    private RecyclerView recyclerView;
    private LentAdapter lentAdapter;
    private ArrayList<String> list;

    public Top10() {
        // Required empty public constructor
    }


    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);

    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        View view=inflater.inflate(R.layout.fragment_top10, container, false);

        list=new ArrayList<>();
        list.add("Hello");
        recyclerView=view.findViewById(R.id.recyclerview);
        lentAdapter=new LentAdapter(list);
        recyclerView.setLayoutManager(new LinearLayoutManager(getContext()));
        recyclerView.setAdapter(lentAdapter);
        // Inflate the layout for this fragment
        return view;
    }
}
PATH: app/src/main/java/com/lendeasy/lendeasy/TransactionModel.java
LINES: 1-15

package com.lendeasy.lendeasy;

import java.sql.Timestamp;

public class TransactionModel {

    String Lender;
    String Borrower;
    String item;
    String Description;
    String interest;
    Timestamp startTime;
    Timestamp endTime;

}
PATH: app/src/main/java/com/lendeasy/lendeasy/ViewPagerAdapter.java
LINES: 1-38

package com.lendeasy.lendeasy;

import androidx.annotation.NonNull;
import androidx.fragment.app.Fragment;
import androidx.fragment.app.FragmentManager;
import androidx.fragment.app.FragmentStatePagerAdapter;

import java.util.ArrayList;
import java.util.List;

public class ViewPagerAdapter extends FragmentStatePagerAdapter {
    private List<androidx.fragment.app.Fragment> Fragment = new ArrayList<>();

    public ViewPagerAdapter(@NonNull FragmentManager fm) {
        super(fm);
    }

    @NonNull
    @Override
    public Fragment getItem(int position) {
        switch (position){
            case 0:
                return new Profile();
            default:
                return new Lend();

        }
    }

    public void add(Fragment Frag) {
        Fragment.add(Frag);
    }

    @Override
    public int getCount() {
        return 2;
    }
}
PATH: app/src/test/java/com/lendeasy/lendeasy/ExampleUnitTest.java
LINES: 1-17

package com.lendeasy.lendeasy;

import org.junit.Test;

import static org.junit.Assert.*;

/**
 * Example local unit test, which will execute on the development machine (host).
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
public class ExampleUnitTest {
    @Test
    public void addition_isCorrect() {
        assertEquals(4, 2 + 2);
    }
}
#Session 2: Logistic Regression

We studied about Linear Regression in the last session. Let us learn about another fundamental concept in Machine Learning: The Logistic Regression.

So let us first start by dissecting the terms and try to get an intuition of what logistic regression means.

### Logistic Regression: an Intuition

You may recognize the term "Regression". Do you recall what Regression means in Machine Learning?

Regression is prediction of a value, given some inputs. This value is calculated by feeding the input to a function. This function is adjusted in a way, that it gives us the best predictions over the data that we provide to it.

Then we studied about Linear Regression, which is a regression technique that uses a linear function as the model. We also studied about non-linear regression as an extension of linear regression. 



Let us now study about Logistic Regression. 

However, there's a small catch here. Logistic Regression is not really Regression, but a classification technique. What is classification?

Classification means to classify items into categories, or classes. For example, among a collection of pictures of dogs and cats, we would like our model to tell which one is which.

We'll be looking at a lot of classification techniques over this course. Logistic Regression will be the first of these.
However, there's a small catch here. Logistic Regression is not really Regression, but a classification technique. What is classification?

Classification means to classify items into categories, or classes. For example, among a collection of pictures of dogs and cats, we would like our model to tell which one is which.

We'll be looking at a lot of classification techniques over this course. Logistic Regression will be the first of these. 

So let us first understand how logistic regression works, and then we'll surely answer the question - *Why is Logistic Regression called Regression, if its a classification technique?*

## Logistic Regression

If you were to break down what Linear Regression is - you would realize, it simply means - **Regression** using a **Linear** Model. By the same logic, Logistic Regression would relate to a Logistic Model (function).

What is a Logistic function?

#### Logistic Function
The logistic function or the sigmoid function is defined as below:

$sigmoid(x) = \frac{1}{1+e^{-x}}$

It looks something like this:
<div>
<img src="https://drive.google.com/uc?id=1_En_37XZ1MLhYrsX0kxQhLml7kisHW1V" width="500"/>
</div>
It looks something like this:
<div>
<img src="https://drive.google.com/uc?id=1_En_37XZ1MLhYrsX0kxQhLml7kisHW1V" width="500"/>
</div>

So, if this is a Logistic function, what would logistic regression mean? Naturally it would mean modeling the data using a logistic function. Ideally it would be helpful in a case where the data is distributed more or less over a sigmoid function.

But we don't usually see data distributed in this fashion - there are almost no practical scenarios where data is distributed in this fashion. Actually, this function is not used for prediction of a value at all. As we said, it is used for classification. But how can we use a function for classification?

Before we start explaining that, let us point out a unique property of this function. Notice how the function strictly lies between 0 and 1. It approaches 1 as the input approaches $+\infty$, and 0 as the input approaches $-\infty$.

Before moving forward, let us make sure, we implement the function in python. Infact, we'll try to implement whatever concept we introduce. And we'll also tell you tricks, like when you don't need to implement a function, and can simply use a pre-implemented version of the code.
But we don't usually see data distributed in this fashion - there are almost no practical scenarios where data is distributed in this fashion. Actually, this function is not used for prediction of a value at all. As we said, it is used for classification. But how can we use a function for classification?

Before we start explaining that, let us point out a unique property of this function. Notice how the function strictly lies between 0 and 1. It approaches 1 as the input approaches $+\infty$, and 0 as the input approaches $-\infty$.

Before moving forward, let us make sure, we implement the function in python. Infact, we'll try to implement whatever concept we introduce. And we'll also tell you tricks, like when you don't need to implement a function, and can simply use a pre-implemented version of the code.

We begin by importing the library PyTorch. As you know, our data would be in the form of Tensors, so PyTorch gives us great tools to handle Tensors. PyTorch is also a great numerical processing library (meaning, it can efficiently handle complex calculation on a large set of numbers simultaneously).

```python
import torch
```

```python
def sigmoid(x): return 1/(1+torch.exp(-x))
```

torch.exp(x) is the same as $e^x$. 

Minor Technical Detail: It does not take any values for input (x). PyTorch expects all inputs to be Tensors, so `x` needs to be a Tensor
```

torch.exp(x) is the same as $e^x$. 

Minor Technical Detail: It does not take any values for input (x). PyTorch expects all inputs to be Tensors, so `x` needs to be a Tensor

Now let's test it on some values.

```python
sigmoid(torch.tensor((100.)))   #sigmoid(100) is so close to 1, that the computer rounds it off to 1 itself. 
                                #But theoretically, yes, it would be very close to 1, but not exactly 1. The round-off does not matter practically!
```

```python
sigmoid(torch.tensor((-100.)))
```

```python
sigmoid(torch.tensor(0.))
```

Moving ahead, as you would know, Computers can only understand numbers. (Even in the last session, you would remember, we had to convert dates into numbers, because computers do not understand dates). 

So it doesn't understand that our classes are 'dogs' or 'cats'. To make the computer understand which one is which - we assign a *label* to each of these classes. By convention, let us name them 0 (for dogs) and 1 (for cats). (You can interchange the labels - there's nothing wrong in that).
```

Moving ahead, as you would know, Computers can only understand numbers. (Even in the last session, you would remember, we had to convert dates into numbers, because computers do not understand dates). 

So it doesn't understand that our classes are 'dogs' or 'cats'. To make the computer understand which one is which - we assign a *label* to each of these classes. By convention, let us name them 0 (for dogs) and 1 (for cats). (You can interchange the labels - there's nothing wrong in that).

So now we come to the core idea of Logistic Regression - The sigmoid function will always produce an output between the values 0 and 1. If the output is closer to 1 than it is to 0 (ie, more than 0.5), we will say, that the model predicts the output to be 1, and if it is closer to 0 than it is to 1 (i.e., less than 0.5), then we would say that the model predicts the output to be 1. Does it make sense logically to you? 

Don't worry if the underlying idea is not yet clear. We would explore all the details in the following sections. 

So this is how Logistic Regression is used for Classification. 

Now coming to the question - *Then why is it called Regression anyways?*
Don't worry if the underlying idea is not yet clear. We would explore all the details in the following sections. 

So this is how Logistic Regression is used for Classification. 

Now coming to the question - *Then why is it called Regression anyways?*

The answer would be clearer as we look into the details. What you should know now, is that, we would ultimately create a Regression Model - a model that predicts a value, which lies between 0 and 1. And then we would create a condition, that would classify the prediction as 0 or 1, based on whether the value of the prediction is less or more than the threshold of 0.5. So we're trying to do classification through a regression model. Fascinating!

#### So how do we model a Logistic Regression Model?
 
By this, what we actually mean is - given some data points, which have multiple features (`x1`,`x2` and so on) and a target value (label , or categories , or classes), how do we form a logistic regression model?

Let us take a look at the logistic function again, because we know have an idea that in the end, our output is going to be the result of the logistic function. 

$sigmoid(a) = \frac{1}{1+e^{-a}}$
Let us take a look at the logistic function again, because we know have an idea that in the end, our output is going to be the result of the logistic function. 

$sigmoid(a) = \frac{1}{1+e^{-a}}$


We only have one parameter in this equation - `a`, where `a` is a real number. But our model has multiple features (`x1`, `x2`, `x3` and so on). How do we model a logistic function in terms of all these multiple features, if we have only one variable as the input?

We already learnt of a technique to combine multiple variables into one single value - the *Linear Regression* Model. 

```
y = w1*x1 + w2*x2 + w3*x3 + ... + b
```
`y` is the variable that contains the characteristics of all input features. 

So, now you can guess how the logistic regression model will be formulated.

$model(x_1,x_2,x_3....) = sigmoid(w_1.x_1 + w_2.x_2 + w_3.x_3 + ..... + b) $

or in other words,

$model(x_1,x_2,x_3....) = \frac{1}{1+e^{w_1x_1 + w_2x_2 + w_3x_3 + ... + b}} $

Thats it! That is the logistic regression model.

#### So how does it work?
What are the parameters of the model? Remember, that parameters are the variables that we change in order to make our model work. We do not have control over the exponential $e$, nor do we have control over the data (x1,x2...). We only have control over w1,w2....,b. These are our parameters.
Thats it! That is the logistic regression model.

#### So how does it work?
What are the parameters of the model? Remember, that parameters are the variables that we change in order to make our model work. We do not have control over the exponential $e$, nor do we have control over the data (x1,x2...). We only have control over w1,w2....,b. These are our parameters.

We would like to adjust our parameters in a way, that for each data point (having features x1,x2...), the linear function `w1*x1 + w2*x2 + .... + b` predicts a value greater than 0 if the target class is a 1 (hence giving the output of the logistic function greater than 0.5, which means, we would say that the model predicts the model to be a 1).

And if the actual target class is 0, we would want the linear function to return a value less than 0, so that the sigmoid (logistic) function returns a value less than 0.5, and thus we would say, that the model predicts the class to be a 0.

Phew! That's a lot to take in at once. Go back and read this again, and make sure you understand the logic. If needed, go back and study the logistic function, the linear regression model, and how they work, and how they all come together to form a classification model. 

We would adjust the parameters w1,w2....,b using an algorithm we already learned about - the *Gradient Descent!*

# Case Study: Identifying Handwritten Digits
Phew! That's a lot to take in at once. Go back and read this again, and make sure you understand the logic. If needed, go back and study the logistic function, the linear regression model, and how they work, and how they all come together to form a classification model. 

We would adjust the parameters w1,w2....,b using an algorithm we already learned about - the *Gradient Descent!*

# Case Study: Identifying Handwritten Digits

Given an image of a handwritten digit, can we build a classifier that can identify what digit it is? Digits refers to integers from 0 to 9, both inclusive.

<div>
<img src="https://drive.google.com/uc?id=1SLXc1HNY9uiO3jGVJ8sTYm0WE-ZpcJNc" width="500"/>
</div>

In the image above, notice, all the different styles in which any digit can be written. The model needs to "learn" the characteristics of digits. What differentiates a 1 from a 2, or a 5 from a 9? To be fair, it is difficult for even humans to express the idea of distinction in words. But we still Can give a vague answer to this question - each number has its own way of writing - for example, a 1 is not likely to have a curved line, but a 2 is likely to have a curve. Its quite amazing that a model can learn these ideas - and remember, all this is learnt in the form of parameters, which are nothing but numbers in themselves!
<div>
<img src="https://drive.google.com/uc?id=1SLXc1HNY9uiO3jGVJ8sTYm0WE-ZpcJNc" width="500"/>
</div>

In the image above, notice, all the different styles in which any digit can be written. The model needs to "learn" the characteristics of digits. What differentiates a 1 from a 2, or a 5 from a 9? To be fair, it is difficult for even humans to express the idea of distinction in words. But we still Can give a vague answer to this question - each number has its own way of writing - for example, a 1 is not likely to have a curved line, but a 2 is likely to have a curve. Its quite amazing that a model can learn these ideas - and remember, all this is learnt in the form of parameters, which are nothing but numbers in themselves!

This dataset is called the MNIST dataset, which contains 28x28 pixel greyscale images for all digits. We however will look at only 2 digits (remember, logistic regression is meant for binary classification). (We will however, look at how to classify among multiple classes too, later on!)
</div>

In the image above, notice, all the different styles in which any digit can be written. The model needs to "learn" the characteristics of digits. What differentiates a 1 from a 2, or a 5 from a 9? To be fair, it is difficult for even humans to express the idea of distinction in words. But we still Can give a vague answer to this question - each number has its own way of writing - for example, a 1 is not likely to have a curved line, but a 2 is likely to have a curve. Its quite amazing that a model can learn these ideas - and remember, all this is learnt in the form of parameters, which are nothing but numbers in themselves!

This dataset is called the MNIST dataset, which contains 28x28 pixel greyscale images for all digits. We however will look at only 2 digits (remember, logistic regression is meant for binary classification). (We will however, look at how to classify among multiple classes too, later on!)
In the image above, notice, all the different styles in which any digit can be written. The model needs to "learn" the characteristics of digits. What differentiates a 1 from a 2, or a 5 from a 9? To be fair, it is difficult for even humans to express the idea of distinction in words. But we still Can give a vague answer to this question - each number has its own way of writing - for example, a 1 is not likely to have a curved line, but a 2 is likely to have a curve. Its quite amazing that a model can learn these ideas - and remember, all this is learnt in the form of parameters, which are nothing but numbers in themselves!

This dataset is called the MNIST dataset, which contains 28x28 pixel greyscale images for all digits. We however will look at only 2 digits (remember, logistic regression is meant for binary classification). (We will however, look at how to classify among multiple classes too, later on!)
In the image above, notice, all the different styles in which any digit can be written. The model needs to "learn" the characteristics of digits. What differentiates a 1 from a 2, or a 5 from a 9? To be fair, it is difficult for even humans to express the idea of distinction in words. But we still Can give a vague answer to this question - each number has its own way of writing - for example, a 1 is not likely to have a curved line, but a 2 is likely to have a curve. Its quite amazing that a model can learn these ideas - and remember, all this is learnt in the form of parameters, which are nothing but numbers in themselves!

This dataset is called the MNIST dataset, which contains 28x28 pixel greyscale images for all digits. We however will look at only 2 digits (remember, logistic regression is meant for binary classification). (We will however, look at how to classify among multiple classes too, later on!)
This dataset is called the MNIST dataset, which contains 28x28 pixel greyscale images for all digits. We however will look at only 2 digits (remember, logistic regression is meant for binary classification). (We will however, look at how to classify among multiple classes too, later on!)

So let us donwload this dataset. Last time, we used the Kaggle API to download the dataset. We can use it this time too! But let us look at another alternative! Its good to know of all possible options to carry out a task - good practitioners should know of their options, because each option, more often than not, has it's pros and cons, and every method is not the best choice for a given task. But still, if you wish to use the Kaggle API, [here](https://www.kaggle.com/c/digit-recognizer/data) is the link to the dataset. Follow the exact procedure as last session. Below is a commented out peice of code from last session to download the dataset. Remember, you need to upload the kaggle.json file, and then enter the Kaggle API command.

In this session, we'll use the API provided by PyTorch. PyTorch provides us tools related to Computer Vision in a separate child library, called `torchvision`, which also includes some famous public datasets, MNIST being one of them. [Here](https://pytorch.org/docs/stable/torchvision/datasets.html) is the documentation for the datasets.

```python
So let us donwload this dataset. Last time, we used the Kaggle API to download the dataset. We can use it this time too! But let us look at another alternative! Its good to know of all possible options to carry out a task - good practitioners should know of their options, because each option, more often than not, has it's pros and cons, and every method is not the best choice for a given task. But still, if you wish to use the Kaggle API, [here](https://www.kaggle.com/c/digit-recognizer/data) is the link to the dataset. Follow the exact procedure as last session. Below is a commented out peice of code from last session to download the dataset. Remember, you need to upload the kaggle.json file, and then enter the Kaggle API command.

In this session, we'll use the API provided by PyTorch. PyTorch provides us tools related to Computer Vision in a separate child library, called `torchvision`, which also includes some famous public datasets, MNIST being one of them. [Here](https://pytorch.org/docs/stable/torchvision/datasets.html) is the documentation for the datasets.

```python
import torchvision
```

```python
mnist_train_ds = torchvision.datasets.MNIST(root='',download=True)
mnist_test_ds  = torchvision.datasets.MNIST(root='',train = False, download=True)
len(mnist_train_ds), len(mnist_test_ds)
```
```python
mnist_train_ds = torchvision.datasets.MNIST(root='',download=True)
mnist_test_ds  = torchvision.datasets.MNIST(root='',train = False, download=True)
len(mnist_train_ds), len(mnist_test_ds)
```

We have 2 variables - `mnist_train_ds` and `mnist_test_ds`. These refer to the Training set and the Test set, which is basically the result of splitting the entire datasets into two subsets. What do they mean?

### Splitting Datasets into training and testing sets

Imagine you have 1000 datapoints to train your model on. You successfully train your model. But now, the question is - *How do you know that your model actually works well?* 

You would say - the model predicts $p\%$ of the images in the dataset correctly, which is a good accuracy. 

But here's the problem - its very much possible, that the model might work well on data you've trained it on (The training dataset), but terrible on data that it hasn't. This is called *Improper Fitting of the model over the data*.  It basically means, that your model has not "learnt" the right things. 

This means that the parameters are set in a manner, that do not represent the general idea of the category, but only the specific characteristics of the images in the training dataset. Naturally, the model performs worse on images it never trained on, because it would come across new settings, that it hasn't learnt to identify.
You would say - the model predicts $p\%$ of the images in the dataset correctly, which is a good accuracy. 

But here's the problem - its very much possible, that the model might work well on data you've trained it on (The training dataset), but terrible on data that it hasn't. This is called *Improper Fitting of the model over the data*.  It basically means, that your model has not "learnt" the right things. 

This means that the parameters are set in a manner, that do not represent the general idea of the category, but only the specific characteristics of the images in the training dataset. Naturally, the model performs worse on images it never trained on, because it would come across new settings, that it hasn't learnt to identify. 

For example, if your model predicts which image is that of a cat or a dog - you would expect your model to learn the general characterisitics of dogs and cats, and not features such as - a picture of a dog is likely to have a green background (grass), while a cat is not (because dogs like to go out of the house, cats don't!). What if all your training images have dogs with a green background, and all cat images with an indoor background? Your model is likely to learn that too!
But here's the problem - its very much possible, that the model might work well on data you've trained it on (The training dataset), but terrible on data that it hasn't. This is called *Improper Fitting of the model over the data*.  It basically means, that your model has not "learnt" the right things. 

This means that the parameters are set in a manner, that do not represent the general idea of the category, but only the specific characteristics of the images in the training dataset. Naturally, the model performs worse on images it never trained on, because it would come across new settings, that it hasn't learnt to identify. 

For example, if your model predicts which image is that of a cat or a dog - you would expect your model to learn the general characterisitics of dogs and cats, and not features such as - a picture of a dog is likely to have a green background (grass), while a cat is not (because dogs like to go out of the house, cats don't!). What if all your training images have dogs with a green background, and all cat images with an indoor background? Your model is likely to learn that too! 

Now, suppose you have a pet dog at home, and click a picture, and feed it to your model. This picture has an indoor setting, so the model is likely to predict it as a cat. That is how improper fitting works.

---
For example, if your model predicts which image is that of a cat or a dog - you would expect your model to learn the general characterisitics of dogs and cats, and not features such as - a picture of a dog is likely to have a green background (grass), while a cat is not (because dogs like to go out of the house, cats don't!). What if all your training images have dogs with a green background, and all cat images with an indoor background? Your model is likely to learn that too! 

Now, suppose you have a pet dog at home, and click a picture, and feed it to your model. This picture has an indoor setting, so the model is likely to predict it as a cat. That is how improper fitting works.

---

### So how do we make sure that the model fits well through the data?

Keep aside a small portion (say, 20%) of the dataset aside. Only train your model on the remaining data. And at the end, see how the model performs on the data that we kept aside (the data that the model has never seen). This would be an indicator of how well the model has learnt the general features of the data. Because if the model works well on data that it never came across, it *has* to be because the model has learnt the right characterisitics.
---

### So how do we make sure that the model fits well through the data?

Keep aside a small portion (say, 20%) of the dataset aside. Only train your model on the remaining data. And at the end, see how the model performs on the data that we kept aside (the data that the model has never seen). This would be an indicator of how well the model has learnt the general features of the data. Because if the model works well on data that it never came across, it *has* to be because the model has learnt the right characterisitics.

This dataset, that we keep aside is called the **Validation Set**, or the **Test Dataset**. These two essentially mean the same thing - a sub-dataset that is used to validate or test the correctness of the model. The model never trains on this dataset, and is used only to test the accuracy of the model. 

If the model does not perform well on the validation dataset, we make changes in the model and training mechanism. More specifically, we change the *hyperparameters* of the model, which are values other than the parameters (w1,w2....), which cannot be learned by the model, but need to be manually set by us. Learning Rate in the Gradient Descent Algorithm is one such example. So you may adjust the learning rate until the validation set accuracy is good enough for our application.

---

Note:
If the model does not perform well on the validation dataset, we make changes in the model and training mechanism. More specifically, we change the *hyperparameters* of the model, which are values other than the parameters (w1,w2....), which cannot be learned by the model, but need to be manually set by us. Learning Rate in the Gradient Descent Algorithm is one such example. So you may adjust the learning rate until the validation set accuracy is good enough for our application.

---

Note: 
1. In many courses, validation set and test set are two different concepts. A validation set refers to a set which is specifically used to set hyperparameters. In doing this, there is a chance we have "memorized" the validation set too, because we manually set the hyperparameters (eg, the learning rate) as a value that works well only for the particular (validation) set. So, we use a test set, which is  a set that we never use for either training or adjustment. It is *completely* unknown to the model. This is a stricter version of the idea we are trying to pursue (to keep aside a dataset that is not to be seen by the model during training).
---

Note: 
1. In many courses, validation set and test set are two different concepts. A validation set refers to a set which is specifically used to set hyperparameters. In doing this, there is a chance we have "memorized" the validation set too, because we manually set the hyperparameters (eg, the learning rate) as a value that works well only for the particular (validation) set. So, we use a test set, which is  a set that we never use for either training or adjustment. It is *completely* unknown to the model. This is a stricter version of the idea we are trying to pursue (to keep aside a dataset that is not to be seen by the model during training). 

 However, for small scale projects, you need not necessarily separate the validation and the test datasets. They can be combined into one dataset, and can be called either the validation set, or the test set. In these sessions, we will interchangeably use the terms validation set and the test set, unless we explicitly mention otherwise. 


2. Improper fitting of the dataset can be of two types - Underfitting and Overfitting. You will learn more of this during the lectures. But if you are curious to know more about these concepts - [here](https://www.youtube.com/watch?v=edxhGBnT-Ps) is great intuitive explanation of the difference between the two!

3.
However, for small scale projects, you need not necessarily separate the validation and the test datasets. They can be combined into one dataset, and can be called either the validation set, or the test set. In these sessions, we will interchangeably use the terms validation set and the test set, unless we explicitly mention otherwise. 


2. Improper fitting of the dataset can be of two types - Underfitting and Overfitting. You will learn more of this during the lectures. But if you are curious to know more about these concepts - [here](https://www.youtube.com/watch?v=edxhGBnT-Ps) is great intuitive explanation of the difference between the two!

3. 
 Why did we not use a validation set in the Linear Regression problem? In many courses, you will find a separate validation set being used for the Linear Regression Problem. This is technically right, but practically, we don't need a validation set for Linear Regression problems. This is because you cannot underfit or overfit linear data. The only extra data you may need in a linear regression model, is to carry out inference (checking the performance of the final model) (testing of the model).

Let us look at how this dataset looks like. According to the documentation, you can index these dataset variables to get a tuple with the image, and the label

```python
mnist_train_ds[0]
```

```python
mnist_train_ds[0][0]
```
```

```python
mnist_train_ds[0][0]
```

#### Case Study: Binary Classification in the MNIST dataset

The MNIST dataset contains 10 labels (0 through 9), but if you remember, logisitic regression is meant to do binary classification. So let us take out two labels, say - 3's and 5's and try to differentiate between them.

```python
threes_ds = [i for i in mnist_train_ds if i[1]==3]
fives_ds  = [i for i in mnist_train_ds if i[1]==5]
len(threes_ds), len(fives_ds)
```

```python
##Similarly, let us extract the test dataset also
threes_test_ds = [i for i in mnist_test_ds if i[1]==3]
fives_test_ds  = [i for i in mnist_test_ds if i[1]==5]
len(threes_test_ds), len(fives_test_ds)
```

And finally, we convert these to PyTorch tensors, and we'll build our model therefrom. The images in our dataset are objects of an Image Processing Library in Python, called PIL. So, to convert them to tensors, we use the `transforms` method of the torchvision library.

```python
def convert_PIL_to_tensors(images):
    images=list(images) #to make sure we can index the collection of images properly. Because of this, the input to this function need
                        #not necessarily be a list, but can a set, tuple, generator or even a dictionary
    return torch.stack(list(map(torchvision.transforms.ToTensor(),images))).float()
```

```python
images=list(images) #to make sure we can index the collection of images properly. Because of this, the input to this function need
                        #not necessarily be a list, but can a set, tuple, generator or even a dictionary
    return torch.stack(list(map(torchvision.transforms.ToTensor(),images))).float()
```

```python
x_threes=convert_PIL_to_tensors([i[0] for i in threes_ds]).view(-1,28*28)
x_fives =convert_PIL_to_tensors([i[0] for i in fives_ds]).view(-1,28*28)

x_dataset = torch.cat((x_threes,x_fives))
y_dataset = torch.stack([torch.tensor(1.)]*len(x_threes) + [torch.tensor(0.)]*len(x_fives))

x_dataset.shape, y_dataset.shape
```

Similarly for the test dataset

```python
x_threes_test=convert_PIL_to_tensors([i[0] for i in threes_test_ds]).view(-1,28*28)
x_fives_test =convert_PIL_to_tensors([i[0] for i in fives_test_ds]).view(-1,28*28)

x_test_dataset = torch.cat((x_threes_test,x_fives_test))
y_test_dataset = torch.stack([torch.tensor(1.)]*len(x_threes_test) + [torch.tensor(0.)]*len(x_fives_test))

x_test_dataset.shape, y_test_dataset.shape
```

A few details.
x_test_dataset.shape, y_test_dataset.shape
```

A few details.

1. `convert_PIL_to_tensors` takes in a list of images, and returns a tensor containing numerical values (pixel values) of tensors. By the way, the pixel values of the MNIST dataset lie between 0 and 9. 0 is a completely black pixel, and 9 represents a white pixel. This scale is called the grayscale. Black and White Images are basically grayscale images. But torch.transform converts all values between 0 and 1. 

2. `.view()` is a pytorch tensor method used to reshape tensors. So we're essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the `-1` as the first argument? It means, "whatever value is needed to account for all the values". So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the `-1` would internally be replaced by `100`, in order to be account for all values. You can even add as many "dimensions" to the tensor as possible, as long as the total number of values in the tensors remains the same.
A few details.

1. `convert_PIL_to_tensors` takes in a list of images, and returns a tensor containing numerical values (pixel values) of tensors. By the way, the pixel values of the MNIST dataset lie between 0 and 9. 0 is a completely black pixel, and 9 represents a white pixel. This scale is called the grayscale. Black and White Images are basically grayscale images. But torch.transform converts all values between 0 and 1. 

2. `.view()` is a pytorch tensor method used to reshape tensors. So we're essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the `-1` as the first argument? It means, "whatever value is needed to account for all the values". So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the `-1` would internally be replaced by `100`, in order to be account for all values. You can even add as many "dimensions" to the tensor as possible, as long as the total number of values in the tensors remains the same.
1. `convert_PIL_to_tensors` takes in a list of images, and returns a tensor containing numerical values (pixel values) of tensors. By the way, the pixel values of the MNIST dataset lie between 0 and 9. 0 is a completely black pixel, and 9 represents a white pixel. This scale is called the grayscale. Black and White Images are basically grayscale images. But torch.transform converts all values between 0 and 1. 

2. `.view()` is a pytorch tensor method used to reshape tensors. So we're essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the `-1` as the first argument? It means, "whatever value is needed to account for all the values". So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the `-1` would internally be replaced by `100`, in order to be account for all values. You can even add as many "dimensions" to the tensor as possible, as long as the total number of values in the tensors remains the same.
1. `convert_PIL_to_tensors` takes in a list of images, and returns a tensor containing numerical values (pixel values) of tensors. By the way, the pixel values of the MNIST dataset lie between 0 and 9. 0 is a completely black pixel, and 9 represents a white pixel. This scale is called the grayscale. Black and White Images are basically grayscale images. But torch.transform converts all values between 0 and 1. 

2. `.view()` is a pytorch tensor method used to reshape tensors. So we're essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the `-1` as the first argument? It means, "whatever value is needed to account for all the values". So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the `-1` would internally be replaced by `100`, in order to be account for all values. You can even add as many "dimensions" to the tensor as possible, as long as the total number of values in the tensors remains the same.
2. `.view()` is a pytorch tensor method used to reshape tensors. So we're essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the `-1` as the first argument? It means, "whatever value is needed to account for all the values". So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the `-1` would internally be replaced by `100`, in order to be account for all values. You can even add as many "dimensions" to the tensor as possible, as long as the total number of values in the tensors remains the same. 

3. What are dimensions of a tensor? A tensor is a multidimensional array. So think of a tensor with 2 dimensions as an "array of arrays", and with 3 dimensions as "an array of arrays of arrays", etc. `shape` is nothing but the values of all dimensions. So if a tensor is of shape (100,784), it means that it is an array containing 100 arrays, each of which contains 784 values. 

4. By convention, in Machine Learning, the first dimension represents the number of items, and the following values represent the shape of the data items. For example, a tensor of shape (100, 784) is a collection of 100 items, each having 784 features.
2. `.view()` is a pytorch tensor method used to reshape tensors. So we're essentially converting a tensor of shape (28,28) (The height and width of the image in pixels) to a single flattened tensor of length 28*28 = 784. What about the `-1` as the first argument? It means, "whatever value is needed to account for all the values". So if there are 100 images of shape (28,28), meaning there are 100x28x28 values in total, the `-1` would internally be replaced by `100`, in order to be account for all values. You can even add as many "dimensions" to the tensor as possible, as long as the total number of values in the tensors remains the same. 

3. What are dimensions of a tensor? A tensor is a multidimensional array. So think of a tensor with 2 dimensions as an "array of arrays", and with 3 dimensions as "an array of arrays of arrays", etc. `shape` is nothing but the values of all dimensions. So if a tensor is of shape (100,784), it means that it is an array containing 100 arrays, each of which contains 784 values. 

4. By convention, in Machine Learning, the first dimension represents the number of items, and the following values represent the shape of the data items. For example, a tensor of shape (100, 784) is a collection of 100 items, each having 784 features.
3. What are dimensions of a tensor? A tensor is a multidimensional array. So think of a tensor with 2 dimensions as an "array of arrays", and with 3 dimensions as "an array of arrays of arrays", etc. `shape` is nothing but the values of all dimensions. So if a tensor is of shape (100,784), it means that it is an array containing 100 arrays, each of which contains 784 values. 

4. By convention, in Machine Learning, the first dimension represents the number of items, and the following values represent the shape of the data items. For example, a tensor of shape (100, 784) is a collection of 100 items, each having 784 features. 

 A tensor of shape (100, 3, 50, 50) is a collection of 100 items, each of which has the dimensions (3,50,50) (this is usually the case with images, which have 3 channels (RGB), and 50x50 is the height x width of the image). For grayscale images in the MNIST dataset, the tensor would be of the shape ($x$, 1, 28, 28), meaning a collection of $x$ items, each having an image shape of (1,28,28), meaning 1 single channel of shape 28x28 pixels. Don't worry if it doesn't make a lot of sense right now. You would get a hold of it through practice.
3. What are dimensions of a tensor? A tensor is a multidimensional array. So think of a tensor with 2 dimensions as an "array of arrays", and with 3 dimensions as "an array of arrays of arrays", etc. `shape` is nothing but the values of all dimensions. So if a tensor is of shape (100,784), it means that it is an array containing 100 arrays, each of which contains 784 values. 

4. By convention, in Machine Learning, the first dimension represents the number of items, and the following values represent the shape of the data items. For example, a tensor of shape (100, 784) is a collection of 100 items, each having 784 features. 

 A tensor of shape (100, 3, 50, 50) is a collection of 100 items, each of which has the dimensions (3,50,50) (this is usually the case with images, which have 3 channels (RGB), and 50x50 is the height x width of the image). For grayscale images in the MNIST dataset, the tensor would be of the shape ($x$, 1, 28, 28), meaning a collection of $x$ items, each having an image shape of (1,28,28), meaning 1 single channel of shape 28x28 pixels. Don't worry if it doesn't make a lot of sense right now. You would get a hold of it through practice.
4. By convention, in Machine Learning, the first dimension represents the number of items, and the following values represent the shape of the data items. For example, a tensor of shape (100, 784) is a collection of 100 items, each having 784 features. 

 A tensor of shape (100, 3, 50, 50) is a collection of 100 items, each of which has the dimensions (3,50,50) (this is usually the case with images, which have 3 channels (RGB), and 50x50 is the height x width of the image). For grayscale images in the MNIST dataset, the tensor would be of the shape ($x$, 1, 28, 28), meaning a collection of $x$ items, each having an image shape of (1,28,28), meaning 1 single channel of shape 28x28 pixels. Don't worry if it doesn't make a lot of sense right now. You would get a hold of it through practice. 

5. For the sake of convenience, we convert all data tensors of shape (1,28,28), to a single array of 784 items. So now, these 784 items act as the features of the data. So correspondingly, our parameters, would be `w1, w2, w3, ....., w784, b`.

6. Note: We have assigned a label of 0 to our 3's and 1 to our 5's.

Let us see how the dataset is formatted. If you pick any image tensor (a tensor of length 784), and reshape it into the original size 28x28, you would get the original image back. To plot the pixels, we use the `matplotlib` library.

```python
import matplotlib.pyplot as plt
6. Note: We have assigned a label of 0 to our 3's and 1 to our 5's.

Let us see how the dataset is formatted. If you pick any image tensor (a tensor of length 784), and reshape it into the original size 28x28, you would get the original image back. To plot the pixels, we use the `matplotlib` library.

```python
import matplotlib.pyplot as plt
plt.imshow(x_dataset[0].view(28,28), cmap='gray')
```

We also combine both the `x` and the `y` into one single variable, and simply call it the `dataset`. This dataset can be indexed to get both the x and y together, without having to handle two variable (x and y).

```python
train_dset=list(zip(x_dataset,y_dataset))
valid_dset=list(zip(x_test_dataset,y_test_dataset))
```

```python
x,y=train_dset[0]
x.shape,y
```

So now, we need 784 weights and 1 bias to create our model. Let us create a function to initiate parameters.

```python
def init_params(size, requires_grad=True): return (torch.randn(size)).requires_grad_()
```

```python
weights=init_params((28*28,1))
bias=init_params(1)
```

Let us now create our model

```python
def logistic_regression_model(x): 
    return torch.sigmoid(x@weights + bias)
```

Lessons in Python!

**The `@` operator**
```

Lessons in Python!

**The `@` operator**

As we know, Python is THE most famous language for data sciences. Matrix multiplications are such an integral part of data sciences, that in 2014, python introduced an operator that is dedicated only for matrix multiplication. Remember, if you want to matrix multiply two matrices (A and B) of shapes (m x n) and (p x q) respectively, p should be equal to n, and the resultant matrix would be of the shape (m x q). If you are unclear about the concept of matrices and matrix multiplication, [here](http://matrixmultiplication.xyz/) is a great source to help you visualize. 

Lets test this model out!

```python
preds = logistic_regression_model(x_dataset)
preds.shape
```

#### The LOSS FUNCTION

So far we've looked at the Mean Square Error Loss Function. It represents the idea, that the larger the distance between two values, the larger the loss. This worked well for the regression case, where we wanted the predictions of the model to be as close as the target values. But that is not applicable for the classification case. We cannot define the difference between two classes, like the dog and class. What we need is a right/wrong approach. Is the prediction right? or is it wrong?

Let us analyse what we would want our model to do.
#### The LOSS FUNCTION

So far we've looked at the Mean Square Error Loss Function. It represents the idea, that the larger the distance between two values, the larger the loss. This worked well for the regression case, where we wanted the predictions of the model to be as close as the target values. But that is not applicable for the classification case. We cannot define the difference between two classes, like the dog and class. What we need is a right/wrong approach. Is the prediction right? or is it wrong?

Let us analyse what we would want our model to do.

We would want our model to predict the value for 3's to be as close to 1., and predict 7's to be as close to 0. Because as we mentioned, if the prediction is closer to a 1 than to a 0, we would classify the result to be a 1. And if the prediction is closer to a 0 than to a 1, we would classify the result to be a 0.If you cannot wrap your hand around this concept, please go back a step and read the concepts again. Because if you understand the concepts well, you can implement the concepts easily too.

```python
def binary_classification_loss(preds,targets):
    assert len(preds)==len(targets)
    return torch.where(targets==1,1-preds,preds).mean()
```
```python
def binary_classification_loss(preds,targets):
    assert len(preds)==len(targets)
    return torch.where(targets==1,1-preds,preds).mean()
```

torch.where(targets==1,1-preds,preds) is the same as `[1-preds if i==1 else preds for i in targets ]`. However, it is a much more efficient method to handle tensors, both in terms of speed and memory.

It basically means, for each data point whose target is 1, the loss is 1-prediction. So if the model prediction is more closer to 0 than to 1 in this case, a higher penalty is assigned. And wherever the target is not 1 (ie , it is 0), the loss is preds. So if the prediction is closer to 1 than 0 in this case, a higher penalty is assigned. 

Finally, we take the mean of all these losses to give the average loss. Let's test if this loss function works!

```python
binary_classification_loss(preds,y_dataset)
```

Note: In a lot of course, you would have seen the binary loss function written something as:

$Loss(pred, target) = mean([-y_i log(pred_i)  (1  y_i) log(1-pred_i)])$
```

Note: In a lot of course, you would have seen the binary loss function written something as:

$Loss(pred, target) = mean([-y_i log(pred_i)  (1  y_i) log(1-pred_i)])$

If you are familiar with this loss function, and are confused with why we have not used this form of expression - the answer is - we have implemented a similar idea. This loss function too penalizes the model based on similar principles. But we haven't used the `log` in our loss function. Actually, the `log` arises during the mathematics behind theory of prediction. When you study the concept of *Maximum Likelihood*, you will learn the exact details. But theory aside, in practice, the loss function we have implemented is correct too! In Practice, Loss function should be thought of as a function that penalizes wrong predictions, and rewards correct predicitions. And it should have some nice mathematical qualites wherever required, such as smoothness and continuity (so that it can be properly differentiated during Gradient Descent, for example).

Our task is to minimize this loss function. We do this through the Gradient Descent Algorithm. But before that, we will learn of an important variant of Gradient Descent - *The Batch Gradient Descent*.

#### Batch Gradient Descent
If you are familiar with this loss function, and are confused with why we have not used this form of expression - the answer is - we have implemented a similar idea. This loss function too penalizes the model based on similar principles. But we haven't used the `log` in our loss function. Actually, the `log` arises during the mathematics behind theory of prediction. When you study the concept of *Maximum Likelihood*, you will learn the exact details. But theory aside, in practice, the loss function we have implemented is correct too! In Practice, Loss function should be thought of as a function that penalizes wrong predictions, and rewards correct predicitions. And it should have some nice mathematical qualites wherever required, such as smoothness and continuity (so that it can be properly differentiated during Gradient Descent, for example).

Our task is to minimize this loss function. We do this through the Gradient Descent Algorithm. But before that, we will learn of an important variant of Gradient Descent - *The Batch Gradient Descent*.

#### Batch Gradient Descent
Our task is to minimize this loss function. We do this through the Gradient Descent Algorithm. But before that, we will learn of an important variant of Gradient Descent - *The Batch Gradient Descent*.

#### Batch Gradient Descent

If you go back to the last lab session (Linear Regression), you would notice, that we passed the entire set of data into the model at once. This worked alright for our case, becaues the number of data points was not very huge. We were dealing with less than 2000 points. But in many cases, the number of datapoints is of the order of tens of thousands to anywhere close to millions of datapoints, and computers cannot handle so many datapoints at once. It leads to a lot of problems - sums tend to approach very large values which computers cannot process. It is also slow and thus, the model rarely works. This problem increases multifold when the number of features increases. 

The solution? We divide the data into smaller chunks, called *Batches*. And we only train the model on one batch at a time. The number of items in a batch, is called the *batch size*. You would usually see batch_sizes as powers of 2 - 2,4,8,16,32,64,128 and so on. The larger each datapoint is in size, the smaller batch size is recommended. Eg - pictures have a lot of features (pixels), and thus, a smaller batch size, like 8 or 16 is recommended.
#### Batch Gradient Descent

If you go back to the last lab session (Linear Regression), you would notice, that we passed the entire set of data into the model at once. This worked alright for our case, becaues the number of data points was not very huge. We were dealing with less than 2000 points. But in many cases, the number of datapoints is of the order of tens of thousands to anywhere close to millions of datapoints, and computers cannot handle so many datapoints at once. It leads to a lot of problems - sums tend to approach very large values which computers cannot process. It is also slow and thus, the model rarely works. This problem increases multifold when the number of features increases. 

The solution? We divide the data into smaller chunks, called *Batches*. And we only train the model on one batch at a time. The number of items in a batch, is called the *batch size*. You would usually see batch_sizes as powers of 2 - 2,4,8,16,32,64,128 and so on. The larger each datapoint is in size, the smaller batch size is recommended. Eg - pictures have a lot of features (pixels), and thus, a smaller batch size, like 8 or 16 is recommended.
If you go back to the last lab session (Linear Regression), you would notice, that we passed the entire set of data into the model at once. This worked alright for our case, becaues the number of data points was not very huge. We were dealing with less than 2000 points. But in many cases, the number of datapoints is of the order of tens of thousands to anywhere close to millions of datapoints, and computers cannot handle so many datapoints at once. It leads to a lot of problems - sums tend to approach very large values which computers cannot process. It is also slow and thus, the model rarely works. This problem increases multifold when the number of features increases. 

The solution? We divide the data into smaller chunks, called *Batches*. And we only train the model on one batch at a time. The number of items in a batch, is called the *batch size*. You would usually see batch_sizes as powers of 2 - 2,4,8,16,32,64,128 and so on. The larger each datapoint is in size, the smaller batch size is recommended. Eg - pictures have a lot of features (pixels), and thus, a smaller batch size, like 8 or 16 is recommended.
If you go back to the last lab session (Linear Regression), you would notice, that we passed the entire set of data into the model at once. This worked alright for our case, becaues the number of data points was not very huge. We were dealing with less than 2000 points. But in many cases, the number of datapoints is of the order of tens of thousands to anywhere close to millions of datapoints, and computers cannot handle so many datapoints at once. It leads to a lot of problems - sums tend to approach very large values which computers cannot process. It is also slow and thus, the model rarely works. This problem increases multifold when the number of features increases. 

The solution? We divide the data into smaller chunks, called *Batches*. And we only train the model on one batch at a time. The number of items in a batch, is called the *batch size*. You would usually see batch_sizes as powers of 2 - 2,4,8,16,32,64,128 and so on. The larger each datapoint is in size, the smaller batch size is recommended. Eg - pictures have a lot of features (pixels), and thus, a smaller batch size, like 8 or 16 is recommended.
The solution? We divide the data into smaller chunks, called *Batches*. And we only train the model on one batch at a time. The number of items in a batch, is called the *batch size*. You would usually see batch_sizes as powers of 2 - 2,4,8,16,32,64,128 and so on. The larger each datapoint is in size, the smaller batch size is recommended. Eg - pictures have a lot of features (pixels), and thus, a smaller batch size, like 8 or 16 is recommended. 

In PyTorch, this is done using the `DataLoader`, which is nothing but a class, that can divide the data into batches, and has some other useful properties, which you will learn as you use it more and more. PyTorch Dataloaders are found in the module `torch.utils.data`

```python
from torch.utils.data import DataLoader
```

```python
dl=DataLoader(train_dset,batch_size=16)
valid_dl=DataLoader(valid_dset,batch_size=32)
```

A dataloader acts as an iterator (meaning something over which you can iterate. List too is an iterator. Remember how we do `for i in list`. We are basically iterating through it).

So, when we do `for x_batch,y_batch in dl`, we will get batches of x and y together. Each batch is a tensor of shape `(batch_size, size_of_datapoint)`. Eg.

```python
x_batch,y_batch=next(iter(dl))
x_batch.shape, y_batch.shape
```
```python
x_batch,y_batch=next(iter(dl))
x_batch.shape, y_batch.shape
```

Next, we define the structure of our Gradient Descent Algorithm. This is the same code we developed for the last session, with only minor changes.

```python
def calc_grad(x_batch,y_batch,model): 
    preds=model(x_batch)
    loss=binary_classification_loss(preds,y_batch)
    loss.backward()

def train_epoch(model,lr,params):
    for x_batch,y_batch in dl: 
        calc_grad(x_batch,y_batch,model)
        for p in params:
            p.data -= lr*p.grad 
            p.grad.zero_()

def batch_accuracy(preds,y_batch):
    return ((preds>=0.5)==y_batch).float().mean()

def validate_epoch(model):
    accs=[batch_accuracy(model(x_batch),y_batch) for x_batch,y_batch in valid_dl]
    return torch.stack(accs).mean().item()
```

```python
#reinitializing the parameters. Do this everytime you want to rerun the model
weights=init_params((28*28,1))
bias=init_params(1)
params=weights,bias
```

```python
for _ in range(50): #run this model for 50 iterations
    train_epoch(logistic_regression_model,0.2,params)
    print(validate_epoch(logistic_regression_model),end=' ')
```

#### The model is about 88% accurate. That means, about 88% of the times, the model is able to tell which image is a three and which is a five. That is incredible!
train_epoch(logistic_regression_model,0.2,params)
    print(validate_epoch(logistic_regression_model),end=' ')
```

#### The model is about 88% accurate. That means, about 88% of the times, the model is able to tell which image is a three and which is a five. That is incredible!

Note: the number of iterations in Gradient Descent are also called as **Epochs**. So for the above case, we train this model for 50 epochs.

Task: (Non-Evaluative) - Try adjusting the learning rate and epochs by hit and trial and try to get as much accuracy as possible. Atleast try getting an accuracy of more than 90%.

So there you have it! You have trained your first classification model! Congratulations!

# Review:
Below we've given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.

### Some Tips:
So there you have it! You have trained your first classification model! Congratulations!

# Review:
Below we've given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.

### Some Tips:
# Review:
Below we've given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.

### Some Tips:
# Review:
Below we've given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.

### Some Tips:
Below we've given some review questions for you to try out. But before that, there are a few tips and review points, that you should go through, think over, and make sure you understand each of them. These points are the most important points of this entire session - and not surprisingly, these are the concepts and ideas that will actually help you be a good Machine Learning practitioner.

### Some Tips:
### Some Tips:
* **Stay up-to-date with the tools of Machine Learning**: As we've mentioned earlier, Machine Learning is an extremely fast moving domain. Every few weeks, new ideas and tools emerge, and within the next few weeks, these will become obsolete too. There are very few concepts and tools that last very long. That shouldn't discourage you. But encourage you to learn the skill of *unlearning* and *learning* fast. For example, a lot of you may have heard of the library NumPy, which is a numerical processing library. A * lot * of practitioners use this library to carry out the calculations (that we have done in this session, such as calculation of the sigmoid function, or matrix multiplications, etc). But notice how we carried out all numerical processing using PyTorch itself. Its not true that we could not have done the processing using NumPy, but it just wasnt the right choice. PyTorch offers the majority of processing tools offered by numpy, with the only difference - it is optimized for Tensors. It clearly was better suitable in this case. However, in another problem, numpy would be a better choice. Its also not true that PyTorch's numerical processing is superior than NumPy, its simply a case of suitability to the environment. We will use NumPy at some point too in these labs.
### Some Tips:
* **Stay up-to-date with the tools of Machine Learning**: As we've mentioned earlier, Machine Learning is an extremely fast moving domain. Every few weeks, new ideas and tools emerge, and within the next few weeks, these will become obsolete too. There are very few concepts and tools that last very long. That shouldn't discourage you. But encourage you to learn the skill of *unlearning* and *learning* fast. For example, a lot of you may have heard of the library NumPy, which is a numerical processing library. A * lot * of practitioners use this library to carry out the calculations (that we have done in this session, such as calculation of the sigmoid function, or matrix multiplications, etc). But notice how we carried out all numerical processing using PyTorch itself. Its not true that we could not have done the processing using NumPy, but it just wasnt the right choice. PyTorch offers the majority of processing tools offered by numpy, with the only difference - it is optimized for Tensors. It clearly was better suitable in this case. However, in another problem, numpy would be a better choice. Its also not true that PyTorch's numerical processing is superior than NumPy, its simply a case of suitability to the environment. We will use NumPy at some point too in these labs.
* **Stay up-to-date with the tools of Machine Learning**: As we've mentioned earlier, Machine Learning is an extremely fast moving domain. Every few weeks, new ideas and tools emerge, and within the next few weeks, these will become obsolete too. There are very few concepts and tools that last very long. That shouldn't discourage you. But encourage you to learn the skill of *unlearning* and *learning* fast. For example, a lot of you may have heard of the library NumPy, which is a numerical processing library. A * lot * of practitioners use this library to carry out the calculations (that we have done in this session, such as calculation of the sigmoid function, or matrix multiplications, etc). But notice how we carried out all numerical processing using PyTorch itself. Its not true that we could not have done the processing using NumPy, but it just wasnt the right choice. PyTorch offers the majority of processing tools offered by numpy, with the only difference - it is optimized for Tensors. It clearly was better suitable in this case. However, in another problem, numpy would be a better choice. Its also not true that PyTorch's numerical processing is superior than NumPy, its simply a case of suitability to the environment. We will use NumPy at some point too in these labs.
This intuition comes through practice. The important takeaway is, be open to new ideas and learn to adapt fast.

* **Proper Structuring of Code**: If you notice the code that we wrote above, you would notice, that we define a lot of function on the way. We didn't have to, we could have simply written the functionality directly, and calculated results. But it is not a good practice to do so. Code Refactoring(Structuring) is important to allow flexibility. Many times, your code simply wont work with some minor changes, if you dont structure your code properly. You also have to write the same piece of code again and again. 

  With proper structuring, you can make sure, you can tweak the functionality of the code with very minimal effort. This is especially important for highly complex codes, which have layers upon layers of code. If the code isn't properly structured, finding even simple bugs will turn into a nightmare. 

* As you develop codes, it is important to test whether the peice of code is working correctly. You would have noticed - at each step, we find out the shape of output tensors, and what all does a function return. 

### Review Questions:
With proper structuring, you can make sure, you can tweak the functionality of the code with very minimal effort. This is especially important for highly complex codes, which have layers upon layers of code. If the code isn't properly structured, finding even simple bugs will turn into a nightmare. 

* As you develop codes, it is important to test whether the peice of code is working correctly. You would have noticed - at each step, we find out the shape of output tensors, and what all does a function return. 

### Review Questions:
These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!

1. We imported many important libraries in this notebook. Whenever a library was imported, the reason and the goal was mentioned. Can you enlist all the libraries used in this notebook, and why they were used?

2. What is a dataloader? What purpose does it serve? What is a batch of data?

3. We have structured the data at many levels - from lists of our `x`'s and `y`'s, all the way to a dataloader. Can you draw a tree explaining the heirarchy of the data? At the top would be the dataloader, and the bottom would be the lists of inputs and targets.
1. We imported many important libraries in this notebook. Whenever a library was imported, the reason and the goal was mentioned. Can you enlist all the libraries used in this notebook, and why they were used?

2. What is a dataloader? What purpose does it serve? What is a batch of data?

3. We have structured the data at many levels - from lists of our `x`'s and `y`'s, all the way to a dataloader. Can you draw a tree explaining the heirarchy of the data? At the top would be the dataloader, and the bottom would be the lists of inputs and targets.

4. What is the sigmoid function? Can you relate the sigmoid function to the concept of probability?

5. Write the formulas in Pseudo Math and in pseudo Code, of both the Mean Square Error Loss, and the logistic Regression Loss.

6. What is the difference between hyperparameters and parameters?

7. Explain the difference between the training set, the validation set and the test set.

# Exercise (Evaluative):

## 1. Who survived the Titanic?
[This problem](https://www.kaggle.com/c/titanic/overview) is one of the most famous problems on Kaggle. It has (real) information of all passengers on the Titanic, such as the age, sex, ticket class, fare price (which may be indicators of their social status), etc. Can you build a logistic regression model to identify who survived the Titanic, and who did not?
# Exercise (Evaluative):

## 1. Who survived the Titanic?
[This problem](https://www.kaggle.com/c/titanic/overview) is one of the most famous problems on Kaggle. It has (real) information of all passengers on the Titanic, such as the age, sex, ticket class, fare price (which may be indicators of their social status), etc. Can you build a logistic regression model to identify who survived the Titanic, and who did not? 

The dataset is available from Kaggle itself, so we need to download the data using the API. Run the code below to upload the kaggle.json file, and download the data. 

Consider the following features: (See the details on the Kaggle page)
* `Sex` (in terms of 0's and 1's)
* `Age`
* `SibSp`
* `Parch`	
* `Fare`

Target Class: `Survived`

You need to *Normalize* the continuous variables (Fare and Age). See the bottom notes to see what Normalization is! 

You also need to use the pandas library to read csv files. Then you need to convert the data into tensors of suitable dimensions. Many values are not available in the dataframe, so for the sake of simplicity, we simply replace all inavailable values with 0's.
Target Class: `Survived`

You need to *Normalize* the continuous variables (Fare and Age). See the bottom notes to see what Normalization is! 

You also need to use the pandas library to read csv files. Then you need to convert the data into tensors of suitable dimensions. Many values are not available in the dataframe, so for the sake of simplicity, we simply replace all inavailable values with 0's.

Finally, we need to manually split the training set into a training dataset and the validation subset, because the test set provided does not have labels. So there is no way to evaluate the performance of the model through this dataset. We will do a 80-20 split. We've written a basic structure for you. Continue from that point, building your model step by step.

```python
%cd 
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json 

!kaggle competitions download -c titanic
!mkdir titanic
!mv train.csv titanic
%cd titanic
```

```python
!ls
```

```python
import pandas as pd
train_df =pd.read_csv('train.csv')
test_df = train_df.iloc[int(len(train_df)*0.8):]
train_df = train_df.iloc[:int(len(train_df)*0.8)]
```

```python
print(len(train_df),len(test_df))
test_df = train_df.iloc[int(len(train_df)*0.8):]
train_df = train_df.iloc[:int(len(train_df)*0.8)]
```

```python
print(len(train_df),len(test_df))
train_df.head()
```

First we use the define the training data.

Steps you need to follow:
1. convert the 'Sex' attribute as a binary (0/1) feature.
2. Clean the data. Replace any non-defined values with a 0, using `df=df.fillna(0)`
3. normalize the required features. This can be done by `x=(x-x.mean)/x.std()`
4. Extract the required features from the dataframe to the tensor. The tensors would be of the shape (x,5) (because of 5 features). You are expected to write this functionality on your own. If however you feel totally stuck, and cannot come up with anything, we provide you with a function structure. It is not totally intuitive, so you still would need to figure out how to work with this function structure. 
```
def get_xtensors_from_dataframe(df,features:list,target: str,normalize:list=[]): 
    x=None
    for feature in features: 
        feature_list = list(df[feature])
        if feature in normalize: feature_list = normalize_feature(feature_list)
        if x is None: 
            x= [torch.tensor(feature_list)]
            continue
        x.append(torch.tensor(feature_list))
    return torch.stack(x).permute(1,0)
```


5. Once you have your tensors, build your dataset, then train your model using gradient descent.
return torch.stack(x).permute(1,0)
```


5. Once you have your tensors, build your dataset, then train your model using gradient descent.

```python
#continue your code from here
train_df=train_df.fillna(0)
test_df=test_df.fillna(0)
train_df['Sex']=train_df['Sex'].map({'male':1, 'female':0})
test_df['Sex']=test_df['Sex'].map({'male':1, 'female':0})
train_df.head()
```

```python
def get_tensors_from_dataframe(df,features:list,target: str,normalize:list=[]): 
 x=None
 for feature in features: 
     feature_list = df[feature]
     if feature in normalize: feature_list = (feature_list-feature_list.mean())/feature_list.std()
     if x is None: 
         x= [torch.tensor(list(feature_list))]
         continue
     x.append(torch.tensor(list(feature_list)))
 y=[torch.tensor(list(df[target]))]
 return torch.stack(x).permute(1,0), torch.stack(y).permute(1,0)
```

```python
x_train, y_train = get_tensors_from_dataframe(train_df, features=['Sex', 'Age', 'SibSp', 'Parch', 'Fare'], target='Survived', normalize=['Fare', 'Age'])
x_test, y_test = get_tensors_from_dataframe(test_df, features=['Sex', 'Age', 'SibSp', 'Parch', 'Fare'], target='Survived', normalize=['Fare', 'Age'])
x_train.shape, y_train.shape
```

```python
weights=init_params((5,1))
bias=init_params(1)
params = weights, bias

train_dset=list(zip(x_train,y_train))
valid_dset=list(zip(x_test,y_test))
bias=init_params(1)
params = weights, bias

train_dset=list(zip(x_train,y_train))
valid_dset=list(zip(x_test,y_test))

dl=DataLoader(train_dset,batch_size=16)
valid_dl=DataLoader(valid_dset,batch_size=32)

for _ in range(50): #run this model for 50 iterations
    train_epoch(logistic_regression_model,0.2,params)
    print(validate_epoch(logistic_regression_model),end=' ')

print('\nAccuracy on train data:')
preds=logistic_regression_model(x_train)
print('%.3f'%((preds>=0.5)==y_train).float().mean().item())
print('Accuracy on test data:')
preds= logistic_regression_model(x_test)
print('%.3f'%((preds>=0.5)==y_test).float().mean().item())
```

### Normalization

Many times, we come accross, data that are of different scales. For example, if you have two features - age and annual salary, age is of the order of $O(10^2)$, while annual salary may be of the order $O(10^5)$. This creates a problem when we build the model, especially during training. The gradients of each parameter is affected by the values of all features, and so it is desirable that all these features have equal and reasonable contribution.
```

### Normalization

Many times, we come accross, data that are of different scales. For example, if you have two features - age and annual salary, age is of the order of $O(10^2)$, while annual salary may be of the order $O(10^5)$. This creates a problem when we build the model, especially during training. The gradients of each parameter is affected by the values of all features, and so it is desirable that all these features have equal and reasonable contribution.

We've already seen an example of normalization in the last session. We divided some values by a number, to make all features comparable. Normalization especially needs to be done for values of very large magnitude, because they often lead to *gradient explosion* (the gradient tends to become infinite (Nan)).

A standard method to normalize values of a feature is to normalize them to a mean of 0 and standard deviation of 1. The way this is carried out is:

$x \sim N(0,1) = \frac{x-mean(x)}{std(x)}$

In general you can adjust the mean and standard deviation of the resultant feature.

$x \sim N(a,b) = b (\frac{x-mean(x)}{std(x)} + a)$

This not only helps training better, but leads to a stable model as well.
# Bayesian Learning
Welcome to the 4th session on Machine Learning practicals. In this session, we'll learn about a classic Machine Learning method - the Bayesian Learning method. The Bayesian approach towards problem solving is completely based on probabilities. This aligns with the idea behind Machine Learning, since in the latter too, every prediction is visualised as a probability. 

For example, in logistic regression, the final output (the output from the sigmoid function) lies between 0 and 1. We visualize the output as the *probability that the prediction is 1*, and `1-output` as the *probability that the prediction is 0*. And we create a threshold, meaning any prediction less than 0.5 is not acceptable as a prediction of 1. (Or in other words, you should be atleast 50% sure that the prediction is a 1). Let us take an example - suppose a logistic regression model returns 0.8 as the output for a particular input, the model is 80% sure that the prediction is equal to a 1, and 20% sure that the prediction is equal to a 1. We do qualify this prediction as a 1. However, the aim of training a model is to adjust the parameters in such a way that this probability is maximized for all inputs.
Welcome to the 4th session on Machine Learning practicals. In this session, we'll learn about a classic Machine Learning method - the Bayesian Learning method. The Bayesian approach towards problem solving is completely based on probabilities. This aligns with the idea behind Machine Learning, since in the latter too, every prediction is visualised as a probability. 

For example, in logistic regression, the final output (the output from the sigmoid function) lies between 0 and 1. We visualize the output as the *probability that the prediction is 1*, and `1-output` as the *probability that the prediction is 0*. And we create a threshold, meaning any prediction less than 0.5 is not acceptable as a prediction of 1. (Or in other words, you should be atleast 50% sure that the prediction is a 1). Let us take an example - suppose a logistic regression model returns 0.8 as the output for a particular input, the model is 80% sure that the prediction is equal to a 1, and 20% sure that the prediction is equal to a 1. We do qualify this prediction as a 1. However, the aim of training a model is to adjust the parameters in such a way that this probability is maximized for all inputs.
For example, in logistic regression, the final output (the output from the sigmoid function) lies between 0 and 1. We visualize the output as the *probability that the prediction is 1*, and `1-output` as the *probability that the prediction is 0*. And we create a threshold, meaning any prediction less than 0.5 is not acceptable as a prediction of 1. (Or in other words, you should be atleast 50% sure that the prediction is a 1). Let us take an example - suppose a logistic regression model returns 0.8 as the output for a particular input, the model is 80% sure that the prediction is equal to a 1, and 20% sure that the prediction is equal to a 1. We do qualify this prediction as a 1. However, the aim of training a model is to adjust the parameters in such a way that this probability is maximized for all inputs.
For example, in logistic regression, the final output (the output from the sigmoid function) lies between 0 and 1. We visualize the output as the *probability that the prediction is 1*, and `1-output` as the *probability that the prediction is 0*. And we create a threshold, meaning any prediction less than 0.5 is not acceptable as a prediction of 1. (Or in other words, you should be atleast 50% sure that the prediction is a 1). Let us take an example - suppose a logistic regression model returns 0.8 as the output for a particular input, the model is 80% sure that the prediction is equal to a 1, and 20% sure that the prediction is equal to a 1. We do qualify this prediction as a 1. However, the aim of training a model is to adjust the parameters in such a way that this probability is maximized for all inputs.
So Bayesian Learning is an important stepping stone towards Machine Learning. However, its a very crude form of Learning - or in other words, a very superficial form of learning - it learns the pattern of *distribution* of data directly, and nothing else. No intrinsic features are learned, all learned features are independent and remain uncorrelated, etc. But still, Many researchers still find Bayesian Learning a useful tool to use in ensemble with modern Machine Learning techniques. This is because despite its straight forward approach, it is the purest form of data representation - it gathers * all * the information that the distribution of the data provides - no more and no less. We will look into these details in the following sections. 

In this session, we will learn about 2 important Bayesian Learning approaaches - the **Naive Bayes Classifier** and **Markov Model**. So let us begin!

## The Naive Bayes Classifier
The Naive Bayes Classifier simply provides us with the information - given a dataset, what is the probability that a particular model gives correct classification on the data.

You must have come across the Bayes Equation:

$ P(E|D) = \frac{P(E) P(D|E)}{P(D)}$

The D refers to our data. We would always have some data that we wish to build a model upon. The $E$ refers to the *evidence*, or simply the prediction of the model. So in simpler terms -
$ P(E|D) = \frac{P(E) P(D|E)}{P(D)}$

The D refers to our data. We would always have some data that we wish to build a model upon. The $E$ refers to the *evidence*, or simply the prediction of the model. So in simpler terms - 


$ P(pred|data) = \frac{P(pred) P(data|pred)}{P(data)}$

For each of different choices of predictions, the Bayes Theorem holds:

$ P(pred=1|data) = \frac{P(pred=1) P(data|pred=1)}{P(data)}$

$ P(pred=0|data) = \frac{P(pred=0) P(data|pred=0)}{P(data)}$

... and so on.

But these math equations don't make a lot of sense. Let us try to get an intuitive understanding of their meanings. 

1. P(data) simply means - what is the probability that a particular data results in the correct prediction. Obviously, one datapoint may have many features. This can be represented as `P(data) = P(feature1 AND feature2 AND .... AND featureN)`, meaning, what is the probability of getting a correct prediction when value of feature1 is such, the value of feature2 is such, and so on. In Probability, when you want to find the probability of *a combination* of different occurences, you can simply multiply the individual probabilities. 

 $P(data = (f1,f2,f3,....fN)) = P(f1)P(f2)...P(fN)$
But these math equations don't make a lot of sense. Let us try to get an intuitive understanding of their meanings. 

1. P(data) simply means - what is the probability that a particular data results in the correct prediction. Obviously, one datapoint may have many features. This can be represented as `P(data) = P(feature1 AND feature2 AND .... AND featureN)`, meaning, what is the probability of getting a correct prediction when value of feature1 is such, the value of feature2 is such, and so on. In Probability, when you want to find the probability of *a combination* of different occurences, you can simply multiply the individual probabilities. 

 $P(data = (f1,f2,f3,....fN)) = P(f1)P(f2)...P(fN)$

 P(f1) simply means - what is the probability of f1 attaining a particular value. This can be found out from the dataset itself, using a *frequentist approach*, meaning, out of all the data, how many of feature f1's have *one* particular value.


2. So, $ P(data|pred)$ means that if whenever the prediction has a value, what would be the probability that if that particular data is passed in the model, this output is achieved. We will look at how to calculate this in the following subsection. T

3. $P(pred)$ is simply the probability of occurence of a particular prediction. Basically, the fraction of data which has a particular *pred* value as the output.
2. So, $ P(data|pred)$ means that if whenever the prediction has a value, what would be the probability that if that particular data is passed in the model, this output is achieved. We will look at how to calculate this in the following subsection. T

3. $P(pred)$ is simply the probability of occurence of a particular prediction. Basically, the fraction of data which has a particular *pred* value as the output.

4. $P(pred|data)$ means, given a data point, having a particular set of features, what is the probability of obtaining a prediction, which is what we want to find out. Infact, this is what all Machine Learning wishes to find out. Even in logistic regression, we build a model that can predict an output with high probability, given some inputs. 

The proof of the Bayes theorem is actually quite simple, and can be found on the internet, or in the lecture classes as well!

But let us see how to build a Naive Bayes Classifier. So first of all we would need some data. Let us try to pick an interesting classification problem.
4. $P(pred|data)$ means, given a data point, having a particular set of features, what is the probability of obtaining a prediction, which is what we want to find out. Infact, this is what all Machine Learning wishes to find out. Even in logistic regression, we build a model that can predict an output with high probability, given some inputs. 

The proof of the Bayes theorem is actually quite simple, and can be found on the internet, or in the lecture classes as well!

But let us see how to build a Naive Bayes Classifier. So first of all we would need some data. Let us try to pick an interesting classification problem.
The proof of the Bayes theorem is actually quite simple, and can be found on the internet, or in the lecture classes as well!

But let us see how to build a Naive Bayes Classifier. So first of all we would need some data. Let us try to pick an interesting classification problem.

Let us pick the [Airplane Passenger Satisfaction](https://www.kaggle.com/teejmahal20/airline-passenger-satisfaction), which contains data about how satisfied passengers have been by the service of their respective flights. They have provided information about the flights and the passengers themselves, such as the class in which they traveled, their experience of food, in-flight entertainment, etc. This is a binary classification problem, meaning the experience reported is either satisfactory, or dissatisfactory. This dataset can be found on kaggle.com, and so, like always, we would need to upload the *kaggle.json* file. You must have it downloaded on your system from the last sessions. If not, you can download a fresh  Run the cell below to upload the kaggle.json file.

```python
%cd 
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json 

!kaggle datasets download -d teejmahal20/airline-passenger-satisfaction
!unzip airline-passenger-satisfaction.zip 
!mkdir airlines && mv test.csv airlines && mv train.csv airlines
%cd airlines
!ls
```

So if you see, this dataset contains two csv files, namely the training dataset and the testing dataset. Let us import the pandas library to read these files, and visualize the dataset.

For the sake of simplicity, we've removed all continuous variables, and only retained categorical variables (i.e. the variables that only take values among finite number of categories), otherwise the concept of probability of occurence of an event doesn not make sense. Ideally, a continuous variable can take infinite values, so how do you find the chances of occurence of a particular value?!

These variables are - 'Departure Delay in Minutes','Arrival Delay in Minutes',and 'Flight Distance', 'Age'

```python
import pandas as pd
```

```python
train=pd.read_csv('train.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance','Age'],axis=1)
```python
import pandas as pd
```

```python
train=pd.read_csv('train.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance','Age'],axis=1) 
test=pd.read_csv('test.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance','Age'],axis=1)
len(train),len(test)
```

```python
train.head()
```

As you can see, each of the features (all columns except the target variable - 'satisfaction') have either one of finite number of values. For example, 'Gender' only takes one of two values - Male or Female. 

But, notice, many of these features have strings as their categories. We know, that computers cannot understand strings. So we need to convert them into numbers. We define a function that converts all values into numerical categories, ranging from 0 to num_categories-1 (num_categories categories in total). Lets call this funtion *categorify*!

```python
def categorify(df):
    df.dictionary={}
    for col in df: 
        numerical_categories = {k:i for i,k in enumerate(sorted(df[col].unique()))}
        df[col]=[numerical_categories[i] for i in df[col]]
        df.dictionary[col]=numerical_categories

categorify(train)
categorify(test)
```

So now, if you take a look at your dataframes, you'll see that all categories are now 0,1,2,...and so on. Even the target (satisfaction) column!

```python
categorify(test)
```

So now, if you take a look at your dataframes, you'll see that all categories are now 0,1,2,...and so on. Even the target (satisfaction) column!

```python
train.head()
```

For convenience, we've also added a method in the dataframe, called *dictionary*, which will help us identify which category refers to which original category!

```python
train.dictionary
```

This is a dictionary, a python object which indexes all values in the form of `{key: value}`. If you type in `dictionary[key]`, you will get the value. So its a great method to store and retreive information. Its just like a real word dictionary. You look up information through a key, which is the word itself. So you want to know the meaning of a word, you don't search for the meaning itself, but the word. And besides the word, we find the definition too!

So now that we are set up with our data, let us build the probabilities. We need to find 3 different probabilities - $P(pred)$,$ P(data)$ and $P(data|pred).$ Using these, we will find $P(pred|data)$. If you're unclear with the meanings of these terms, *Please* go back up and read it through again.
```

This is a dictionary, a python object which indexes all values in the form of `{key: value}`. If you type in `dictionary[key]`, you will get the value. So its a great method to store and retreive information. Its just like a real word dictionary. You look up information through a key, which is the word itself. So you want to know the meaning of a word, you don't search for the meaning itself, but the word. And besides the word, we find the definition too!

So now that we are set up with our data, let us build the probabilities. We need to find 3 different probabilities - $P(pred)$,$ P(data)$ and $P(data|pred).$ Using these, we will find $P(pred|data)$. If you're unclear with the meanings of these terms, *Please* go back up and read it through again.

Let us start with building $P(pred)$. Meaning, what is the probability of getting a particular prediction (0 or 1, which correspond to the satisfaction of the passenger) (Go take a look at the 'satisfaction' key in the dataframe's dictionary, which we printed just above this cell). 

This probability can be simply found by counting the fraction of the number of times in the whole dataset, that the prediction was 1 or 0.

```python
def get_ppred(df,tgt='satisfaction'):
    f"get P(pred) as a dictionary, where tgt is the target variable and does not count as a feature." #this is a comment for the user
This probability can be simply found by counting the fraction of the number of times in the whole dataset, that the prediction was 1 or 0.

```python
def get_ppred(df,tgt='satisfaction'):
    f"get P(pred) as a dictionary, where tgt is the target variable and does not count as a feature." #this is a comment for the user
    p_pred=df[tgt].value_counts().to_dict()
    for p in p_pred: p_pred[p]/=len(df) # convert the frequencies into a ratio of occurence
    return p_pred
```

```python
p_pred = get_ppred(train,'satisfaction')
p_pred
```

We see, that `p_pred` (which stands for $P(pred)$), is a dictionary, which tells us the probabilities of occurences of all possible predictions (that is, 0 or 1). 

Next, let us calculate $P(data)$. This is a tricky one. Our data is made up of multiple features, like Gender, Class, Seat Comfort, etc. As we mentioned above, we simply need to calculate the probability of occurence of all these features separately. In the end, we multiply the probabilities of all features. So for now, let us calculate the individual probabilities of all features (except ofcourse, the target class Satisfaction)

```python
def get_pdata(df,tgt='satisfaction'): return {k:get_ppred(df,tgt=k) for k in df.keys().drop(tgt)}
```

```python
p_data=get_pdata(train)
p_data
```
```python
p_data=get_pdata(train)
p_data
```

This is again a dictionary, with individual features as the keys in the dictionary. The value of each key is again a probability, which contains the probability of occurence of each value that the feature can take. For example, the 'Customer Type' feature takes 2 values (0 and 1) and so we have calculated the probability of occurence of each of these values. How? by simply calculating the fraction of times these two features occur in the whole training dataset

```python
p_data['Customer Type']
```

In fact, you can ensure that the sum of all these probabilities for any feature is always 1.

```python
sum(p_data['Customer Type'].values())
```

Let us run a small snippet of code to ensure the sum of probabilities of all features is equal to 1. In the code snippet below, we ensure that the difference between 1 and the sum of all probabilities is almost negligible (it might not be 0, because the individual probabilities may have lower precision. For example, a 0.33333333333333333 + 0.66666666666666666 is in all practical sense, equal to 1. But according to the computer, not equal to 1, but a 0.99999999999999999 .

```python
for key in p_data: 
    assert abs(sum(p_data[key].values())-1)<1e-6, f"probabilities of the key: {key} does not sum up to 1"
```
```python
for key in p_data: 
    assert abs(sum(p_data[key].values())-1)<1e-6, f"probabilities of the key: {key} does not sum up to 1"
```

Now let us define P(data|pred). This is final component of our model. This can be thought of sub-cases of P(data) itself. Basically, it means - what is the probability of the occurence of a data, given pred assumes a certain value?! So, if you look at P(data), it is the collection of the probabilities of occurence of different features for all the datapoints. 

Now if we calculate the occurences of a feature only for a particular target(pred) value, we get P(data|pred)

Obviously, if you add up the P(data|pred) for all possible prediction values, it will result in the corresponding P(data) itself. We define `get_pdata_given_pred`, which calculates this for us.

```python
def get_pdata_given_pred(df,tgt='satisfaction'): return {k:calc_pdata_given_pred(df,tgt,key=k) for k in df[tgt].unique()}

def calc_pdata_given_pred(df,tgt,key):
    ret={f:None for f in df if f!=tgt}
    l=len(df)
    df=df[df[tgt]==key]
    for f in ret:
        conditional_probs=df[f].value_counts().to_dict()
        for o in conditional_probs: conditional_probs[o]/=l
        ret[f]=conditional_probs
    return ret
```

```python
p_data_given_pred = get_pdata_given_pred(train)
p_data_given_pred
```
```python
p_data_given_pred = get_pdata_given_pred(train)
p_data_given_pred
```

`p_data_given_pred` is simply a dictionary with keys as the different possible predictions (0 and 1), each of whose values are a dictionary containing the corresponding P(data) dictionaries (dictionary containing information about the probability of occurence of each individual feature value)

Let us also verify that the for each feature, the sum of $P(data|pred)$ for each individual prediction is equal to the overall $P(data)$

```python
for feature in train.keys().drop('satisfaction'): 
    assert p_data_given_pred[1][feature].get(0,0) + p_data_given_pred[0][feature].get(0,0) == p_data[feature].get(0)
```

#### Lessons in Python: The `get` method.

We've seen how we can access a particular item of a list of a dictionary, using square brackets. Internally, this is possible by defining a `def __get__` (pronounced dunder get) method in the respective class definition (of the list, dictionary, etc.). In addition to `__get__`, dictionaries also have a `def get` method defined, which can take a key, and returns the associated value. Additionally, if a particular key is not found, then you can decide what default value is returned. By default, it is `None`. So the syntax is as follows:

`dict.get(key,default=None)`
#### Lessons in Python: The `get` method.

We've seen how we can access a particular item of a list of a dictionary, using square brackets. Internally, this is possible by defining a `def __get__` (pronounced dunder get) method in the respective class definition (of the list, dictionary, etc.). In addition to `__get__`, dictionaries also have a `def get` method defined, which can take a key, and returns the associated value. Additionally, if a particular key is not found, then you can decide what default value is returned. By default, it is `None`. So the syntax is as follows:

`dict.get(key,default=None)`

Here, we've write the code to get the value of the `0` key, and if the 0 key does not exist (which is the case when the associated probability is 0), we choose to return a default value of 0 itself.

Finally, let us now define a function that can caluculate the final predictions.

```python
def get_prediction_accuracy(df,tgt,p_data,p_pred,p_data_given_pred):
    probability_of_being_1=[]
    probability_of_being_0=[]
    features=df.keys().drop(tgt)
    for _,row in df.iterrows(): 
        probability_of_being_1.append(calculate_prob(row,features,p_data,p_pred,p_data_given_pred,tgt=1))
        probability_of_being_0.append(calculate_prob(row,features,p_data,p_pred,p_data_given_pred,tgt=0))
probability_of_being_0=[]
    features=df.keys().drop(tgt)
    for _,row in df.iterrows(): 
        probability_of_being_1.append(calculate_prob(row,features,p_data,p_pred,p_data_given_pred,tgt=1))
        probability_of_being_0.append(calculate_prob(row,features,p_data,p_pred,p_data_given_pred,tgt=0))
    
    pred=[p_one>=p_zero for p_one,p_zero in zip(probability_of_being_1,probability_of_being_0)]
    return (pred==df[tgt]).mean()

def calculate_prob(row: pd.Series,features,p_data,p_pred,p_data_given_pred,tgt=1):
    p_pred_tgt = p_pred[tgt] #p_pred for target=1
    p_data_1=1. #initializing value
    p_data_given_pred_1 =1. #initialize

    for f in features:
        p_data_1*=p_data[f][row[f]]
        p_data_given_pred_1*=p_data_given_pred[tgt][f].get(row[f],0)

    return p_pred_tgt*p_data_given_pred_1/p_data_1
```

Let us check the prediction accuracy on the training and testing dataset

```python
get_prediction_accuracy(train,'satisfaction',p_data,p_pred,p_data_given_pred)
```

```python
get_prediction_accuracy(test,'satisfaction',p_data,p_pred,p_data_given_pred)
#note that p_data,p_pred and p_data_given_pred are probabilities corresponding to the training dataset, not the testing dataset. 
#THis is because we build our model using the training dataset only. Testing dataset is not used for finding parameters.
```
```python
get_prediction_accuracy(test,'satisfaction',p_data,p_pred,p_data_given_pred)
#note that p_data,p_pred and p_data_given_pred are probabilities corresponding to the training dataset, not the testing dataset. 
#THis is because we build our model using the training dataset only. Testing dataset is not used for finding parameters.
```

So there you have it. You have built a Naive Bayes Classifier from scratch. Obviously, we don't use this classifier independently in real word applications anymore, because there exist much more sophisticated models, but it is an important concept, that somehow made way for more advanced techniques. And even today, you can find a lot of research combining modern AI techniques with traditional probabilistic techniques.


This is a good time to understand why this classifier does not perform exceptionally well. Or, to put it in better words, why modern techniques can easily outperform such a probabilistic model.
So there you have it. You have built a Naive Bayes Classifier from scratch. Obviously, we don't use this classifier independently in real word applications anymore, because there exist much more sophisticated models, but it is an important concept, that somehow made way for more advanced techniques. And even today, you can find a lot of research combining modern AI techniques with traditional probabilistic techniques.


This is a good time to understand why this classifier does not perform exceptionally well. Or, to put it in better words, why modern techniques can easily outperform such a probabilistic model.

Modern models can understand relations between different features, which can be helpful to understand which features are more meaningful and important than others. Understanding relations can also help derive much complex information from the data, the lack of which causes the Bayesian model to be very superficial, since every feature is considered independently, and no analysis is done over the feature. The frequency of occurence is the sole factor that drives the predictions.

## Markov Chains
 
<figure><center>
<img src='https://drive.google.com/uc?id=12QN6EHTg_-fFxX2fYAhdu66-elyO1GNU' width='20%'>
<figcaption>Andrei A. Markov - Russian Mathematician</figcaption>

</center></figure>
<figure><center>
<img src='https://drive.google.com/uc?id=12QN6EHTg_-fFxX2fYAhdu66-elyO1GNU' width='20%'>
<figcaption>Andrei A. Markov - Russian Mathematician</figcaption>

</center></figure>

Now let us look at another interesting Bayesian Model, called *Markov Model*. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.
<figcaption>Andrei A. Markov - Russian Mathematician</figcaption>

</center></figure>

Now let us look at another interesting Bayesian Model, called *Markov Model*. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.
</center></figure>

Now let us look at another interesting Bayesian Model, called *Markov Model*. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.
</center></figure>

Now let us look at another interesting Bayesian Model, called *Markov Model*. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.
Now let us look at another interesting Bayesian Model, called *Markov Model*. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.

Let us understand this with an example. Suppose you know that today is a Cloudy Day, what is the probability of a Sunny Day tomorrow, or a windy day, or a Raining day. How is this calculated? By analysing *data*. Suppose you gather data on the weather conditions of each day. The probability of the occurence of a particular weather condition is simply calculated by a probabilistic model - how many times, of all days in the data that the weather was like today, was the weather of the next day Sunny? or windy?. More occurences of Sunny after a day in the past, on which there was weather condition was similar to today,  higher the  probability of the occurence of a sunny weather tomorrow too. Hope the idea is clear. This is pretty straighforward, and makes practical sense too.
Now let us look at another interesting Bayesian Model, called *Markov Model*. Its a fairly simple probabilistic approach. Markov Models, also known as Markov Chains, are used to predict continuously from a set of possible predictions, and this completely depends on probabilities. To put the whole idea in one sentence, A Markov Model tries to give the next prediction given a current prediction. So given a prediction, we try to find out what is the probability of the next prediction. So based on these probabilities, the next prediction is made. Predictions with more probability are more likely to be made.

Let us understand this with an example. Suppose you know that today is a Cloudy Day, what is the probability of a Sunny Day tomorrow, or a windy day, or a Raining day. How is this calculated? By analysing *data*. Suppose you gather data on the weather conditions of each day. The probability of the occurence of a particular weather condition is simply calculated by a probabilistic model - how many times, of all days in the data that the weather was like today, was the weather of the next day Sunny? or windy?. More occurences of Sunny after a day in the past, on which there was weather condition was similar to today,  higher the  probability of the occurence of a sunny weather tomorrow too. Hope the idea is clear. This is pretty straighforward, and makes practical sense too.
Let us understand this with an example. Suppose you know that today is a Cloudy Day, what is the probability of a Sunny Day tomorrow, or a windy day, or a Raining day. How is this calculated? By analysing *data*. Suppose you gather data on the weather conditions of each day. The probability of the occurence of a particular weather condition is simply calculated by a probabilistic model - how many times, of all days in the data that the weather was like today, was the weather of the next day Sunny? or windy?. More occurences of Sunny after a day in the past, on which there was weather condition was similar to today,  higher the  probability of the occurence of a sunny weather tomorrow too. Hope the idea is clear. This is pretty straighforward, and makes practical sense too. 

<figure><center>
<img src='https://drive.google.com/uc?id=1dAjzO1-DLj-BYGYC8v1qmjjqViGI0s1h' width='40%'>
</center></figure>

This modeling technique can be used in many scenarios. Obviously it can be used to predict a sequence of occurences, like the weather conditions. But there are some more advanced and interesting applications. For example, You can create an **Auto text generator** using this. Let us see how this works:

### Case Study: Auto Text Generator
</center></figure>

This modeling technique can be used in many scenarios. Obviously it can be used to predict a sequence of occurences, like the weather conditions. But there are some more advanced and interesting applications. For example, You can create an **Auto text generator** using this. Let us see how this works:

### Case Study: Auto Text Generator

The idea is the same, suppose I start with a single word. We will build a Markiv Chain Model, that predicts the next word by analysing data. The data can be a huge corpus of text. Suppose the first word is "The". We find out the occurence of different words after "The" in the data. The word that occurs the highest number of times has a higher chance of being the next word. Lets say, that 60% of the times, the word that follows "The" is "Apple", 20% of the times is "Banana", 10% of times "Orange" and 10% of the times "Horse". So out of 100 times the word "The" appears in our predicted text, the Markov model would predict the next word to be "Apple" approximately 60 times, and so on.

Let us pick a random text as our data. We use the most boring text there could be ([for real, apparantly!](https://www.brade.zone/2008/09/13/boring/)).

```python
The idea is the same, suppose I start with a single word. We will build a Markiv Chain Model, that predicts the next word by analysing data. The data can be a huge corpus of text. Suppose the first word is "The". We find out the occurence of different words after "The" in the data. The word that occurs the highest number of times has a higher chance of being the next word. Lets say, that 60% of the times, the word that follows "The" is "Apple", 20% of the times is "Banana", 10% of times "Orange" and 10% of the times "Horse". So out of 100 times the word "The" appears in our predicted text, the Markov model would predict the next word to be "Apple" approximately 60 times, and so on.

Let us pick a random text as our data. We use the most boring text there could be ([for real, apparantly!](https://www.brade.zone/2008/09/13/boring/)).

```python
The idea is the same, suppose I start with a single word. We will build a Markiv Chain Model, that predicts the next word by analysing data. The data can be a huge corpus of text. Suppose the first word is "The". We find out the occurence of different words after "The" in the data. The word that occurs the highest number of times has a higher chance of being the next word. Lets say, that 60% of the times, the word that follows "The" is "Apple", 20% of the times is "Banana", 10% of times "Orange" and 10% of the times "Horse". So out of 100 times the word "The" appears in our predicted text, the Markov model would predict the next word to be "Apple" approximately 60 times, and so on.

Let us pick a random text as our data. We use the most boring text there could be ([for real, apparantly!](https://www.brade.zone/2008/09/13/boring/)).

```python
Let us pick a random text as our data. We use the most boring text there could be ([for real, apparantly!](https://www.brade.zone/2008/09/13/boring/)).

```python
Let us pick a random text as our data. We use the most boring text there could be ([for real, apparantly!](https://www.brade.zone/2008/09/13/boring/)).

```python
```python
input=f"""I am writing something. Yes, I plan to make it the most boring thing ever written. I go to the store. A car is parked. Many cars are parked or moving. Some are blue. Some are tan. They have windows. In the store, there are items for sale. These include such things as soap, detergent, magazines, and lettuce. You can enhance your life with these products. Soap can be used for bathing, be it in a bathtub or in a shower. Apply the soap to your body and rinse. Detergent is used to wash clothes. Place your dirty clothes into a washing machine and add some detergent as directed on the box. Select the appropriate settings on your washing machine and you should be ready to begin. Magazines are stapled reading material made with glossy paper, and they cover a wide variety of topics, ranging from news and politics to business and stock market information. Some magazines are concerned with more recreational topics, like sports card collecting or different kinds of hairstyles. Lettuce is a vegetable. It is usually green and leafy, and is the main ingredient of salads. You may have an appliance at home that can quickly shred lettuce for use in salads. Lettuce is also used as an optional item for hamburgers and deli sandwiches. Some people even eat lettuce by itself. I have not done this. So you can purchase many types of things at stores.
```python
input=f"""I am writing something. Yes, I plan to make it the most boring thing ever written. I go to the store. A car is parked. Many cars are parked or moving. Some are blue. Some are tan. They have windows. In the store, there are items for sale. These include such things as soap, detergent, magazines, and lettuce. You can enhance your life with these products. Soap can be used for bathing, be it in a bathtub or in a shower. Apply the soap to your body and rinse. Detergent is used to wash clothes. Place your dirty clothes into a washing machine and add some detergent as directed on the box. Select the appropriate settings on your washing machine and you should be ready to begin. Magazines are stapled reading material made with glossy paper, and they cover a wide variety of topics, ranging from news and politics to business and stock market information. Some magazines are concerned with more recreational topics, like sports card collecting or different kinds of hairstyles. Lettuce is a vegetable. It is usually green and leafy, and is the main ingredient of salads. You may have an appliance at home that can quickly shred lettuce for use in salads. Lettuce is also used as an optional item for hamburgers and deli sandwiches. Some people even eat lettuce by itself. I have not done this. So you can purchase many types of things at stores.
input=f"""I am writing something. Yes, I plan to make it the most boring thing ever written. I go to the store. A car is parked. Many cars are parked or moving. Some are blue. Some are tan. They have windows. In the store, there are items for sale. These include such things as soap, detergent, magazines, and lettuce. You can enhance your life with these products. Soap can be used for bathing, be it in a bathtub or in a shower. Apply the soap to your body and rinse. Detergent is used to wash clothes. Place your dirty clothes into a washing machine and add some detergent as directed on the box. Select the appropriate settings on your washing machine and you should be ready to begin. Magazines are stapled reading material made with glossy paper, and they cover a wide variety of topics, ranging from news and politics to business and stock market information. Some magazines are concerned with more recreational topics, like sports card collecting or different kinds of hairstyles. Lettuce is a vegetable. It is usually green and leafy, and is the main ingredient of salads. You may have an appliance at home that can quickly shred lettuce for use in salads. Lettuce is also used as an optional item for hamburgers and deli sandwiches. Some people even eat lettuce by itself. I have not done this. So you can purchase many types of things at stores.
If I drive around, I sometimes notice the houses and buildings all around. There are also pieces of farm land that are very large. Houses can be built from different kinds of materials. The most common types are brick, wood, and vinyl or synthetic siding. Houses have lawns that need to be tended. Lawns need to be mowed regularly. Most people use riding lawnmowers to do this. You can also use a push mower. These come in two varieties: gas-powered and manual. You dont see manual push-mowers very much anymore, but they are a good option if you do not want to pollute the air with smoke from a gas-powered lawnmower. I notice that many families designate the lawnmowing responsibility to a teenager in the household. Many of these teenagers are provided with an allowance for mowing the yard, as well as performing other chores, like taking out the trash, washing the dishes, making their bed, and keeping the house organized. Allowances are small amounts of money given by parents to their children, usually on a weekly basis. These usually range from 5 dollars to 15 dollars, sometimes even 20 dollars. Many parents feel that teenagers can learn financial responsibility with this system.
If I drive around, I sometimes notice the houses and buildings all around. There are also pieces of farm land that are very large. Houses can be built from different kinds of materials. The most common types are brick, wood, and vinyl or synthetic siding. Houses have lawns that need to be tended. Lawns need to be mowed regularly. Most people use riding lawnmowers to do this. You can also use a push mower. These come in two varieties: gas-powered and manual. You dont see manual push-mowers very much anymore, but they are a good option if you do not want to pollute the air with smoke from a gas-powered lawnmower. I notice that many families designate the lawnmowing responsibility to a teenager in the household. Many of these teenagers are provided with an allowance for mowing the yard, as well as performing other chores, like taking out the trash, washing the dishes, making their bed, and keeping the house organized. Allowances are small amounts of money given by parents to their children, usually on a weekly basis. These usually range from 5 dollars to 15 dollars, sometimes even 20 dollars. Many parents feel that teenagers can learn financial responsibility with this system.
Now I will talk about farm land. Farm land can be identified by some common features. They almost always consist of a very large patch of dirt with small green plants lined up in very long rows. You may sometimes see farm equipment riding over these rows, like tractors or combines. These machines help farmers grow more crops in less time. They are a very helpful invention. Some different types of crops are soybeans, cotton, corn, tomatoes, tobacco, and lettuce (which I mentioned earlier). Most crops are used as food, and can be defined as either fruits or vegetables. Some are commonly eaten raw, after being rinsed in water to remove any dirt. Some are often cooked, which helps give them a more pleasant taste and makes them easier to chew. A very versatile vegetable is the potato. It can be eaten raw, or it can be cooked in a variety of ways. They can be baked, and many people like to add butter to them. They can be mashed, and a lot of times brown gravy or milk gravy is poured on top of them. They can be cut into thin strips and fried. Typically a large amount of grease is required to prepare potatoes in this style, but they are easy to make and easy to eat. You can order them at several fast-food restaurants. Potatoes can also be boiled, stewed, and scalloped. There is a wide variety of options available to you when cooking potatoes.
Now I will talk about farm land. Farm land can be identified by some common features. They almost always consist of a very large patch of dirt with small green plants lined up in very long rows. You may sometimes see farm equipment riding over these rows, like tractors or combines. These machines help farmers grow more crops in less time. They are a very helpful invention. Some different types of crops are soybeans, cotton, corn, tomatoes, tobacco, and lettuce (which I mentioned earlier). Most crops are used as food, and can be defined as either fruits or vegetables. Some are commonly eaten raw, after being rinsed in water to remove any dirt. Some are often cooked, which helps give them a more pleasant taste and makes them easier to chew. A very versatile vegetable is the potato. It can be eaten raw, or it can be cooked in a variety of ways. They can be baked, and many people like to add butter to them. They can be mashed, and a lot of times brown gravy or milk gravy is poured on top of them. They can be cut into thin strips and fried. Typically a large amount of grease is required to prepare potatoes in this style, but they are easy to make and easy to eat. You can order them at several fast-food restaurants. Potatoes can also be boiled, stewed, and scalloped. There is a wide variety of options available to you when cooking potatoes.
Some other types of crops grown on farm land are used for other purposes. Cotton is used to make clothing (which I also mentioned earlier). It is a very versatile and inexpensive material for clothes. Such items as shirts, pants, socks, and underwear can be made from cotton. The process of converting cotton from a cotton plant to clothing is fairly complicated. Today, cotton is harvested more efficiently through the use of the cotton gin, invented by Eli Whitney many years ago.

Tobacco is another type of crop. It is used in making cigarettes. A lot of people smoke cigarettes, even though many medical sources have identified them as harmful to peoples health. Warnings are printed on cigarette packages reminding people of possible dangers resulting from smoking. Cigarettes are available in several brands, including Marlboro, Salem, and Virginia Slims. There is a brand called Kool, but I dont know whether they are still available at most outlets. Tobacco farming is a large industry, and currently there is debate about it. Recently the government decided on some regulations that cost tobacco companies a large amount of money.
Some other types of crops grown on farm land are used for other purposes. Cotton is used to make clothing (which I also mentioned earlier). It is a very versatile and inexpensive material for clothes. Such items as shirts, pants, socks, and underwear can be made from cotton. The process of converting cotton from a cotton plant to clothing is fairly complicated. Today, cotton is harvested more efficiently through the use of the cotton gin, invented by Eli Whitney many years ago.

Tobacco is another type of crop. It is used in making cigarettes. A lot of people smoke cigarettes, even though many medical sources have identified them as harmful to peoples health. Warnings are printed on cigarette packages reminding people of possible dangers resulting from smoking. Cigarettes are available in several brands, including Marlboro, Salem, and Virginia Slims. There is a brand called Kool, but I dont know whether they are still available at most outlets. Tobacco farming is a large industry, and currently there is debate about it. Recently the government decided on some regulations that cost tobacco companies a large amount of money.
Tobacco is another type of crop. It is used in making cigarettes. A lot of people smoke cigarettes, even though many medical sources have identified them as harmful to peoples health. Warnings are printed on cigarette packages reminding people of possible dangers resulting from smoking. Cigarettes are available in several brands, including Marlboro, Salem, and Virginia Slims. There is a brand called Kool, but I dont know whether they are still available at most outlets. Tobacco farming is a large industry, and currently there is debate about it. Recently the government decided on some regulations that cost tobacco companies a large amount of money.
Tobacco is another type of crop. It is used in making cigarettes. A lot of people smoke cigarettes, even though many medical sources have identified them as harmful to peoples health. Warnings are printed on cigarette packages reminding people of possible dangers resulting from smoking. Cigarettes are available in several brands, including Marlboro, Salem, and Virginia Slims. There is a brand called Kool, but I dont know whether they are still available at most outlets. Tobacco farming is a large industry, and currently there is debate about it. Recently the government decided on some regulations that cost tobacco companies a large amount of money.
If you notice, some farm lands have animals living on them. Most of these are cows, and there are also pigs, sheep, and goats living on farms. Some are raised for the milk they provide. This milk goes through several processes to ensure that it is not contaminated before it is made available to consumers at stores (which I mentioned earlier). Another use for farm animals is meat. Three popular types of meat are beef, pork, and chicken. Beef comes from cows. Pork comes from pigs. Chicken comes from chickens, but you probably knew that. These animals are raised to become plump and healthy, then they are killed, sometimes at slaughter houses. The meat is then removed from their bodies, cleaned, and made available at a variety of stores and restaurants. Sometimes this process can seem gross, but it is part of an advanced ecological food chain on earth. Just like birds eat worms and tigers eat deer, human beings eat cows and pigs. The main difference is that we dont eat animals raw. We cook the meat to remove blood, fat, and germs from it. We also season our meat with salt or different kinds of sauces. The end result is food that is very tasty and is healthy for us.
If you notice, some farm lands have animals living on them. Most of these are cows, and there are also pigs, sheep, and goats living on farms. Some are raised for the milk they provide. This milk goes through several processes to ensure that it is not contaminated before it is made available to consumers at stores (which I mentioned earlier). Another use for farm animals is meat. Three popular types of meat are beef, pork, and chicken. Beef comes from cows. Pork comes from pigs. Chicken comes from chickens, but you probably knew that. These animals are raised to become plump and healthy, then they are killed, sometimes at slaughter houses. The meat is then removed from their bodies, cleaned, and made available at a variety of stores and restaurants. Sometimes this process can seem gross, but it is part of an advanced ecological food chain on earth. Just like birds eat worms and tigers eat deer, human beings eat cows and pigs. The main difference is that we dont eat animals raw. We cook the meat to remove blood, fat, and germs from it. We also season our meat with salt or different kinds of sauces. The end result is food that is very tasty and is healthy for us.
Farmers do not like trespassers. If a farmer sees one, he will sometimes shoot at them with a shotgun that he owns. Trespassing is against the law. Laws are created by government to prevent people from living in fear. They are meant to provide safety for citizens. Our government in America consists of a legislative branch, an executive branch, and a judicial branch. The legislative branch makes laws based on the concerns of citizens they represent. The executive branch consists of the President. This person enforces the law, and he has certain other duties like declaring war and approving bills prepared by members of the legislative branch. The President is also considered the leader of our country. The judicial branch interprets the laws. This branch consists of the courts and the trials held in them. Here a judge and jury determine from evidence presented by lawyers whether someone is guilty of breaking a law. Initial law enforcement takes place among police officers. They are the first people to encounter situations where a law is being broken. If a criminal (law-breaker) becomes too violent or hostile, they will use guns or mace or nightsticks to administer immediate punishment. Their goal is to bring the criminal under control, so that he can receive a punishment determined by members of the judicial branch of government. Punishments mostly include time in jail, but they can also include fines and, in extreme cases, the death penalty. There is controversy surrounding the death penalty.
Children play with toys. This is common to almost all kids. Toys come in a very wide variety. Boys tend to like cars, action figures, and toy weapons. Girls tend to like dolls, toy kitchens, and make-up. Both of them like building or assembling things, be it with Legos, blocks, Play-Doh, or something similar. Toys can be found at most stores, and these days entire stores are dedicated to selling only toys. The most popular of these is Toys R Us (with a backwards R). Their mascot is Geoffrey the Giraffe. Children love to go to Toys R Us and look at the wide variety of toys available. Most children receive the greatest quanitity of toys on their birthdays, or during the holiday season in December. For the majority of children, this holiday is Christmas. For Jewish children, the holiday is Channakuh. Either way, the kid gets presents during this time, and most of these presents are toys.
Children play with toys. This is common to almost all kids. Toys come in a very wide variety. Boys tend to like cars, action figures, and toy weapons. Girls tend to like dolls, toy kitchens, and make-up. Both of them like building or assembling things, be it with Legos, blocks, Play-Doh, or something similar. Toys can be found at most stores, and these days entire stores are dedicated to selling only toys. The most popular of these is Toys R Us (with a backwards R). Their mascot is Geoffrey the Giraffe. Children love to go to Toys R Us and look at the wide variety of toys available. Most children receive the greatest quanitity of toys on their birthdays, or during the holiday season in December. For the majority of children, this holiday is Christmas. For Jewish children, the holiday is Channakuh. Either way, the kid gets presents during this time, and most of these presents are toys.
Christmas is a holiday which has gradually become centered around the character Santa Claus and his elves and reindeer. Children are told that Santas elves build their toys, and Santa delivers them personally to each house in the world by riding in an airborne sleigh hauled by nine reindeer, including Rudolph the red-nosed reindeer, who leads the way. Another popular Christmas character is Frosty the Snowman. Frosty is basically any snowman that comes to life. So during Christmas, many children build snowmen, and some of them hope that theirs might come to life. But all of these characters are myths. The true origin of Christmas is a celebration of the birth of Jesus, who founded the religion of Christianity a couple of thousand years ago. Many popular Christmas carols deal with his story, such as Joy to the World and Silent Night.
Christmas is a holiday which has gradually become centered around the character Santa Claus and his elves and reindeer. Children are told that Santas elves build their toys, and Santa delivers them personally to each house in the world by riding in an airborne sleigh hauled by nine reindeer, including Rudolph the red-nosed reindeer, who leads the way. Another popular Christmas character is Frosty the Snowman. Frosty is basically any snowman that comes to life. So during Christmas, many children build snowmen, and some of them hope that theirs might come to life. But all of these characters are myths. The true origin of Christmas is a celebration of the birth of Jesus, who founded the religion of Christianity a couple of thousand years ago. Many popular Christmas carols deal with his story, such as Joy to the World and Silent Night.
Other holidays include Thanksgiving, Halloween, and Independence Day. Thanksgiving has become a tradition of preparing large quantities of food for a large gathering of people, mainly family and friends. This meal usually features turkey or ham as the main course. Turkey and ham are both kinds of meat (which I mentioned earlier). The meal usually also consists of dressing and a wide assortment of vegetables (which I also mentioned earlier). The origin of Thanksgiving is usually traced to the days of the pilgrims, who were the first settlers in America. They made peace with the native people, the Indians, and together enjoyed a large feast, thanking God for providing them with such an abundance. (Their concepts of God were probably very different.)
Other holidays include Thanksgiving, Halloween, and Independence Day. Thanksgiving has become a tradition of preparing large quantities of food for a large gathering of people, mainly family and friends. This meal usually features turkey or ham as the main course. Turkey and ham are both kinds of meat (which I mentioned earlier). The meal usually also consists of dressing and a wide assortment of vegetables (which I also mentioned earlier). The origin of Thanksgiving is usually traced to the days of the pilgrims, who were the first settlers in America. They made peace with the native people, the Indians, and together enjoyed a large feast, thanking God for providing them with such an abundance. (Their concepts of God were probably very different.)
Halloween is the holiday when people dress in costumes to look like other characters. Most of these are children, who go from door to door in different neighborhoods to request candy from the people living there. They usually say trick or treat then receive a treat. Very rarely does the person in the house respond with a trick. Halloween has some sort of demonic origin that I am not quite sure about, but the name derives from All Hallows Eve. I will not say much about Independence Day, but it is the day Americans celebrate the anniversary of our independence from Britain. Most families purchase fireworks during this holiday and set them off in their lawns (which I mentioned earlier).
Halloween is the holiday when people dress in costumes to look like other characters. Most of these are children, who go from door to door in different neighborhoods to request candy from the people living there. They usually say trick or treat then receive a treat. Very rarely does the person in the house respond with a trick. Halloween has some sort of demonic origin that I am not quite sure about, but the name derives from All Hallows Eve. I will not say much about Independence Day, but it is the day Americans celebrate the anniversary of our independence from Britain. Most families purchase fireworks during this holiday and set them off in their lawns (which I mentioned earlier).
America gained independence from Britain in the late 1700s after the Revolutionary War. Britain was hoping to extend its empire across the Atlantic Ocean, but the colonists who settled the territory did not want to be under Britains control, with their various taxes and regulations. Both sides were very passionate about their position on the issue, so a war occurred. This war featured a few heroes, including George Washington and Paul Revere. George Washington became Americas first president when we gained independence. I am not sure what happened to Paul Revere. The Declaration of Independence was written before the war by Thomas Jefferson in 1776 and made clear the position of the colonists. It was signed by many important people, including Ben Franklin and John Hancock. Ben Franklin is well-known for many things. One of these is inventing electrical conductors in the form of lightning rods. A famous tale is that he flew a kite with a small piece of metal somewhere on the string during a lightning storm. This was an effective way to test his theory. Another thing Ben Franklin is known for is publishing Poor Richards Almanac. This was like a magazine and contained some of his famous writings and quotations. One famous quote was Tell me, I forget. Teach me, I remember. Involve me, I learn. Maybe this had something to do with why he flew that kite.
America gained independence from Britain in the late 1700s after the Revolutionary War. Britain was hoping to extend its empire across the Atlantic Ocean, but the colonists who settled the territory did not want to be under Britains control, with their various taxes and regulations. Both sides were very passionate about their position on the issue, so a war occurred. This war featured a few heroes, including George Washington and Paul Revere. George Washington became Americas first president when we gained independence. I am not sure what happened to Paul Revere. The Declaration of Independence was written before the war by Thomas Jefferson in 1776 and made clear the position of the colonists. It was signed by many important people, including Ben Franklin and John Hancock. Ben Franklin is well-known for many things. One of these is inventing electrical conductors in the form of lightning rods. A famous tale is that he flew a kite with a small piece of metal somewhere on the string during a lightning storm. This was an effective way to test his theory. Another thing Ben Franklin is known for is publishing Poor Richards Almanac. This was like a magazine and contained some of his famous writings and quotations. One famous quote was Tell me, I forget. Teach me, I remember. Involve me, I learn. Maybe this had something to do with why he flew that kite.
Trees are one of our most important natural resources. They are made of wood, and wood can be made into a variety of products. Some of the more obvious kinds are furniture, houses, and toothpicks. However, wood can also be made into paper. When I first heard this, I was skeptical, but it is true. Paper is a very important product in our society. Writers and artists have greatly benefited from the invention of paper. With only some paper and a pen or pencil, a writer can produce stories and poems that can captivate readers. They can also write down historical facts about their society. Actually, these writings dont become historical until years later. At the time, the writings could probably be considered news. Artists use paper for their drawings and paintings. They can also use canvas. Drawings and paintings can be very beautiful. They can depict a wide variety of subjects, including flowers, animals, landscapes, and people. They can be realistic or impressionistic. Some paintings also attempt to convey emotions merely by the way the colors are combined and the brushstrokes are applied. This is a modern or contemporary approach to art. Many people think this approach does not require as much talent as the realistic styles.
Trees are one of our most important natural resources. They are made of wood, and wood can be made into a variety of products. Some of the more obvious kinds are furniture, houses, and toothpicks. However, wood can also be made into paper. When I first heard this, I was skeptical, but it is true. Paper is a very important product in our society. Writers and artists have greatly benefited from the invention of paper. With only some paper and a pen or pencil, a writer can produce stories and poems that can captivate readers. They can also write down historical facts about their society. Actually, these writings dont become historical until years later. At the time, the writings could probably be considered news. Artists use paper for their drawings and paintings. They can also use canvas. Drawings and paintings can be very beautiful. They can depict a wide variety of subjects, including flowers, animals, landscapes, and people. They can be realistic or impressionistic. Some paintings also attempt to convey emotions merely by the way the colors are combined and the brushstrokes are applied. This is a modern or contemporary approach to art. Many people think this approach does not require as much talent as the realistic styles.
I will end my writing here. I have tried to make it very boring, and I hope I have succeeded. There are plenty of boring documents available for you to read. Check your public library for more information. You can also find boring materials at a bookstore or on websites. Sometimes this information can be found in magazines (which I mentioned earlier)."""
```

For the sake of simplicity, we remove all special characters (except a blank space) and even convert all text into lowercase text. To remove all special characters, we use a special type of language in python, called *Regex*, which stands for *Regular Expressions*. Its like telling python to do tasks which are otherwise hard to execute by code. For example, "take all alphabetical letters, except the letter 'a','b' and 'z', but only uptil you encounter an underscore, after which, you only omit the letter 'z' ", and so on. You get the idea!
Its extremely useful to know regex in Python, and there are great tutorials online. But many times, you can find prewritten regex for your particular task on forums, and so not knowing regex is not necessarily a barrier!

So regex can be used after importing the `re` library.

```python
import re
```
So regex can be used after importing the `re` library.

```python
import re
```

We define a function that can remove all special characters from a text, and convert everything to lowercase. It then returns a list containing the words in the text in the exact same sequence as they occur in the text.

```python
def simplify(text: str): return re.sub(r"\W+|_", " ", text).lower().split() #returns a list of words
```

```python
text =simplify(input)
text[:10],len(text)
```

So now, in order to create a model, we will create a data structure, which contains the next occurences of all words, because that's what we're concerned with. So we use a python dictionary, with keys as words, and their corresponding values as a list of all the successive occurences in all of the data (the text above).

The idea behind this is as follows. We will start with a word. Then we look up the successive occurences to the word from this dictionary, and choose a random word. Now this word is our present prediction. Now we will predict the next word through the successive occurences of this present prediction, and this cycle goes on. You can visualize this using the representation below. Each prediction acts as an input for the next prediction!

<figure><center>
<img src='https://drive.google.com/uc?id=1SNp15ppS1BVfoFSU48ox90xtHlG2EzvJ' width='30%'>
</center></figure>
<figure><center>
<img src='https://drive.google.com/uc?id=1SNp15ppS1BVfoFSU48ox90xtHlG2EzvJ' width='30%'>
</center></figure>


(Credits: Joshua Payne for [this](https://joshua-payne.medium.com/an-introduction-to-recurrent-neural-networks-8151823daeb7) article)

Each word's corresponding occurence list can have one word appear more than once. So we will simply make a list of all the successive occurences in the text - and a word can occur in this list as many times as the combination of the present word and a particular successor occur together. This will infact help us. Since we will choose a random word as our prediction, more the occurences of a particular word, more likely it is to be chosen, right?! So that aligns with our concepts very well.

```python
def get_occurences(text: list):
    """
    This function takes in a sequence of text as a list, and creates a dictionary of all words in the text, with their values being a list of all words that
    have been seen to occur after the word, in the text.
    Sometimes, a word may not have a successor at all (like the ending word of the text). We don't want our predictions to stop. So we simply add all the words 
    from the text to the value of that word key, so that the model will randomly choose any word from the entire corpus, and start predicting all over again.

    """
    dictionary={k:[] for k in set(text)}
have been seen to occur after the word, in the text.
    Sometimes, a word may not have a successor at all (like the ending word of the text). We don't want our predictions to stop. So we simply add all the words 
    from the text to the value of that word key, so that the model will randomly choose any word from the entire corpus, and start predicting all over again.

    """
    dictionary={k:[] for k in set(text)}
    for k,i in enumerate(text[:-1]):
        dictionary.get(i).append(text[k+1])
    for key in dictionary: 
        if not dictionary.get(key): dictionary.get(key).extend(list(set(text)))
    
    return dictionary
```

```python
text_dict=get_occurences(text)
print(text_dict)
```

We can now finally build our Markov Model. We will randomly choose a word, and continuously predict the following words. To choose random items, we will use a library called `random`.

```python
import random
```

```python
word=random.choice(text) #initialize with a random word
word
```

```python
for _ in range(1000): 
    print(word,end=' ')
    word=random.choice(text_dict[word])
```

And, with that, you have successfully built your text generation system.
print(word,end=' ')
    word=random.choice(text_dict[word])
```

And, with that, you have successfully built your text generation system.

You would notice that this generated text does not make a lot of sense (Though it does make sense somewhere, atleast in short phrases!). Thats because the model does not understand contexts. It only predicts words based on the current word, and does not consider the past words at all! This is called a *memoryless model*, meaning it does not have memory of past words, and only works based on the current input. 

It also makes random predictions based on probabilities. This randomness often creates meaningless predictions. Nowadays, we use much more sophisticated models, such as Recurrent Neural Networks, that can understand very complex contexts and even hidden meanings in the text.

## Review
In this session, we learnt about two very interesting Bayesian models, or probabilistic models - the Naive Bayes Classifier and the Markov Chain Model. These both work in different ways and have great applications in their own domains. We built a classifier using no iterative learning, and only through analyzing the frequencies of occurence of a particular value.

Simple probabilistic models are very crude, because of the fact that there exists unguided randomness in these models. But yet, there does exist logical sense in these models.
## Review
In this session, we learnt about two very interesting Bayesian models, or probabilistic models - the Naive Bayes Classifier and the Markov Chain Model. These both work in different ways and have great applications in their own domains. We built a classifier using no iterative learning, and only through analyzing the frequencies of occurence of a particular value.

Simple probabilistic models are very crude, because of the fact that there exists unguided randomness in these models. But yet, there does exist logical sense in these models.

## Review Questions:
These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!

1. What is the significance of *probability* in Machine Learning?
2. Explain what $P(pred)$,$ P(data)$ and $P(data|pred)$ mean in essence?, and how are they related to our classification model?
3. How have we represented each of $P(pred)$,$ P(data)$ and $P(data|pred)$ in code? Meaning, what data structures have been used, and how are they organised?
4. Go to the section where we define `get_prediction_accuracy` function. Understand the working of this code, and in 4-5 lines of plain English, explain the working in pseudo terms.
These are non-evaluative, but highly recommended to go through. Make sure you clearly know the answer to each of these concepts. The answers to all these questions are somewhere in this notebook, so if you find yourself unclear with a concept, go back up and find the answer!

1. What is the significance of *probability* in Machine Learning?
2. Explain what $P(pred)$,$ P(data)$ and $P(data|pred)$ mean in essence?, and how are they related to our classification model?
3. How have we represented each of $P(pred)$,$ P(data)$ and $P(data|pred)$ in code? Meaning, what data structures have been used, and how are they organised?
4. Go to the section where we define `get_prediction_accuracy` function. Understand the working of this code, and in 4-5 lines of plain English, explain the working in pseudo terms. 
5. Can you give 5 different everyday examples, where the Markov Chain can be used?
6. What is Regex?
7. When would a Markov Chain result in an infinite looping situation?

# Exercise (Evaluative):

## Problem 1: Extended Naive Bayes Classifier
7. When would a Markov Chain result in an infinite looping situation?

# Exercise (Evaluative):

## Problem 1: Extended Naive Bayes Classifier

In the above problem, we simply dropped all continuous variables (Why?). But there are ways to convert continuous variables into categorical variables. One such method is *binning*, which means, we divide the data into separate categories, or bins, based on their value. For example, values from 0 to 5 lie in one bin, 5 to 10 in another and so on. So let us try to include continuous variables as well into our data, and see if we can get a better Bayesian accuracy.

So first do it for the variable Age. According to the dataset Age lies between 0 and 85. So you can create bins of 10 years (ie, 0 to 10, 10 to 20 and so on).

Hint: You can do it easily using list comprehensions:

```
train['Age'] = [some_function(i) for i in train['Age'] #Do it for the testing dataset too
```

```python
train=pd.read_csv('train.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance'],axis=1) 
test=pd.read_csv('test.csv',index_col=0).drop(['id','Departure Delay in Minutes','Arrival Delay in Minutes','Flight Distance'],axis=1)
```

```python
#continue your code from here
def categorify_continuous(df, cols:list, bins:list):#Function to categorify continuous variables of the data
  assert len(cols)==len(bins)
```

```python
#continue your code from here
def categorify_continuous(df, cols:list, bins:list):#Function to categorify continuous variables of the data
  assert len(cols)==len(bins)
  for i in range(len(cols)):
    df[cols[i]]=[int(j)//bins[i] for j in df[cols[i]]]
```

Build a Naive Bayes classifier with the new dataframes now. Is there a significant improvement/deterioration in the accuracy? (There need not necessarily be, thats okay!)

Next, build a classifier with various combinations of continuous variables, in addition to all categorical variables. Bin the continuous variables using an appropriate bin size. Keep the following things in mind - the number of bins should not be too large, or there is no difference between this and a continuous variable (both take large number of values, hence the corresponding probabilities of occurence would be very low), or too small either (otherwise there is loss of information). A moderate number of bins is sufficient. Try to push the accuracy as far as possible, and report which combination of features gives the best informatoin about customer satisfaction.
```

Build a Naive Bayes classifier with the new dataframes now. Is there a significant improvement/deterioration in the accuracy? (There need not necessarily be, thats okay!)

Next, build a classifier with various combinations of continuous variables, in addition to all categorical variables. Bin the continuous variables using an appropriate bin size. Keep the following things in mind - the number of bins should not be too large, or there is no difference between this and a continuous variable (both take large number of values, hence the corresponding probabilities of occurence would be very low), or too small either (otherwise there is loss of information). A moderate number of bins is sufficient. Try to push the accuracy as far as possible, and report which combination of features gives the best informatoin about customer satisfaction. 

As a final step, try to think of which *categorical* variables do not affect customer satisfaction (through your own personal understanding), and remove upto 3 such variables, and see how far the accuracy can be achieved. This answer is subjective, so accuracies may vary for everyone.

For your reference, the following are all the features of the Airplane Passenger Satisfaction Dataset.

```
['id', 'Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',
'Flight Distance', 'Inflight wifi service',
For your reference, the following are all the features of the Airplane Passenger Satisfaction Dataset.

```
['id', 'Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',
'Flight Distance', 'Inflight wifi service',
'Departure/Arrival time convenient', 'Ease of Online booking',
'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',
'Inflight entertainment', 'On-board service', 'Leg room service',
'Baggage handling', 'Checkin service', 'Inflight service',
'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes',
'satisfaction']
```
`id` will be always dropped (removed) from the dataframe

```python
#Checking with binning Age column
categorify_continuous(train, ['Age'], [10])
categorify_continuous(test, ['Age'], [10])

categorify(train)
categorify(test)

p_data=get_pdata(train)
p_data_given_pred = get_pdata_given_pred(train)
p_pred = get_ppred(train,'satisfaction')

train_accuracy = get_prediction_accuracy(train,'satisfaction',p_data,p_pred,p_data_given_pred)
test_accuracy = get_prediction_accuracy(test,'satisfaction',p_data,p_pred,p_data_given_pred)

print("Train accuracy: " + str(train_accuracy))
print("Test accuracy: " + str(test_accuracy))
```

```python
def get_results(droplist=[], continuous_categories=[], bins=[]):#function to get training and test accuracy based on different continuous categories
print("Train accuracy: " + str(train_accuracy))
print("Test accuracy: " + str(test_accuracy))
```

```python
def get_results(droplist=[], continuous_categories=[], bins=[]):#function to get training and test accuracy based on different continuous categories
  train=pd.read_csv('train.csv',index_col=0).drop(droplist,axis=1).fillna(0)
  test=pd.read_csv('test.csv',index_col=0).drop(droplist,axis=1).fillna(0)

  categorify_continuous(train, continuous_categories, bins)
  categorify_continuous(test, continuous_categories, bins)
  categorify(train)
  categorify(test)

  p_data=get_pdata(train)
  p_data_given_pred = get_pdata_given_pred(train)
  p_pred = get_ppred(train,'satisfaction')

  train_accuracy = get_prediction_accuracy(train,'satisfaction',p_data,p_pred,p_data_given_pred)
  test_accuracy = get_prediction_accuracy(test,'satisfaction',p_data,p_pred,p_data_given_pred)

  print("Train accuracy: " + str(train_accuracy))
  print("Test accuracy: " + str(test_accuracy))
```

```python
#getting accuracies for some continuous categories combinations
get_results(droplist=['id'], continuous_categories=['Age','Departure Delay in Minutes', 'Arrival Delay in Minutes','Flight Distance'], bins=[10, 5, 5,500])
get_results(droplist=['id', 'Age','Flight Distance'], continuous_categories=['Departure Delay in Minutes', 'Arrival Delay in Minutes'], bins=[5, 5])
```

```python
#getting accuracies for some continuous categories combinations
get_results(droplist=['id'], continuous_categories=['Age','Departure Delay in Minutes', 'Arrival Delay in Minutes','Flight Distance'], bins=[10, 5, 5,500])
get_results(droplist=['id', 'Age','Flight Distance'], continuous_categories=['Departure Delay in Minutes', 'Arrival Delay in Minutes'], bins=[5, 5])
get_results(droplist=['id','Departure Delay in Minutes', 'Arrival Delay in Minutes'], continuous_categories=['Age', 'Flight Distance'], bins=[10, 500])
get_results(droplist=['id', 'Age'], continuous_categories=['Departure Delay in Minutes', 'Arrival Delay in Minutes', 'Flight Distance'], bins=[5, 5, 500])
get_results(droplist=['id', 'Age','Departure Delay in Minutes', 'Arrival Delay in Minutes'], continuous_categories=['Flight Distance'], bins=[500])
```

```python
#Since only 'Flight Distance' gives best accuracy
#Dropping 'Gate location', 'Ease of Online booking', 'Customer Type' in categorical and considering only 'Flight Distance' in continuous category 
get_results(droplist=['id', 'Gate location' , 'Ease of Online booking', 'Customer Type', 'Age', 'Departure Delay in Minutes', 'Arrival Delay in Minutes'], continuous_categories=['Flight Distance'], bins=[500])
```

```python
#Since only 'Flight Distance' gives best accuracy
#Dropping 'Gate location', 'Ease of Online booking', 'Customer Type' in categorical and considering only 'Flight Distance' in continuous category 
get_results(droplist=['id', 'Gate location' , 'Ease of Online booking', 'Customer Type', 'Age', 'Departure Delay in Minutes', 'Arrival Delay in Minutes'], continuous_categories=['Flight Distance'], bins=[500])
```

```python
#Dropping 'Gate location', 'Ease of Online booking', 'Customer Type' in categorical and dropping none in continuous category
get_results(droplist=['id', 'Gate location' , 'Ease of Online booking', 'Customer Type'], continuous_categories=['Flight Distance', 'Age', 'Departure Delay in Minutes', 'Arrival Delay in Minutes'], bins=[500, 10, 5, 5])
```

## Problem 2: Markov? Shakespeare? What is in the name!

We learnt that the next prediction of a Markov Model only depends on the present prediction/value, i.e. the model is a *memoryless* model.

Let us build a memory model, ie a model, which also takes into account the predecessor of the current prediction, along with the current prediction itself. Meaning, what would be the next word, given the last *2* predictions?!
## Problem 2: Markov? Shakespeare? What is in the name!

We learnt that the next prediction of a Markov Model only depends on the present prediction/value, i.e. the model is a *memoryless* model.

Let us build a memory model, ie a model, which also takes into account the predecessor of the current prediction, along with the current prediction itself. Meaning, what would be the next word, given the last *2* predictions?!

But this most probably won't work on the text we wrote above (the boring text), because it contains only 2400 words. So there is a very low probability that the successor of two consecutive words will have more than one possible options. We will end up getting the exact same text as the text we wrote, or almost on the same lines. 

Let us download a bigger text file - how about a whole Shakespeare play. And we will try to build a model that can predict text in the style of Shakespeare!

Let us download this dataset and put it in a string.

```python
!kaggle datasets download -d adarshpathak/shakespeare-text
!unzip shakespeare-text
```

```python
from pathlib import Path
txt = Path('text.txt').read_text()
```

Now we need to define our functions. We've written the code to incorporate two consecutive words at once.

```python
def simplify(text: str):
    words = re.sub(r"\W+|_", " ", text).lower().split()
Now we need to define our functions. We've written the code to incorporate two consecutive words at once.

```python
def simplify(text: str):
    words = re.sub(r"\W+|_", " ", text).lower().split()
    return [' '.join((words[i],words[i+1])) for i,_ in enumerate(words[:-1])], words
```

```python
def get_occurences(text: list,original_wordlist):

    dictionary={k:[] for k in set(text)}
    # dictionary['unk']=text
    for k,i in enumerate(text[:-1]):
        dictionary.get(i).append(original_wordlist[k+2])
    for key in dictionary: 
        if not dictionary.get(key): dictionary.get(key).extend(original_wordlist)
    
    return dictionary
```

Continue the code from here, and write the loop function to generate the Markov Chain Model's predictions. 

Randomly choose a word, and predict the next 100 words.

```python
sh_text,words =simplify(txt)
sh_text_dict=get_occurences(sh_text, words)
#print(word)
```

```python
phrase=random.choice(sh_text)
word=random.choice(sh_text_dict[phrase])
for _ in range(100): 
    print(word,end=' ')
    phrase=phrase.split()[1]+" "+word
    word=random.choice(sh_text_dict[phrase])
```
# 07 Decision Trees and Random Forests

Welcome to the 2nd half of this course, and the 7th practical session in Machine Learning. This session is on Decision Trees and Random Forests. 

Till now we've learnt about classical Machine Learning techniques. Now we're going to shift towards the *modern era* of Machine Learning, that lead to the present state of Artificial Intelligence. The state of Artificial Intelligence has become quite sophisticated.

At present, methods like Deep Learning and Reinforcement Learning are state of the art, and will remain so in the near future. These methods have only emerged recently.

In early 2000s, *Random Forests* was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.
Till now we've learnt about classical Machine Learning techniques. Now we're going to shift towards the *modern era* of Machine Learning, that lead to the present state of Artificial Intelligence. The state of Artificial Intelligence has become quite sophisticated.

At present, methods like Deep Learning and Reinforcement Learning are state of the art, and will remain so in the near future. These methods have only emerged recently.

In early 2000s, *Random Forests* was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.
At present, methods like Deep Learning and Reinforcement Learning are state of the art, and will remain so in the near future. These methods have only emerged recently.

In early 2000s, *Random Forests* was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.
At present, methods like Deep Learning and Reinforcement Learning are state of the art, and will remain so in the near future. These methods have only emerged recently.

In early 2000s, *Random Forests* was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.
In early 2000s, *Random Forests* was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.
In early 2000s, *Random Forests* was introduced, and it started becoming a popular method for Regression and Classification in Machine Learning. Part of the reason for its popularity was because of its ability to handle complexity well, and the other part of its popularity was its simplicity and intuitiveness.
Infact, even after the emergence of sophisticated methods like Deep Learning, Random Forests are still very significant. Deep Learning can solve a lot of complex problems, but is not very effective in many domains. For example, Deep Learning does not perform really well wherever traditional multi-layer perceptron type networks are used (it has reached to good levels of performance, buts its not sophisticated yet). For example, tabular data prediction is solved using traditional types of Neural Networks, and they do not perform very well as of now. But, Random Forests perform very well in the case of tabular data. Random Forests are infact more popular and effective than Neural Networks presently. (Though in 2019, some research showed that good feature engineering can help Neural Networks perform better, but still, Random Forests still outperform Neural Networks). Having said that, it is an important and really interesting topic to learn. We've tried to solve many problems using tabular data, so today we'll be learning of the State of the Art (for real) modeling technique in Machine Learning to predict on tabular data.

Random Forests are an extension of *Decision Trees*. So in this session, we'll first learn about Decision Trees, and then about Random Forests.

## Decision Trees
Infact, even after the emergence of sophisticated methods like Deep Learning, Random Forests are still very significant. Deep Learning can solve a lot of complex problems, but is not very effective in many domains. For example, Deep Learning does not perform really well wherever traditional multi-layer perceptron type networks are used (it has reached to good levels of performance, buts its not sophisticated yet). For example, tabular data prediction is solved using traditional types of Neural Networks, and they do not perform very well as of now. But, Random Forests perform very well in the case of tabular data. Random Forests are infact more popular and effective than Neural Networks presently. (Though in 2019, some research showed that good feature engineering can help Neural Networks perform better, but still, Random Forests still outperform Neural Networks). Having said that, it is an important and really interesting topic to learn. We've tried to solve many problems using tabular data, so today we'll be learning of the State of the Art (for real) modeling technique in Machine Learning to predict on tabular data.

Random Forests are an extension of *Decision Trees*. So in this session, we'll first learn about Decision Trees, and then about Random Forests.

## Decision Trees
Random Forests are an extension of *Decision Trees*. So in this session, we'll first learn about Decision Trees, and then about Random Forests.

## Decision Trees
Decision Trees are a very intuitive way to process information. They are not specific to Machine Learning, and actually come from psychology. Decision Trees were initially used to create logic models. Lets understand the beauty of Decision Trees with an example. 

<figure><center>
<img src='https://drive.google.com/uc?id=1pv7em1Ewu_90NllqHGdE-ww4pcmdrkss' width='60%'>
</figure>

[Figure Credits](https://medium.com/machine-learning-for-grandma/decision-tree-e6ab1037df16)

When one applies for a loan, the bank needs takes into consideration many factors. And there can be many "chains" of queries(or questions, or  that can determine the final outcome (whether or not a loan should be given out to the applicant). For example, the above figure shows the following chain of decisions to be made while considering a loan application. 

1. Do they have a credit history? Meaning, have they taken a loan before?
[Figure Credits](https://medium.com/machine-learning-for-grandma/decision-tree-e6ab1037df16)

When one applies for a loan, the bank needs takes into consideration many factors. And there can be many "chains" of queries(or questions, or  that can determine the final outcome (whether or not a loan should be given out to the applicant). For example, the above figure shows the following chain of decisions to be made while considering a loan application. 

1. Do they have a credit history? Meaning, have they taken a loan before?

2. If yes, do they have a pledge? (Meaning an asset that is on another loan or mortgage). If no, then it doesn't matter whether they have a pledge or not. The important question now is, how much debt they have. If its less than some amount, then its a yes for the loan. If the amount is greater than a value, then its a no. 

and so on...

You can think of many examples in real life where you think through such a structured chain of thought. 

Decision Trees come with an idea of features ranked by importance. For example, in the above example, whether or not an applicant has a pledged asset is not necessarily a deciding factor, but whether or not a person has a credit history is!

So, how do Decision Trees work?
You can think of many examples in real life where you think through such a structured chain of thought. 

Decision Trees come with an idea of features ranked by importance. For example, in the above example, whether or not an applicant has a pledged asset is not necessarily a deciding factor, but whether or not a person has a credit history is!

So, how do Decision Trees work?

Decision Trees ask a series of binary questions (yes/no questions) to arrive to a conclusion. That is the entire model! Now once you have this model, you can simply traverse through the tree to find the final answer. 

A decision Tree starts with one question, that splits into a yes, and a no. Each one of those too split into further yes and no's. Each question is called a *node*.

The first question that is asked (or the first node. In this case, the question about whether or not an applicant has a credit history) is the most important question. Its called the *root* of the tree. The root *branches out* into *children nodes*. Each child further branches out, until we reach the final node in any branch. This node is called the *leaf* node.

How do we decide which feature is more important and which node is less important? And how do we decide the correct sequence of decisions that need to be taken? We'll briefly describe the method. 

Before that, let us pick an interesting problem.
The first question that is asked (or the first node. In this case, the question about whether or not an applicant has a credit history) is the most important question. Its called the *root* of the tree. The root *branches out* into *children nodes*. Each child further branches out, until we reach the final node in any branch. This node is called the *leaf* node.

How do we decide which feature is more important and which node is less important? And how do we decide the correct sequence of decisions that need to be taken? We'll briefly describe the method. 

Before that, let us pick an interesting problem.

### Case Study: Tic Tac Toe 

<figure><center>
<img src='https://drive.google.com/uc?id=11R5WhfpfjNG0fs0fxg_gAd3N4cBMi_vt' width='15%'>
</figure>

You must have played a game of tic-tac-toe. Two players mark either a X or an O on a grid of 3x3 cells. If anyone manages to mark a complete row, column or diagonal, they win the game. We can visually look at the board and understand the meaning of a row, cell or column. But how would you hardcode the logic to a tic tac toe game app that you can find on the internet?

One of the ways is to build a tree, that goes as follows.
1. Mark all the cells as 1 through 9.
2. If a person marks cell, does the adjacent cell have the same mark?
3. If yes, then is does the same mark exist in the next cell _in the same direction_?
You must have played a game of tic-tac-toe. Two players mark either a X or an O on a grid of 3x3 cells. If anyone manages to mark a complete row, column or diagonal, they win the game. We can visually look at the board and understand the meaning of a row, cell or column. But how would you hardcode the logic to a tic tac toe game app that you can find on the internet?

One of the ways is to build a tree, that goes as follows.
1. Mark all the cells as 1 through 9.
2. If a person marks cell, does the adjacent cell have the same mark?
3. If yes, then is does the same mark exist in the next cell _in the same direction_? 
4. If yes, the player wins the game.

[This](https://www.kaggle.com/aungpyaeap/tictactoe-endgame-dataset-uci) dataset contains all the possible configurations of the tic-tac-toe grid that result in one of the players winning the game. Each cell has three possible values - X, O or blank (b). Configurations that lead player 'X' to win are labeled as positive, and the ones that lead player 'O' to win, are labeled as negative. 

Our job is to build a classifier that can take in a board configuration and come to a conclusion about which player won the game. 

Let us download this dataset. Since we are downloading this dataset from *Kaggle*, we need to first upload the kaggle.json file, that you may have downloaded earlier.

```python
%cd 
from google.colab import files
Let us download this dataset. Since we are downloading this dataset from *Kaggle*, we need to first upload the kaggle.json file, that you may have downloaded earlier.

```python
%cd 
from google.colab import files
uploaded = files.upload()
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json
```

```python
!kaggle datasets download -d aungpyaeap/tictactoe-endgame-dataset-uci
!mkdir -p tictactoe && unzip tictactoe-endgame-dataset-uci.zip -d tictactoe
%cd tictactoe
!ls
```

We have a csv file containing data. So we'll use the pandas library to read and process this library.

```python
import pandas as pd
df=pd.read_csv('tic-tac-toe-endgame.csv')
df.head()
```

If the data is not intuitive to you, let us visualize the first row in a way that we are more used to seeing.

```python
import numpy as np
```

```python
config_=df.iloc[0]
config=config_[:-1]
config=[o if o!='b' else '' for o in config]
print(f"winner: player {'X' if config_[-1]=='positive' else 'O'}")
pd.DataFrame(data=np.reshape(config,(3,3)))
```

How many rows does this dataset contain in total?

```python
len(df)
```

And are there any rows which contain incomplete information in this dataset?

```python
df.isna().any().any()
```
And are there any rows which contain incomplete information in this dataset?

```python
df.isna().any().any()
```

No! There are no empty features in this dataset. We've now setup our data. The next step is to feed the data into the model. But before that, we need to *preprocess* the data. Meaning, we have to first convert each feature into numbers, because computers cannot understand anything other than numbers. So we need to categorize all variables into numbers.

We defined a function in Session 4 (On Bayesian Learning) called `categorify`, which categorizes each categorical variable, and also adds a method to the dataframe, called the `dictionary`, which keeps track of which categorical numbers refer to which original category. We'll categorify our dataframe below.

```python
def categorify(df):
    df.dictionary={}
    for col in df: 
        numerical_categories = {k:i for i,k in enumerate(sorted(df[col].unique()))}
        df[col]=[numerical_categories[i] for i in df[col]]
        df.dictionary[col]=numerical_categories
categorify(df)
```

```python
df.head()
```

You can see that all feaures in the dataframe are now numbers. Lets see which features refer to which

```python
df.dictionary
```

Finally, lets split this data into the input and output , or `x` and `y`.

```python
x=df.drop(['V10'],axis=1)
y=df['V10']
len(x),len(y)
```
```python
x=df.drop(['V10'],axis=1)
y=df['V10']
len(x),len(y)
```

Finally lets build the model that we will use to build the classifier. 

We won't build a Decision Tree from scratch, but use an already existing implementation that is provided by the scikitlearn library.

```python
from sklearn.tree import DecisionTreeClassifier
```

According to the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) here's the syntax to build a classifier.

```python
model = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=5)
```

To understand how this model works, Let's explain how Decision Trees are built!

As we mentioned before, Decision Trees are built upon the principle of ranking different features. For example, in the bank loan example, whether a person had a credit history is the most important feature.

Here are the steps to build a Decision Tree.

1. Go through each feature of the dataset one by one.
2. For each possible value in the feature column, classify all relevant datapoints using that value as the threshold.
3. Whichever threshold (among all values of all features) gives the best result becomes the criteria of classification at that node, and the corresponding feature becomes the node of the tree.
Here are the steps to build a Decision Tree.

1. Go through each feature of the dataset one by one.
2. For each possible value in the feature column, classify all relevant datapoints using that value as the threshold.
3. Whichever threshold (among all values of all features) gives the best result becomes the criteria of classification at that node, and the corresponding feature becomes the node of the tree.
4. Continue till all datapoints till you reach a satisfactory result, or reach a maximum number of iterations. 

For example, while going through all features, when the model comes across the feature "credit history", and looks for all possible values. The possible values are 0 or 1. So the decision is, whether the value of credit history is 0 or 1. The ones for which credit history is 1, are classified as "positive" (for whether a loan should be given or not) "negative for the other". So there are, say, `a` number of datapoints in the branch where credit history is 1, and `b` datapoints in 0. All of the 'a' datapoints are *predicted* to be positive by this model at this point. And all of 
'b' datapoints are predicted as negative. Note, a+b is the total number of datapoints. And the reason that "credit history" was chosen as the node among all features, because it could classify the points the best among all features. Not perfect, but still the closest.
3. Whichever threshold (among all values of all features) gives the best result becomes the criteria of classification at that node, and the corresponding feature becomes the node of the tree.
4. Continue till all datapoints till you reach a satisfactory result, or reach a maximum number of iterations. 

For example, while going through all features, when the model comes across the feature "credit history", and looks for all possible values. The possible values are 0 or 1. So the decision is, whether the value of credit history is 0 or 1. The ones for which credit history is 1, are classified as "positive" (for whether a loan should be given or not) "negative for the other". So there are, say, `a` number of datapoints in the branch where credit history is 1, and `b` datapoints in 0. All of the 'a' datapoints are *predicted* to be positive by this model at this point. And all of 
'b' datapoints are predicted as negative. Note, a+b is the total number of datapoints. And the reason that "credit history" was chosen as the node among all features, because it could classify the points the best among all features. Not perfect, but still the closest.
4. Continue till all datapoints till you reach a satisfactory result, or reach a maximum number of iterations. 

For example, while going through all features, when the model comes across the feature "credit history", and looks for all possible values. The possible values are 0 or 1. So the decision is, whether the value of credit history is 0 or 1. The ones for which credit history is 1, are classified as "positive" (for whether a loan should be given or not) "negative for the other". So there are, say, `a` number of datapoints in the branch where credit history is 1, and `b` datapoints in 0. All of the 'a' datapoints are *predicted* to be positive by this model at this point. And all of 
'b' datapoints are predicted as negative. Note, a+b is the total number of datapoints. And the reason that "credit history" was chosen as the node among all features, because it could classify the points the best among all features. Not perfect, but still the closest. 

Now, For the a datapoints, we go through all features again (excluding credit history, because we've already covered it for this particular branch.) and choose the feature that further gives the best accuracy while doing a split. And we do this until we reach a point where either all the datapoints have been correctly classified, or we've reached a limit on the *depth* or levels of the tree.
For example, while going through all features, when the model comes across the feature "credit history", and looks for all possible values. The possible values are 0 or 1. So the decision is, whether the value of credit history is 0 or 1. The ones for which credit history is 1, are classified as "positive" (for whether a loan should be given or not) "negative for the other". So there are, say, `a` number of datapoints in the branch where credit history is 1, and `b` datapoints in 0. All of the 'a' datapoints are *predicted* to be positive by this model at this point. And all of 
'b' datapoints are predicted as negative. Note, a+b is the total number of datapoints. And the reason that "credit history" was chosen as the node among all features, because it could classify the points the best among all features. Not perfect, but still the closest. 

Now, For the a datapoints, we go through all features again (excluding credit history, because we've already covered it for this particular branch.) and choose the feature that further gives the best accuracy while doing a split. And we do this until we reach a point where either all the datapoints have been correctly classified, or we've reached a limit on the *depth* or levels of the tree.

In the syntax of the line `model = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=5)`
'b' datapoints are predicted as negative. Note, a+b is the total number of datapoints. And the reason that "credit history" was chosen as the node among all features, because it could classify the points the best among all features. Not perfect, but still the closest. 

Now, For the a datapoints, we go through all features again (excluding credit history, because we've already covered it for this particular branch.) and choose the feature that further gives the best accuracy while doing a split. And we do this until we reach a point where either all the datapoints have been correctly classified, or we've reached a limit on the *depth* or levels of the tree.

In the syntax of the line `model = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=5)`

`criterion` is the metric which helps decide which feature is better than the other. Its similar to a loss function we've seen before. We've studied about Entropy Loss in Logistic Regression. Here we will use the 'entropy' criterion.

We will also put a limit to the maximum number of leaf nodes there can be, so that the model does not try to fit every single datapoint correctly. That would be similar to *overfitting*. We don't want that. So we'll stop the model to go all the way to the end.

Let us now train (aka fit) the model. According to the documentation, here's the syntax to do so.

```python
model.fit(x, y);
```
Let us now train (aka fit) the model. According to the documentation, here's the syntax to do so.

```python
model.fit(x, y);
```

And we have built a Decision Tree. We would like to visualize this tree. We will do so by using the tree module in sklearn. Matplotlib is used to display the figure in Colab.

```python
import matplotlib.pyplot as plt
from sklearn import tree
```

```python
plt.figure(figsize=(6,8))
tree.plot_tree(model,filled=True)
plt.show()
```

For each node, the branch on the left corresponds to the 'Yes' case, and the right corresponds to 'No'. At each node, entropy of the decision tree is mentioned (basically, how well the tree is able to predict on the data , the lower - the better). You can also see that as we go down a tree, the entropy reduces. That is exactly what we want. 

`Samples` tells us about the number of datapoints that are classified as positive or negative by the node above it. You can verify that at any node, the number of datapoints on its left and right child nodes (ie the datapoints classified as positive or negative) add up to the number of datapoints in that node.
```

For each node, the branch on the left corresponds to the 'Yes' case, and the right corresponds to 'No'. At each node, entropy of the decision tree is mentioned (basically, how well the tree is able to predict on the data , the lower - the better). You can also see that as we go down a tree, the entropy reduces. That is exactly what we want. 

`Samples` tells us about the number of datapoints that are classified as positive or negative by the node above it. You can verify that at any node, the number of datapoints on its left and right child nodes (ie the datapoints classified as positive or negative) add up to the number of datapoints in that node.

And finally, we would like to see how good the model performs on the dataset. We will do this by finding the accuracy of predictions. We haven't used a validation/testing dataset for this problem. We'll simply examine the trend of model performance on the training dataset itself.

```python
from sklearn.metrics import accuracy_score
y_pred=model.predict(x)
accuracy_score(y_pred,y)
```

Ideally if you put no condition on the depth of the tree, the model will keep splitting the data until every datapoint is correctly classified. But that probably won't work well on real world data, because of overfitting. But never the less, lets try it out.

```python
model = DecisionTreeClassifier(criterion = 'entropy')
model.fit(x, y);
Ideally if you put no condition on the depth of the tree, the model will keep splitting the data until every datapoint is correctly classified. But that probably won't work well on real world data, because of overfitting. But never the less, lets try it out.

```python
model = DecisionTreeClassifier(criterion = 'entropy')
model.fit(x, y);
y_pred=model.predict(x)
accuracy_score(y_pred,y)
```

```python
plt.figure(figsize=(6,8))
tree.plot_tree(model,filled=True)
plt.show()
```

This is quite a deep and complex decision tree! So, if you slowly increase the number of `max_leaf_nodes` in the model, your accuracy will increase. Initially we set it as 5, now lets set it as 20

```python
model = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=20)
model.fit(x, y);
```

```python
y_pred=model.predict(x)
accuracy_score(y_pred,y)
```

```python
plt.figure(figsize=(8,8))
tree.plot_tree(model,filled=True)
plt.show()
```

You can see that the accuracy has increased!

So that is how Decision Trees are built in python!

Now let us understand what Random Forests are...

## Random Forests
So that is how Decision Trees are built in python!

Now let us understand what Random Forests are...

## Random Forests

In 1994, Leo Breiman, a recently retired UC Berkley professor of statistics introduced *Bagging*, which is a statistics technique used to improve the performance of models (It is actually used by almost all corporates in any Machine Learning application). The idea is to not rely on a single Machine Learning model to get predictions, because there might be errors in the model predictions. 

However, if you *aggregate* the results of multiple Machine Learning model (using a democracy rule), you are likely to get stronger predictions. For example, a particular model might predict an input wrongly, but it is quite less likely for 10 models to predict wrong on the same input.

Breiman proposed to do this by building multiple models, each with a random subset of the data. Because of the randomness, each model is independent of the other. The final result of the model is the aggregate of the predictions of all models. For example, for calssification, this  aggregation may be done by a majority-voting method. 

In 2001, Breiman demonstrated this method for Decision Trees, and that came to be called *Random Forests*. 

(Forests because forests have trees, and Random because of the randomness in the bagging technique.)
Breiman proposed to do this by building multiple models, each with a random subset of the data. Because of the randomness, each model is independent of the other. The final result of the model is the aggregate of the predictions of all models. For example, for calssification, this  aggregation may be done by a majority-voting method. 

In 2001, Breiman demonstrated this method for Decision Trees, and that came to be called *Random Forests*. 

(Forests because forests have trees, and Random because of the randomness in the bagging technique.)

Sklearn provides us with a class that build a Random Forest Classifier for us. We will use this..

```python
from sklearn.ensemble import RandomForestClassifier
```

We again set a limit on the maximum number of leaf nodes in any decision tree.

`n_estimators` is the number of decision trees in a Random Forest. Its default value is 100, meaning a 100 DTs will be built, and the final result will be an aggregate of all these 100 decision trees. 

There's also an attribute called as `max_samples` in the RandomForest function, which tells the model, what fraction of the entier dataset must be chosen to build any  of the 100 decision trees.

```python
model=RandomForestClassifier(criterion='entropy',max_leaf_nodes=5,n_estimators=100,max_samples=0.5)
model.fit(x,y);
```

```python
y_pred=model.predict(x)
accuracy_score(y_pred,y)
```
```python
y_pred=model.predict(x)
accuracy_score(y_pred,y)
```

You can see that the performance of a RandomForest for the same limit of `max_leaf_nodes` is better than its Decision Tree counterpart.

Try playing around with the parameters of the model function and see the effect on the performance.

# Excercise (Evaluative)

Your task is to build both Decision Trees and Random Forests for another task. The task we will use in this question is the [Mushroom Classification](https://www.kaggle.com/uciml/mushroom-classification) Dataset, which gives information about the mushrooms appearance, habitat, etc. We need to identify which mushroom is poisenous, and which is edible (given in the `class` column of the dataframe)

```python
%cd
!kaggle datasets download -d uciml/mushroom-classification
!mkdir mushroom && unzip mushroom-classification.zip -d mushroom
%cd mushroom
!ls
```

```python
df=pd.read_csv('mushrooms.csv')
df
```

Some columns have `?`, which is equivalent to NaNs. We need to remove these blank values.

```python
(df=='?').any()
```

'stalk-root' contains such ?'s. So we'll drop the entire column for the sake of this problem

```python
df=df.drop('stalk-root',axis=1)
```

Exercise:
1. Build a Decision Tree for this dataset. Run this decision tree for `max_leaf_nodes` varying from 2 to 20. Store the accuracy of the model for each of these decision trees in a list.
```python
df=df.drop('stalk-root',axis=1)
```

Exercise:
1. Build a Decision Tree for this dataset. Run this decision tree for `max_leaf_nodes` varying from 2 to 20. Store the accuracy of the model for each of these decision trees in a list.
2. Similarly build a Random Forest Model for `max_leaf_nodes` varying from 2 to 20. Store the accuracies in another list
3. Plot the accuracies of the two models in a single graph, using matplotlib.
4. Also plot another graph showing the effect of changing the `n_estimator` parameter of the Random FOrest constructor from 1 to 100, keeping `max_leaf_nodes` as 5.

```python
categorify(df)
x=df.drop(['class'],axis=1)
y=df['class']
len(x), len(y)
```

```python
dec_tree_acc=[]
max_leaf_nodes_list = [ i for i in range(2,21)]
for max_leaf_nodes in max_leaf_nodes_list:
  model = DecisionTreeClassifier(criterion = 'entropy', max_leaf_nodes=max_leaf_nodes)
  model.fit(x, y);
  y_pred=model.predict(x)
  dec_tree_acc.append(accuracy_score(y_pred,y))
```

```python
rand_forr_acc=[]
for max_leaf_nodes in max_leaf_nodes_list:
  model = RandomForestClassifier(criterion='entropy',max_leaf_nodes=max_leaf_nodes,n_estimators=100,max_samples=0.5)
  model.fit(x, y);
  y_pred=model.predict(x)
  rand_forr_acc.append(accuracy_score(y_pred,y))
```

```python
plt.plot(max_leaf_nodes_list, dec_tree_acc)
plt.plot(max_leaf_nodes_list, rand_forr_acc)
rand_forr_acc.append(accuracy_score(y_pred,y))
```

```python
plt.plot(max_leaf_nodes_list, dec_tree_acc)
plt.plot(max_leaf_nodes_list, rand_forr_acc)
plt.legend(['Decision Tree', 'Random Forrest'])
plt.xlabel('Max Leaf Nodes')
plt.ylabel('Accuracy')
```

```python
rand_forr_acc=[]
n_estimators_list = [i for i in range(1, 101)]
for n_estimators in n_estimators_list:
  model = RandomForestClassifier(criterion='entropy',max_leaf_nodes=5,n_estimators=n_estimators,max_samples=0.5)
  model.fit(x, y);
  y_pred=model.predict(x)
  rand_forr_acc.append(accuracy_score(y_pred,y))
```

```python
plt.plot(n_estimators_list, rand_forr_acc)
plt.xlabel('N_estimators')
plt.ylabel('Accuracy')
```
# Session 08 - Neural Networks: Part 1



> Before Starting, we will change the accelerator of this notebook from a CPU to a GPU, since we'll be running our Neural Networks on the GPU. 

> Go to Runtime --> Change Runtime Type --> GPU 




Welcome to the 8th practical session on Machine Learning. Today we're going to start learning about the most important and widely applied avenue of Machine Learning - *Deep Learning*. Deep Learning is nothing but a fancy word for Machine Learning applications with *Neural Networks*. 

But why does Deep Learning get to have a whole phrase dedicated to it? That's because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. ([This](https://www.eff.org/ai/metrics) notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).
Welcome to the 8th practical session on Machine Learning. Today we're going to start learning about the most important and widely applied avenue of Machine Learning - *Deep Learning*. Deep Learning is nothing but a fancy word for Machine Learning applications with *Neural Networks*. 

But why does Deep Learning get to have a whole phrase dedicated to it? That's because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. ([This](https://www.eff.org/ai/metrics) notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).
Welcome to the 8th practical session on Machine Learning. Today we're going to start learning about the most important and widely applied avenue of Machine Learning - *Deep Learning*. Deep Learning is nothing but a fancy word for Machine Learning applications with *Neural Networks*. 

But why does Deep Learning get to have a whole phrase dedicated to it? That's because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. ([This](https://www.eff.org/ai/metrics) notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).
Welcome to the 8th practical session on Machine Learning. Today we're going to start learning about the most important and widely applied avenue of Machine Learning - *Deep Learning*. Deep Learning is nothing but a fancy word for Machine Learning applications with *Neural Networks*. 

But why does Deep Learning get to have a whole phrase dedicated to it? That's because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. ([This](https://www.eff.org/ai/metrics) notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).
But why does Deep Learning get to have a whole phrase dedicated to it? That's because Deep Learning is so flexible and efficient, that it can solve almost any problem in the modern age. Neural Networks have reached a point, where they can process almost any kind of data - videos, images, audio, text, tabular data, etc. And in a lot of applications, Neural Networks have been claimed to perform better than human performance. ([This](https://www.eff.org/ai/metrics) notebook provides a comparison of a lot of models on various benchmark problems along with human performance. You would get a general idea about the current state of Deep Learning).
 
That's crazy if you think about it, since Artificial Intelligence literally aspires to be as good as human intelligence. We consider human intelligence to be the most superior, and every form of intelligence, including animal intelligence, and Artificial Intelligence is compared to human intelligence. So, you would now understand why there is so much hype about Deep Learning. Almost all research (including corporate research too) in Machine Learning is about Deep Learning now.

## What are Neural Networks?

You may have seen a visual representation of Neural Networks before, something like the picture below.



![](https://drive.google.com/uc?id=1oXydloc3m8yNmM17Z4CvI1L8F4APRJiN)
You may have seen a visual representation of Neural Networks before, something like the picture below.



![](https://drive.google.com/uc?id=1oXydloc3m8yNmM17Z4CvI1L8F4APRJiN)

This is nothing but an augmented version of the Linear Model we learned about in the Linear/Logistic Regression sessions. 

In the Linear Regression Model, we fed an input tensor (with different features $x_1$,$x_2$,...$x_n$ into the model $f(x) = w_1.x_1 + w_2.x_2 + w_3.x_3 + ..... + b$. (If you don't remember this, head back to the Linear Regression and Logisitic Regression sessions, and come back once you're clear with that concept!)

Neural Networks are nothing but multiple Linear functions stacked on top of each other. 

So the output of linear model is fed to the next, and this is done multiple times. Finally we get an output just like we did in Linear/Logistic Regression models. 

But wait, why do we even need to stack multiple Linear models on top of each other?

So, if you remember the Linear Regression session, you would know that the "model" is nothing but some mathematical function in the space of the feature vectors ($x_1$, $x_2$ and so on). Our job is to find that mathematical function that best solves a task (like a Cat vs Dog classification problem).
So the output of linear model is fed to the next, and this is done multiple times. Finally we get an output just like we did in Linear/Logistic Regression models. 

But wait, why do we even need to stack multiple Linear models on top of each other?

So, if you remember the Linear Regression session, you would know that the "model" is nothing but some mathematical function in the space of the feature vectors ($x_1$, $x_2$ and so on). Our job is to find that mathematical function that best solves a task (like a Cat vs Dog classification problem). 

But most of the times, the model is not necessarily as simple as a single Linear Function. Neural Networks is a series of linear functions, that apparantly, can represent *any* complexity of mathematical functions in space. The more the number of *layers*, or the number of linear stacks (in other words, the *deeper* the network) (Hence the name *Deep* Learning), the more complexity the model can handle. Isn't that an interesting approach?

While Deep Learning may have gained popularity in the past decade only, Neural Networks are definitely not a new concept. They existed as back as in 1940s. There is a wonderful article by Stanford University on the history of Neural Nets. [Check it out](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html) if you want to know more!
So, if you remember the Linear Regression session, you would know that the "model" is nothing but some mathematical function in the space of the feature vectors ($x_1$, $x_2$ and so on). Our job is to find that mathematical function that best solves a task (like a Cat vs Dog classification problem). 

But most of the times, the model is not necessarily as simple as a single Linear Function. Neural Networks is a series of linear functions, that apparantly, can represent *any* complexity of mathematical functions in space. The more the number of *layers*, or the number of linear stacks (in other words, the *deeper* the network) (Hence the name *Deep* Learning), the more complexity the model can handle. Isn't that an interesting approach?

While Deep Learning may have gained popularity in the past decade only, Neural Networks are definitely not a new concept. They existed as back as in 1940s. There is a wonderful article by Stanford University on the history of Neural Nets. [Check it out](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html) if you want to know more! 

![](https://drive.google.com/uc?id=1zSbF-sQtR_-Ddf85vzjnfqCetOgAgJ2y)
But most of the times, the model is not necessarily as simple as a single Linear Function. Neural Networks is a series of linear functions, that apparantly, can represent *any* complexity of mathematical functions in space. The more the number of *layers*, or the number of linear stacks (in other words, the *deeper* the network) (Hence the name *Deep* Learning), the more complexity the model can handle. Isn't that an interesting approach?

While Deep Learning may have gained popularity in the past decade only, Neural Networks are definitely not a new concept. They existed as back as in 1940s. There is a wonderful article by Stanford University on the history of Neural Nets. [Check it out](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html) if you want to know more! 

![](https://drive.google.com/uc?id=1zSbF-sQtR_-Ddf85vzjnfqCetOgAgJ2y)

This is how a Neural Networks work. Remember? in the Linear/Logistic Regression Model we fed an input, and passed it through the linear function, and based on the output, made our predictions! Here too, we feed the input through multiple linear functions, and based on the final output, make our predictions. We will look at this in more detail below!
While Deep Learning may have gained popularity in the past decade only, Neural Networks are definitely not a new concept. They existed as back as in 1940s. There is a wonderful article by Stanford University on the history of Neural Nets. [Check it out](https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html) if you want to know more! 

![](https://drive.google.com/uc?id=1zSbF-sQtR_-Ddf85vzjnfqCetOgAgJ2y)

This is how a Neural Networks work. Remember? in the Linear/Logistic Regression Model we fed an input, and passed it through the linear function, and based on the output, made our predictions! Here too, we feed the input through multiple linear functions, and based on the final output, make our predictions. We will look at this in more detail below!
![](https://drive.google.com/uc?id=1zSbF-sQtR_-Ddf85vzjnfqCetOgAgJ2y)

This is how a Neural Networks work. Remember? in the Linear/Logistic Regression Model we fed an input, and passed it through the linear function, and based on the output, made our predictions! Here too, we feed the input through multiple linear functions, and based on the final output, make our predictions. We will look at this in more detail below!

In this session, we will learn how to build our own Neural Networks with PyTorch. We'll be learning how to use PyTorch's inbuilt functionality to vreate Neural Networks with minimal effort. Then we'll slowly breakdown each component and learn how to write them in python from scratch. That being said, this lab session has some really good lessons on python development, especially Object Oriented Programming concepts, which will help you develop better Machine Learning code for the rest of your life. Most Deep Learning practitioners are not good software developers, and so it is a valuable skill to master in order to develop efficient tools and techniques.

## Case Study: A Classifier using Neural Networks
This is how a Neural Networks work. Remember? in the Linear/Logistic Regression Model we fed an input, and passed it through the linear function, and based on the output, made our predictions! Here too, we feed the input through multiple linear functions, and based on the final output, make our predictions. We will look at this in more detail below!

In this session, we will learn how to build our own Neural Networks with PyTorch. We'll be learning how to use PyTorch's inbuilt functionality to vreate Neural Networks with minimal effort. Then we'll slowly breakdown each component and learn how to write them in python from scratch. That being said, this lab session has some really good lessons on python development, especially Object Oriented Programming concepts, which will help you develop better Machine Learning code for the rest of your life. Most Deep Learning practitioners are not good software developers, and so it is a valuable skill to master in order to develop efficient tools and techniques.

## Case Study: A Classifier using Neural Networks

Let us pick a problem and solve it using Neural Networks. For the purpose of this lab, we'll be using the [*Fruits Dataset*](https://www.kaggle.com/moltean/fruits), which contains images of 44 fruits and vegetables, which we wish to classify. For simplicity, we'll only be classifying bwtween any 2 categories for now.
In this session, we will learn how to build our own Neural Networks with PyTorch. We'll be learning how to use PyTorch's inbuilt functionality to vreate Neural Networks with minimal effort. Then we'll slowly breakdown each component and learn how to write them in python from scratch. That being said, this lab session has some really good lessons on python development, especially Object Oriented Programming concepts, which will help you develop better Machine Learning code for the rest of your life. Most Deep Learning practitioners are not good software developers, and so it is a valuable skill to master in order to develop efficient tools and techniques.

## Case Study: A Classifier using Neural Networks

Let us pick a problem and solve it using Neural Networks. For the purpose of this lab, we'll be using the [*Fruits Dataset*](https://www.kaggle.com/moltean/fruits), which contains images of 44 fruits and vegetables, which we wish to classify. For simplicity, we'll only be classifying bwtween any 2 categories for now. 

Lets us dowload this data from kaggle. First of all, upload your *kaggle.json* file.

```python
%cd 
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
%cd 
from google.colab import files
uploaded = files.upload()
for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json 

!kaggle datasets download -d moltean/fruits
!unzip fruits.zip >./tmp
%cd fruits-360
!ls
```

The following are the classes that we are dealing with.

```python
!cd Training && ls
```

This dataset contains 100x100 pixel images for any category. So, for example, let us look at two categories  - Pears and Cauliflowers. Just fr visualization purposes, we'll be using the PIL library

```python
from PIL import Image
```

```python
im=Image.open('Training/Cauliflower/236_100.jpg')
print(im.size)
im
```

```python
im=Image.open('Training/Pear/236_100.jpg')
print(im.size)
im
```

### Converting the data into PyTorch tensors

As you may remember, PyTorch processes all data in the form of *tensors*, which are nothing but N-dimensional arrays. Our first job is to find a way to convert images to tensors and vice versa (just in case we wish to visualize tensors as images).
```

### Converting the data into PyTorch tensors

As you may remember, PyTorch processes all data in the form of *tensors*, which are nothing but N-dimensional arrays. Our first job is to find a way to convert images to tensors and vice versa (just in case we wish to visualize tensors as images). 

A quick google search tells us that the *torchvision* module of the PyTorch library functions contains some classes that can do this for us. Let us create objects of these classes, namely `img2tensor` and `tensor2img`. The names are pretty self explanatory!



> Lessons in Python:

> *Class*

> A class is a collection of different things, all under one roof. These things can be anything - data or functions  (these are the two broad categories that everything in programming can be put into, if you think about it). The data can be anything like - numbers, strings, *objects* of other classes *we'l look at *objects* next!), and functions are the peices of code that operate on this data, to provide us users with some functionality. (These functions within a class are also called as methods). 

> So, if you remember building models with SKLearn, we always used classes to build models. We first named our class something, and first gave it some *data* (usually our x's and y's) and then used the `.fit()` method (or function) to train the model!
> *Class*

> A class is a collection of different things, all under one roof. These things can be anything - data or functions  (these are the two broad categories that everything in programming can be put into, if you think about it). The data can be anything like - numbers, strings, *objects* of other classes *we'l look at *objects* next!), and functions are the peices of code that operate on this data, to provide us users with some functionality. (These functions within a class are also called as methods). 

> So, if you remember building models with SKLearn, we always used classes to build models. We first named our class something, and first gave it some *data* (usually our x's and y's) and then used the `.fit()` method (or function) to train the model!

> Objects are nothing but the real world implementations of classes. Classes are only templates about how data and functions (attributes and methods, to be more precise) can exist together and function together to provide usefulness to the user! An object is like the physical version of a class, that Actually does tasks and stores data physically.
> A class is a collection of different things, all under one roof. These things can be anything - data or functions  (these are the two broad categories that everything in programming can be put into, if you think about it). The data can be anything like - numbers, strings, *objects* of other classes *we'l look at *objects* next!), and functions are the peices of code that operate on this data, to provide us users with some functionality. (These functions within a class are also called as methods). 

> So, if you remember building models with SKLearn, we always used classes to build models. We first named our class something, and first gave it some *data* (usually our x's and y's) and then used the `.fit()` method (or function) to train the model!

> Objects are nothing but the real world implementations of classes. Classes are only templates about how data and functions (attributes and methods, to be more precise) can exist together and function together to provide usefulness to the user! An object is like the physical version of a class, that Actually does tasks and stores data physically.
> So, if you remember building models with SKLearn, we always used classes to build models. We first named our class something, and first gave it some *data* (usually our x's and y's) and then used the `.fit()` method (or function) to train the model!

> Objects are nothing but the real world implementations of classes. Classes are only templates about how data and functions (attributes and methods, to be more precise) can exist together and function together to provide usefulness to the user! An object is like the physical version of a class, that Actually does tasks and stores data physically.

> If you're not aware of the concept of classes and objects at all, it would be useful to spend some time learning about them. [Here](https://www.youtube.com/watch?v=n-DVyV2RjiY) is a video to help you understand the intuition behind them. The video teaches this concept in JAVA, which is a really old (classic) but popular language to learn object oriented programming (which is nothing but the style of programming, which contains lots and lots of classes and objects). 

> One we dive a little deeper into classes in python, we'll provide with more resources to learn about the more technical details.
> If you're not aware of the concept of classes and objects at all, it would be useful to spend some time learning about them. [Here](https://www.youtube.com/watch?v=n-DVyV2RjiY) is a video to help you understand the intuition behind them. The video teaches this concept in JAVA, which is a really old (classic) but popular language to learn object oriented programming (which is nothing but the style of programming, which contains lots and lots of classes and objects). 

> One we dive a little deeper into classes in python, we'll provide with more resources to learn about the more technical details.


Python is a fully object oreiented programming language, meaning everything in python is an object of some class. Turns out, that objects are essential in data sciences, because of the flexibility they provide in bundling different data and functionality together, which is also one of the reasons why python is so famous for data sciences. 

Let us start with creating `img2tensor` and `tensor2img`, which are objects of the classes `ToTensor` and `ToPILImage`.

```python
from torchvision.transforms import ToTensor,ToPILImage
img2tensor = ToTensor()
tensor2img=ToPILImage()
```

Let us see these classes in action!

```python
img2tensor(im) #gives us a tensor
```

```python
img2tensor(im).shape
```

`img2tensor` takes in a PIL image as input, and gives a tensor as an output.
```python
img2tensor(im).shape
```

`img2tensor` takes in a PIL image as input, and gives a tensor as an output. 

Notice how `img2tensor` is an object of a class, but is used like function. This is a special feature of python. We have the liberty to define the functionality of an object, when it is used like a function. (like `output = object_name(input)` We will learn about this in more detail below. 

But first let us move ahead. Next, we need to go to the location where all images are, and convert each image to a tensor so that we can get the final `x` and `y` for our model!. To handle paths, we use the `pathlib` library. And to navigate through directories, we use the `os` library in python.

```python
import os
from pathlib import Path
Path.ls =lambda x: os.listdir(x) # this funcitonality gives us the contents of a directory, in a LINUX style command ("ls")
```

Next, we've written a function that takes in the paths of all the directories where are images are (for example, if our model contains data on cauliflowers and pears, we will pass in the paths where cauliflower and pear images are). And the model returns the `x` tensors and the corresponding ground truth labels `y`. 

Now, since we are dealing with tensors, we need the PyTorch library (`torch`), to handle tensors. So we import that!

```python
import torch
```

```python
```python
import torch
```

```python
def get_tensors_from_folder(folder_paths:list=None,normalize_x=True,preprocess_y=True):
    f"""
    Takes in the paths of all the categories that need to be considered, and returns a collective tensor for the input tensors and the target tensors. 
    normalize_x: Default True. If True, the function will automatically Gaussian normalize the input tensor x.
    preprocess_y: Default True. If True, the function will convert the labels to a series of numbers ranging from 0 to N-1, where N is the number of classes(categories). 
                                This is important if your classes are strings, or unordered. Computers cannot understand anything other than numbers. More than that, 
                                Neural Networks, as you will see, only understand categories that go as 0,1,2,....N-1 (if there are N categories to be classified in total.)
    """
    x = None 
    y = []
    folder_paths=list(folder_paths)

    for folder_path in folder_paths:
        for img_path in folder_path.ls():
            x=torch.cat((img2tensor(Image.open(folder_path/img_path))[None],x)) if x is not None else img2tensor(Image.open(folder_path/img_path))[None]
            # y=torch.cat((torch.Tensor(folder_path.stem),y)) if y is not None else torch.Tensor(folder_path.stem)
            y.append(folder_path.stem)

    if normalize_x:
for img_path in folder_path.ls():
            x=torch.cat((img2tensor(Image.open(folder_path/img_path))[None],x)) if x is not None else img2tensor(Image.open(folder_path/img_path))[None]
            # y=torch.cat((torch.Tensor(folder_path.stem),y)) if y is not None else torch.Tensor(folder_path.stem)
            y.append(folder_path.stem)

    if normalize_x: 
        x.sub_(x.mean()).div_(x.std())
    if preprocess_y:
        proc={v:k for k,v in enumerate(sorted(set(y)))}
        y=[proc[v] for v in y]

    y=torch.Tensor(y).long()
    return x.reshape(x.shape[0],-1),y
```

So, now that we've defined the function to retrieve the relevant tensors, we will use it to derive the training `x` and `y` tensors and the testing `x` and `y` tensors.

```python
path_cauliflowers = Path('Training/Cauliflower')
path_pears=Path('Training/Pear')

x_train,y_train=get_tensors_from_folder([path_cauliflowers,path_pears])
x_train.shape, y_train.shape
```

```python
path_cauliflowers = Path('Test/Cauliflower')
path_pears = Path('Test/Pear')

x_test,y_test = get_tensors_from_folder([path_pears,path_cauliflowers])
x_test.shape,y_test.shape
```

## Datasets and Dataloaders

Next, we need to learn about 2 important concepts in datasciences - Dataset and Dataloaders.
```

## Datasets and Dataloaders

Next, we need to learn about 2 important concepts in datasciences - Dataset and Dataloaders.

Dataset: A Dataset is nothing but a class which bundles the `x` and `y`. This is important, because its very inefficient to have to specify x and y separately everytime. They always go together. So the dataset class makes our life easier, by simply bundling x and y. You can simply retrieve the individual x and y by calling the attributes `dataset.x` and `dataset.y`.

Below is a simple implementation of the Dataset class. 

Any dataset class should have three properties

* It should have `x` and `y` as its attributes. 
* We should be able to get the length of the dataset (with the `__len__` (pronounced dunder len) method)
* We should be able to get a specific value by index (with the `__getitem__` method)

```python
class Dataset(): # to wrap x and y together
    def __init__(self, x,y):self.x,self.y=x,y # __init__ is pythons way to initiate classes into objects. self.x is basically the process of setting x as an attribute of the object
    def __len__(self): return self.x.shape[0]
    def __getitem__(self,i): return self.x[i], self.y[i]
```

```python
#Let us derive the training dataset
train_ds=Dataset(x_train,y_train)
len(train_ds) # we can use len on train_ds, because of the __len__ method
```

```python
train_ds[0] # __getitem__
```

```python
```python
train_ds[0] # __getitem__
```

```python
#similarly let us derive the Validation Dataset
valid_ds=Dataset(x_test,y_test)
len(valid_ds)
```

Lessons in Python.
You may have used len and list_name[i] very frequently in the case of lists. Even list is a class in python, which ultimately implements the `__len__` and `__getitem__` methods

```python
my_list=[1,2,3,4,5]
len(my_list), my_list.__len__()
```

```python
my_list[0], my_list.__getitem__(0)
```

The next concept is dataloaders. 
A dataloader is a convenient class in python that does 2 things.
1. It loads the data into the model (that we are going to build). Sometimes, it is not a wise idea to feed all the data into the model at once, because of memory constraints, so the dataloader can also feed data to the model in batches.
2. The dataloader can also make sure that the data is loaded onto the correct device (CPU or GPU). By default, everything is on the CPU, so you need to explicitly load the data on the GPU, if you wish so. So, dataloaders can save us from this unnecessary effort!

> What is a GPU?
A dataloader is a convenient class in python that does 2 things.
1. It loads the data into the model (that we are going to build). Sometimes, it is not a wise idea to feed all the data into the model at once, because of memory constraints, so the dataloader can also feed data to the model in batches.
2. The dataloader can also make sure that the data is loaded onto the correct device (CPU or GPU). By default, everything is on the CPU, so you need to explicitly load the data on the GPU, if you wish so. So, dataloaders can save us from this unnecessary effort!

> What is a GPU?

> A GPU stands for Graphical Processing Unit (Just like CPU stands for Central Processing Unit). Its also known as a graphics card. If anyone of you is interested in gaming, you would know what a graphics card is! A GPU is like a CPU, but multitudes faster (sometimes millions of times faster). A GPU can do matrix multiplications and additions very fast, which is exactly what the graphics of any video game are - lots and lots of matrix operations. A CPU is much more capable of handling more complex operations than matrix operations, but a GPU, even though less capable of doing complex operations, is very fast at doing these basic operations. Turns out, that Deep Learning is nothing but a lot of matrix multiplications and additions, which is why we use GPUs for Deep Learning.
2. The dataloader can also make sure that the data is loaded onto the correct device (CPU or GPU). By default, everything is on the CPU, so you need to explicitly load the data on the GPU, if you wish so. So, dataloaders can save us from this unnecessary effort!

> What is a GPU?

> A GPU stands for Graphical Processing Unit (Just like CPU stands for Central Processing Unit). Its also known as a graphics card. If anyone of you is interested in gaming, you would know what a graphics card is! A GPU is like a CPU, but multitudes faster (sometimes millions of times faster). A GPU can do matrix multiplications and additions very fast, which is exactly what the graphics of any video game are - lots and lots of matrix operations. A CPU is much more capable of handling more complex operations than matrix operations, but a GPU, even though less capable of doing complex operations, is very fast at doing these basic operations. Turns out, that Deep Learning is nothing but a lot of matrix multiplications and additions, which is why we use GPUs for Deep Learning.

> There's a newer type of device, called the TPU, or the Tensor Processing Unit, which is a GPU, but meant specifically for Tensor Operations. So, a TPU can ideally handle much less complexity than a GPU, but can perform Deep Learning related operations at a much faster rate. They are still not commercially used, because
> What is a GPU?

> A GPU stands for Graphical Processing Unit (Just like CPU stands for Central Processing Unit). Its also known as a graphics card. If anyone of you is interested in gaming, you would know what a graphics card is! A GPU is like a CPU, but multitudes faster (sometimes millions of times faster). A GPU can do matrix multiplications and additions very fast, which is exactly what the graphics of any video game are - lots and lots of matrix operations. A CPU is much more capable of handling more complex operations than matrix operations, but a GPU, even though less capable of doing complex operations, is very fast at doing these basic operations. Turns out, that Deep Learning is nothing but a lot of matrix multiplications and additions, which is why we use GPUs for Deep Learning.

> There's a newer type of device, called the TPU, or the Tensor Processing Unit, which is a GPU, but meant specifically for Tensor Operations. So, a TPU can ideally handle much less complexity than a GPU, but can perform Deep Learning related operations at a much faster rate. They are still not commercially used, because 
1. They are still very costly
2. Because the community awareness is still low, there isnt much support for TPUs in deep learning libraries. It will take a few years, before TPUs become mainstream.

PyTorch implements DataLoaders for us. We will use that!

```python
1. They are still very costly
2. Because the community awareness is still low, there isnt much support for TPUs in deep learning libraries. It will take a few years, before TPUs become mainstream.

PyTorch implements DataLoaders for us. We will use that!

```python
from torch.utils.data import DataLoader
```

```python
train_dl=DataLoader(train_ds,batch_size=64,shuffle=True)
valid_dl=DataLoader(valid_ds,batch_size=128,shuffle=True) #batch_size double the batch_size of train_dl since valid_dl doesnt require gradients, so less space will be used, and thus batch_size can be increased.
```

As you can see that, the DataLoader takes in a dataset, along with `batch_size` and an option to specify whether the data should be returned in random order, or in the same order as given in the dataset.

This dataloader now works just like an iterator.

```python
len(train_dl)
```

```python
for x,y in train_dl: print(x.shape, end=' ')
```

We can simply feed in one batch of size 64 to the model at once.

Now that we are set up with our data, let us move onto building our Neural Networks

## Creating our own Neural Network

PyTorch provides us with a really easy way to define Neural Networks, by simply stacking the different components (or layers) in a function called `Sequential`. 

PyTorch provides a module called `nn` which contains all Neural Network architecture related stuff

```python
PyTorch provides us with a really easy way to define Neural Networks, by simply stacking the different components (or layers) in a function called `Sequential`. 

PyTorch provides a module called `nn` which contains all Neural Network architecture related stuff

```python
from torch import nn
```

```python
def model(in_features,hidden_features,out_features): 
    return nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid())
```

What does 

```nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features), nn.Sigmoid())``` 

do ?

First of all, what are nn.Linear, nn.ReLU, and nn.Sigmoid?

nn.Linear is nothing but a linear model class. It is the implementation of the linear model we defined in the Linear/Logistic Regression session. That's the basic building block of a Neural Network.

nn.ReLU is the implementation of the *ReLU* Activation function, or the Rectified Linear Unit. We've seen the sigmoid activation function. The ReLU activation function looks like this.  

![](https://drive.google.com/uc?id=1YzqTmE-VLvLRzAj19Hde1vlcXv1JJNBr)

And is simply defined as 

$$ReLU(z) = max(0,z)$$

What is an activation function anyways?

### Activation Functions
$$ReLU(z) = max(0,z)$$

What is an activation function anyways?

### Activation Functions

As you know, a Neural Network is nothing but a stack different linear functions. But if you analyze a simple linear stack mathematically, as follows...

$$y_1=w_1.x + b_1$$
$$y_2 = w_2.y_1 + b_2 $$

$y_2$ can be simplified as:

$$y_2 = w_2(w_1.x_1 + b_1) + b_2 $$
$$ y_2 = w_2.w_1.x_1 + (w_2.b_1 + b_2)$$

which is nothing but another linear function

$$ y_2 = W.x_1 + B $$

which is exactly what $y_1$ could have represented. Then what was the point of stacking multiple linear layers? The ultimate point of stacking linear layers was because a single linear layer could not represent the complexity of functions in space. 

You can try stacking 3,4, or even more linear layers together, and write them mathematically, then simplify them, and see, that the final result will still be equivalent to one linear layer.
$$ y_2 = W.x_1 + B $$

which is exactly what $y_1$ could have represented. Then what was the point of stacking multiple linear layers? The ultimate point of stacking linear layers was because a single linear layer could not represent the complexity of functions in space. 

You can try stacking 3,4, or even more linear layers together, and write them mathematically, then simplify them, and see, that the final result will still be equivalent to one linear layer. 

So, how do we overcome this problem? This is where Activation Functions come into use. Activation Functions are non-linearities put in between of two linear layers. It turns out that that because of this non-linearity, the resulting model can achieve any level of complexity, with as little as 2 layers. Infact, a research proposed that with just 2 layers of Neural Networks (excluding the output layer), with a non-linearity activation function, can achieve arbitrarily any level of model complexity. Obviously, this is only in theory, and in practice you need more than 2 layers to achieve a level of complexity, which is why many Neural Nets have 100s, maybe even 1000s of layers. 

For now, we'll be focussing on only 2 layers (the input layer, and the hidden layer). (Hidden Layer is basically any linear layer in between the output and the input layer, both excluded. Refer to the figure in the beginning of this notebook).
You can try stacking 3,4, or even more linear layers together, and write them mathematically, then simplify them, and see, that the final result will still be equivalent to one linear layer. 

So, how do we overcome this problem? This is where Activation Functions come into use. Activation Functions are non-linearities put in between of two linear layers. It turns out that that because of this non-linearity, the resulting model can achieve any level of complexity, with as little as 2 layers. Infact, a research proposed that with just 2 layers of Neural Networks (excluding the output layer), with a non-linearity activation function, can achieve arbitrarily any level of model complexity. Obviously, this is only in theory, and in practice you need more than 2 layers to achieve a level of complexity, which is why many Neural Nets have 100s, maybe even 1000s of layers. 

For now, we'll be focussing on only 2 layers (the input layer, and the hidden layer). (Hidden Layer is basically any linear layer in between the output and the input layer, both excluded. Refer to the figure in the beginning of this notebook).

There are many activation functions. Even Sigmoid is an activation function. Infact, in the earlier days, sigmoid was used in place of ReLU. But now, we only use ReLU as the activation function. 

 ---
For now, we'll be focussing on only 2 layers (the input layer, and the hidden layer). (Hidden Layer is basically any linear layer in between the output and the input layer, both excluded. Refer to the figure in the beginning of this notebook).

There are many activation functions. Even Sigmoid is an activation function. Infact, in the earlier days, sigmoid was used in place of ReLU. But now, we only use ReLU as the activation function. 

 ---

Finally nn.Sigmoid is the implementation of the sigmoid function. Just like in the logistic regression model, we use sigmoid to scale the output between 0 and 1. This helps us interpret the model output better, in terms of probability. 

nn.Sequential stacks all these components together.
When we write

``` nn.Sequential(nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid())```

When you feed data into this, the data fill be fed into the first layer of the stack, ie nn.Linear. The output of this Linear layer is fed into ReLU, the output of the ReLU is fed into the second Linear Layer, and finally the output of the Linear layer is fed to the Sigmoid function. 

The output of the Sigmoid function is our final output function, and we will make predictions based on that

```python
my_model = model(x_train.shape[-1],50,2)
my_model #prints out all the modules # uses the __repr__ method
```
```python
my_model = model(x_train.shape[-1],50,2)
my_model #prints out all the modules # uses the __repr__ method
```

You can see the different layers in `my_model`.

nn.Sequential has some wonderful properties. It provides methods like the `.parameters` to return the list of learnable parameters in the model. So, if you remember the gradient descent algorithm from the Linear/Logistic Regression session, you would know that we need to update parameters. So nn.Sequential makes it easy for the model to recognize which data are the parameters.

```python
for p in my_model.parameters(): print(p.shape)
```

And we can simply use my_model as a function (even though its a class object). As seen below. This comes by implementing the `__call__` method, which tells the object to do something, if the object is directly used as a function. So the `__call__` method of the nn.Sequential must be implementing the functionality to pass the input through all the layers in the model and returning the output

```python
xb,yb=next(iter(train_dl))
pred = my_model(xb)
```

```python
??my_model.__call__
# You can see, that there is some implementation of __call__, in the form of _call_impl . Details dont matter as of now
```

What do the predictions from the model look like? Let us analyze the `pred` tensor

```python
pred.shape
```
What do the predictions from the model look like? Let us analyze the `pred` tensor

```python
pred.shape
```

`pred` is basically a tensor of 2 values for every one of the 64 inputs. These 2 outputs per input is basically the prediction of the probability of each one of the 2 categories (pears and cauliflowers). As we saw earlier, whichever neuron has the highest value, is basically the prediction of the model for that input.

```python
pred
```

## Gradient Descent on Neural Networks.

Next, we will simply apply the **Gradient Descent** Algoithm to optimize our Neural Nets. If you don't remember what Gradient Descent is, head back to the Linear Regression and Logistic Regression sessions to understand it, and then come back!

The steps are the same. 
1. Get predictions from the model
2. Calculate loss function
3. Derive Gradients of parameters with respect to the loss function
4. Update the parameters by a specific step size (called as learning rate)
5. Calculate the accuracy on the validation/test dataset to find out how good your model is performing
6. Repeat this process for many iterations (or *epochs*)

We implemented this from scratch earlier. But now, we will use PyTorch's built in Gradient Descent Algorithm.

```python
learning_rate = 0.001
optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate)
```
```python
learning_rate = 0.001
optimizer = torch.optim.SGD(my_model.parameters(), lr=learning_rate)
```

And we will also use the Cross Entropy Loss function inbuilt in PyTorch. If you dont remember what the Cross Entropy loss function does, head back to the Logisitic Regression session!

```python
loss_fn=torch.nn.CrossEntropyLoss()
```

Next, we will pass the input through the model, and calculate the loss. Passing the data through the model is called a **forward pass** through the model.

```python
# FORWARD PASS
pred=my_model(xb) # we only pass one batch at a time. Because many a times, the entire dataset is too large for the model to handle at once
loss=loss_fn(pred,yb)
```

And we calculate the gradients after this. You don't have to implement the gradient calculations on your own. PyTorch does that for you!

You would remember from the Linear/Logistic Regression session that we simply wrote `loss.backward` to calculate the *gradient of all parameters with respect to the loss`. (Now you understand why nn.Sequential's .parameter method is useful. With this, the model keeps track of where parameters and present, so that gradients can be calculated and they can be updated).
```

And we calculate the gradients after this. You don't have to implement the gradient calculations on your own. PyTorch does that for you!

You would remember from the Linear/Logistic Regression session that we simply wrote `loss.backward` to calculate the *gradient of all parameters with respect to the loss`. (Now you understand why nn.Sequential's .parameter method is useful. With this, the model keeps track of where parameters and present, so that gradients can be calculated and they can be updated).

The method is called `.backward` because traditionally, the process of calculating gradients is called a **backward pass**. Its nothing but an implementation of the chain rule for derivative calculations.

```python
# BACKWARD PASS
loss.backward()
```

So, just to understand the state of the model at this point - We haven't trained the Neural Net at all right now. So let us see how the model is performing on the dataset right now, and we will compare it with the performance after we train the model.

So for that, we define an accuracy function, which basically checks how many predictions are the same as the target value (ground truth value).
```

So, just to understand the state of the model at this point - We haven't trained the Neural Net at all right now. So let us see how the model is performing on the dataset right now, and we will compare it with the performance after we train the model.

So for that, we define an accuracy function, which basically checks how many predictions are the same as the target value (ground truth value).

The neural Network has 2 neurons at the output layer, corresponding to the two classes (pears and cauliflowers). Whichever neuron has the highest value, is the prediction for the given input. `torch.argmax` basically returns the index of the neuron with gives the highest value.

```python
def accuracy(pred,targ): return (torch.argmax(pred,dim=1)==targ).float().mean() #understand this line of code. Search on google for the functionality of a term, if you dont understand
```

```python
accuracy(pred,yb)
```

Now, if we update the parameters once, using the `optimizer.step` method of the `optimizer` class, let us see how the accuracy increases.

```python
optimizer.step()
optimizer.zero_grad()
```

```python
accuracy(my_model(xb),yb)
```

You can see that the accuracy has increased!

What is `optimizer.zero_grad()`? We mentioned this in the earlier sessions too, when we were discussing gradient descent, but we didnt describe it back then. Let us understand it now.
```

You can see that the accuracy has increased!

What is `optimizer.zero_grad()`? We mentioned this in the earlier sessions too, when we were discussing gradient descent, but we didnt describe it back then. Let us understand it now.

The idea is simple. By default, PyTorch *accumulates* (or keeps adding up ) gradients if you keep calculating gradients. This is not a bad thing, infact, its quite helpful when you have very small batch sizes, and you wish to accumulate gradients of multiple batches, (more the data used for parameter update, the better generalisation occurs in the final output). At the end, you have to manually *clear* the gradients, so that fresh gradients can be calculated. `.zero_grad` does exactly that. It replaces the gradients with a zero value. So the next time `.backward` is called, gradients are added to 0, which basically means, that fresh gradients are calculated. 

This is it. You've built your own Neural Network!

## Refactoring the entire code to a convenient class

The above code is quite scattered and clunky. We will refactor the code to make the entire Neural Network more understandable and easy to work with.
This is it. You've built your own Neural Network!

## Refactoring the entire code to a convenient class

The above code is quite scattered and clunky. We will refactor the code to make the entire Neural Network more understandable and easy to work with. 

Now is a good time to understand what `__init__` does. [Here](https://www.youtube.com/watch?v=ZDa-Z5JzLYM) is a great video by Cory Schafer on YouTube that explains this.  Actually, Cory Schafer has one of the best tutorials on object oriented programming in Python. So go check it out if you're interested!

```python
class NeuralNetwork():
    def __init__(self,train_dl,valid_dl,model,loss_fn=nn.CrossEntropyLoss(),optimizer=None):
        f"valid_dl can be None. In that case, the model will be evaluated on the training set only"

        self.train_dl = train_dl
        self.valid_dl = valid_dl
        self.model = model
        self.loss_fn=loss_fn
        self.optimizer = torch.optim.SGD(self.model.parameters(),lr=0.1) if not optimizer else optimizer
        
    def train(self,epochs=1,lr=None,loss_fn=None):
        self.train_loss,self.valid_loss=[],[]
        if lr: self.opt.defaults['lr']=learning_rate
        if loss_fn: self.loss_fn=loss_fn

        for epoch in range(epochs):
            # self.model=self.model.train()
            for x,y in train_dl:
if lr: self.opt.defaults['lr']=learning_rate
        if loss_fn: self.loss_fn=loss_fn

        for epoch in range(epochs):
            # self.model=self.model.train()
            for x,y in train_dl:                     
                loss=self.loss_fn(self.model(x),y)
                loss.backward()
                self.optimizer.step()
                self.optimizer.zero_grad()
                self.train_loss.append(loss)

            if self.valid_dl: print(f'epoch {epoch+1}:validation accuracy - {self.get_validation_accuracy()}')
            else: print(print(f'epoch {epoch+1}: training accuracy - {self.get_validation_accuracy(validation=False)}'))

    def get_validation_accuracy(self,validation=True):
        dl=self.valid_dl if validation else self.train_dl
        with torch.no_grad(): acc=[accuracy(self.model(x),y) for x,y in dl]
        return torch.Tensor(acc).mean().float()
```

```python
my_model = model(x_train.shape[-1],50,2) # reinitializing the model
optimizer=torch.optim.Adam(my_model.parameters(),lr=0.0001)
my_network = NeuralNetwork(train_dl,valid_dl,my_model,loss_fn=loss_fn,optimizer=optimizer)
```

```python
my_network.get_validation_accuracy() # checking the model performance before training, for comparison
```

```python
my_network.train(1)
```

You can see that with just one epoch, the model has improved significantly.
```python
my_network.train(1)
```

You can see that with just one epoch, the model has improved significantly.

Try writing the above class from scratch, without looking at it. You'll be surprised by how much you learn about python and Neural Nets with just this one class definition. It should take you about 3-4 hours to write it. Peeking at the original function once in a while is okay, but your aim should be to understand everything, and being able to write it from scratch.

## Implementing nn.Sequential from scatch

We've implemented a model using nn.Sequential from scratch. But lets see what goes on under the hood. Let's actually look at the source code of the nn.Sequential function.

In Jupyter notebooks, you can look up the source code of any function using the `??` sign. 

So let us first go to the source code of the model function which we defined above.

```python
??model
```

As expected, it would show us the definition of the model, as defined above. We used nn.Sequential within the `model` function.

So next, let us look up the source code of the nn.Sequential class.

```python
??nn.Sequential
```

First of all, notice that nn.Sequential *inherits* from nn.Module. 

`nn.Module` is the base class for all functions in the `nn` (torch.nn). It contains basic information about how nn functions should behave. The `__call__` for all functions is implemented in nn.Module .
??nn.Sequential
```

First of all, notice that nn.Sequential *inherits* from nn.Module. 

`nn.Module` is the base class for all functions in the `nn` (torch.nn). It contains basic information about how nn functions should behave. The `__call__` for all functions is implemented in nn.Module .

```
class Sequential(Module):
```

```python
??nn.Module.__call__
```

There's a lot of technical code over there, but look for this main line of code.

```
result = self.forward(*input, **kwargs)
```

So, behind the scenes, nn functions call a function called `forward`. This is nothing but the function corresponding to the forward pass. Now you know why you can do a forward pass by simply calling the object like a function, with input as the argument to the function.

This forward function is all you need to define for any nn function. Even nn.Linear, nn.ReLU, nn.Signmoid define only a forward function, and the rest is done by the `__call__` method. 

Next, lets take a look at the `forward` function of the nn.Sequential function

```python
??nn.Sequential.forward
```

You would see this.

```
def forward(self, input):
        for module in self:
            input = module(input)
        return input
```
def forward(self, input):
        for module in self:
            input = module(input)
        return input
```

what is a module? A module is the basic unit in nn.Sequential function. In our case, nn.Linear, nn.ReLU, nn.Sigmoid, etc are the basic units. Let us run this ourself to understand this better

```python
#my_model is a nn.Sequential object
for module in my_model: print(module)
```

As you can see, module is nothing but the different building layers of our model. 
So what nn.Sequential.forward is doing, is that it iterates through each layer, and passes the output from the previous layer to the next. 

Now that we know how it works, let a model using this behaviour from scratch!

```python
class model(nn.Module):
    def __init__(self,layers):
        super().__init__()
        self.layers=layers
        for k,v in enumerate(self.layers): self.add_module(f"layer {k}",v) #nn.Sequential automatically added each layer as a module. We didnt have to do that. 
                                                                           #But now we have to do it ourselves. nn.Module has a function called add_module that can add any layer as a module of the class.
```

Now all we need to do is, define a forward function in this model class. But before that, lets see, if the model behaves properly

```python
in_features=x_train.shape[-1]
hidden_features=50
out_features=2
```python
in_features=x_train.shape[-1]
hidden_features=50
out_features=2

layers=[nn.Linear(in_features,hidden_features), nn.ReLU(), nn.Linear(hidden_features,out_features),nn.Sigmoid()]
my_model=model(layers)
```

```python
my_model
```

Yes, it does. You can see, that all the building blocks have been added as modules. Now we can create a forward function. 

Let us actually verify that this model class will use the `forward` function when called

```python
# this will show an error at this point because forward is not implemented
#my_model(x_train)    #Uncomment to run
```

You can see that the error occurs at the line `result = self.forward(*input, **kwargs)`. Meaning, all we need to do is, define a forward function, and nn.Module will take care of the rest.

```python
class model(nn.Module):
    def __init__(self,layers):
        super().__init__()
        self.layers=layers
        for k,v in enumerate(self.layers): self.add_module(f"layer {k}",v)
    
    def forward(self,x):
        for layer in self.layers: x=layer(x)
        return x
```

```python
my_model=model(layers)
pred=my_model(x_train)
pred.shape
```

Yes, the model prediction is working correctly now!

## Review
```

Yes, the model prediction is working correctly now!

## Review

That's it for this session. In the next session, we'll look at more advanced concepts in Neural Networks, and following that, we'll dive into specific applications in Deep Learning, like Computer Vision, NLP and so on. 

In this session, we learnt how to build our own Neural Networks from scratch. We dived deep into PyTorch's elegant API and even learnt a lot new stuff in python. 

You are not expected to learn all this stuff in one go. There were too many concepts involved in this session, so its okay to take some time to understand all this stuff. Go through the whole notebook again if you need to, and make sure you understand everything well. 

We built Neural Networks the practical way - no math involved. Thats because libraries that exist today make sure we don't have to involve ourselves in unnecessary bookish mathematics. Infact, we saw how PyTorch provides us with functionalities to build our own Neural Networks in just one line of code - `nn.Sequential(...)`. Yes, we went much deeper than that. The sole reason behind that was to show you how things work at the foundational level of code.
In this session, we learnt how to build our own Neural Networks from scratch. We dived deep into PyTorch's elegant API and even learnt a lot new stuff in python. 

You are not expected to learn all this stuff in one go. There were too many concepts involved in this session, so its okay to take some time to understand all this stuff. Go through the whole notebook again if you need to, and make sure you understand everything well. 

We built Neural Networks the practical way - no math involved. Thats because libraries that exist today make sure we don't have to involve ourselves in unnecessary bookish mathematics. Infact, we saw how PyTorch provides us with functionalities to build our own Neural Networks in just one line of code - `nn.Sequential(...)`. Yes, we went much deeper than that. The sole reason behind that was to show you how things work at the foundational level of code. 

This is important when you want to modify the functionality of a code. If you simply don't know how nn.Sequential works, you cannot make it work in any other way than the default way. However, once you know how things work at the grassroot level,  you can make your code work in whatever way you want!

## Exercise (Evaluative)
We built Neural Networks the practical way - no math involved. Thats because libraries that exist today make sure we don't have to involve ourselves in unnecessary bookish mathematics. Infact, we saw how PyTorch provides us with functionalities to build our own Neural Networks in just one line of code - `nn.Sequential(...)`. Yes, we went much deeper than that. The sole reason behind that was to show you how things work at the foundational level of code. 

This is important when you want to modify the functionality of a code. If you simply don't know how nn.Sequential works, you cannot make it work in any other way than the default way. However, once you know how things work at the grassroot level,  you can make your code work in whatever way you want!

## Exercise (Evaluative)

Your task is to define the nn.Linear, nn.Sigmoid and nn.ReLU from scratch. You only have to implement the `forward` function in each of these classes. Once you do that, build a nn.Sequential object using not nn.Linear, etc, but the functions you define below. Run the Pear vs Cauliflower Classifier and make sure you get the same results.

You can use any python or pytorch inbuilt function if needed, but you cannot directly copy paste the code from the repective forward definitions of nn.Linear, etc.

```python
import math
```

```python
class linear(nn.Module):
```python
import math
```

```python
class linear(nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features))
        self.reset_parameters() #used to kaiming initialize parameters
    
    def reset_parameters(self): #implementation of the kaiming initialization. IGNORE THIS
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
        bound = 1 / math.sqrt(fan_in)
        nn.init.uniform_(self.bias, -bound, bound)

    def forward(self,x):
        x = torch.add(torch.matmul(x, torch.transpose(self.weight,0,1)), self.bias)
        return x
```

```python
class relu(nn.Module):
    def __init__(self):
        super().__init__()
        
    def forward(self,x):
        x[x<0] = 0
        return x
```

```python
class sigmoid(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self,x):
        return 1.0/(1.0+torch.exp(-1.0*x))
```

```python
in_features=x_train.shape[-1]
hidden_features=50
out_features=2

layers=[linear(in_features,hidden_features), relu(), linear(hidden_features,out_features),sigmoid()]
```python
in_features=x_train.shape[-1]
hidden_features=50
out_features=2

layers=[linear(in_features,hidden_features), relu(), linear(hidden_features,out_features),sigmoid()]
my_model=nn.Sequential(*layers)
```

```python
my_model
```

```python
optimizer=torch.optim.Adam(my_model.parameters(),lr=0.0001)
```

```python
mynet=NeuralNetwork(train_dl,valid_dl,my_model,optimizer=optimizer)
```

```python
mynet.get_validation_accuracy() #checking accuracy before the training cycle
```

```python
mynet.train()
```
# Machine-Learning-Lab-BITSP
Repository of lab notebooks for the course Machine Learning (BITS F464) for the academic year 2020-21.
```python
import numpy as np
import pandas as pd
import h5py
import matplotlib.pyplot as plt
import scipy
from PIL import Image
from scipy import ndimage

%matplotlib inline
from keras import layers
from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D,Add
from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D
from keras.models import Model
from keras.preprocessing import image
from keras.initializers import glorot_uniform
from keras.utils import layer_utils
from keras.utils.data_utils import get_file
from keras.applications.imagenet_utils import preprocess_input
```

```python
df= pd.read_csv('Train.csv')
train_y=df['label'].values.T
train_x=df.iloc[:,1:].values.astype('float32')
train_x=train_x.reshape(-1,28,28,1)
train_x.shape
```

```python
temp_y=np.zeros((42000,10))
for i in range(42000):
    temp_y[i,train_y[i]]=1
train_y=temp_y.astype('int32')
train_x=train_x/255
train_y
```

```python
print(train_y.shape)
plt.imshow(train_x[41997,:,:,0])
```

```python
def convolutional_block(X, f, filters, stage, block, s = 2):
    
    # defining name basis
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    
    # Retrieve Filters
    F1, F2, F3 = filters
    
    # Save the input value
    X_shortcut = X
F1, F2, F3 = filters
    
    # Save the input value
    X_shortcut = X


    ##### MAIN PATH #####
    # First component of main path 
    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + '2a',padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)
    X = Activation('relu')(X)
    

    # Second component of main path (3 lines)
    X = Conv2D(F2, (f, f), strides = (1,1), name = conv_name_base + '2b',padding = 'same', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)
    X = Activation('relu')(X)

    # Third component of main path (2 lines)
    X = Conv2D(F3, (1, 1), strides = (1,1), name = conv_name_base + '2c',padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)

    ##### SHORTCUT PATH #### (2 lines)
    X_shortcut = Conv2D(F3, (1, 1), strides = (s,s), name = conv_name_base + '1',padding = 'valid', kernel_initializer = glorot_uniform(seed=0))(X_shortcut)
    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + '1')(X_shortcut)

    # Final step: Add shortcut value to main path, and pass it through a RELU activation (2 lines)
    X = Add()([X,X_shortcut])
    X = Activation('relu')(X)
    
    
    return X
```
X = Activation('relu')(X)
    
    
    return X
```

```python
def identity_block(X, f, filters, stage, block):
   
    
    # defining name basis
    conv_name_base = 'res' + str(stage) + block + '_branch'
    bn_name_base = 'bn' + str(stage) + block + '_branch'
    
    # Retrieve Filters
    F1, F2, F3 = filters
    
    # Save the input value. You'll need this later to add back to the main path. 
    X_shortcut = X
    
    # First component of main path
    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)
    X = Activation('relu')(X)
    
    
    # Second component of main path (3 lines)
    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1,1), padding = 'same', name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)
    X = Activation('relu')(X)

    # Third component of main path (2 lines)
    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)

    X = Add()([X,X_shortcut])
    X = Activation('relu')(X)
X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1,1), padding = 'valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)

    X = Add()([X,X_shortcut])
    X = Activation('relu')(X)
    
    
    return X
```

```python
def ResNet50(input_shape = (28, 28, 1), classes = 10):
    
    # Define the input as a tensor with shape input_shape
    X_input = Input(input_shape)

    
    # Zero-Padding
    X = ZeroPadding2D((3, 3))(X_input)
    
    # Stage 1
    X = Conv2D(64, (7, 7), strides = (2, 2), name = 'conv1', kernel_initializer = glorot_uniform(seed=0))(X)
    X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)
    X = Activation('relu')(X)
    X = MaxPooling2D((3, 3), strides=(2, 2))(X)

    # Stage 2
    X = convolutional_block(X, f = 3, filters = [64, 64, 256], stage = 2, block='a', s = 1)
    X = identity_block(X, 3, [64, 64, 256], stage=2, block='b')
    X = identity_block(X, 3, [64, 64, 256], stage=2, block='c')


    # AVGPOOL 
    X = AveragePooling2D(pool_size=(2, 2))(X)
    

    # output layer
    X = Flatten()(X)
    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)
    
    
    # Create model
    model = Model(inputs = X_input, outputs = X, name='ResNet50')

    return model
```
# Create model
    model = Model(inputs = X_input, outputs = X, name='ResNet50')

    return model
```

```python
def DigitModel(input_shape = (28, 28, 1), classes = 10):
    
    X_input = Input(input_shape)
    
    X = Conv2D(filters = 32, kernel_size = (3, 3), strides = (1,1), padding = 'same', name = 'conv1', activation = 'relu', kernel_initializer = glorot_uniform(seed=0))(X_input)
    #X = BatchNormalization(axis = 3, name = 'bn_conv1')(X)
    X = MaxPooling2D(pool_size=(2, 2), strides = (2,2))(X)
    
    X = Conv2D(filters = 64, kernel_size = (5, 5), strides = (1,1), padding = 'same', name = 'conv2', activation = 'relu', kernel_initializer = glorot_uniform(seed=0))(X)
    #X = BatchNormalization(axis = 3, name = 'bn_conv2')(X)
    X = AveragePooling2D(pool_size=(2, 2), strides = (2,2))(X)
    
    X = Flatten()(X)
    X = Dropout(0.5,input_shape = (None,3136))(X)
    X = Dense(128, name = 'fc1', kernel_initializer = glorot_uniform(seed=0))(X)
    #X = Dense(84, activation = 'relu', name = 'fc2', kernel_initializer = glorot_uniform(seed=0))(X)
    X = Dropout(0.5,input_shape = (None, 128))(X)
    X = Dense(classes, activation = 'softmax', name = 'fc2', kernel_initializer = glorot_uniform(seed=0))(X)
    
     # Create model
    model = Model(inputs = X_input, outputs = X, name='LeNet5')

    return model
```

```python
model = Model(inputs = X_input, outputs = X, name='LeNet5')

    return model
```

```python
model = DigitModel(input_shape = (28, 28, 1), classes = 10)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(train_x, train_y, epochs = 100, batch_size = 420)
```

```python
df1= pd.read_csv('Test.csv')
test_x=df1.values.astype('float32')
test_x=test_x.reshape(-1,28,28,1)
test_x.shape
```

```python
results = model.predict(test_x)
```

```python
results = np.argmax(results, axis=1)
print(results)
results = pd.Series(results, name='Label')
```

```python
submission = pd.concat([pd.Series(range(1,28001), name='ImageId'), results], axis=1)
submission.to_csv('submission.csv', index=False)
```

```python
model.summary()
```

```python
import torch
```
PATH: app/src/androidTest/java/com/example/mysafety/ExampleInstrumentedTest.java
LINES: 1-26

package com.example.mysafety;

import android.content.Context;
import android.support.test.InstrumentationRegistry;
import android.support.test.runner.AndroidJUnit4;

import org.junit.Test;
import org.junit.runner.RunWith;

import static org.junit.Assert.*;

/**
 * Instrumented test, which will execute on an Android device.
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
@RunWith(AndroidJUnit4.class)
public class ExampleInstrumentedTest {
    @Test
    public void useAppContext() {
        // Context of the app under test.
        Context appContext = InstrumentationRegistry.getTargetContext();

        assertEquals("com.example.mysafety", appContext.getPackageName());
    }
}
PATH: app/src/main/java/com/example/mysafety/Categories.java
LINES: 1-45

package com.example.mysafety;

import android.content.Context;
import android.net.Uri;
import android.os.Bundle;

import androidx.annotation.NonNull;
import androidx.fragment.app.Fragment;

import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ListView;
import android.widget.TextView;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;

import java.util.ArrayList;
import java.util.List;


/**
 * A simple {@link Fragment} subclass.
 * Activities that contain this fragment must implement the
 * {@link Categories.OnFragmentInteractionListener} interface
 * to handle interaction events.
 * Use the {@link Categories#newInstance} factory method to
 * create an instance of this fragment.
 */
public class Categories extends Fragment {
    // the fragment initialization parameters, e.g. ARG_ITEM_NUMBER
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";
    TextView nocomplaints;
    ListView listView;
    FirebaseFirestore db;
    static Bundle args;

    private String mParam1;
PATH: app/src/main/java/com/example/mysafety/Categories.java
LINES: 46-89

private String mParam2;

    private OnFragmentInteractionListener mListener;

    public Categories() {
        // Required empty public constructor
    }

    /**
     * Use this factory method to create a new instance of
     * this fragment using the provided parameters.
     *
     * @param param1 Parameter 1.
     * @return A new instance of fragment Categories.
     */
    public static Categories newInstance(String param1) {
        Categories fragment = new Categories();
        args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        // Inflate the layout for this fragment
        View view=inflater.inflate(R.layout.fragment_categories, container, false);
        nocomplaints=view.findViewById(R.id.nocomplaints);
        nocomplaints.setVisibility(View.GONE);

        db=FirebaseFirestore.getInstance();

        listView=view.findViewById(R.id.list);
PATH: app/src/main/java/com/example/mysafety/Categories.java
LINES: 90-114

db.collection(getString(R.string.complaint)).whereEqualTo("Department",mParam1).get()
                .addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                    @Override
                    public void onComplete(@NonNull Task<QuerySnapshot> task) {
                        List<Details> details= new ArrayList<>();
                        if(task.isSuccessful()){
                            for (QueryDocumentSnapshot document : task.getResult()){
                                Details details1=new Details(document.getString("Date"),document.getString("User"),document.getString("Complaint"));
                                details.add(details1);
                            }
                            CustomAdapter customAdapter=new CustomAdapter(getActivity(),details);
                            listView.setAdapter(customAdapter);
                        }
                        if(task.getResult().isEmpty())nocomplaints.setVisibility(View.VISIBLE);
                    }
                }).addOnFailureListener(new OnFailureListener() {
            @Override
            public void onFailure(@NonNull Exception e) {
                Log.w("TAG","Error in getting documnets");
            }
        });//Displaying complaints sorted by department name as per the tab opened currently

        return view;
    }
PATH: app/src/main/java/com/example/mysafety/Categories.java
LINES: 115-151

public void onButtonPressed(Uri uri) {
        if (mListener != null) {
            mListener.onFragmentInteraction(uri);
        }
    }

    @Override
    public void onAttach(Context context) {
        super.onAttach(context);
        if (context instanceof OnFragmentInteractionListener) {
            mListener = (OnFragmentInteractionListener) context;
        } else {
            throw new RuntimeException(context.toString()
                    + " must implement OnFragmentInteractionListener");
        }
    }

    @Override
    public void onDetach() {
        super.onDetach();
        mListener = null;
    }

    /**
     * This interface must be implemented by activities that contain this
     * fragment to allow an interaction in this fragment to be communicated
     * to the activity and potentially other fragments contained in that
     * activity.
     * <p>
     * See the Android Training lesson <a href=
     * "http://developer.android.com/training/basics/fragments/communicating.html"
     * >Communicating with Other Fragments</a> for more information.
     */
    public interface OnFragmentInteractionListener {
        void onFragmentInteraction(Uri uri);
    }
}
PATH: app/src/main/java/com/example/mysafety/Complaints.java
LINES: 1-31

package com.example.mysafety;

import androidx.appcompat.app.AppCompatActivity;
import androidx.viewpager.widget.ViewPager;

import android.net.Uri;
import android.os.Bundle;
import com.google.android.material.tabs.TabLayout;



public class Complaints extends AppCompatActivity implements Categories.OnFragmentInteractionListener {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_complaints);

        ViewPager viewPager=(ViewPager)findViewById(R.id.viewpager);
        SimpleFragmentPagerAdapter adapter = new SimpleFragmentPagerAdapter(getSupportFragmentManager(),getApplicationContext());
        viewPager.setAdapter(adapter);//Setting viewpager for navigating between tabs by swiping
        TabLayout tabLayout=(TabLayout)findViewById(R.id.sliding_tabs);
        tabLayout.setupWithViewPager(viewPager);//tablayout for tabs

    }

    @Override
    public void onFragmentInteraction(Uri uri) {

    }
}
PATH: app/src/main/java/com/example/mysafety/CustomAdapter.java
LINES: 1-37

package com.example.mysafety;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ArrayAdapter;
import android.widget.TextView;

import java.util.List;

public class CustomAdapter extends ArrayAdapter<Details> {
    public CustomAdapter( Context context, List<Details> objects) {
        super(context,0, objects);
    }

    @Override
    public View getView(int position, View convertView, ViewGroup parent) {
        View listitemview=convertView;
        if(listitemview==null)//Check if the existing view is being used if not inflate a new view
            listitemview = LayoutInflater.from(getContext()).inflate(R.layout.complaint, parent, false);

        Details details=getItem(position);

        TextView date=listitemview.findViewById(R.id.date);
        date.setText(details.getDate());

        TextView username=listitemview.findViewById(R.id.user);
        username.setText(details.getUsername());

        TextView complaint=listitemview.findViewById(R.id.complaint);
        complaint.setText(details.getComplaint());


        return listitemview;
    }
}
PATH: app/src/main/java/com/example/mysafety/CustomAdapter1.java
LINES: 1-43

package com.example.mysafety;

import android.content.Context;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ArrayAdapter;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;

import com.example.mysafety.MyComplaint;
import com.example.mysafety.R;

import java.util.List;

public class CustomAdapter1 extends ArrayAdapter<MyComplaint> {
    public CustomAdapter1(Context context, List<MyComplaint> objects) {
        super(context, 0, objects);
    }

    @NonNull
    @Override
    public View getView(int position, @Nullable View convertView, @NonNull ViewGroup parent) {
        View listitemview=convertView;
        if(listitemview==null)//Check if the existing view is being used if not inflate a new view
            listitemview = LayoutInflater.from(getContext()).inflate(R.layout.mycomplaint, parent, false);

        MyComplaint myComplaint=getItem(position);

        TextView date=listitemview.findViewById(R.id.date1);
        date.setText(myComplaint.getDate());

        TextView department=listitemview.findViewById(R.id.mydepartment);
        department.setText(myComplaint.getDepartment());

        TextView complaint=listitemview.findViewById(R.id.complaint1);
        complaint.setText(myComplaint.getComplaint());

        return listitemview;
    }
}
PATH: app/src/main/java/com/example/mysafety/Details.java
LINES: 1-28

package com.example.mysafety;

public class Details {
    private String date;
    private String username;
    private String complaint;

    public Details(){}

    public Details(String date, String username, String complaint) {
        this.date = date;
        this.username = username;
        this.complaint = complaint;
    }

    public String getDate() {
        return date;
    }

    public String getUsername() {
        return username;
    }

    public String getComplaint() {
        return complaint;
    }

}
PATH: app/src/main/java/com/example/mysafety/DisplayImage.java
LINES: 1-46

package com.example.mysafety;

import android.content.Context;
import android.net.Uri;
import android.os.Bundle;

import androidx.fragment.app.Fragment;

import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ImageView;
import android.widget.TextView;

import com.bumptech.glide.Glide;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.DocumentSnapshot;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;


/**
 * A simple {@link Fragment} subclass.
 * Activities that contain this fragment must implement the
 * {@link DisplayImage.OnFragmentInteractionListener} interface
 * to handle interaction events.
 * Use the {@link DisplayImage#newInstance} factory method to
 * create an instance of this fragment.
 */
public class DisplayImage extends Fragment {
    // the fragment initialization parameters, e.g. ARG_ITEM_NUMBER
    private static final String ARG_PARAM1 = "param1";
    private static final String ARG_PARAM2 = "param2";

    private String mParam1;
    private String mParam2;
    private String path;

    private OnFragmentInteractionListener mListener;

    public DisplayImage() {
        // Required empty public constructor
    }

    /**
PATH: app/src/main/java/com/example/mysafety/DisplayImage.java
LINES: 47-81

* Use this factory method to create a new instance of
     * this fragment using the provided parameters.
     *
     * @param param1 Parameter 1.
     * @param param2 Parameter 2.
     * @return A new instance of fragment DisplayImage.
     */
    private static DisplayImage newInstance(String param1, String param2) {
        DisplayImage fragment = new DisplayImage();
        Bundle args = new Bundle();
        args.putString(ARG_PARAM1, param1);
        args.putString(ARG_PARAM2, param2);
        fragment.setArguments(args);
        return fragment;
    }

    @Override
    public void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        if (getArguments() != null) {
            mParam1 = getArguments().getString(ARG_PARAM1);
            mParam2 = getArguments().getString(ARG_PARAM2);
            path=getArguments().getString("path");
        }
    }

    @Override
    public View onCreateView(LayoutInflater inflater, ViewGroup container,
                             Bundle savedInstanceState) {
        View view= inflater.inflate(R.layout.fragment_display_image, container, false);

        final ImageView imageView=view.findViewById(R.id.imagedisplay);

        FirebaseStorage firebaseStorage=FirebaseStorage.getInstance();
        StorageReference storage=firebaseStorage.getReference().child("MySafety");
PATH: app/src/main/java/com/example/mysafety/DisplayImage.java
LINES: 82-121

StorageReference childreference=storage.child(path);
        childreference.getDownloadUrl()
                .addOnSuccessListener(new OnSuccessListener<Uri>() {
                    @Override
                    public void onSuccess(Uri uri) {
                        String url=uri.toString();
                        Glide.with(getContext())
                                .load(url)
                                .into(imageView);
                    }
                });

        String pathref=path.substring(0,path.lastIndexOf(' '));

        final TextView Label=view.findViewById(R.id.labeltext);

        FirebaseFirestore db=FirebaseFirestore.getInstance();
        db.collection("Images").document(pathref).get()
                .addOnSuccessListener(new OnSuccessListener<DocumentSnapshot>() {
                    @Override
                    public void onSuccess(DocumentSnapshot documentSnapshot) {
                        Label.setText(documentSnapshot.getString("Label"));//Displaying the label retrieved from firebase above the photo
                    }
                });

        // Inflate the layout for this fragment
        return view;


    }


    public void onButtonPressed(Uri uri) {
        if (mListener != null) {
            mListener.onFragmentInteraction(uri);
        }
    }

    @Override
    public void onAttach(Context context) {
PATH: app/src/main/java/com/example/mysafety/DisplayImage.java
LINES: 122-150

super.onAttach(context);
        if (context instanceof OnFragmentInteractionListener) {
            mListener = (OnFragmentInteractionListener) context;
        } else {
            throw new RuntimeException(context.toString()
                    + " must implement OnFragmentInteractionListener");
        }
    }

    @Override
    public void onDetach() {
        super.onDetach();
        mListener = null;
    }

    /**
     * This interface must be implemented by activities that contain this
     * fragment to allow an interaction in this fragment to be communicated
     * to the activity and potentially other fragments contained in that
     * activity.
     * <p>
     * See the Android Training lesson <a href=
     * "http://developer.android.com/training/basics/fragments/communicating.html"
     * >Communicating with Other Fragments</a> for more information.
     */
    public interface OnFragmentInteractionListener {
        void onFragmentInteraction(Uri uri);
    }
}
PATH: app/src/main/java/com/example/mysafety/EmailLogin.java
LINES: 1-42

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.TextView;
import android.widget.Toast;

import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInAccount;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;

public class EmailLogin extends AppCompatActivity {

    EditText memail,mpassword;
    TextView forgotpasswd;
    String email,password;
    Button login;
    FirebaseAuth mAuth;
    FirebaseAuth.AuthStateListener authStateListener;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_email_login);
        memail=findViewById(R.id.email);
        mpassword=findViewById(R.id.password);
        forgotpasswd=findViewById(R.id.forgotpassword);
        login=findViewById(R.id.login2);
PATH: app/src/main/java/com/example/mysafety/EmailLogin.java
LINES: 43-74

mAuth=FirebaseAuth.getInstance();

        authStateListener=new FirebaseAuth.AuthStateListener() {
            @Override
            public void onAuthStateChanged(@NonNull FirebaseAuth firebaseAuth) {

                FirebaseUser user=firebaseAuth.getCurrentUser();
                if(user!=null){
                    Intent intent=new Intent(EmailLogin.this,MainPage.class);
                    intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                    startActivity(intent);
                    finish();
                }
            }
        };


        login.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                email=memail.getText().toString();
                email=email.trim();
                password=mpassword.getText().toString();//getting strings from the edit text to perform options accordingly

                if(email.trim().isEmpty()) {
                    Toast.makeText(getApplicationContext(), "Enter email", Toast.LENGTH_SHORT).show();
                    return;
                }
                if(password.trim().isEmpty() && !email.trim().isEmpty()) {
                    Toast.makeText(getApplicationContext(), "Enter password", Toast.LENGTH_SHORT).show();
                    return;
                }
PATH: app/src/main/java/com/example/mysafety/EmailLogin.java
LINES: 75-107

SharedPreferences sharedPreferences=EmailLogin.this.getSharedPreferences("Userdetails",MODE_PRIVATE);
                SharedPreferences.Editor editor=sharedPreferences.edit();
                editor.putString("User",email);
                editor.apply();

                email=email.trim()+"@mysafety.com";//adding fake domain to store user in firebase
                FirebaseAuthWithEmail();
            }
        });//dealing with all cases when user clicks login button

        forgotpasswd.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                email=memail.getText().toString();

                if(email.trim().isEmpty()) {
                    Toast.makeText(getApplicationContext(), "Enter email", Toast.LENGTH_SHORT).show();
                    return;
                }

                Forgotpassword();
            }
        });
    }

    private void FirebaseAuthWithEmail()
    {
        mAuth.signInWithEmailAndPassword(email, password)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {
                            // Sign in success, update UI with the signed-in user's information
PATH: app/src/main/java/com/example/mysafety/EmailLogin.java
LINES: 108-135

Log.d("TAG", "signInWithEmail:success");
                            FirebaseUser user = mAuth.getCurrentUser();
                            //updateUI(user);
                        } else {
                            // If sign in fails, display a message to the user.
                            Log.w("TAG", "signInWithEmail:failure", task.getException());
                            Toast.makeText(getApplicationContext(),"Invalid Email/Password",Toast.LENGTH_SHORT).show();
                            //updateUI(null);
                        }

                    }
                }).addOnFailureListener(new OnFailureListener() {
            @Override
            public void onFailure(@NonNull Exception e) {
                Toast.makeText(EmailLogin.this,"Error Logging in",Toast.LENGTH_SHORT).show();
            }
        });
    }

    private void Forgotpassword(){
        mAuth.sendPasswordResetEmail(email)
                .addOnCompleteListener(new OnCompleteListener<Void>() {
                    @Override
                    public void onComplete(@NonNull Task<Void> task) {
                        if (task.isSuccessful()) {
                            Log.d("TAG", "Email sent.");
                            Toast.makeText(getApplicationContext(),"Email sent",Toast.LENGTH_SHORT).show();
                        }
PATH: app/src/main/java/com/example/mysafety/EmailLogin.java
LINES: 136-155

else
                            Toast.makeText(getApplicationContext(),"Error",Toast.LENGTH_SHORT).show();
                    }
                });
    }

    @Override
    public void onStart() {
        super.onStart();
        mAuth.addAuthStateListener(authStateListener);
    }

    @Override
    public void onStop() {
        super.onStop();
        if (authStateListener != null) {
            mAuth.removeAuthStateListener(authStateListener);
        }
    }
}
PATH: app/src/main/java/com/example/mysafety/Gallery.java
LINES: 1-43

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;
import androidx.fragment.app.FragmentManager;
import androidx.fragment.app.FragmentTransaction;

import android.net.Uri;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.AdapterView;
import android.widget.GridView;
import android.widget.SearchView;
import android.widget.TextView;
import android.widget.Toast;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.firestore.DocumentReference;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;

import java.util.ArrayList;
import java.util.List;

public class Gallery extends AppCompatActivity implements DisplayImage.OnFragmentInteractionListener {

    GridView gridView;
    FirebaseStorage firebaseStorage;
    FirebaseFirestore db;
    TextView NoImage;
    ImageAdapter imageAdapter;
    List<String> images;


    @Override
    protected void onCreate(Bundle savedInstanceState) {
PATH: app/src/main/java/com/example/mysafety/Gallery.java
LINES: 44-73

super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_gallery);

        firebaseStorage=FirebaseStorage.getInstance();
        db=FirebaseFirestore.getInstance();

        NoImage=findViewById(R.id.error);
        NoImage.setVisibility(View.GONE);

        gridView=findViewById(R.id.gridview);
        gridView.setVisibility(View.VISIBLE);//gridview to display images


        db.collection("Images").get()
                .addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                    @Override
                    public void onComplete(@NonNull Task<QuerySnapshot> task) {
                        images=new ArrayList<>();
                        if(task.isSuccessful()){
                            for(QueryDocumentSnapshot document: task.getResult()){
                                String path=document.getString("Time")+" "+document.getString("Image");
                                images.add(path.trim());
                            }
                            imageAdapter=new ImageAdapter(getApplicationContext(),images);
                            gridView.setAdapter(imageAdapter);
                            if(task.getResult().isEmpty())NoImage.setVisibility(View.VISIBLE);
                        }
                    }
                }).addOnFailureListener(new OnFailureListener() {
            @Override
PATH: app/src/main/java/com/example/mysafety/Gallery.java
LINES: 74-106

public void onFailure(@NonNull Exception e) {
                Log.w("TAG","Error in getting documnets");
                Toast.makeText(Gallery.this,"Error Loading Photos",Toast.LENGTH_SHORT).show();
            }
        });//Getting images to be displayed in gridview and setting them in the gridview

        gridView.setOnItemClickListener(new AdapterView.OnItemClickListener() {
            @Override
            public void onItemClick(AdapterView<?> parent, View view, int position, long id) {
                String path=imageAdapter.getItem(position);

                Bundle bundle=new Bundle();
                bundle.putString("path",path);

                FragmentManager fragmentManager = getSupportFragmentManager();
                FragmentTransaction fragmentTransaction = fragmentManager.beginTransaction();

                DisplayImage displayImage=new DisplayImage();
                displayImage.setArguments(bundle);

                gridView.setVisibility(View.GONE);

                fragmentTransaction.add(R.id.galleryview,displayImage);
                fragmentTransaction.addToBackStack(null);
                fragmentTransaction.commit();
            }
        });//Displaying enlarged image on clicking every element

    }

    @Override
    public void onBackPressed(){
        int fragments=getSupportFragmentManager().getBackStackEntryCount();
PATH: app/src/main/java/com/example/mysafety/Gallery.java
LINES: 107-118

if(fragments==1){
            getSupportFragmentManager().popBackStack();
            gridView.setVisibility(View.VISIBLE);
        }
        else
            super.onBackPressed();
    }
    @Override
    public void onFragmentInteraction(Uri uri) {

    }
}
PATH: app/src/main/java/com/example/mysafety/ImageAdapter.java
LINES: 1-45

package com.example.mysafety;

import android.content.Context;
import android.net.Uri;
import android.util.Log;
import android.view.LayoutInflater;
import android.view.View;
import android.view.ViewGroup;
import android.widget.ArrayAdapter;
import android.widget.ImageView;
import android.widget.TextView;

import androidx.annotation.NonNull;
import androidx.annotation.Nullable;

import com.bumptech.glide.Glide;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;

import java.util.List;

class ImageAdapter extends ArrayAdapter<String> {

    private Context context;

    public ImageAdapter(@NonNull Context context, List<String> objects) {
        super(context,0, objects);
        this.context=context;
    }

    @NonNull
    @Override
    public View getView(int position, View convertView, ViewGroup parent) {


        View listitemview=convertView;
        if(listitemview==null)//Check if the existing view is being used if not inflate a new view
            listitemview = LayoutInflater.from(getContext()).inflate(R.layout.image, parent, false);

        String path=getItem(position);
        final ImageView imageView=listitemview.findViewById(R.id.imageview);


        FirebaseStorage firebaseStorage=FirebaseStorage.getInstance();
PATH: app/src/main/java/com/example/mysafety/ImageAdapter.java
LINES: 46-61

StorageReference storage=firebaseStorage.getReference().child("MySafety");
        StorageReference childreference=storage.child(path);
        childreference.getDownloadUrl()
                .addOnSuccessListener(new OnSuccessListener<Uri>() {
                    @Override
                    public void onSuccess(Uri uri) {
                        String url=uri.toString();
                        Glide.with(context)
                                .load(url)
                                .into(imageView);
                    }
                });

        return listitemview;
    }
}
PATH: app/src/main/java/com/example/mysafety/Login.java
LINES: 1-37

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.Toast;

import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInAccount;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.common.SignInButton;
import com.google.android.gms.common.api.ApiException;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.AuthCredential;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;
import com.google.firebase.auth.GoogleAuthProvider;

public class Login extends AppCompatActivity {
    private GoogleSignInClient mGoogleSignInClient;
    private final int RC_SIGN_IN=1000;
    private FirebaseAuth mAuth;
    Button login,signup;
    SignInButton signInButton;
    FirebaseAuth.AuthStateListener authStateListener;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
PATH: app/src/main/java/com/example/mysafety/Login.java
LINES: 38-71

super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_login);

        GoogleSignInOptions gso = new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestIdToken(getString(R.string.web_client_id))
                .requestEmail()
                .build();

        mGoogleSignInClient= GoogleSignIn.getClient(this,gso);
        signInButton=findViewById(R.id.google);

        mAuth = FirebaseAuth.getInstance();

        authStateListener=new FirebaseAuth.AuthStateListener() {
            @Override
            public void onAuthStateChanged(@NonNull FirebaseAuth firebaseAuth) {

                FirebaseUser user=firebaseAuth.getCurrentUser();
                if(user!=null){
                    Intent intent=new Intent(Login.this,MainPage.class);
                    intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                    startActivity(intent);
                    finish();
                }
            }
        };

        login=findViewById(R.id.login1);
        signup=findViewById(R.id.signup);

        login.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent intent=new Intent(Login.this,EmailLogin.class);
PATH: app/src/main/java/com/example/mysafety/Login.java
LINES: 72-107

intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP | Intent.FLAG_ACTIVITY_CLEAR_TASK);
                startActivity(intent);
                finish();
            }
        });
        signup.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(Login.this,SignUp.class));
            }
        });


        signInButton.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                signIn();
            }
        });
    }

    private void signIn() {
        Intent signInIntent = mGoogleSignInClient.getSignInIntent();
        startActivityForResult(signInIntent, RC_SIGN_IN);//starting activity for google sign in(google accounts options)
    }

    @Override
    public void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        // Result returned from launching the Intent from GoogleSignInApi.getSignInIntent(...);
        if (requestCode == RC_SIGN_IN) {
            Task<GoogleSignInAccount> task = GoogleSignIn.getSignedInAccountFromIntent(data);
            try {
                // Google Sign In was successful, authenticate with Firebase
                GoogleSignInAccount account = task.getResult(ApiException.class);
PATH: app/src/main/java/com/example/mysafety/Login.java
LINES: 108-133

firebaseAuthWithGoogle(account);

                SharedPreferences sharedPreferences=this.getSharedPreferences("Userdetails",MODE_PRIVATE);
                SharedPreferences.Editor editor=sharedPreferences.edit();
                editor.putString("User",account.getDisplayName());
                editor.apply();//Storing username for storing in database

            } catch (ApiException e) {
                // Google Sign In failed, update UI appropriately
                Log.w("TAG", "Google sign in failed", e);
                Toast.makeText(Login.this,"Error",Toast.LENGTH_SHORT).show();
            }
        }
    }

    private void firebaseAuthWithGoogle(GoogleSignInAccount acct) {
        Log.d("TAG", "firebaseAuthWithGoogle:" + acct.getId());

        AuthCredential credential = GoogleAuthProvider.getCredential(acct.getIdToken(), null);
        mAuth.signInWithCredential(credential)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
                    public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {
                            // Sign in success, update UI with the signed-in user's information
                            Log.d("TAG", "signInWithCredential:success");
PATH: app/src/main/java/com/example/mysafety/Login.java
LINES: 134-158

FirebaseUser user = mAuth.getCurrentUser();
                        } else {
                            // If sign in fails, display a message to the user.
                            Log.w("TAG", "signInWithCredential:failure", task.getException());
                        }

                        // ...
                    }
                });
    }

    @Override
    public void onStart() {
        super.onStart();
        mAuth.addAuthStateListener(authStateListener);
    }

    @Override
    public void onStop() {
        super.onStop();
        if (authStateListener != null) {
            mAuth.removeAuthStateListener(authStateListener);
        }
    }
}
PATH: app/src/main/java/com/example/mysafety/MainActivity.java
LINES: 1-36

package com.example.mysafety;

import android.content.Intent;
import androidx.appcompat.app.AppCompatActivity;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;

public class MainActivity extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        Button complaints=findViewById(R.id.complaints);//Connecting button object with it's xml element
        complaints.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(MainActivity.this,Complaints.class));//opening complaints activity
            }
        });//Adding onClick listener to perform action when button is clicked

        Button login=findViewById(R.id.login);
        login.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(MainActivity.this,Login.class));
            }
        });

        Button gallery=findViewById(R.id.gallery1);
        gallery.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(MainActivity.this,Gallery.class));
            }
PATH: app/src/main/java/com/example/mysafety/MainActivity.java
LINES: 37-39

});
    }
}
PATH: app/src/main/java/com/example/mysafety/MainPage.java
LINES: 1-39

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.view.View;
import android.widget.Button;

import com.google.android.gms.auth.api.signin.GoogleSignIn;
import com.google.android.gms.auth.api.signin.GoogleSignInClient;
import com.google.android.gms.auth.api.signin.GoogleSignInOptions;
import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;

public class MainPage extends AppCompatActivity {

    Button Regcomp,Signout,mycomplaints,myuploads;
    GoogleSignInClient googleSignInClient;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main_page);

        Regcomp=findViewById(R.id.regcomp);
        Signout=findViewById(R.id.signout);
        mycomplaints=findViewById(R.id.mycomplaints);
        myuploads=findViewById(R.id.myuploads);

        GoogleSignInOptions gso=new GoogleSignInOptions.Builder(GoogleSignInOptions.DEFAULT_SIGN_IN)
                .requestIdToken(getString(R.string.web_client_id))
                .requestEmail()
                .build();
PATH: app/src/main/java/com/example/mysafety/MainPage.java
LINES: 40-77

googleSignInClient=GoogleSignIn.getClient(this,gso);

        Regcomp.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(MainPage.this,SpeechtoText.class));
            }
        });

        Signout.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                signout();
            }
        });

        mycomplaints.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(MainPage.this,MyComplaints.class));
            }
        });

        myuploads.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                startActivity(new Intent(MainPage.this,MyGallery.class));
            }
        });
    }
    public void signout(){
        googleSignInClient.signOut().addOnCompleteListener(this, new OnCompleteListener<Void>() {
            @Override
            public void onComplete(@NonNull Task<Void> task) {

                SharedPreferences sharedPreferences=getSharedPreferences("Userdetails",MODE_PRIVATE);
                SharedPreferences.Editor editor = sharedPreferences.edit();
                editor.clear();
PATH: app/src/main/java/com/example/mysafety/MainPage.java
LINES: 78-91

//clears out the user data of the application in shared preference when the user logs out
                editor.commit();

                FirebaseAuth.getInstance().signOut();//signing out from firebase

                Intent intent=new Intent(MainPage.this,MainActivity.class);
                intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                startActivity(intent);
            }
        });// Signing out the user
    }


}
PATH: app/src/main/java/com/example/mysafety/MyComplaint.java
LINES: 1-34

package com.example.mysafety;

public class MyComplaint {
    private String date;
    private String complaint;
    private String time;
    private String department;


    public MyComplaint(){}

    public MyComplaint(String date, String complaint, String time,String department) {
        this.date = date;
        this.complaint = complaint;
        this.time=time;
        this.department=department;
    }

    public String getDate() {
        return date;
    }

    public String getComplaint() {
        return complaint;
    }

    public String getTime() {
        return time;
    }

    public String getDepartment() {
        return department;
    }
}
PATH: app/src/main/java/com/example/mysafety/MyComplaints.java
LINES: 1-47

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AlertDialog;
import androidx.appcompat.app.AppCompatActivity;

import android.content.DialogInterface;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.AdapterView;
import android.widget.ListView;
import android.widget.TextView;
import android.widget.Toast;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;

import java.util.ArrayList;
import java.util.List;


public class MyComplaints extends AppCompatActivity {

    TextView error;
    ListView listView;
    FirebaseFirestore db;
    CustomAdapter1 customAdapter1;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_my_complaints);

        error=findViewById(R.id.error1);
        listView=findViewById(R.id.list1);
        error.setVisibility(View.GONE);

        db=FirebaseFirestore.getInstance();
PATH: app/src/main/java/com/example/mysafety/MyComplaints.java
LINES: 48-68

SharedPreferences sharedPreferences=this.getSharedPreferences("Userdetails",MODE_PRIVATE);
        String user=sharedPreferences.getString("User","");

        db.collection(getString(R.string.complaint)).whereEqualTo("User",user).get()//querying according to username in firebase
                .addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {

                    @Override
                    public void onComplete(@NonNull Task<QuerySnapshot> task) {
                        List<MyComplaint> myComplaints=new ArrayList<>();
                        if(task.isSuccessful()){
                            for(QueryDocumentSnapshot document: task.getResult()){
                                MyComplaint myComplaint=new MyComplaint(document.getString("Date"),document.getString("Complaint"),document.getString("Time"),document.getString("Department"));
                                myComplaints.add(myComplaint);
                            }
                            customAdapter1 = new CustomAdapter1(getApplicationContext(),myComplaints);
                            listView.setAdapter(customAdapter1);
                        }
                        if(task.getResult().isEmpty())error.setVisibility(View.VISIBLE);
                    }
                }).addOnFailureListener(new OnFailureListener() {
            @Override
PATH: app/src/main/java/com/example/mysafety/MyComplaints.java
LINES: 69-102

public void onFailure(@NonNull Exception e) {
                Log.d("TAG","Error accessing documents");
            }
        });

        listView.setLongClickable(true);

        listView.setOnItemLongClickListener(new AdapterView.OnItemLongClickListener() {
            @Override
            public boolean onItemLongClick(AdapterView<?> parent, View view, int position, long id) {

                MyComplaint myComplaint=customAdapter1.getItem(position);
                buildDialog(myComplaint);//building a dialog to ask user whether to delete when the item is pressed for long time
                return true;
            }
        });
    }

    private void buildDialog(final MyComplaint myComplaint){

        AlertDialog.Builder builder=new AlertDialog.Builder(this);
        builder.setMessage("Do you want to delete the complaint?");

        builder.setPositiveButton("Yes",
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        DataDelete(myComplaint);
                    }
                });
        builder.setNegativeButton("No",
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
PATH: app/src/main/java/com/example/mysafety/MyComplaints.java
LINES: 103-131

Log.d("TAG",""+myComplaint.getTime());
                        dialog.cancel();
                    }
                });//Setting attributes and functionalities for the dialog buttons
        builder.show();
    }

    private void DataDelete(final MyComplaint myComplaint){
        db.collection("Complaints").document(myComplaint.getTime())
                .delete()
                .addOnSuccessListener(new OnSuccessListener<Void>() {
                    @Override
                    public void onSuccess(Void aVoid) {
                        Log.d("TAG", "DocumentSnapshot successfully deleted!");
                        Toast.makeText(MyComplaints.this,"Complaint Deleted",Toast.LENGTH_SHORT).show();
                        customAdapter1.remove(myComplaint);
                        if(customAdapter1.isEmpty())
                            error.setVisibility(View.VISIBLE);
                    }
                })
                .addOnFailureListener(new OnFailureListener() {
                    @Override
                    public void onFailure(@NonNull Exception e) {
                        Log.w("TAG", "Error deleting document", e);
                    }
                });//deleting the data as per the item selected by the user

    }
}
PATH: app/src/main/java/com/example/mysafety/MyGallery.java
LINES: 1-41

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AlertDialog;
import androidx.appcompat.app.AppCompatActivity;
import androidx.fragment.app.FragmentManager;
import androidx.fragment.app.FragmentTransaction;

import android.content.DialogInterface;
import android.content.SharedPreferences;
import android.net.Uri;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.AdapterView;
import android.widget.GridView;
import android.widget.TextView;
import android.widget.Toast;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.QueryDocumentSnapshot;
import com.google.firebase.firestore.QuerySnapshot;
import com.google.firebase.storage.FirebaseStorage;

import java.util.ArrayList;
import java.util.List;

public class MyGallery extends AppCompatActivity implements DisplayImage.OnFragmentInteractionListener {

    GridView gridView;
    FirebaseStorage firebaseStorage;
    FirebaseFirestore db;
    TextView NoImage;
    ImageAdapter imageAdapter;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
PATH: app/src/main/java/com/example/mysafety/MyGallery.java
LINES: 42-67

super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_my_gallery);

        firebaseStorage=FirebaseStorage.getInstance();
        db=FirebaseFirestore.getInstance();

        NoImage=findViewById(R.id.myerror);
        NoImage.setVisibility(View.GONE);

        gridView=findViewById(R.id.mygridview);

        SharedPreferences sharedPreferences=this.getSharedPreferences("Userdetails",MODE_PRIVATE);
        String user=sharedPreferences.getString("User","");//getting value fron shared preference which was stored while logging in

        db.collection("Images").whereEqualTo("User",user).get()//querying in firebase by username
                .addOnCompleteListener(new OnCompleteListener<QuerySnapshot>() {
                    @Override
                    public void onComplete(@NonNull Task<QuerySnapshot> task) {
                        List<String> images=new ArrayList<>();
                        if(task.isSuccessful()){
                            for(QueryDocumentSnapshot document: task.getResult()){
                                String path=document.getString("Time")+" "+document.getString("Image");
                                images.add(path.trim());
                            }
                            imageAdapter=new ImageAdapter(getApplicationContext(),images);
                            gridView.setAdapter(imageAdapter);
PATH: app/src/main/java/com/example/mysafety/MyGallery.java
LINES: 68-100

if(task.getResult().isEmpty())NoImage.setVisibility(View.VISIBLE);
                        }

                    }
                }).addOnFailureListener(new OnFailureListener() {
            @Override
            public void onFailure(@NonNull Exception e) {
                Toast.makeText(MyGallery.this,"Error Loading Photos",Toast.LENGTH_SHORT).show();
            }
        });

        gridView.setOnItemLongClickListener(new AdapterView.OnItemLongClickListener() {
            @Override
            public boolean onItemLongClick(AdapterView<?> parent, View view, int position, long id) {

                String path=imageAdapter.getItem(position);
                buildDialog(path);
                return true;
            }
        });

        gridView.setOnItemClickListener(new AdapterView.OnItemClickListener() {
            @Override
            public void onItemClick(AdapterView<?> parent, View view, int position, long id) {
                String path=imageAdapter.getItem(position);

                Bundle bundle=new Bundle();
                bundle.putString("path",path);

                FragmentManager fragmentManager = getSupportFragmentManager();
                FragmentTransaction fragmentTransaction = fragmentManager.beginTransaction();

                DisplayImage displayImage=new DisplayImage();
PATH: app/src/main/java/com/example/mysafety/MyGallery.java
LINES: 101-136

displayImage.setArguments(bundle);

                gridView.setVisibility(View.GONE);

                fragmentTransaction.add(R.id.mygalleryview,displayImage);
                fragmentTransaction.addToBackStack(null);
                fragmentTransaction.commit();
            }
        });
    }
    private void buildDialog(final String path){
        AlertDialog.Builder builder=new AlertDialog.Builder(this);
        builder.setMessage("Do you want to delete the photo?");

        builder.setPositiveButton("Yes",
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        PhotoDelete(path);
                        int index=path.lastIndexOf(' ');
                        String time=path.substring(0,index);
                        ReferenceDelete(time);
                    }
                });
        builder.setNegativeButton("No",
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        dialog.cancel();
                    }
                });
        builder.show();
    }

    private void PhotoDelete(final String path){
        firebaseStorage.getReference().child("MySafety").child(path)
PATH: app/src/main/java/com/example/mysafety/MyGallery.java
LINES: 137-173

.delete().addOnSuccessListener(new OnSuccessListener<Void>() {
            @Override
            public void onSuccess(Void aVoid) {
                Toast.makeText(MyGallery.this,"Photo deleted",Toast.LENGTH_SHORT).show();
                imageAdapter.remove(path);
            }
        }).addOnFailureListener(new OnFailureListener() {
            @Override
            public void onFailure(@NonNull Exception e) {
                Toast.makeText(MyGallery.this,"Error Deleting Photo",Toast.LENGTH_SHORT).show();
            }
        });
    }

    private void ReferenceDelete(String time){
            db.collection("Images").document(time)
                    .delete()
                    .addOnSuccessListener(new OnSuccessListener<Void>() {
                        @Override
                        public void onSuccess(Void aVoid) {
                            Log.d("TAG","Document deleted");
                        }
                    }).addOnFailureListener(new OnFailureListener() {
                @Override
                public void onFailure(@NonNull Exception e) {
                    Log.w("TAG", "Error deleting document", e);
                }
            });
    }

    @Override
    public void onFragmentInteraction(Uri uri) {

    }

    @Override
    public void onBackPressed(){
PATH: app/src/main/java/com/example/mysafety/MyGallery.java
LINES: 174-182

int fragments=getSupportFragmentManager().getBackStackEntryCount();
        if(fragments==1){
            getSupportFragmentManager().popBackStack();
            gridView.setVisibility(View.VISIBLE);
        }
        else
            super.onBackPressed();
    }
}
PATH: app/src/main/java/com/example/mysafety/SignUp.java
LINES: 1-43

package com.example.mysafety;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.content.SharedPreferences;
import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.Toast;

import com.google.android.gms.tasks.OnCompleteListener;
import com.google.android.gms.tasks.Task;
import com.google.firebase.auth.AuthResult;
import com.google.firebase.auth.FirebaseAuth;
import com.google.firebase.auth.FirebaseUser;

public class SignUp extends AppCompatActivity {

    FirebaseAuth mAuth;
    EditText memail,mpassword,mconfirmpw;
    String email,password,confirmpw;
    Button createuser;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_sign_up);

        memail=findViewById(R.id.email1);
        mpassword=findViewById(R.id.password1);
        mconfirmpw=findViewById(R.id.confirmpw);

        createuser=findViewById(R.id.create);
        mAuth=FirebaseAuth.getInstance();

        createuser.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                email=memail.getText().toString().trim();
PATH: app/src/main/java/com/example/mysafety/SignUp.java
LINES: 44-75

password=mpassword.getText().toString();
                confirmpw=mconfirmpw.getText().toString();

                if(email.trim().isEmpty()) {
                    Toast.makeText(getApplicationContext(), "Enter email", Toast.LENGTH_SHORT).show();
                    return;
                }
                if(!email.trim().isEmpty() && password.trim().isEmpty()) {
                    Toast.makeText(getApplicationContext(), "Enter password", Toast.LENGTH_SHORT).show();
                    return;
                }
                if(!confirmpw.trim().equals(password.trim())) {
                    Toast.makeText(getApplicationContext(), "Password and Confirm password not same", Toast.LENGTH_SHORT).show();
                    return;
                }

                SharedPreferences sharedPreferences=SignUp.this.getSharedPreferences("Userdetails",MODE_PRIVATE);
                SharedPreferences.Editor editor=sharedPreferences.edit();
                editor.putString("User",email);
                editor.apply();

                email=email.trim()+"@mysafety.com";

                firebasecreateuser();
            }
        });
    }

    private void firebasecreateuser() {
        mAuth.createUserWithEmailAndPassword(email, password)
                .addOnCompleteListener(this, new OnCompleteListener<AuthResult>() {
                    @Override
PATH: app/src/main/java/com/example/mysafety/SignUp.java
LINES: 76-95

public void onComplete(@NonNull Task<AuthResult> task) {
                        if (task.isSuccessful()) {
                            // Sign in success, update UI with the signed-in user's information
                            Log.d("TAG", "createUserWithEmail:success");
                            FirebaseUser user = mAuth.getCurrentUser();
                            Toast.makeText(getApplicationContext(),"Account successfully created!!",Toast.LENGTH_SHORT).show();
                        }
                         else {
                            // If sign in fails, display a message to the user.
                            Log.w("TAG", "createUserWithEmail:failure", task.getException());
                            Toast.makeText(getApplicationContext(), "Invalid Email/Password",
                                    Toast.LENGTH_SHORT).show();
                        }


                    }
                });//creating user and subsequently logging in
    }

}
PATH: app/src/main/java/com/example/mysafety/SimpleFragmentPagerAdapter.java
LINES: 1-30

package com.example.mysafety;

import android.content.Context;

import androidx.fragment.app.Fragment;
import androidx.fragment.app.FragmentManager;
import androidx.fragment.app.FragmentPagerAdapter;

class SimpleFragmentPagerAdapter extends FragmentPagerAdapter {

    private Context context;
    private String tabtitles[]=new String[]{"Electrical","Instrumentation","Mechanical","General"};
    public SimpleFragmentPagerAdapter(FragmentManager fm,Context context) {
        super(fm);
        this.context=context;
    }

    @Override
    public Fragment getItem(int position) {
        return Categories.newInstance(tabtitles[position]);
    }

    @Override
    public int getCount() {
        return tabtitles.length;
    }
    public CharSequence getPageTitle(int position){
        return tabtitles[position];
    }
}
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 1-43

package com.example.mysafety;

import android.Manifest;
import android.app.AlertDialog;
import android.content.ActivityNotFoundException;
import android.content.Context;
import android.content.DialogInterface;
import android.content.Intent;
import android.content.SharedPreferences;
import android.content.pm.PackageManager;
import android.graphics.Bitmap;
import android.net.Uri;
import android.provider.MediaStore;
import android.speech.RecognizerIntent;

import androidx.annotation.NonNull;
import androidx.appcompat.app.AppCompatActivity;
import androidx.core.app.ActivityCompat;
import androidx.core.content.ContextCompat;

import android.os.Bundle;
import android.util.Log;
import android.view.View;
import android.widget.Button;
import android.widget.EditText;
import android.widget.ImageButton;
import android.widget.Toast;

import com.google.android.gms.tasks.OnFailureListener;
import com.google.android.gms.tasks.OnSuccessListener;
import com.google.firebase.firestore.FirebaseFirestore;
import com.google.firebase.firestore.SetOptions;
import com.google.firebase.storage.FirebaseStorage;
import com.google.firebase.storage.StorageReference;

import java.io.ByteArrayOutputStream;
import java.text.SimpleDateFormat;
import java.util.ArrayList;
import java.util.Calendar;
import java.util.Date;
import java.util.HashMap;
import java.util.Locale;
import java.util.Map;
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 44-84

import java.util.TimeZone;

public class SpeechtoText extends AppCompatActivity {

    static private EditText txtSpeechInput;
    private ImageButton btnSpeak;
    private final int REQ_CODE_SPEECH_INPUT=100;
    private final int REQ_IMAGE_CAPTURE=1;
    private final int REQ_PHOTO_PICKER=2;
    Button next,photo,gallery;
    static AlertDialog.Builder dialog,options;
    FirebaseFirestore db;
    FirebaseStorage firebaseStorage;
    String department;
    EditText label;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_speechto_text);

        txtSpeechInput = (EditText) findViewById(R.id.txtSpeechInput);//edittext to write complaint
        btnSpeak = (ImageButton) findViewById(R.id.btnSpeak);
        next=findViewById(R.id.next);
        db=FirebaseFirestore.getInstance();
        firebaseStorage=FirebaseStorage.getInstance();
        department="";

        btnSpeak.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                promptSpeechInput();
            }
        });

        next=findViewById(R.id.next);
        photo=findViewById(R.id.photo);
        gallery=findViewById(R.id.gallery);

        final String[] Options={"Electrical","Instrumentation","Mechanical","General"};
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 85-118

options=new AlertDialog.Builder(this);
        options.setTitle("Choose a category").setCancelable(true);
        options.setItems(Options, new DialogInterface.OnClickListener() {
            @Override
            public void onClick(DialogInterface dialog, int which) {
                department=Options[which];
                showDialog();
            }
        });//dialog to show options for department

        next.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                if(txtSpeechInput.getText().toString().isEmpty())
                    Toast.makeText(getApplicationContext(),"No text entered",Toast.LENGTH_SHORT).show();
                else
                options.show();
            }
        });

        photo.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Takepicture();
            }
        });

        gallery.setOnClickListener(new View.OnClickListener() {
            @Override
            public void onClick(View v) {
                Intent intent = new Intent(Intent.ACTION_GET_CONTENT);
                intent.setType("image/jpeg");
                intent.putExtra(Intent.EXTRA_LOCAL_ONLY, true);
                startActivityForResult(Intent.createChooser(intent, "Complete action using"), REQ_PHOTO_PICKER);
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 119-150

}
        });

    }

    private void promptSpeechInput() {
        Intent intent = new Intent(RecognizerIntent.ACTION_RECOGNIZE_SPEECH);
        intent.putExtra(RecognizerIntent.EXTRA_LANGUAGE_MODEL,
                RecognizerIntent.LANGUAGE_MODEL_FREE_FORM);
        intent.putExtra(RecognizerIntent.EXTRA_LANGUAGE, Locale.getDefault());
        intent.putExtra(RecognizerIntent.EXTRA_PROMPT,
                getString(R.string.speech_prompt));
        try {
            startActivityForResult(intent, REQ_CODE_SPEECH_INPUT);//starting the speechtotext activity
        } catch (ActivityNotFoundException a) {
            Toast.makeText(getApplicationContext(),
                    getString(R.string.speech_not_supported),
                    Toast.LENGTH_SHORT).show();
        }
    }

    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent data) {
        super.onActivityResult(requestCode, resultCode, data);

        TimeZone tz=TimeZone.getTimeZone("Asia/Kolkata");
        Calendar calendar=Calendar.getInstance(tz);
        SimpleDateFormat mdformat = new SimpleDateFormat("dd-MM-yyyy", Locale.getDefault());
        SimpleDateFormat timeformat=new SimpleDateFormat("HH:mm:ss",Locale.getDefault());

        String strDate = mdformat.format(calendar.getTime());
        String strTime=timeformat.format(calendar.getTime());
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 151-173

String finaldate=strDate+" "+strTime;//storing current date and time in finaldate

        switch (requestCode) {
            case REQ_CODE_SPEECH_INPUT: {//for voice to text activity
                if (resultCode == RESULT_OK && null != data) {

                    ArrayList<String> result = data
                            .getStringArrayListExtra(RecognizerIntent.EXTRA_RESULTS);//getting text from voice input
                    txtSpeechInput.setText(result.get(0));//storing it in the edittext
                }
                break;
            }

            case REQ_IMAGE_CAPTURE:{//for camera activity
                if (resultCode == RESULT_OK) {
                    Bundle extras = data.getExtras();
                    Bitmap imageBitmap = (Bitmap) extras.get("data");
                    if (ContextCompat.checkSelfPermission(this, Manifest.permission.WRITE_EXTERNAL_STORAGE)
                            != PackageManager.PERMISSION_GRANTED) {
                        if (ActivityCompat.shouldShowRequestPermissionRationale(this,
                                Manifest.permission.READ_CONTACTS)) {
                            // Show an explanation to the user *asynchronously* -- don't block
                            // this thread waiting for the user's response! After the user
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 174-195

// sees the explanation, try again to request the permission.
                        } else {
                            // No explanation needed; request the permission
                            ActivityCompat.requestPermissions(this,
                                    new String[]{Manifest.permission.READ_CONTACTS},
                                    0);

                            // MY_PERMISSIONS_REQUEST_READ_CONTACTS is an
                            // app-defined int constant. The callback method gets the
                            // result of the request.
                        }
                    }
                    Log.d("TAG",""+imageBitmap);
                    try{
                    Uri imageuri=getImageUri(this,imageBitmap);//getting image URI of image clicked
                    StorageReference storageReference=firebaseStorage.getReference().child("MySafety");
                    String path=finaldate.trim()+" "+imageuri.getLastPathSegment().trim();

                        StorageReference childreference = storageReference.child(path);
                        childreference.putFile(imageuri);//storing image in firebase storage
                        addPhoto(imageuri.getLastPathSegment(),finaldate);
                        Toast.makeText(SpeechtoText.this, "Photo Successfully Uploaded", Toast.LENGTH_SHORT).show();
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 196-226

}catch (NullPointerException e){
                        Toast.makeText(SpeechtoText.this,"Error in uploading photo",Toast.LENGTH_SHORT).show();
                    }

                }
                break;
            }

            case REQ_PHOTO_PICKER:{//for photo picker
                if(resultCode==RESULT_OK) {
                    Uri imageuri = data.getData();
                    StorageReference storageReference = firebaseStorage.getReference().child("MySafety");
                    String path=finaldate.trim()+" "+imageuri.getLastPathSegment().trim();
                    try {
                        StorageReference childreference = storageReference.child(path);
                        childreference.putFile(imageuri);
                        addPhoto(imageuri.getLastPathSegment(),finaldate);
                        Toast.makeText(SpeechtoText.this, "Photo Successfully Uploaded", Toast.LENGTH_SHORT).show();
                    } catch (NullPointerException e) {
                        Toast.makeText(SpeechtoText.this, "Error in uploading photo", Toast.LENGTH_SHORT).show();
                    }
                    break;
                }
            }
        }
    }

    public void showDialog(){
        dialog = new AlertDialog.Builder(this);
        dialog.setMessage("Any other complaint?").setCancelable(true);
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 227-256

dialog.setPositiveButton("Yes",
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        addComplaint(txtSpeechInput.getText().toString().trim());
                        txtSpeechInput.setText("");
                        dialog.cancel();
                    }
                });

        dialog.setNegativeButton("No",
                new DialogInterface.OnClickListener() {
                    @Override
                    public void onClick(DialogInterface dialog, int which) {
                        Intent intent=new Intent(SpeechtoText.this,MainPage.class);
                        intent.setFlags(Intent.FLAG_ACTIVITY_CLEAR_TOP);
                        startActivity(intent);
                        addComplaint(txtSpeechInput.getText().toString().trim());
                        txtSpeechInput.setText("");
                        dialog.cancel();
                        finish();
                    }
                });
        dialog.show();
    }

    public void addComplaint(String complaint){
        TimeZone tz=TimeZone.getTimeZone("Asia/Kolkata");
        Calendar calendar=Calendar.getInstance(tz);
        SimpleDateFormat mdformat = new SimpleDateFormat("dd-MM-yyyy", Locale.getDefault());
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 257-285

SimpleDateFormat timeformat=new SimpleDateFormat("HH:mm:ss",Locale.getDefault());

        String strDate = mdformat.format(calendar.getTime());
        String strTime=timeformat.format(calendar.getTime());
        String finaldate=strDate+" "+strTime;

        SharedPreferences sharedPreferences=this.getSharedPreferences("Userdetails",MODE_PRIVATE);
        String user=sharedPreferences.getString("User","");

        Map<String,Object> details=new HashMap<>();
        details.put("User",user);
        details.put("Date",strDate);
        details.put("Complaint",complaint);
        details.put("Time",finaldate);
        details.put("Department",department);

        db.collection(getString(R.string.complaint)).document(finaldate)
                .set(details, SetOptions.merge())
                .addOnSuccessListener(new OnSuccessListener<Void>() {
                    @Override
                    public void onSuccess(Void aVoid) {
                        Log.d("TAG", "Complaint added with ID: " );
                        Toast.makeText(SpeechtoText.this,"Complaint registered",Toast.LENGTH_SHORT).show();
                    }

                }).addOnFailureListener(new OnFailureListener() {
            @Override
            public void onFailure(@NonNull Exception e) {
                Log.w("TAG", "Error adding document", e);
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 286-323

Toast.makeText(SpeechtoText.this,"Unable to register complaint",Toast.LENGTH_SHORT).show();
            }
        });

    }

    public void addPhoto(final String url,final String finaldate){

        SharedPreferences sharedPreferences=this.getSharedPreferences("Userdetails",MODE_PRIVATE);
        final String user=sharedPreferences.getString("User","");

        label = new EditText(SpeechtoText.this);
        label.setHint("Enter Label");

        AlertDialog.Builder alert=new AlertDialog.Builder(SpeechtoText.this);
        alert.setTitle("Label");
        alert.setView(label);
        alert.setCancelable(false);

        alert.setPositiveButton("Ok", new DialogInterface.OnClickListener() {
            @Override
            public void onClick(DialogInterface dialog, int which) {
                Map<String,Object> details=new HashMap<>();
                details.put("Image",url.trim());
                details.put("User",user);
                details.put("Time",finaldate);
                details.put("Label",label.getText().toString().trim());

                db.collection("Images").document(finaldate)
                        .set(details,SetOptions.merge());
            }
        });//storing photo related details in firebase

        alert.show();
        return;
    }

    public void Takepicture() {
PATH: app/src/main/java/com/example/mysafety/SpeechtoText.java
LINES: 324-344

PackageManager pm = this.getPackageManager();

        if (pm.hasSystemFeature(PackageManager.FEATURE_CAMERA)) {
            Intent takePictureIntent = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
            if (takePictureIntent.resolveActivity(getPackageManager()) != null) {//checking if phone has camera start the activity for getting picture
                startActivityForResult(takePictureIntent, REQ_IMAGE_CAPTURE);
            }
        }
        else
            Toast.makeText(this,"Camera unavailable",Toast.LENGTH_SHORT).show();
    }


    private Uri getImageUri(Context context, Bitmap inImage) {//obtaining image uri from its bitmap
        ByteArrayOutputStream bytes = new ByteArrayOutputStream();
        inImage.compress(Bitmap.CompressFormat.JPEG, 100, bytes);
        String path = MediaStore.Images.Media.insertImage(context.getContentResolver(), inImage, "Title", null);
        return Uri.parse(path);
    }

}
PATH: app/src/main/java/com/example/mysafety/SplashActivity.java
LINES: 1-25

package com.example.mysafety;

import androidx.appcompat.app.AppCompatActivity;

import android.content.Intent;
import android.os.Bundle;
import android.os.Handler;

public class SplashActivity extends AppCompatActivity {

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_splash);

        new Handler().postDelayed(new Runnable() {
            @Override
            public void run() {
                startActivity(new Intent(SplashActivity.this,MainActivity.class));
                finish();
            }
        },2000);//the splash activity(first screen that appears when the app is opened) being displayed for 2 seconds

    }
}
PATH: app/src/test/java/com/example/mysafety/ExampleUnitTest.java
LINES: 1-17

package com.example.mysafety;

import org.junit.Test;

import static org.junit.Assert.*;

/**
 * Example local unit test, which will execute on the development machine (host).
 *
 * @see <a href="http://d.android.com/tools/testing">Testing documentation</a>
 */
public class ExampleUnitTest {
    @Test
    public void addition_isCorrect() {
        assertEquals(4, 2 + 2);
    }
}
```python
import numpy as np
import logging
import os
import time
import numpy as np
import glob
import sys
import math
import random

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
from nltk import ngrams
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from collections import Counter
import editdistance
```

```python
CUDA = torch.cuda.is_available()
CUDA #Boolean variable to check the presence of CUDA
```

```python
class CorpusSearcher(object):
    """
      To create corpuses and retreive attributes closest to the given query.

    """

    def __init__(self, query_corpus, key_corpus, value_corpus, vectorizer, make_binary=True):
        self.vectorizer = vectorizer
        self.vectorizer.fit(key_corpus)

        self.query_corpus = query_corpus
        self.key_corpus = key_corpus
        self.value_corpus = value_corpus
        
        # rows = docs, cols = features
        self.key_corpus_matrix = self.vectorizer.transform(key_corpus)
        if make_binary:
            # make binary
            self.key_corpus_matrix = (self.key_corpus_matrix != 0).astype(int)

        
    def most_similar(self, key_idx, n=10):
        """
# make binary
            self.key_corpus_matrix = (self.key_corpus_matrix != 0).astype(int)

        
    def most_similar(self, key_idx, n=10):
        """ 
          Score the query against the key corpus and return the values corresponding to the 
          top N scores from the value corpus.
          Used for retrieving attributes from sentences with similar content to the query sentence.

          Parameter:
          - key_idx : id of the attribute in the query corpus
          - n : number of closest attributes to be returned

          Returns:
          - selected : list of closest N values in the value corpus with their index and score 

        """

        query = self.query_corpus[key_idx]
        query_vec = self.vectorizer.transform([query])

        scores = np.dot(self.key_corpus_matrix, query_vec.T)
        scores = np.squeeze(scores.toarray())
        scores_indices = zip(scores, range(len(scores)))
        selected = sorted(scores_indices, reverse=True)[:n]

        # use the retrieved index 'i' to pick examples from the VALUE corpus
        selected = [ (self.value_corpus[i], i, score) for (score, i) in selected ]

        return selected
```

```python
def build_vocab_maps(vocab_file):
    """
      Creates and returns two dictionaries, one to map vocabulary words to unique ids and
```

```python
def build_vocab_maps(vocab_file):
    """
      Creates and returns two dictionaries, one to map vocabulary words to unique ids and 
      one to map the unique ids back to the vocabulary words.
      
      Parameters: 
      - vocab_file : os path to vocabulary file
      
      Returns:
      - tok_to_id : dictionary which fetches id from token key
      - id_to_tok : dictionary which fetches token from id key

    """

    assert os.path.exists(vocab_file), "The vocab file %s does not exist" % vocab_file
    unk = '<unk>'
    pad = '<pad>'
    sos = '<s>'
    eos = '</s>'

    lines = [x.strip() for x in open(vocab_file)]

    assert lines[0] == unk and lines[1] == pad and lines[2] == sos and lines[3] == eos, \
        "The first words in %s are not %s, %s, %s, %s" % (vocab_file, unk, pad, sos, eos)

    tok_to_id = {}
    id_to_tok = {}
    for i, vi in enumerate(lines):
        tok_to_id[vi] = i
        id_to_tok[i] = vi

    # appending an extra vocab item for empty attribute lines
    empty_tok_idx =  len(id_to_tok)
    tok_to_id['<empty>'] = empty_tok_idx
    id_to_tok[empty_tok_idx] = '<empty>'

    return tok_to_id, id_to_tok
```

```python
def extract_attributes(line, attribute_vocab, use_ngrams=False):
    """
      Split the given sentence into its attribute markers and attribute-independent content.
      This is the 'DELETE' process in the paper
```python
def extract_attributes(line, attribute_vocab, use_ngrams=False):
    """
      Split the given sentence into its attribute markers and attribute-independent content.
      This is the 'DELETE' process in the paper

      Parameters:
      - line: the given sentence
      - attribute_vocab: the complete vocabulary of attributes
      - use_ngrams: boolean, True if ngrams of the sentence should be checked instead of individual words

      Returns:
      - line
      - content : attribute-independent content remaining after attributes are deleted
      - attribute_markers : list of attributes from the given sentence

    """

    if use_ngrams:
        # generate all ngrams for the sentence
        grams = []
        for i in range(1, 5):
            try:
                i_grams = [ " ".join(gram) for gram in ngrams(line, i) ]
                grams.extend(i_grams)
            except RuntimeError:
                continue

        # filter ngrams by whether they appear in the attribute_vocab
        candidate_markers = [ (gram, attribute_vocab[gram]) for gram in grams if gram in attribute_vocab ]

        # sort attribute markers by score and prepare for 'deletion'
        content = " ".join(line)
        candidate_markers.sort(key=lambda x: x[1], reverse=True)

        candidate_markers = [marker for (marker, score) in candidate_markers]
# sort attribute markers by score and prepare for 'deletion'
        content = " ".join(line)
        candidate_markers.sort(key=lambda x: x[1], reverse=True)

        candidate_markers = [marker for (marker, score) in candidate_markers]
        
        # seperate attributes and attribute-independent content
        attribute_markers = []
        for marker in candidate_markers:
            if marker in content:
                attribute_markers.append(marker)
                content = content.replace(marker, "")
        content = content.split()
        
    else:
        # same thing, but without the use of ngrams
        content = []
        attribute_markers = []
        for tok in line:
            if tok in attribute_vocab:
                attribute_markers.append(tok)
            else:
                content.append(tok)

    return line, content, attribute_markers
```

```python
def read_nmt_data(src, config, tgt, attribute_vocab, train_src=None, train_tgt=None, ngram_attributes=False):
    """
      Initializer function to read data from files and store it in 'src' and 'tgt'

      Parameters:
      - src, tgt : os paths to files containging source and target sentences respectively
      - config : contains path to vocab files
      - attribute_vocab : path to attribute vocabulary 
      - train_src, train_tgt :
Parameters:
      - src, tgt : os paths to files containging source and target sentences respectively
      - config : contains path to vocab files
      - attribute_vocab : path to attribute vocabulary 
      - train_src, train_tgt : 
      - ngram_attributes : boolean, if True then attributes are ngrams instead of direct mappings

      Returns:
      - src, tgt: dictionaries containing source and target date respectively

    """
    
    if ngram_attributes:
        # read attribute vocab as a dictionary mapping attributes to scores
        pre_attr = {}
        post_attr = {}
        with open(attribute_vocab) as attr_file:
            next(attr_file) # skip header
            for line in attr_file:
                parts = line.strip().split()
                pre_salience = float(parts[-2])
                post_salience = float(parts[-1])
                attr = ' '.join(parts[:-2])
                pre_attr[attr] = pre_salience
                post_attr[attr] = post_salience
    else:
        pre_attr = post_attr = set([x.strip() for x in open(attribute_vocab)])

    src_lines = [l.strip().lower().split() for l in open(src, 'r')]
    src_lines, src_content, src_attribute = list(zip(*[extract_attributes(line, pre_attr, pre_attr) for line in src_lines]))
    src_tok2id, src_id2tok = build_vocab_maps(config['data']['src_vocab'])
pre_attr = post_attr = set([x.strip() for x in open(attribute_vocab)])

    src_lines = [l.strip().lower().split() for l in open(src, 'r')]
    src_lines, src_content, src_attribute = list(zip(*[extract_attributes(line, pre_attr, pre_attr) for line in src_lines]))
    src_tok2id, src_id2tok = build_vocab_maps(config['data']['src_vocab'])

    # during train time, just pick attributes that are close to the current (using word distance)
    # we don't need to do the TFIDF thing with the source because test is strictly in the src => tgt direction. 
    # But we still measure both src and tgt dist because training is bidirectional
    # (i.e., we're autoencoding src and tgt sentences during training)

    src_dist_measurer = CorpusSearcher(
        query_corpus=[' '.join(x) for x in src_attribute],
        key_corpus=[' '.join(x) for x in src_attribute],
        value_corpus=[' '.join(x) for x in src_attribute],
        vectorizer=CountVectorizer(vocabulary=src_tok2id),
        make_binary=True
    )
    src = {
        'data': src_lines, 'content': src_content, 'attribute': src_attribute,
        'tok2id': src_tok2id, 'id2tok': src_id2tok, 'dist_measurer': src_dist_measurer
    }

    tgt_lines = [l.strip().lower().split() for l in open(tgt, 'r')] if tgt else None
src = {
        'data': src_lines, 'content': src_content, 'attribute': src_attribute,
        'tok2id': src_tok2id, 'id2tok': src_id2tok, 'dist_measurer': src_dist_measurer
    }

    tgt_lines = [l.strip().lower().split() for l in open(tgt, 'r')] if tgt else None
    tgt_lines, tgt_content, tgt_attribute = list(zip(*[extract_attributes(line, post_attr, post_attr) for line in tgt_lines]))
    tgt_tok2id, tgt_id2tok = build_vocab_maps(config['data']['tgt_vocab'])

    # during train time, just pick attributes that are close to the current (using word distance)
    # since this is only used to noise the inputs

    if train_src is None or train_tgt is None:
        tgt_dist_measurer = CorpusSearcher(
            query_corpus=[' '.join(x) for x in tgt_attribute],
            key_corpus=[' '.join(x) for x in tgt_attribute],
            value_corpus=[' '.join(x) for x in tgt_attribute],
            vectorizer=CountVectorizer(vocabulary=tgt_tok2id),
            make_binary=True
        )

    # during test time, scan through train content (using tfidf) and retrieve corresponding attributes
    
    else:
        tgt_dist_measurer = CorpusSearcher(
            query_corpus=[' '.join(x) for x in src_content],
            key_corpus=[' '.join(x) for x in train_tgt['content']],
            value_corpus=[' '.join(x) for x in train_tgt['attribute']],
else:
        tgt_dist_measurer = CorpusSearcher(
            query_corpus=[' '.join(x) for x in src_content],
            key_corpus=[' '.join(x) for x in train_tgt['content']],
            value_corpus=[' '.join(x) for x in train_tgt['attribute']],
            vectorizer=TfidfVectorizer(vocabulary=tgt_tok2id),
            make_binary=False
        )
    tgt = {
        'data': tgt_lines, 'content': tgt_content, 'attribute': tgt_attribute,
        'tok2id': tgt_tok2id, 'id2tok': tgt_id2tok, 'dist_measurer': tgt_dist_measurer
    }

    return src, tgt
```

```python
def sample_replace(lines, dist_measurer, sample_rate, corpus_idx):
    """
      Replace sample_rate * batch_size lines with nearby examples (according to dist_measurer).
      This is not exactly the same as the paper (words are shared during train) but its essentially the same idea and easier to implement.

      Parameters:
      - lines : list of sentences
      - dist_measurer : object of CorpusSearcher
      - sample_rate : percentage of samples to be replaced with examples close to given one
      - corpus_idx : given sample

      Returns:
      - out: list of sentences after changing

    """

    out = [None for _ in range(len(lines))]
    for i, line in enumerate(lines):
        if random.random() < sample_rate:
            # ignore first line since top match is the current line
"""

    out = [None for _ in range(len(lines))]
    for i, line in enumerate(lines):
        if random.random() < sample_rate:
            # ignore first line since top match is the current line
            sims = dist_measurer.most_similar(corpus_idx + i)[1:]
            
            try:
                line = next( (
                    tgt_attr.split() for tgt_attr, _, _ in sims
                    if set(tgt_attr.split()) != set(line[1:-1]) # and tgt_attr != ''
                ) )
            # all the matches are blanks
            except StopIteration:
                line = []
            line = ['<s>'] + line + ['</s>']

        # special empty token for empty sequences (just start/end tok)
        if len(line) == 2:
            line.insert(1, '<empty>')
        out[i] = line

    return out
```

```python
def get_minibatch(lines, tok2id, index, batch_size, max_len, sort=False, idx=None, dist_measurer=None, sample_rate=0.0):
    """
      To prepare minibatch. No sorting since we care about the order of outputs. Also acts as a helper function to implement 'Retrieve' 

      Parameters:
      - lines : list of sentences
      - tok2id : dictionary which fetches id from token key
      - index : list of indices for which output is wanted
      - batch_size : size of the minibatch
      - max_len : maximum allowed length for sentences
Parameters:
      - lines : list of sentences
      - tok2id : dictionary which fetches id from token key
      - index : list of indices for which output is wanted
      - batch_size : size of the minibatch
      - max_len : maximum allowed length for sentences
      - sort : boolean, False by default
      - idx : optional; if passed the fn will only return values for those ids
      - dist_measurer : distance measuring function, used for sample_replace
      - sample rate : between 0 and 1

      Returns:
      - input_lines : tokenized input lines
      - output_lines : tokenized output lines
      - lens : list of lengths of sentences
      - mask : 0/1 masking for each sentence
      - idx : idx
    
    """

    lines = [
        ['<s>'] + line[:max_len] + ['</s>']
        for line in lines[index:index + batch_size]
    ]

    if dist_measurer is not None:
        lines = sample_replace(lines, dist_measurer, sample_rate, index)

    lens = [len(line) - 1 for line in lines]
    max_len = max(lens)

    unk_id = tok2id['<unk>']
    input_lines = [
        [tok2id.get(w, unk_id) for w in line[:-1]] +
        [tok2id['<pad>']] * (max_len - len(line) + 1)
        for line in lines
    ]

    output_lines = [
        [tok2id.get(w, unk_id) for w in line[1:]] +
        [tok2id['<pad>']] * (max_len - len(line) + 1)
        for line in lines
    ]

    mask = [
[tok2id.get(w, unk_id) for w in line[1:]] +
        [tok2id['<pad>']] * (max_len - len(line) + 1)
        for line in lines
    ]

    mask = [
        ([1] * l) + ([0] * (max_len - l))
        for l in lens
    ]

    if sort:
        idx = [x[0] for x in sorted(enumerate(lens), key=lambda x: -x[1])]

    if idx is not None:
        lens = [lens[j] for j in idx]
        input_lines = [input_lines[j] for j in idx]
        output_lines = [output_lines[j] for j in idx]
        mask = [mask[j] for j in idx]

    input_lines = Variable(torch.LongTensor(input_lines))
    output_lines = Variable(torch.LongTensor(output_lines))
    mask = Variable(torch.FloatTensor(mask))

    if CUDA:
        input_lines = input_lines.cuda()
        output_lines = output_lines.cuda()
        mask = mask.cuda()

    return input_lines, output_lines, lens, mask, idx
```

```python
def minibatch(src, tgt, idx, batch_size, max_len, model_type, is_test=False):
    """
      Fetches the inputs, outputs and attributes using get_minibatch depending on model type.

      Parameters:
      - src, tgt : source and target data dictionaries
      - idx : list of indices for which output is wanted
      - batch_size : size of the minibatch
      - max_len : maximum allowed length for each sentence
      - model_type: determines which model is followed:
          - 'delete' for DeleleOnly
- src, tgt : source and target data dictionaries
      - idx : list of indices for which output is wanted
      - batch_size : size of the minibatch
      - max_len : maximum allowed length for each sentence
      - model_type: determines which model is followed:
          - 'delete' for DeleleOnly
          - 'delete_retrieve' for DeleteAndRetrieve
          - 'seq2seq' for TemplateBased

      Returns:
      - inputs : input_lines from get_minibatch
      - outputs : output_lines from get_minibatch
      - attributes : attributes generated

    """
    if not is_test:
        use_src = random.random() < 0.5
        in_dataset = src if use_src else tgt
        out_dataset = in_dataset
        attribute_id = 0 if use_src else 1
    else:
        in_dataset = src
        out_dataset = tgt
        attribute_id = 1

    if model_type == 'delete':
        inputs = get_minibatch(in_dataset['content'], in_dataset['tok2id'], idx, batch_size, max_len, sort=True)
        outputs = get_minibatch(out_dataset['data'], out_dataset['tok2id'], idx, batch_size, max_len, idx=inputs[-1])

        # since true length could be less than batch_size at end of data
        batch_len = len(outputs[0])
        attribute_ids = [attribute_id for _ in range(batch_len)]
        attribute_ids = Variable(torch.LongTensor(attribute_ids))
        if CUDA:
            attribute_ids = attribute_ids.cuda()
# since true length could be less than batch_size at end of data
        batch_len = len(outputs[0])
        attribute_ids = [attribute_id for _ in range(batch_len)]
        attribute_ids = Variable(torch.LongTensor(attribute_ids))
        if CUDA:
            attribute_ids = attribute_ids.cuda()

        attributes = (attribute_ids, None, None, None, None)

    elif model_type == 'delete_retrieve':
        inputs =  get_minibatch(in_dataset['content'], in_dataset['tok2id'], idx, batch_size, max_len, sort=True)
        outputs = get_minibatch(out_dataset['data'], out_dataset['tok2id'], idx, batch_size, max_len, idx=inputs[-1])

        if is_test:
            # This dist_measurer has sentence attributes for values, so setting 
            # the sample rate to 1 means the output is always replaced with an
            # attribute. So we're still getting attributes even though
            # the method is being fed content. 
            attributes =  get_minibatch(
                in_dataset['content'], out_dataset['tok2id'], idx, 
                batch_size, max_len, idx=inputs[-1],
                dist_measurer=out_dataset['dist_measurer'], sample_rate=1.0)
        else:
            attributes =  get_minibatch(
                out_dataset['attribute'], out_dataset['tok2id'], idx, 
                batch_size, max_len, idx=inputs[-1],
batch_size, max_len, idx=inputs[-1],
                dist_measurer=out_dataset['dist_measurer'], sample_rate=1.0)
        else:
            attributes =  get_minibatch(
                out_dataset['attribute'], out_dataset['tok2id'], idx, 
                batch_size, max_len, idx=inputs[-1],
                dist_measurer=out_dataset['dist_measurer'], sample_rate=0.1)
            
        attributes = (None, None, None, None, None)

    else:
        raise Exception('Unsupported model_type: %s' % model_type)

    return inputs, attributes, outputs


def unsort(arr, idx):
    """
      Unsort a list given a list of each element's original index
    """
    unsorted_arr = arr[:]
    for i, origin in enumerate(idx):
        unsorted_arr[origin] = arr[i]
    return unsorted_arr
```

```python
#Encoder
class Encoder(nn.Module):
    """ 
      Bi-directional LSTM to encode sentence+attributes.
    """

    def __init__(self, emb_dim, hidden_dim, layers, bidirectional, dropout, pack=True):
        super(Encoder, self).__init__()

        self.num_directions = 2 if bidirectional else 1

        self.lstm = nn.LSTM(
            emb_dim,
            hidden_dim // self.num_directions,
            layers,
            bidirectional=bidirectional,
            batch_first=True,
            dropout=dropout)

        self.pack = pack

    def init_state(self, input):
batch_first=True,
            dropout=dropout)

        self.pack = pack

    def init_state(self, input):
        batch_size = input.size(0) 
        h0 = Variable(torch.zeros(
            self.lstm.num_layers * self.num_directions,
            batch_size,
            self.lstm.hidden_size
        ), requires_grad=False)
        c0 = Variable(torch.zeros(
            self.lstm.num_layers * self.num_directions,
            batch_size,
            self.lstm.hidden_size
        ), requires_grad=False)

        if CUDA:
            return h0.cuda(), c0.cuda()
        else:
            return h0, c0


    def forward(self, src_embedding, srclens, srcmask, temp=1):
        h0, c0 = self.init_state(src_embedding)

        if self.pack:
            inputs = pack_padded_sequence(src_embedding, srclens, batch_first=True)
        else:
            inputs = src_embedding

        outputs, (h_final, c_final) = self.lstm(inputs, (h0, c0))

        if self.pack:
            outputs, _ = pad_packed_sequence(outputs, batch_first=True)

        return outputs, (h_final, c_final)
```

```python
#Decoders
class BilinearAttention(nn.Module):
    """ 
      Bilinear attention layer: score(H_j, q) = H_j^T W_a q (where W_a = self.in_projection)
    """

    def __init__(self, hidden):
        super(BilinearAttention, self).__init__()
""" 
      Bilinear attention layer: score(H_j, q) = H_j^T W_a q (where W_a = self.in_projection)
    """

    def __init__(self, hidden):
        super(BilinearAttention, self).__init__()
        self.in_projection = nn.Linear(hidden, hidden, bias=False)
        self.softmax = nn.Softmax()
        self.out_projection = nn.Linear(hidden * 2, hidden, bias=False)
        self.tanh = nn.Tanh()

    def forward(self, query, keys, srcmask=None, values=None):
        """
            query: [batch, hidden]
            keys: [batch, len, hidden]
            values: [batch, len, hidden] (optional, if none will = keys)

            compare query to keys, use the scores to find weighted sum of values
            if no value is specified, then values = keys
        """
        
        if values is None:
            values = keys
    
        # [Batch, Hidden, 1]
        decoder_hidden = self.in_projection(query).unsqueeze(2)
        # [Batch, Source length]
        attn_scores = torch.bmm(keys, decoder_hidden).squeeze(2)
        if srcmask is not None:
            attn_scores = attn_scores.masked_fill(srcmask, -float('inf'))
            
        attn_probs = self.softmax(attn_scores)
        # [Batch, 1, source length]
        attn_probs_transposed = attn_probs.unsqueeze(1)
        # [Batch, hidden]
        weighted_context = torch.bmm(attn_probs_transposed, values).squeeze(1)
attn_probs = self.softmax(attn_scores)
        # [Batch, 1, source length]
        attn_probs_transposed = attn_probs.unsqueeze(1)
        # [Batch, hidden]
        weighted_context = torch.bmm(attn_probs_transposed, values).squeeze(1)

        context_query_mixed = torch.cat((weighted_context, query), 1)
        context_query_mixed = self.tanh(self.out_projection(context_query_mixed))

        return weighted_context, context_query_mixed, attn_probs
```

```python
class AttentionalLSTM(nn.Module):
    """
      A LSTM cell with attention.
    """

    def __init__(self, input_dim, hidden_dim, config, attention):
        super(AttentionalLSTM, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.num_layers = 1
        self.use_attention = attention
        self.config = config
        self.cell = nn.LSTMCell(input_dim, hidden_dim)

        if self.use_attention:
            self.attention_layer = BilinearAttention(hidden_dim)


    def forward(self, input, hidden, ctx, srcmask, kb=None):
        input = input.transpose(0, 1)

        output = []
        timesteps = range(input.size(0))
        for i in timesteps:
            hy, cy = self.cell(input[i], hidden)
            if self.use_attention:
                _, h_tilde, alpha = self.attention_layer(hy, ctx, srcmask)
                hidden = h_tilde, cy
timesteps = range(input.size(0))
        for i in timesteps:
            hy, cy = self.cell(input[i], hidden)
            if self.use_attention:
                _, h_tilde, alpha = self.attention_layer(hy, ctx, srcmask)
                hidden = h_tilde, cy
                output.append(h_tilde)
            else: 
                hidden = hy, cy
                output.append(hy)

        # combine outputs, and get into [time, batch, dim]
        output = torch.cat(output, 0).view(input.size(0), *output[0].size())

        output = output.transpose(0, 1)

        return output, hidden
```

```python
class StackedAttentionLSTM(nn.Module):
    """ 
      Stacked LSTM with input feeding.
    """

    def __init__(self, cell_class=AttentionalLSTM, config=None):
        super(StackedAttentionLSTM, self).__init__()
        self.options=config['model']

        self.dropout = nn.Dropout(self.options['dropout'])

        self.layers = []
        input_dim = self.options['emb_dim']
        hidden_dim = self.options['tgt_hidden_dim']
        for i in range(self.options['tgt_layers']):
            layer = cell_class(input_dim, hidden_dim, config, config['model']['attention'])
            self.add_module('layer_%d' % i, layer)
            self.layers.append(layer)
            input_dim = hidden_dim

    def forward(self, input, hidden, ctx, srcmask, kb=None):
layer = cell_class(input_dim, hidden_dim, config, config['model']['attention'])
            self.add_module('layer_%d' % i, layer)
            self.layers.append(layer)
            input_dim = hidden_dim

    def forward(self, input, hidden, ctx, srcmask, kb=None):
        h_final, c_final = [], []
        for i, layer in enumerate(self.layers):
            output, (h_final_i, c_final_i) = layer(input, hidden, ctx, srcmask, kb)

            input = output

            if i != len(self.layers):
                input = self.dropout(input)

            h_final.append(h_final_i)
            c_final.append(c_final_i)

        h_final = torch.stack(h_final)
        c_final = torch.stack(c_final)

        return input, (h_final, c_final)
```

```python
def get_latest_ckpt(ckpt_dir):
    """
      Fetch latest checkpoint.
    """
    ckpts = glob.glob(os.path.join(ckpt_dir, '*.ckpt'))
    # if no checkpoints are found, continue with fresh parameters
    if len(ckpts) == 0:
        return -1, None
    ckpts = map(lambda ckpt: (int(ckpt.split('.')[1]), ckpt), ckpts)
    # get most recent checkpoint
    epoch, ckpt_path = sorted(ckpts)[-1]
    return epoch, ckpt_path


def attempt_load_model(model, checkpoint_dir=None, checkpoint_path=None):
    """
      Load model from latest checkpoint (get_latest_ckpt).
    """
    assert checkpoint_dir or checkpoint_path
def attempt_load_model(model, checkpoint_dir=None, checkpoint_path=None):
    """
      Load model from latest checkpoint (get_latest_ckpt).
    """
    assert checkpoint_dir or checkpoint_path

    if checkpoint_dir:
        epoch, checkpoint_path = get_latest_ckpt(checkpoint_dir)
    else:
        epoch = int(checkpoint_path.split('.')[-2])

    if checkpoint_path:
        model.load_state_dict(torch.load(checkpoint_path))
        print('Load from %s sucessful!' % checkpoint_path)
        return model, epoch + 1
    else:
        return model, 0
```

```python
class SeqModel(nn.Module):
    """
      Sequential Model
    """

    def __init__(self, src_vocab_size, tgt_vocab_size, pad_id_src, pad_id_tgt, config=None):
        super(SeqModel, self).__init__()
        self.src_vocab_size = src_vocab_size
        self.tgt_vocab_size = tgt_vocab_size
        self.pad_id_src = pad_id_src
        self.pad_id_tgt = pad_id_tgt
        self.batch_size = config['data']['batch_size']
        self.config = config
        self.options = config['model']
        self.model_type = config['model']['model_type']

        self.src_embedding = nn.Embedding(self.src_vocab_size, self.options['emb_dim'], self.pad_id_src)

        if self.config['data']['share_vocab']:
            self.tgt_embedding = self.src_embedding
        else:
            self.tgt_embedding = nn.Embedding(
self.src_embedding = nn.Embedding(self.src_vocab_size, self.options['emb_dim'], self.pad_id_src)

        if self.config['data']['share_vocab']:
            self.tgt_embedding = self.src_embedding
        else:
            self.tgt_embedding = nn.Embedding(
                self.tgt_vocab_size,
                self.options['emb_dim'],
                self.pad_id_tgt)

        if self.options['encoder'] == 'lstm':
            self.encoder = Encoder(
                self.options['emb_dim'],
                self.options['src_hidden_dim'],
                self.options['src_layers'],
                self.options['bidirectional'],
                self.options['dropout'])
            self.ctx_bridge = nn.Linear(
                self.options['src_hidden_dim'],
                self.options['tgt_hidden_dim'])

        else:
            raise NotImplementedError('unknown encoder type')
        
        if self.model_type == 'delete':
            self.attribute_embedding = nn.Embedding(num_embeddings=2, embedding_dim=self.options['emb_dim'])
            attr_size = self.options['emb_dim']

        elif self.model_type == 'delete_retrieve':
            self.attribute_encoder = Encoder(
                self.options['emb_dim'],
                self.options['src_hidden_dim'],
                self.options['src_layers'],
                self.options['bidirectional'],
elif self.model_type == 'delete_retrieve':
            self.attribute_encoder = Encoder(
                self.options['emb_dim'],
                self.options['src_hidden_dim'],
                self.options['src_layers'],
                self.options['bidirectional'],
                self.options['dropout'],
                pack=False)
            attr_size = self.options['src_hidden_dim']

        else:
            raise NotImplementedError('unknown model type')

        self.c_bridge = nn.Linear(attr_size + self.options['src_hidden_dim'], self.options['tgt_hidden_dim'])
        self.h_bridge = nn.Linear(attr_size + self.options['src_hidden_dim'], self.options['tgt_hidden_dim'])

        self.decoder = StackedAttentionLSTM(config=config)

        self.output_projection = nn.Linear(self.options['tgt_hidden_dim'], tgt_vocab_size)

        self.softmax = nn.Softmax(dim=-1)

        self.init_weights()

    def init_weights(self):
        """Initialize weights."""
        initrange = 0.1
        self.src_embedding.weight.data.uniform_(-initrange, initrange)
        self.tgt_embedding.weight.data.uniform_(-initrange, initrange)
        self.h_bridge.bias.data.fill_(0)
        self.c_bridge.bias.data.fill_(0)
        self.output_projection.bias.data.fill_(0)

    def forward(self, input_src, input_tgt, srcmask, srclens, input_attr, attrlens, attrmask):
self.tgt_embedding.weight.data.uniform_(-initrange, initrange)
        self.h_bridge.bias.data.fill_(0)
        self.c_bridge.bias.data.fill_(0)
        self.output_projection.bias.data.fill_(0)

    def forward(self, input_src, input_tgt, srcmask, srclens, input_attr, attrlens, attrmask):
        src_emb = self.src_embedding(input_src)

        srcmask = (1-srcmask).byte()

        src_outputs, (src_h_t, src_c_t) = self.encoder(src_emb, srclens, srcmask)

        if self.options['bidirectional']:
            h_t = torch.cat((src_h_t[-1], src_h_t[-2]), 1)
            c_t = torch.cat((src_c_t[-1], src_c_t[-2]), 1)
        else:
            h_t = src_h_t[-1]
            c_t = src_c_t[-1]

        src_outputs = self.ctx_bridge(src_outputs)

        if self.model_type == 'delete':
            a_ht = self.attribute_embedding(input_attr)
            c_t = torch.cat((c_t, a_ht), -1)
            h_t = torch.cat((h_t, a_ht), -1)

        elif self.model_type == 'delete_retrieve':
            attr_emb = self.src_embedding(input_attr)
            _, (a_ht, a_ct) = self.attribute_encoder(attr_emb, attrlens, attrmask)
            if self.options['bidirectional']:
                a_ht = torch.cat((a_ht[-1], a_ht[-2]), 1)
                a_ct = torch.cat((a_ct[-1], a_ct[-2]), 1)

            h_t = torch.cat((h_t, a_ht), -1)
            c_t = torch.cat((c_t, a_ct), -1)
a_ht = torch.cat((a_ht[-1], a_ht[-2]), 1)
                a_ct = torch.cat((a_ct[-1], a_ct[-2]), 1)

            h_t = torch.cat((h_t, a_ht), -1)
            c_t = torch.cat((c_t, a_ct), -1)
            
        c_t = self.c_bridge(c_t)
        h_t = self.h_bridge(h_t)

        tgt_emb = self.tgt_embedding(input_tgt)
        tgt_outputs, (_, _) = self.decoder(tgt_emb, (h_t, c_t), src_outputs, srcmask)

        tgt_outputs_reshape = tgt_outputs.contiguous().view(
            tgt_outputs.size()[0] * tgt_outputs.size()[1],
            tgt_outputs.size()[2])
        decoder_logit = self.output_projection(tgt_outputs_reshape)
        decoder_logit = decoder_logit.view(
            tgt_outputs.size()[0],
            tgt_outputs.size()[1],
            decoder_logit.size()[1])

        probs = self.softmax(decoder_logit)

        return decoder_logit, probs

    def count_params(self):
        n_params = 0
        for param in self.parameters():
            n_params += np.prod(param.data.cpu().numpy().shape)
        return n_params
```

```python
# BLEU functions

def bleu_stats(hypothesis, reference):
    """
      Compute statistics for BLEU.
    """
    
    stats = []
    stats.append(len(hypothesis))
    stats.append(len(reference))
    for n in range(1, 5):
        s_ngrams = Counter(
stats = []
    stats.append(len(hypothesis))
    stats.append(len(reference))
    for n in range(1, 5):
        s_ngrams = Counter(
            [tuple(hypothesis[i:i + n]) for i in range(len(hypothesis) + 1 - n)]
        )
        r_ngrams = Counter(
            [tuple(reference[i:i + n]) for i in range(len(reference) + 1 - n)]
        )
        stats.append(max([sum((s_ngrams & r_ngrams).values()), 0]))
        stats.append(max([len(hypothesis) + 1 - n, 0]))
    return stats

def bleu(stats):
    """
      Compute BLEU given n-gram statistics.
    """
    if len(list(filter(lambda x: x == 0, stats))) > 0:
        return 0
    (c, r) = stats[:2]
    log_bleu_prec = sum([math.log(float(x) / y) for x, y in zip(stats[2::2], stats[3::2])]) / 4.
    return math.exp(min([0, 1 - float(r) / c]) + log_bleu_prec)

def get_bleu(hypotheses, reference):
    """
      Get validation BLEU score for dev set.
    """
    stats = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])
    for hyp, ref in zip(hypotheses, reference):
        stats += np.array(bleu_stats(hyp, ref))
    return 100 * bleu(stats)

def get_edit_distance(hypotheses, reference):
    ed = 0
    for hyp, ref in zip(hypotheses, reference):
        ed += editdistance.eval(hyp, ref)

    return ed * 1.0 / len(hypotheses)
for hyp, ref in zip(hypotheses, reference):
        ed += editdistance.eval(hyp, ref)

    return ed * 1.0 / len(hypotheses)


def decode_minibatch(max_len, start_id, model, src_input, srclens, srcmask, aux_input, auxlens, auxmask):
    """ 
      Decoding minibatch
    """
    # Initialize target with <s> for every sentence
    tgt_input = Variable(torch.LongTensor([[start_id] for i in range(src_input.size(0))]))
    if CUDA:
        tgt_input = tgt_input.cuda()

    for i in range(max_len):
        # run input through the model
        decoder_logit, word_probs = model(src_input, tgt_input, srcmask, srclens, aux_input, auxmask, auxlens)
        decoder_argmax = word_probs.data.cpu().numpy().argmax(axis=-1)
        # select the predicted "next" tokens, attach to target-side inputs
        next_preds = Variable(torch.from_numpy(decoder_argmax[:, -1]))
        if CUDA:
            next_preds = next_preds.cuda()
        tgt_input = torch.cat((tgt_input, next_preds.unsqueeze(1)), dim=1)

    return tgt_input

def decode_dataset(model, src, tgt, config):
    """
      Evaluate model.
    """
    inputs = []
    preds = []
    auxs = []
    ground_truths = []

    for j in range(0, len(src['data']), config['data']['batch_size']):
        sys.stdout.write("\r%s/%s..." % (j, len(src['data'])))
        sys.stdout.flush()

        # get batch
for j in range(0, len(src['data']), config['data']['batch_size']):
        sys.stdout.write("\r%s/%s..." % (j, len(src['data'])))
        sys.stdout.flush()

        # get batch
        input_content, input_aux, output = minibatch(
            src, tgt, j, 
            config['data']['batch_size'], 
            config['data']['max_len'], 
            config['model']['model_type'],
            is_test=True)
        input_lines_src, output_lines_src, srclens, srcmask, indices = input_content
        input_ids_aux, _, auxlens, auxmask, _ = input_aux
        input_lines_tgt, output_lines_tgt, _, _, _ = output

        tgt_pred = decode_minibatch(
            config['data']['max_len'], tgt['tok2id']['<s>'], 
            model, input_lines_src, srclens, srcmask,
            input_ids_aux, auxlens, auxmask)

        # convert seqs to tokens
        def ids_to_toks(tok_seqs, id2tok):
            out = []
            # take off the gpu
            tok_seqs = tok_seqs.cpu().numpy()
            # convert to toks, cut off at </s>, delete any start tokens (preds were kickstarted w them)
            for line in tok_seqs:
                toks = [id2tok[x] for x in line]
                if '<s>' in toks: 
                    toks.remove('<s>')
                cut_idx = toks.index('</s>') if '</s>' in toks else len(toks)
                out.append( toks[:cut_idx] )
            # unsort
toks = [id2tok[x] for x in line]
                if '<s>' in toks: 
                    toks.remove('<s>')
                cut_idx = toks.index('</s>') if '</s>' in toks else len(toks)
                out.append( toks[:cut_idx] )
            # unsort
            out = unsort(out, indices)
            return out

        # convert inputs/preds/targets/aux to human-readable form
        inputs += ids_to_toks(output_lines_src, src['id2tok'])
        preds += ids_to_toks(tgt_pred, tgt['id2tok'])
        ground_truths += ids_to_toks(output_lines_tgt, tgt['id2tok'])
        
        if config['model']['model_type'] == 'delete':
            auxs += [[str(x)] for x in input_ids_aux.data.cpu().numpy()] 
        elif config['model']['model_type'] == 'delete_retrieve':
            auxs += ids_to_toks(input_ids_aux, tgt['id2tok'])
        elif config['model']['model_type'] == 'seq2seq':
            auxs += ['None' for _ in range(len(tgt_pred))]

    return inputs, preds, ground_truths, auxs


def inference_metrics(model, src, tgt, config):
    """ 
      Decode and evaluate BLEU scores. 
    """

    inputs, preds, ground_truths, auxs = decode_dataset(
        model, src, tgt, config)

    bleu = get_bleu(preds, ground_truths)
    edit_distance = get_edit_distance(preds, ground_truths)

    inputs = [' '.join(seq) for seq in inputs]
model, src, tgt, config)

    bleu = get_bleu(preds, ground_truths)
    edit_distance = get_edit_distance(preds, ground_truths)

    inputs = [' '.join(seq) for seq in inputs]
    preds = [' '.join(seq) for seq in preds]
    ground_truths = [' '.join(seq) for seq in ground_truths]
    auxs = [' '.join(seq) for seq in auxs]

    return bleu, edit_distance, inputs, preds, ground_truths, auxs


def evaluate_lpp(model, src, tgt, config):
    """ 
      Evaluate log perplexity WITHOUT decoding (i.e., with teacher forcing)
    """
    
    weight_mask = torch.ones(len(tgt['tok2id']))
    if CUDA:
        weight_mask = weight_mask.cuda()
    weight_mask[tgt['tok2id']['<pad>']] = 0
    loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)
    if CUDA:
        loss_criterion = loss_criterion.cuda()

    losses = []
    for j in range(0, len(src['data']), config['data']['batch_size']):
        sys.stdout.write("\r%s/%s..." % (j, len(src['data'])))
        sys.stdout.flush()

        # get batch
        input_content, input_aux, output = minibatch(
            src, tgt, j, 
            config['data']['batch_size'], 
            config['data']['max_len'], 
            config['model']['model_type'],
            is_test=True)
        input_lines_src, _, srclens, srcmask, _ = input_content
        input_ids_aux, _, auxlens, auxmask, _ = input_aux
config['data']['batch_size'], 
            config['data']['max_len'], 
            config['model']['model_type'],
            is_test=True)
        input_lines_src, _, srclens, srcmask, _ = input_content
        input_ids_aux, _, auxlens, auxmask, _ = input_aux
        input_lines_tgt, output_lines_tgt, _, _, _ = output

        decoder_logit, decoder_probs = model(
            input_lines_src, input_lines_tgt, srcmask, srclens,
            input_ids_aux, auxlens, auxmask)

        loss = loss_criterion(
            decoder_logit.contiguous().view(-1, len(tgt['tok2id'])),
            output_lines_tgt.view(-1)
        )
        losses.append(loss.item())

    return np.mean(losses)
```

```python
Bleu = True
overfit = False
```

```python
# config file which has our fine-tuned model
config = {
  "training": {
    "optimizer": "adam",
    "learning_rate": 0.0003,
    "max_norm": 3.0,
    "epochs": 45,
    "batches_per_report": 200,
    "batches_per_sampling": 500,
    "random_seed": 1
  },
  "data": {
    "src": "/content/drive/MyDrive/data/yelp/sentiment.train.0",
    "tgt": "/content/drive/MyDrive/data/yelp/sentiment.train.1",
    "src_test": "/content/drive/MyDrive/data/yelp/reference.test.0",
    "tgt_test": "/content/drive/MyDrive/data/yelp/reference.test.1",
    "src_vocab": "/content/drive/MyDrive/data/yelp/vocab",
"data": {
    "src": "/content/drive/MyDrive/data/yelp/sentiment.train.0",
    "tgt": "/content/drive/MyDrive/data/yelp/sentiment.train.1",
    "src_test": "/content/drive/MyDrive/data/yelp/reference.test.0",
    "tgt_test": "/content/drive/MyDrive/data/yelp/reference.test.1",
    "src_vocab": "/content/drive/MyDrive/data/yelp/vocab",
    "tgt_vocab": "/content/drive/MyDrive/data/yelp/vocab",
    "share_vocab": True,
    "attribute_vocab": "/content/drive/MyDrive/data/yelp/ngram.15.attribute",
    "ngram_attributes": True,
    "batch_size": 256,
    "max_len": 50,
    "working_dir": "/content/drive/MyDrive/data/working_dir"
  },
    "model": {
        "model_type": "delete",
        "emb_dim": 128,
        "attention": False,
        "encoder": "lstm",
        "src_hidden_dim": 512,
        "src_layers": 1,
        "bidirectional": True,
        "tgt_hidden_dim": 512,
        "tgt_layers": 1,
        "decode": "greedy",
        "dropout": 0.2
    }
}
```

```python
train_losses = []
scores = []
```

```python
working_dir = config['data']['working_dir']

if not os.path.exists(working_dir):
    os.makedirs(working_dir)

# set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='%s/train_log' % working_dir,
)

console = logging.StreamHandler()
console.setLevel(logging.INFO)
format='%(asctime)s - %(levelname)s - %(message)s',
    filename='%s/train_log' % working_dir,
)

console = logging.StreamHandler()
console.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
console.setFormatter(formatter)
logging.getLogger('').addHandler(console)

logging.info('Reading data ...')
src, tgt = read_nmt_data(
    src=config['data']['src'],
    config=config,
    tgt=config['data']['tgt'],
    attribute_vocab=config['data']['attribute_vocab'],
    ngram_attributes=config['data']['ngram_attributes']
)

src_test, tgt_test = read_nmt_data(
    src=config['data']['src_test'],
    config=config,
    tgt=config['data']['tgt_test'],
    attribute_vocab=config['data']['attribute_vocab'],
    ngram_attributes=config['data']['ngram_attributes'],
    train_src=src,
    train_tgt=tgt
)
logging.info('...done!')


batch_size = config['data']['batch_size']
max_length = config['data']['max_len']
src_vocab_size = len(src['tok2id'])
tgt_vocab_size = len(tgt['tok2id'])


weight_mask = torch.ones(tgt_vocab_size)
weight_mask[tgt['tok2id']['<pad>']] = 0
loss_criterion = nn.CrossEntropyLoss(weight=weight_mask)
if CUDA:
    weight_mask = weight_mask.cuda()
    loss_criterion = loss_criterion.cuda()

torch.manual_seed(config['training']['random_seed'])
np.random.seed(config['training']['random_seed'])

model = SeqModel(
loss_criterion = loss_criterion.cuda()

torch.manual_seed(config['training']['random_seed'])
np.random.seed(config['training']['random_seed'])

model = SeqModel(
    src_vocab_size=src_vocab_size,
    tgt_vocab_size=tgt_vocab_size,
    pad_id_src=src['tok2id']['<pad>'],
    pad_id_tgt=tgt['tok2id']['<pad>'],
    config=config
)

logging.info('MODEL HAS %s params' %  model.count_params())
model, start_epoch = attempt_load_model(
    model=model,
    checkpoint_dir=working_dir)
if CUDA:
    model = model.cuda()


if config['training']['optimizer'] == 'adam':
    lr = config['training']['learning_rate']
    optimizer = optim.Adam(model.parameters(), lr=lr)
elif config['training']['optimizer'] == 'sgd':
    lr = config['training']['learning_rate']
    optimizer = optim.SGD(model.parameters(), lr=lr)
elif config['training']['optimizer'] == 'adadelta':
    lr = config['training']['learning_rate']
    optimizer = optim.Adadelta(model.parameters(), lr=lr)
else:
    raise NotImplementedError("Learning method not recommend for task")

epoch_loss = []
start_since_last_report = time.time()

words_since_last_report = 0
losses_since_last_report = []
best_metric = 0.0
best_epoch = 0
cur_metric = 0.0 # log perplexity or BLEU
num_examples = min(len(src['content']), len(tgt['content']))
num_batches = num_examples / batch_size

STEP = 0
best_epoch = 0
cur_metric = 0.0 # log perplexity or BLEU
num_examples = min(len(src['content']), len(tgt['content']))
num_batches = num_examples / batch_size

STEP = 0
for epoch in range(start_epoch, config['training']['epochs']):
    if cur_metric > best_metric:
        # delete old checkpoint to save memory
        for ckpt_path in glob.glob(working_dir + '/model.*'):
            os.system("rm %s" % ckpt_path)
        # replace with new checkpoint
        torch.save(model.state_dict(), working_dir + '/model.%s.ckpt' % epoch)

        best_metric = cur_metric
        best_epoch = epoch - 1

    losses = []
    for i in range(0, num_examples, batch_size):

        if overfit:
            i = 50

        batch_idx = i / batch_size

        input_content, input_aux, output = minibatch(
            src, tgt, i, batch_size, max_length, config['model']['model_type'])
        input_lines_src, _, srclens, srcmask, _ = input_content
        input_ids_aux, _, auxlens, auxmask, _ = input_aux
        input_lines_tgt, output_lines_tgt, _, _, _ = output
        
        decoder_logit, decoder_probs = model(
            input_lines_src, input_lines_tgt, srcmask, srclens,
            input_ids_aux, auxlens, auxmask)

        optimizer.zero_grad()

        loss = loss_criterion(
            decoder_logit.contiguous().view(-1, tgt_vocab_size),
            output_lines_tgt.view(-1)
        )
loss = loss_criterion(
            decoder_logit.contiguous().view(-1, tgt_vocab_size),
            output_lines_tgt.view(-1)
        )

        losses.append(loss.item())
        losses_since_last_report.append(loss.item())
        epoch_loss.append(loss.item())
        loss.backward()
        norm = nn.utils.clip_grad_norm_(model.parameters(), config['training']['max_norm'])


        optimizer.step()

        if overfit or batch_idx % config['training']['batches_per_report'] == 0:

            s = float(time.time() - start_since_last_report)
            eps = (batch_size * config['training']['batches_per_report']) / s
            avg_loss = np.mean(losses_since_last_report)
            info = (epoch, batch_idx, num_batches, eps, avg_loss, cur_metric)
            logging.info('EPOCH: %s ITER: %s/%s EPS: %.2f LOSS: %.4f METRIC: %.4f' % info)
            start_since_last_report = time.time()
            words_since_last_report = 0
            losses_since_last_report = []

        STEP += 1

    logging.info('EPOCH %s COMPLETE. EVALUATING...' % epoch)
    start = time.time()
    model.eval()
    dev_loss = evaluate_lpp(
            model, src_test, tgt_test, config)

    if Bleu and epoch >= config['training'].get('inference_start_epoch', 1):
        cur_metric, edit_distance, _, preds, _, _ = inference_metrics(
            model, src_test, tgt_test, config)
model, src_test, tgt_test, config)

    if Bleu and epoch >= config['training'].get('inference_start_epoch', 1):
        cur_metric, edit_distance, _, preds, _, _ = inference_metrics(
            model, src_test, tgt_test, config)

        with open(working_dir + '/preds.%s' % epoch, 'w') as f:
            f.write('\n'.join(preds) + '\n')


    else:
        cur_metric = dev_loss

    model.train()

    logging.info('METRIC: %s. TIME: %.2fs CHECKPOINTING...' % (
        cur_metric, (time.time() - start)))
    avg_loss = np.mean(epoch_loss)
    train_losses.append(avg_loss)
    scores.append(cur_metric)
    epoch_loss = []
```

```python
# Delete_retrieve model
import matplotlib.pyplot as plt
plt.plot(train_losses)
plt.title('DeleteAndRetrieve model -- train and dev losses')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.show()
```

```python
plt.plot(scores)
plt.title('scores')
plt.ylabel('DeleteAndRetrieve model -- scores')
plt.xlabel('epoch')
plt.show()
```

```python
#Delete model
import matplotlib.pyplot as plt
plt.plot(train_losses)
plt.plot(dev_losses)
plt.title("Delete model -- train and dev losses")
plt.ylabel('losses')
plt.xlabel('epoch')
plt.show()
```

```python
plt.plot(scores)
plt.title('Delete model -- scores')
plt.ylabel('scores')
plt.xlaebl('epoch')
plt.show()
```

```python
!python eval.py
```
PATH: eval.py
LINES: 1-39

import matplotlib.pyplot as plt
import json

f = open('train_test_vals.json')

data = json.load(f)

delete_bleu_history = data['delete']['bleu_history']
delete_retireve_bleu_history = data['delete_retrieve']['bleu_history']
delete_loss_history = data['delete']['loss_history']
delete_retrieve_loss_history = data['delete_retrieve']['loss_history']

plt.plot(delete_bleu_history)
plt.xlabel('Epochs')
plt.ylabel('Bleu Scores')
plt.title('DeleteOnly Model Bleu Scores')
plt.savefig('DeleteOnly_Bleu.png')
plt.show()

plt.plot(delete_loss_history)
plt.xlabel('Epochs')
plt.ylabel('Losses')
plt.title('DeleteOnly Model Training Losses')
plt.savefig('DeleteOnly_Loss.png')
plt.show()

plt.plot(delete_retireve_bleu_history)
plt.xlabel('Epochs')
plt.ylabel('Bleu Scores')
plt.title('DeleteAndRetrieve Model Bleu Scores')
plt.savefig('DeleteAndRetrieve_Bleu.png')
plt.show()

plt.plot(delete_retrieve_loss_history)
plt.xlabel('Epochs')
plt.ylabel('Losses')
plt.title('DeleteAndRetrieve Model Training Losses')
plt.savefig('DeleteAndRetrieve_Loss.png')
plt.show()
PATH: Flask_server.py
LINES: 1-8

from Functions import *

app = Flask(__name__)

detector = None 
json_config = None

@app.route('/image/', methods=['POST'])
PATH: Flask_server.py
LINES: 9-32

def image_request():
    if(request.method=='POST'):

        json_body= request.get_json() #get json from request

        #get data from the request json
        objects, minimum_probability, detection_speed, unique_id = handle_user_request(json_body, json_config)

        image_name, image_type, image_file = handle_image_request(json_body, unique_id)

        #upload the retrieved image to the uploads directory 
        image_path = upload_image(json_config, image_file, image_name, image_type, unique_id)

        # detect the objects in the image as per user input
        t0=time.perf_counter()
        detections = Detection(json_config, detector, objects, image_path, minimum_probability, unique_id)
        t1=time.perf_counter()
        
        print("Time elapsed in detection:", t1-t0)

        #return the response
        return jsonify(
            objects = json.dumps(detections, default= int)
        )
PATH: Flask_server.py
LINES: 35-36

def hello():
    return "Hello"
PATH: Functions.py
LINES: 1-11

from flask import Flask, request, jsonify, abort, Response
import tensorflow
import keras
import json
import base64
import time
from datetime import date, datetime
import os
from imageai.Detection import ObjectDetection
PATH: Functions.py
LINES: 12-15

def check_dirs(dirs):# function to check if the passed directories exist else create them
    for dir in dirs:
        if(not os.path.isdir(dir)):
            os.mkdir(dir)
PATH: Functions.py
LINES: 17-22

def get_config(file_name):#function to get the config file else return exception
    try:
        json_config = json.load(open(file_name))
        return json_config
    except FileNotFoundError as error:
        abort(500)
PATH: Functions.py
LINES: 25-31

def load_model(model_path , detection_speed):
    detector = ObjectDetection()
    detector.setModelTypeAsYOLOv3()
    detector.setModelPath(model_path)
    detector.loadModel(detection_speed=detection_speed)

    return detector
PATH: Functions.py
LINES: 34-73

def Detection(json_config, detector, objects, image_path, minimum_probability, unique_id):
    curr_date = str(date.today())#get the current date and time
    curr_time = str(datetime.now().strftime("%H-%M-%S"))

    #get the path to upload and detected directories 
    detected_dir = json_config['default_parameters']['detected_directory']
    upload_dir = json_config['default_parameters']['upload_directory']
    
    #get path to sub directories in uploads and detected named as per the current data 
    input_dir = upload_dir + curr_date + '/'
    output_dir =  detected_dir + curr_date + '/'

    #check for the existence of the above sub directories
    check_dirs([detected_dir, input_dir, output_dir])

    #getting path for the input and output images 
    input_path = input_dir + image_path
    output_path = output_dir + image_path

    if len(objects)!=0: #if objects are passed in request use custom detector
        custom = detector.CustomObjects() 

        for item in objects: #updating the dictionary to detect for passed objects 
            custom[item]='valid'

        try : #detect custom objects
            detections = detector.detectCustomObjectsFromImage(custom_objects = custom, input_image= input_path, output_image_path= output_path,minimum_percentage_probability=minimum_probability, thread_safe=True)
        except Exception as error:
            logger(error, unique_id)
            abort(Response('Error - {}'.format(error), status= 400))
    else : # if no objects are passed(empty list) detect all 80 objects possible by the model

        try:
            detections = detector.detectObjectsFromImage(input_image= input_path, output_image_path= output_path,minimum_percentage_probability=minimum_probability, thread_safe=True)
        except Exception as error:
            logger(error, unique_id)
            abort(Response('Error - {}'.format(error), status= 400))  

    #delete_image(input_path)
    return detections
PATH: Functions.py
LINES: 75-95

def upload_image(json_config, image_file, image_name, image_type, unique_id):

    curr_date = str(date.today())
    curr_time = str(datetime.now().strftime("%H-%M-%S"))

    uploaded_dir = json_config['default_parameters']['upload_directory']

    input_dir = uploaded_dir + curr_date + '/'

    check_dirs([uploaded_dir, input_dir])

    #storing name of image to be uploaded in name_date_time.image_type format 
    image_path = image_name + '_' + curr_date + '_' + curr_time + "." + image_type

    try: #converting the base64 image received to jpeg/png image
        with open(input_dir + image_path, "wb") as fh:
            fh.write(base64.b64decode(image_file))
    except Exception as error:
        logger(error, unique_id)
        abort(Response('Error - {}'.format(error), status= 400))
    return image_path
PATH: Functions.py
LINES: 97-99

def delete_image(input_path): #function to delete image
    if(os.path.exists(input_path)):
        os.remove(input_path)
PATH: Functions.py
LINES: 102-112

def handle_image_request(json_body, unique_id):
    image_file = json_body.get('image')
    image_name = json_body.get('image_name')
    image_type = json_body.get('image_type')

    #check if the data type of request values is valid for our purpose
    if not isinstance(image_file, str) or not isinstance(image_name, str) or not isinstance(image_type, str) :
        logger('Error - Invalid Request', unique_id)
        abort(Response('Error - Invalid Request', status= 400))

    return image_name, image_type, image_file
PATH: Functions.py
LINES: 115-127

def handle_user_request(json_body, json_config):
    #assigning default values in case objects, min probability and detection speed haven't been specified
    objects = json_body.get('objects') if json_body.get('objects')!=None else json_config['default_objects']
    minimum_probability = json_body.get('minimum_probability') if json_body.get('minimum_probability')!=None else json_config['default_parameters']['default_probability']
    detection_speed = json_body.get('detection_speed') if json_body.get('detection_speed')!=None else json_config['default_parameters']['default_speed']
    unique_id = json_body.get('id')

    #check if the data type of request values is valid for our purpose
    if not isinstance(objects, list) or not isinstance(minimum_probability, int) and not isinstance(minimum_probability, float) or not isinstance(detection_speed, str) or not isinstance(unique_id, str) :
        logger('Error - Invalid Request', unique_id)
        abort(Response('Error - Invalid Request', status= 400))

    return objects, minimum_probability, detection_speed, unique_id
PATH: Functions.py
LINES: 130-136

def logger(error, unique_id):
    #write the errors in error.log file in time_unique_id_error format
    with open("error.log","a+") as log:
        curr_date = str(date.today())
        curr_time = str(datetime.now().strftime("%H-%M-%S"))
        log_time = curr_date + '_' + curr_time 
        log.write('\n'+log_time + unique_id + '-' + str(error))
```python
#from google.colab import drive
#drive.mount('/content/drive')
```

```python
!pip3 install tensorflow==1.13.1
#!pip3 install opencv-python
#!pip3 install keras==2.0.8
#!pip3 install imageai --upgrade
```

```python
from imageai.Detection import ObjectDetection
import time
```

```python
detector = ObjectDetection()
```

```python
detector.setModelTypeAsYOLOv3()
```

```python
detector.setModelPath("/content/drive/My Drive/yolo.h5")
```

```python
detector.loadModel(detection_speed= 'fastest')
```

```python
custom = detector.CustomObjects(person=True, car=False)
t0=time.clock()
detections = detector.detectCustomObjectsFromImage(custom_objects = custom, input_image="/content/drive/My Drive/dat.jpg", output_image_path="/content/drive/My Drive/PennFudanPed/PNGImages/Test6.png", minimum_percentage_probability=35)
t1=time.clock()
print("Time elapsed:", t1-t0)
```

```python
for eachObject in detections:
    print(eachObject["name"] , " : ", eachObject["percentage_probability"], " : ", eachObject["box_points"] )
    print("--------------------------------")
```

```python
#!pip uninstall tensorflow
#!pip3 install tensorflow-gpu==1.13.1
#!pip3 install keras==2.0.8
#!pip3 install imageai --upgrade
```

```python
from imageai.Detection.Custom import DetectionModelTrainer
```

```python
trainer = DetectionModelTrainer()
```

```python
trainer.setModelTypeAsYOLOv3()
```
```

```python
trainer.setModelTypeAsYOLOv3()
```

```python
trainer.setDataDirectory(data_directory='/content/drive/My Drive/headset')
```

```python
trainer.setTrainConfig(object_names_array=["hololens"], batch_size=4, num_experiments=20, train_from_pretrained_model="/content/drive/My Drive/pretrained-yolov3.h5")
trainer.trainModel()
```

```python
metrics = trainer.evaluateModel(model_path='/content/drive/My Drive/headset/models', json_path='/content/drive/My Drive/headset/json/detection_config.json', iou_threshold=0.5, object_threshold=0.3, nms_threshold=0.5)
```
# IMAGE AI
Detect objects in an image using ImageAi library.
The server takes image input from the user as JSON request as a base64 encoded and decodes and stores it and uses the Image Ai API to detect objects as specified by the user. The model used is YOLO which can detect upto 80 objects in the image.
The specifics of detection like speed, threshold probability, objects to be detected can be set by the user.
The API gives an output image showing bounding boxes, class and probability detected by the model along with a dictionary consisting of details associated with the detected objects. The server returns this dicitonary as response in JSON format.
PATH: ImageAI/asgi.py
LINES: 1-16

"""
ASGI config for ImageAI project.

It exposes the ASGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/3.0/howto/deployment/asgi/
"""

import os

from django.core.asgi import get_asgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ImageAI.settings')

application = get_asgi_application()
PATH: ImageAI/settings.py
LINES: 1-120

"""
Django settings for ImageAI project.

Generated by 'django-admin startproject' using Django 3.0.8.

For more information on this file, see
https://docs.djangoproject.com/en/3.0/topics/settings/

For the full list of settings and their values, see
https://docs.djangoproject.com/en/3.0/ref/settings/
"""

import os

# Build paths inside the project like this: os.path.join(BASE_DIR, ...)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))


# Quick-start development settings - unsuitable for production
# See https://docs.djangoproject.com/en/3.0/howto/deployment/checklist/

# SECURITY WARNING: keep the secret key used in production secret!


# SECURITY WARNING: don't run with debug turned on in production!
DEBUG = True

ALLOWED_HOSTS = []
CSRF_COOKIE_SECURE = False

# Application definition

INSTALLED_APPS = [
    'django.contrib.admin',
    'django.contrib.auth',
    'django.contrib.contenttypes',
    'django.contrib.sessions',
    'django.contrib.messages',
    'django.contrib.staticfiles',
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'django.contrib.sessions.middleware.SessionMiddleware',
    'django.middleware.common.CommonMiddleware',
    'django.middleware.csrf.CsrfViewMiddleware',
    'django.contrib.auth.middleware.AuthenticationMiddleware',
    'django.contrib.messages.middleware.MessageMiddleware',
    'django.middleware.clickjacking.XFrameOptionsMiddleware',
]

ROOT_URLCONF = 'ImageAI.urls'

TEMPLATES = [
    {
        'BACKEND': 'django.template.backends.django.DjangoTemplates',
        'DIRS': [],
        'APP_DIRS': True,
        'OPTIONS': {
            'context_processors': [
                'django.template.context_processors.debug',
                'django.template.context_processors.request',
                'django.contrib.auth.context_processors.auth',
                'django.contrib.messages.context_processors.messages',
            ],
        },
    },
]

WSGI_APPLICATION = 'ImageAI.wsgi.application'


# Database
# https://docs.djangoproject.com/en/3.0/ref/settings/#databases

DATABASES = {
    'default': {
        'ENGINE': 'django.db.backends.sqlite3',
        'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),
    }
}


# Password validation
# https://docs.djangoproject.com/en/3.0/ref/settings/#auth-password-validators

AUTH_PASSWORD_VALIDATORS = [
    {
        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',
    },
    {
        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',
    },
]


# Internationalization
# https://docs.djangoproject.com/en/3.0/topics/i18n/

LANGUAGE_CODE = 'en-us'

TIME_ZONE = 'UTC'

USE_I18N = True

USE_L10N = True

USE_TZ = True


# Static files (CSS, JavaScript, Images)
# https://docs.djangoproject.com/en/3.0/howto/static-files/

STATIC_URL = '/static/'
PATH: ImageAI/urls.py
LINES: 1-22

"""ImageAI URL Configuration

The `urlpatterns` list routes URLs to views. For more information please see:
    https://docs.djangoproject.com/en/3.0/topics/http/urls/
Examples:
Function views
    1. Add an import:  from my_app import views
    2. Add a URL to urlpatterns:  path('', views.home, name='home')
Class-based views
    1. Add an import:  from other_app.views import Home
    2. Add a URL to urlpatterns:  path('', Home.as_view(), name='home')
Including another URLconf
    1. Import the include() function: from django.urls import include, path
    2. Add a URL to urlpatterns:  path('blog/', include('blog.urls'))
"""
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('', include('objdet.urls'))
]
PATH: ImageAI/wsgi.py
LINES: 1-16

"""
WSGI config for ImageAI project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/3.0/howto/deployment/wsgi/
"""

import os

from django.core.wsgi import get_wsgi_application

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ImageAI.settings')

application = get_wsgi_application()
# Object_Detection_Django
Detect objects in an image using ImageAi library using a Django backend.
The server takes image input from the user as JSON request as a base64 encoded and decodes and stores it and uses the Image Ai API to detect objects as specified by the user. The model used is YOLO which can detect upto 80 objects in the image.
The specifics of detection like speed, threshold probability, objects to be detected can be set by the user.
The API gives an output image showing bounding boxes, class and probability detected by the model alomg with a dictionary consisting of details associated with the detected objects. The server returns this dicitonary as response in JSON format.
PATH: manage.py
LINES: 1-6

#!/usr/bin/env python
"""Django's command-line utility for administrative tasks."""
import os
import sys
PATH: manage.py
LINES: 7-17

def main():
    os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'ImageAI.settings')
    try:
        from django.core.management import execute_from_command_line
    except ImportError as exc:
        raise ImportError(
            "Couldn't import Django. Are you sure it's installed and "
            "available on your PYTHONPATH environment variable? Did you "
            "forget to activate a virtual environment?"
        ) from exc
    execute_from_command_line(sys.argv)
PATH: objdet/Functions.py
LINES: 1-11

from django.http import HttpResponseBadRequest, HttpResponseServerError
import tensorflow
import keras
import json
import base64
import time
from datetime import date, datetime
import os
from imageai.Detection import ObjectDetection
PATH: objdet/Functions.py
LINES: 12-15

def check_dirs(dirs):# function to check if the passed directories exist else create them
    for dir in dirs:
        if(not os.path.isdir(dir)):
            os.mkdir(dir)
PATH: objdet/Functions.py
LINES: 17-22

def get_config(file_name):#function to get the config file else return exception
    try:
        json_config = json.load(open(file_name))
        return json_config
    except FileNotFoundError as error:
        raise HttpResponseServerError('File Not Found')
PATH: objdet/Functions.py
LINES: 25-31

def load_model(model_path , detection_speed):
    detector = ObjectDetection()
    detector.setModelTypeAsYOLOv3()
    detector.setModelPath(model_path)
    detector.loadModel(detection_speed=detection_speed)

    return detector
PATH: objdet/Functions.py
LINES: 34-73

def Detection(json_config, detector, objects, image_path, minimum_probability, unique_id):
    curr_date = str(date.today())#get the current date and time
    curr_time = str(datetime.now().strftime("%H-%M-%S"))

    #get the path to upload and detected directories 
    detected_dir = json_config['default_parameters']['detected_directory']
    upload_dir = json_config['default_parameters']['upload_directory']
    
    #get path to sub directories in uploads and detected named as per the current data 
    input_dir = upload_dir + curr_date + '/'
    output_dir =  detected_dir + curr_date + '/'

    #check for the existence of the above sub directories
    check_dirs([detected_dir, input_dir, output_dir])

    #getting path for the input and output images 
    input_path = input_dir + image_path
    output_path = output_dir + image_path

    if len(objects)!=0: #if objects are passed in request use custom detector
        custom = detector.CustomObjects() 

        for item in objects: #updating the dictionary to detect for passed objects 
            custom[item]='valid'

        try : #detect custom objects
            detections = detector.detectCustomObjectsFromImage(custom_objects = custom, input_image= input_path, output_image_path= output_path,minimum_percentage_probability=minimum_probability, thread_safe=True)
        except Exception as error:
            logger(error, unique_id)
            raise HttpResponseBadRequest('Error - {}'.format(error))
    else : # if no objects are passed(empty list) detect all 80 objects possible by the model

        try:
            detections = detector.detectObjectsFromImage(input_image= input_path, output_image_path= output_path,minimum_percentage_probability=minimum_probability, thread_safe=True)
        except Exception as error:
            logger(error, unique_id)
            raise HttpResponseBadRequest('Error - {}'.format(error))  

    #delete_image(input_path)
    return detections
PATH: objdet/Functions.py
LINES: 75-95

def upload_image(json_config, image_file, image_name, image_type, unique_id):

    curr_date = str(date.today())
    curr_time = str(datetime.now().strftime("%H-%M-%S"))

    uploaded_dir = json_config['default_parameters']['upload_directory']

    input_dir = uploaded_dir + curr_date + '/'

    check_dirs([uploaded_dir, input_dir])

    #storing name of image to be uploaded in name_date_time.image_type format 
    image_path = image_name + '_' + curr_date + '_' + curr_time + "." + image_type

    try: #converting the base64 image received to jpeg/png image
        with open(input_dir + image_path, "wb") as fh:
            fh.write(base64.b64decode(image_file))
    except Exception as error:
        logger(error, unique_id)
        raise HttpResponseBadRequest('Error - {}'.format(error))
    return image_path
PATH: objdet/Functions.py
LINES: 97-99

def delete_image(input_path): #function to delete image
    if(os.path.exists(input_path)):
        os.remove(input_path)
PATH: objdet/Functions.py
LINES: 102-112

def handle_image_request(json_body, unique_id):
    image_file = json_body.get('image')
    image_name = json_body.get('image_name')
    image_type = json_body.get('image_type')

    #check if the data type of request values is valid for our purpose
    if not isinstance(image_file, str) or not isinstance(image_name, str) or not isinstance(image_type, str) :
        logger('Error - Invalid Request', unique_id)
        raise HttpResponseBadRequest('Error - Invalid Request')

    return image_name, image_type, image_file
PATH: objdet/Functions.py
LINES: 115-127

def handle_user_request(json_body, json_config):
    #assigning default values in case objects, min probability and detection speed haven't been specified
    objects = json_body.get('objects') if json_body.get('objects')!=None else json_config['default_objects']
    minimum_probability = json_body.get('minimum_probability') if json_body.get('minimum_probability')!=None else json_config['default_parameters']['default_probability']
    detection_speed = json_body.get('detection_speed') if json_body.get('detection_speed')!=None else json_config['default_parameters']['default_speed']
    unique_id = json_body.get('id')

    #check if the data type of request values is valid for our purpose
    if not isinstance(objects, list) or not isinstance(minimum_probability, int) and not isinstance(minimum_probability, float) or not isinstance(detection_speed, str) or not isinstance(unique_id, str) :
        logger('Error - Invalid Request', unique_id)
        raise HttpResponseBadRequest('Error - Invalid Request')

    return objects, minimum_probability, detection_speed, unique_id
PATH: objdet/Functions.py
LINES: 130-136

def logger(error, unique_id):
    #write the errors in error.log file in time_unique_id_error format
    with open("error.log","a+") as log:
        curr_date = str(date.today())
        curr_time = str(datetime.now().strftime("%H-%M-%S"))
        log_time = curr_date + '_' + curr_time 
        log.write('\n'+log_time + unique_id + '-' + str(error))
PATH: objdet/admin.py
LINES: 1-3

from django.contrib import admin

# Register your models here.
PATH: objdet/apps.py
LINES: 1-3

from django.apps import AppConfig
PATH: objdet/apps.py
LINES: 4-5

class ObjdetConfig(AppConfig):
    name = 'objdet'
PATH: objdet/models.py
LINES: 1-3

from django.db import models

# Create your models here.
PATH: objdet/tests.py
LINES: 1-3

from django.test import TestCase

# Create your tests here.
PATH: objdet/urls.py
LINES: 1-7

from django.urls import path

from . import views

urlpatterns = [
    path('image', views.detect, name = 'image')
]
PATH: objdet/views.py
LINES: 1-21

from django.shortcuts import render
from django.http import JsonResponse
from .Functions import *
from django.views.decorators.csrf import csrf_exempt

# Create your views here.
json_config = get_config('config.json')
upload_path = json_config['default_parameters']['upload_directory']
detected_path = json_config['default_parameters']['detected_directory']

#checking if directories for uploaded and detected images exist if not create them
check_dirs([upload_path, detected_path])

#loading the model before start of server
t0=time.perf_counter()
detector = load_model(json_config['model_file']['default'], json_config['default_parameters']['default_speed']) 
t1=time.perf_counter()

print("Time elapsed in loading model:", t1-t0)

@csrf_exempt
PATH: objdet/views.py
LINES: 22-44

def detect(request):
    if request.method == 'POST':
        json_body = json.loads(request.body)

        #get data from the request json
        objects, minimum_probability, detection_speed, unique_id = handle_user_request(json_body, json_config)

        image_name, image_type, image_file = handle_image_request(json_body, unique_id)

        #upload the retrieved image to the uploads directory 
        image_path = upload_image(json_config, image_file, image_name, image_type, unique_id)

        # detect the objects in the image as per user input
        t0=time.perf_counter()
        detections = Detection(json_config, detector, objects, image_path, minimum_probability, unique_id)
        t1=time.perf_counter()

        data = {
            'objects' : json.dumps(detections, default= int)
        }
        print("Time elapsed in detection:", t1-t0)

    return JsonResponse(data)
# Omdb_web_project
PATH: omdb-backend/app.js
LINES: 1-28

const express = require('express');
const path = require('path');
const cookieParser = require('cookie-parser');
const logger = require('morgan');
const cors = require('cors');

const omdbRouter = require('./routes/omdb');

const app = express();

// These will not send a response
app.use(cors())
app.use(logger('dev'));
app.use(express.json());
app.use(express.urlencoded({ extended: false }));
app.use(cookieParser());

// These can send a response or pass it on
// Based on url you can do one of two things
// 1. Return a file
// 2. Execute a function and return a response
app.use(express.static(path.join(__dirname, 'public')));
app.use('/omdb', omdbRouter);
app.get('/*', function (req, res) {
    res.sendFile(path.join(__dirname, 'public', 'index.html'));
});

module.exports = app;
<!doctype html><html lang="en"><head><meta charset="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="viewport" content="width=device-width,initial-scale=1"/><meta name="theme-color" content="#000000"/><meta name="description" content="Web site created using create-react-app"/><link rel="apple-touch-icon" href="/logo192.png"/><link rel="manifest" href="/manifest.json"/><title>React App</title><link href="/static/css/2.11829350.chunk.css" rel="stylesheet"><link href="/static/css/main.d180690c.chunk.css" rel="stylesheet"></head><body><noscript>You need to enable JavaScript to run this app.</noscript><div id="root"></div><script>!function(e){function r(r){for(var n,i,l=r[0],f=r[1],a=r[2],c=0,s=[];c<l.length;c++)i=l[c],Object.prototype.hasOwnProperty.call(o,i)&&o[i]&&s.push(o[i][0]),o[i]=0;for(n in f)Object.prototype.hasOwnProperty.call(f,n)&&(e[n]=f[n]);for(p&&p(r);s.length;)s.shift()();return u.push.apply(u,a||[]),t()}function t(){for(var e,r=0;r<u.length;r++){for(var t=u[r],n=!0,l=1;l<t.length;l++){var f=t[l];0!==o[f]&&(n=!1)}n&&(u.splice(r--,1),e=i(i.s=t[0]))}return e}var n={},o={1:0},u=[];function i(r){if(n[r])return n[r].exports;var t=n[r]={i:r,l:!1,exports:{}};return e[r].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.m=e,i.c=n,i.d=function(e,r,t){i.o(e,r)||Object.defineProperty(e,r,{enumerable:!0,get:t})},i.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.t=function(e,r){if(1&r&&(e=i(e)),8&r)return e;if(4&r&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&r&&"string"!=typeof e)for(var n in e)i.d(t,n,function(r){return e[r]}.bind(null,n));return t},i.n=function(e){var r=e&&e.__esModule?function(){return e.default}:function(){return e};return i.d(r,"a",r),r},i.o=function(e,r){return Object.prototype.hasOwnProperty.call(e,r)},i.p="/";var l=this["webpackJsonpomdb-ui"]=this["webpackJsonpomdb-ui"]||[],f=l.push.bind(l);l.push=r,l=l.slice();for(var a=0;a<l.length;a++)r(l[a]);var p=f;t()}([])</script><script src="/static/js/2.fb38bfe7.chunk.js"></script><script src="/static/js/main.66ea98dc.chunk.js"></script></body></html>
PATH: omdb-backend/public/precache-manifest.b95082ab14ed4f2cbac8e2bf83d1cf95.js
LINES: 1-30

self.__precacheManifest = (self.__precacheManifest || []).concat([
  {
    "revision": "2e71abb1994308a68ddaa8623567c5ce",
    "url": "/index.html"
  },
  {
    "revision": "79abd535e1fa91d9c46e",
    "url": "/static/css/2.11829350.chunk.css"
  },
  {
    "revision": "6267f747ee7b563ae7db",
    "url": "/static/css/main.d180690c.chunk.css"
  },
  {
    "revision": "79abd535e1fa91d9c46e",
    "url": "/static/js/2.fb38bfe7.chunk.js"
  },
  {
    "revision": "3453b8997016469371284a28c0e873e2",
    "url": "/static/js/2.fb38bfe7.chunk.js.LICENSE.txt"
  },
  {
    "revision": "6267f747ee7b563ae7db",
    "url": "/static/js/main.66ea98dc.chunk.js"
  },
  {
    "revision": "87f9cc7c2d39d40ed91e",
    "url": "/static/js/runtime-main.1d58dd66.js"
  }
]);
PATH: omdb-backend/public/service-worker.js
LINES: 1-39

/**
 * Welcome to your Workbox-powered service worker!
 *
 * You'll need to register this file in your web app and you should
 * disable HTTP caching for this file too.
 * See https://goo.gl/nhQhGp
 *
 * The rest of the code is auto-generated. Please don't update this file
 * directly; instead, make changes to your Workbox build configuration
 * and re-run your build process.
 * See https://goo.gl/2aRDsh
 */

importScripts("https://storage.googleapis.com/workbox-cdn/releases/4.3.1/workbox-sw.js");

importScripts(
  "/precache-manifest.b95082ab14ed4f2cbac8e2bf83d1cf95.js"
);

self.addEventListener('message', (event) => {
  if (event.data && event.data.type === 'SKIP_WAITING') {
    self.skipWaiting();
  }
});

workbox.core.clientsClaim();

/**
 * The workboxSW.precacheAndRoute() method efficiently caches and responds to
 * requests for URLs in the manifest.
 * See https://goo.gl/S9QRab
 */
self.__precacheManifest = [].concat(self.__precacheManifest || []);
workbox.precaching.precacheAndRoute(self.__precacheManifest, {});

workbox.routing.registerNavigationRoute(workbox.precaching.getCacheKeyForURL("/index.html"), {
  
  blacklist: [/^\/_/,/\/[^\/?]+\.[^\/]+$/],
});
PATH: omdb-backend/public/static/js/2.fb38bfe7.chunk.js
LINES: 1-1

/*! For license information please see 2.fb38bfe7.chunk.js.LICENSE.txt */
PATH: omdb-backend/public/static/js/2.fb38bfe7.chunk.js
LINES: 2-2

(this["webpackJsonpomdb-ui"]=this["webpackJsonpomdb-ui"]||[]).push([[2],[function(e,t,n){"use strict";e.exports=n(31)},function(e,t,n){e.exports=n(54)()},function(e,t,n){"use strict";function r(){return(r=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(e[r]=n[r])}return e}).apply(this,arguments)}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";n.d(t,"a",(function(){return w})),n.d(t,"b",(function(){return T})),n.d(t,"c",(function(){return h})),n.d(t,"d",(function(){return N})),n.d(t,"e",(function(){return m})),n.d(t,"f",(function(){return k})),n.d(t,"g",(function(){return R})),n.d(t,"h",(function(){return j}));var r=n(7),a=n(0),i=n.n(a),o=(n(1),n(9)),l=n(15),u=n(8),c=n(2),s=n(16),f=n.n(s),d=(n(26),n(4)),p=(n(28),function(e){var t=Object(l.a)();return t.displayName=e,t}("Router-History")),m=function(e){var t=Object(l.a)();return t.displayName=e,t}("Router"),h=function(e){function t(t){var n;return(n=e.call(this,t)||this).state={location:t.history.location},n._isMounted=!1,n._pendingLocation=null,t.staticContext||(n.unlisten=t.history.listen((function(e){n._isMounted?n.setState({location:e}):n._pendingLocation=e}))),n}Object(r.a)(t,e),t.computeRootMatch=function(e){return{path:"/",url:"/",params:{},isExact:"/"===e}};var n=t.prototype;return n.componentDidMount=function(){this._isMounted=!0,this._pendingLocation&&this.setState({location:this._pendingLocation})},n.componentWillUnmount=function(){this.unlisten&&this.unlisten()},n.render=function(){return i.a.createElement(m.Provider,{value:{history:this.props.history,location:this.state.location,match:t.computeRootMatch(this.state.location.pathname),staticContext:this.props.staticContext}},i.a.createElement(p.Provider,{children:this.props.children||null,value:this.props.history}))},t}(i.a.Component);i.a.Component;var v=function(e){function t(){return e.apply(this,arguments)||this}Object(r.a)(t,e);var n=t.prototype;return n.componentDidMount=function(){this.props.onMount&&this.props.onMount.call(this,this)},n.componentDidUpdate=function(e){this.props.onUpdate&&this.props.onUpdate.call(this,this,e)},n.componentWillUnmount=function(){this.props.onUnmount&&this.props.onUnmount.call(this,this)},n.render=function(){return null},t}(i.a.Component);var g={},y=0;function b(e,t){return void 0===e&&(e="/"),void 0===t&&(t={}),"/"===e?e:function(e){if(g[e])return g[e];var t=f.a.compile(e);return y<1e4&&(g[e]=t,y++),t}(e)(t,{pretty:!0})}function w(e){var t=e.computedMatch,n=e.to,r=e.push,a=void 0!==r&&r;return i.a.createElement(m.Consumer,null,(function(e){e||Object(u.a)(!1);var r=e.history,l=e.staticContext,s=a?r.push:r.replace,f=Object(o.c)(t?"string"===typeof n?b(n,t.params):Object(c.a)({},n,{pathname:b(n.pathname,t.params)}):n);return l?(s(f),null):i.a.createElement(v,{onMount:function(){s(f)},onUpdate:function(e,t){var n=Object(o.c)(t.to);Object(o.f)(n,Object(c.a)({},f,{key:n.key}))||s(f)},to:n})}))}var x={},E=0;function k(e,t){void 0===t&&(t={}),("string"===typeof t||Array.isArray(t))&&(t={path:t});var n=t,r=n.path,a=n.exact,i=void 0!==a&&a,o=n.strict,l=void 0!==o&&o,u=n.sensitive,c=void 0!==u&&u;return[].concat(r).reduce((function(t,n){if(!n&&""!==n)return null;if(t)return t;var r=function(e,t){var n=""+t.end+t.strict+t.sensitive,r=x[n]||(x[n]={});if(r[e])return r[e];var a=[],i={regexp:f()(e,a,t),keys:a};return E<1e4&&(r[e]=i,E++),i}(n,{end:i,strict:l,sensitive:c}),a=r.regexp,o=r.keys,u=a.exec(e);if(!u)return null;var s=u[0],d=u.slice(1),p=e===s;return i&&!p?null:{path:n,url:"/"===n&&""===s?"/":s,isExact:p,params:o.reduce((function(e,t,n){return e[t.name]=d[n],e}),{})}}),null)}var T=function(e){function t(){return e.apply(this,arguments)||this}return Object(r.a)(t,e),t.prototype.render=function(){var e=this;return i.a.createElement(m.Consumer,null,(function(t){t||Object(u.a)(!1);var n=e.props.location||t.location,r=e.props.computedMatch?e.props.computedMatch:e.props.path?k(n.pathname,e.props):t.match,a=Object(c.a)({},t,{location:n,match:r}),o=e.props,l=o.children,s=o.component,f=o.render;return Array.isArray(l)&&0===l.length&&(l=null),i.a.createElement(m.Provider,{value:a},a.match?l?"function"===typeof l?l(a):l:s?i.a.createElement(s,a):f?f(a):null:"function"===typeof l?l(a):null)}))},t}(i.a.Component);function S(e){return"/"===e.charAt(0)?e:"/"+e}function C(e,t){if(!e)return t;var n=S(e);return 0!==t.pathname.indexOf(n)?t:Object(c.a)({},t,{pathname:t.pathname.substr(n.length)})}function P(e){return"string"===typeof e?e:Object(o.e)(e)}function _(e){return function(){Object(u.a)(!1)}}function O(){}i.a.Component;var N=function(e){function t(){return e.apply(this,arguments)||this}return Object(r.a)(t,e),t.prototype.render=function(){var e=this;return i.a.createElement(m.Consumer,null,(function(t){t||Object(u.a)(!1);var n,r,a=e.props.location||t.location;return i.a.Children.forEach(e.props.children,(function(e){if(null==r&&i.a.isValidElement(e)){n=e;var o=e.props.path||e.props.from;r=o?k(a.pathname,Object(c.a)({},e.props,{path:o})):t.match}})),r?i.a.cloneElement(n,{location:a,computedMatch:r}):null}))},t}(i.a.Component);var M=i.a.useContext;function R(){return M(p)}function j(){var e=M(m).match;return e?e.params:{}}},function(e,t,n){"use strict";function r(e,t){if(null==e)return{};var n,r,a={},i=Object.keys(e);for(r=0;r<i.length;r++)n=i[r],t.indexOf(n)>=0||(a[n]=e[n]);return a}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";n.d(t,"a",(function(){return o})),n.d(t,"b",(function(){return u}));var r,a=n(1),i=n.n(a);function o(e,t){return void 0===e&&(e=""),void 0===t&&(t=r),t?e.split(" ").map((function(e){return t[e]||e})).join(" "):e}var l="object"===typeof window&&window.Element||function(){};i.a.oneOfType([i.a.string,i.a.func,function(e,t,n){if(!(e[t]instanceof l))return new Error("Invalid prop `"+t+"` supplied to `"+n+"`. Expected prop to be an instance of Element. Validation failed.")},i.a.shape({current:i.a.any})]);var u=i.a.oneOfType([i.a.func,i.a.string,i.a.shape({$$typeof:i.a.symbol,render:i.a.func}),i.a.arrayOf(i.a.oneOfType([i.a.func,i.a.string,i.a.shape({$$typeof:i.a.symbol,render:i.a.func})]))]);"undefined"===typeof window||!window.document||window.document.createElement},function(e,t,n){"use strict";var r=n(18),a=Object.prototype.toString;function i(e){return"[object Array]"===a.call(e)}function o(e){return"undefined"===typeof e}function l(e){return null!==e&&"object"===typeof e}function u(e){return"[object Function]"===a.call(e)}function c(e,t){if(null!==e&&"undefined"!==typeof e)if("object"!==typeof e&&(e=[e]),i(e))for(var n=0,r=e.length;n<r;n++)t.call(null,e[n],n,e);else for(var a in e)Object.prototype.hasOwnProperty.call(e,a)&&t.call(null,e[a],a,e)}e.exports={isArray:i,isArrayBuffer:function(e){return"[object ArrayBuffer]"===a.call(e)},isBuffer:function(e){return null!==e&&!o(e)&&null!==e.constructor&&!o(e.constructor)&&"function"===typeof e.constructor.isBuffer&&e.constructor.isBuffer(e)},isFormData:function(e){return"undefined"!==typeof FormData&&e instanceof FormData},isArrayBufferView:function(e){return"undefined"!==typeof ArrayBuffer&&ArrayBuffer.isView?ArrayBuffer.isView(e):e&&e.buffer&&e.buffer instanceof ArrayBuffer},isString:function(e){return"string"===typeof e},isNumber:function(e){return"number"===typeof e},isObject:l,isUndefined:o,isDate:function(e){return"[object Date]"===a.call(e)},isFile:function(e){return"[object File]"===a.call(e)},isBlob:function(e){return"[object Blob]"===a.call(e)},isFunction:u,isStream:function(e){return l(e)&&u(e.pipe)},isURLSearchParams:function(e){return"undefined"!==typeof URLSearchParams&&e instanceof URLSearchParams},isStandardBrowserEnv:function(){return("undefined"===typeof navigator||"ReactNative"!==navigator.product&&"NativeScript"!==navigator.product&&"NS"!==navigator.product)&&("undefined"!==typeof window&&"undefined"!==typeof document)},forEach:c,merge:function e(){var t={};function n(n,r){"object"===typeof t[r]&&"object"===typeof n?t[r]=e(t[r],n):t[r]=n}for(var r=0,a=arguments.length;r<a;r++)c(arguments[r],n);return t},deepMerge:function e(){var t={};function n(n,r){"object"===typeof t[r]&&"object"===typeof n?t[r]=e(t[r],n):t[r]="object"===typeof n?e({},n):n}for(var r=0,a=arguments.length;r<a;r++)c(arguments[r],n);return t},extend:function(e,t,n){return c(t,(function(t,a){e[a]=n&&"function"===typeof t?r(t,n):t})),e},trim:function(e){return e.replace(/^\s*/,"").replace(/\s*$/,"")}}},function(e,t,n){"use strict";function r(e,t){e.prototype=Object.create(t.prototype),e.prototype.constructor=e,e.__proto__=t}n.d(t,"a",(function(){return r}))},function(e,t,n){"use strict";t.a=function(e,t){if(!e)throw new Error("Invariant failed")}},function(e,t,n){"use strict";n.d(t,"a",(function(){return x})),n.d(t,"b",(function(){return C})),n.d(t,"d",(function(){return _})),n.d(t,"c",(function(){return h})),n.d(t,"f",(function(){return v})),n.d(t,"e",(function(){return m}));var r=n(2);function a(e){return"/"===e.charAt(0)}function i(e,t){for(var n=t,r=n+1,a=e.length;r<a;n+=1,r+=1)e[n]=e[r];e.pop()}var o=function(e,t){void 0===t&&(t="");var n,r=e&&e.split("/")||[],o=t&&t.split("/")||[],l=e&&a(e),u=t&&a(t),c=l||u;if(e&&a(e)?o=r:r.length&&(o.pop(),o=o.concat(r)),!o.length)return"/";if(o.length){var s=o[o.length-1];n="."===s||".."===s||""===s}else n=!1;for(var f=0,d=o.length;d>=0;d--){var p=o[d];"."===p?i(o,d):".."===p?(i(o,d),f++):f&&(i(o,d),f--)}if(!c)for(;f--;f)o.unshift("..");!c||""===o[0]||o[0]&&a(o[0])||o.unshift("");var m=o.join("/");return n&&"/"!==m.substr(-1)&&(m+="/"),m};function l(e){return e.valueOf?e.valueOf():Object.prototype.valueOf.call(e)}var u=function e(t,n){if(t===n)return!0;if(null==t||null==n)return!1;if(Array.isArray(t))return Array.isArray(n)&&t.length===n.length&&t.every((function(t,r){return e(t,n[r])}));if("object"===typeof t||"object"===typeof n){var r=l(t),a=l(n);return r!==t||a!==n?e(r,a):Object.keys(Object.assign({},t,n)).every((function(r){return e(t[r],n[r])}))}return!1},c=n(8);function s(e){return"/"===e.charAt(0)?e:"/"+e}function f(e){return"/"===e.charAt(0)?e.substr(1):e}function d(e,t){return function(e,t){return 0===e.toLowerCase().indexOf(t.toLowerCase())&&-1!=="/?#".indexOf(e.charAt(t.length))}(e,t)?e.substr(t.length):e}function p(e){return"/"===e.charAt(e.length-1)?e.slice(0,-1):e}function m(e){var t=e.pathname,n=e.search,r=e.hash,a=t||"/";return n&&"?"!==n&&(a+="?"===n.charAt(0)?n:"?"+n),r&&"#"!==r&&(a+="#"===r.charAt(0)?r:"#"+r),a}function h(e,t,n,a){var i;"string"===typeof e?(i=function(e){var t=e||"/",n="",r="",a=t.indexOf("#");-1!==a&&(r=t.substr(a),t=t.substr(0,a));var i=t.indexOf("?");return-1!==i&&(n=t.substr(i),t=t.substr(0,i)),{pathname:t,search:"?"===n?"":n,hash:"#"===r?"":r}}(e)).state=t:(void 0===(i=Object(r.a)({},e)).pathname&&(i.pathname=""),i.search?"?"!==i.search.charAt(0)&&(i.search="?"+i.search):i.search="",i.hash?"#"!==i.hash.charAt(0)&&(i.hash="#"+i.hash):i.hash="",void 0!==t&&void 0===i.state&&(i.state=t));try{i.pathname=decodeURI(i.pathname)}catch(l){throw l instanceof URIError?new URIError('Pathname "'+i.pathname+'" could not be decoded. This is likely caused by an invalid percent-encoding.'):l}return n&&(i.key=n),a?i.pathname?"/"!==i.pathname.charAt(0)&&(i.pathname=o(i.pathname,a.pathname)):i.pathname=a.pathname:i.pathname||(i.pathname="/"),i}function v(e,t){return e.pathname===t.pathname&&e.search===t.search&&e.hash===t.hash&&e.key===t.key&&u(e.state,t.state)}function g(){var e=null;var t=[];return{setPrompt:function(t){return e=t,function(){e===t&&(e=null)}},confirmTransitionTo:function(t,n,r,a){if(null!=e){var i="function"===typeof e?e(t,n):e;"string"===typeof i?"function"===typeof r?r(i,a):a(!0):a(!1!==i)}else a(!0)},appendListener:function(e){var n=!0;function r(){n&&e.apply(void 0,arguments)}return t.push(r),function(){n=!1,t=t.filter((function(e){return e!==r}))}},notifyListeners:function(){for(var e=arguments.length,n=new Array(e),r=0;r<e;r++)n[r]=arguments[r];t.forEach((function(e){return e.apply(void 0,n)}))}}}var y=!("undefined"===typeof window||!window.document||!window.document.createElement);function b(e,t){t(window.confirm(e))}function w(){try{return window.history.state||{}}catch(e){return{}}}function x(e){void 0===e&&(e={}),y||Object(c.a)(!1);var t=window.history,n=function(){var e=window.navigator.userAgent;return(-1===e.indexOf("Android 2.")&&-1===e.indexOf("Android 4.0")||-1===e.indexOf("Mobile Safari")||-1!==e.indexOf("Chrome")||-1!==e.indexOf("Windows Phone"))&&(window.history&&"pushState"in window.history)}(),a=!(-1===window.navigator.userAgent.indexOf("Trident")),i=e,o=i.forceRefresh,l=void 0!==o&&o,u=i.getUserConfirmation,f=void 0===u?b:u,v=i.keyLength,x=void 0===v?6:v,E=e.basename?p(s(e.basename)):"";function k(e){var t=e||{},n=t.key,r=t.state,a=window.location,i=a.pathname+a.search+a.hash;return E&&(i=d(i,E)),h(i,r,n)}function T(){return Math.random().toString(36).substr(2,x)}var S=g();function C(e){Object(r.a)(I,e),I.length=t.length,S.notifyListeners(I.location,I.action)}function P(e){(function(e){return void 0===e.state&&-1===navigator.userAgent.indexOf("CriOS")})(e)||N(k(e.state))}function _(){N(k(w()))}var O=!1;function N(e){if(O)O=!1,C();else{S.confirmTransitionTo(e,"POP",f,(function(t){t?C({action:"POP",location:e}):function(e){var t=I.location,n=R.indexOf(t.key);-1===n&&(n=0);var r=R.indexOf(e.key);-1===r&&(r=0);var a=n-r;a&&(O=!0,z(a))}(e)}))}}var M=k(w()),R=[M.key];function j(e){return E+m(e)}function z(e){t.go(e)}var A=0;function L(e){1===(A+=e)&&1===e?(window.addEventListener("popstate",P),a&&window.addEventListener("hashchange",_)):0===A&&(window.removeEventListener("popstate",P),a&&window.removeEventListener("hashchange",_))}var D=!1;var I={length:t.length,action:"POP",location:M,createHref:j,push:function(e,r){var a=h(e,r,T(),I.location);S.confirmTransitionTo(a,"PUSH",f,(function(e){if(e){var r=j(a),i=a.key,o=a.state;if(n)if(t.pushState({key:i,state:o},null,r),l)window.location.href=r;else{var u=R.indexOf(I.location.key),c=R.slice(0,u+1);c.push(a.key),R=c,C({action:"PUSH",location:a})}else window.location.href=r}}))},replace:function(e,r){var a=h(e,r,T(),I.location);S.confirmTransitionTo(a,"REPLACE",f,(function(e){if(e){var r=j(a),i=a.key,o=a.state;if(n)if(t.replaceState({key:i,state:o},null,r),l)window.location.replace(r);else{var u=R.indexOf(I.location.key);-1!==u&&(R[u]=a.key),C({action:"REPLACE",location:a})}else window.location.replace(r)}}))},go:z,goBack:function(){z(-1)},goForward:function(){z(1)},block:function(e){void 0===e&&(e=!1);var t=S.setPrompt(e);return D||(L(1),D=!0),function(){return D&&(D=!1,L(-1)),t()}},listen:function(e){var t=S.appendListener(e);return L(1),function(){L(-1),t()}}};return I}var E={hashbang:{encodePath:function(e){return"!"===e.charAt(0)?e:"!/"+f(e)},decodePath:function(e){return"!"===e.charAt(0)?e.substr(1):e}},noslash:{encodePath:f,decodePath:s},slash:{encodePath:s,decodePath:s}};function k(e){var t=e.indexOf("#");return-1===t?e:e.slice(0,t)}function T(){var e=window.location.href,t=e.indexOf("#");return-1===t?"":e.substring(t+1)}function S(e){window.location.replace(k(window.location.href)+"#"+e)}function C(e){void 0===e&&(e={}),y||Object(c.a)(!1);var t=window.history,n=(window.navigator.userAgent.indexOf("Firefox"),e),a=n.getUserConfirmation,i=void 0===a?b:a,o=n.hashType,l=void 0===o?"slash":o,u=e.basename?p(s(e.basename)):"",f=E[l],v=f.encodePath,w=f.decodePath;function x(){var e=w(T());return u&&(e=d(e,u)),h(e)}var C=g();function P(e){Object(r.a)(F,e),F.length=t.length,C.notifyListeners(F.location,F.action)}var _=!1,O=null;function N(){var e,t,n=T(),r=v(n);if(n!==r)S(r);else{var a=x(),o=F.location;if(!_&&(t=a,(e=o).pathname===t.pathname&&e.search===t.search&&e.hash===t.hash))return;if(O===m(a))return;O=null,function(e){if(_)_=!1,P();else{C.confirmTransitionTo(e,"POP",i,(function(t){t?P({action:"POP",location:e}):function(e){var t=F.location,n=z.lastIndexOf(m(t));-1===n&&(n=0);var r=z.lastIndexOf(m(e));-1===r&&(r=0);var a=n-r;a&&(_=!0,A(a))}(e)}))}}(a)}}var M=T(),R=v(M);M!==R&&S(R);var j=x(),z=[m(j)];function A(e){t.go(e)}var L=0;function D(e){1===(L+=e)&&1===e?window.addEventListener("hashchange",N):0===L&&window.removeEventListener("hashchange",N)}var I=!1;var F={length:t.length,action:"POP",location:j,createHref:function(e){var t=document.querySelector("base"),n="";return t&&t.getAttribute("href")&&(n=k(window.location.href)),n+"#"+v(u+m(e))},push:function(e,t){var n=h(e,void 0,void 0,F.location);C.confirmTransitionTo(n,"PUSH",i,(function(e){if(e){var t=m(n),r=v(u+t);if(T()!==r){O=t,function(e){window.location.hash=e}(r);var a=z.lastIndexOf(m(F.location)),i=z.slice(0,a+1);i.push(t),z=i,P({action:"PUSH",location:n})}else P()}}))},replace:function(e,t){var n=h(e,void 0,void 0,F.location);C.confirmTransitionTo(n,"REPLACE",i,(function(e){if(e){var t=m(n),r=v(u+t);T()!==r&&(O=t,S(r));var a=z.indexOf(m(F.location));-1!==a&&(z[a]=t),P({action:"REPLACE",location:n})}}))},go:A,goBack:function(){A(-1)},goForward:function(){A(1)},block:function(e){void 0===e&&(e=!1);var t=C.setPrompt(e);return I||(D(1),I=!0),function(){return I&&(I=!1,D(-1)),t()}},listen:function(e){var t=C.appendListener(e);return D(1),function(){D(-1),t()}}};return F}function P(e,t,n){return Math.min(Math.max(e,t),n)}function _(e){void 0===e&&(e={});var t=e,n=t.getUserConfirmation,a=t.initialEntries,i=void 0===a?["/"]:a,o=t.initialIndex,l=void 0===o?0:o,u=t.keyLength,c=void 0===u?6:u,s=g();function f(e){Object(r.a)(w,e),w.length=w.entries.length,s.notifyListeners(w.location,w.action)}function d(){return Math.random().toString(36).substr(2,c)}var p=P(l,0,i.length-1),v=i.map((function(e){return h(e,void 0,"string"===typeof e?d():e.key||d())})),y=m;function b(e){var t=P(w.index+e,0,w.entries.length-1),r=w.entries[t];s.confirmTransitionTo(r,"POP",n,(function(e){e?f({action:"POP",location:r,index:t}):f()}))}var w={length:v.length,action:"POP",location:v[p],index:p,entries:v,createHref:y,push:function(e,t){var r=h(e,t,d(),w.location);s.confirmTransitionTo(r,"PUSH",n,(function(e){if(e){var t=w.index+1,n=w.entries.slice(0);n.length>t?n.splice(t,n.length-t,r):n.push(r),f({action:"PUSH",location:r,index:t,entries:n})}}))},replace:function(e,t){var r=h(e,t,d(),w.location);s.confirmTransitionTo(r,"REPLACE",n,(function(e){e&&(w.entries[w.index]=r,f({action:"REPLACE",location:r}))}))},go:b,goBack:function(){b(-1)},goForward:function(){b(1)},canGo:function(e){var t=w.index+e;return t>=0&&t<w.entries.length},block:function(e){return void 0===e&&(e=!1),s.setPrompt(e)},listen:function(e){return s.appendListener(e)}};return w}},function(e,t,n){var r;!function(){"use strict";var n={}.hasOwnProperty;function a(){for(var e=[],t=0;t<arguments.length;t++){var r=arguments[t];if(r){var i=typeof r;if("string"===i||"number"===i)e.push(r);else if(Array.isArray(r)&&r.length){var o=a.apply(null,r);o&&e.push(o)}else if("object"===i)for(var l in r)n.call(r,l)&&r[l]&&e.push(l)}}return e.join(" ")}e.exports?(a.default=a,e.exports=a):void 0===(r=function(){return a}.apply(t,[]))||(e.exports=r)}()},function(e,t,n){"use strict";function r(e,t){(null==t||t>e.length)&&(t=e.length);for(var n=0,r=new Array(t);n<t;n++)r[n]=e[n];return r}function a(e,t){return function(e){if(Array.isArray(e))return e}(e)||function(e,t){if("undefined"!==typeof Symbol&&Symbol.iterator in Object(e)){var n=[],r=!0,a=!1,i=void 0;try{for(var o,l=e[Symbol.iterator]();!(r=(o=l.next()).done)&&(n.push(o.value),!t||n.length!==t);r=!0);}catch(u){a=!0,i=u}finally{try{r||null==l.return||l.return()}finally{if(a)throw i}}return n}}(e,t)||function(e,t){if(e){if("string"===typeof e)return r(e,t);var n=Object.prototype.toString.call(e).slice(8,-1);return"Object"===n&&e.constructor&&(n=e.constructor.name),"Map"===n||"Set"===n?Array.from(n):"Arguments"===n||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(n)?r(e,t):void 0}}(e,t)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}n.d(t,"a",(function(){return a}))},function(e,t,n){"use strict";n.d(t,"a",(function(){return f}));var r=n(3),a=n(7),i=n(0),o=n.n(i),l=n(9),u=(n(1),n(2)),c=n(4),s=n(8),f=function(e){function t(){for(var t,n=arguments.length,r=new Array(n),a=0;a<n;a++)r[a]=arguments[a];return(t=e.call.apply(e,[this].concat(r))||this).history=Object(l.a)(t.props),t}return Object(a.a)(t,e),t.prototype.render=function(){return o.a.createElement(r.c,{history:this.history,children:this.props.children})},t}(o.a.Component);o.a.Component;var d=function(e,t){return"function"===typeof e?e(t):e},p=function(e,t){return"string"===typeof e?Object(l.c)(e,null,null,t):e},m=function(e){return e},h=o.a.forwardRef;"undefined"===typeof h&&(h=m);var v=h((function(e,t){var n=e.innerRef,r=e.navigate,a=e.onClick,i=Object(c.a)(e,["innerRef","navigate","onClick"]),l=i.target,s=Object(u.a)({},i,{onClick:function(e){try{a&&a(e)}catch(t){throw e.preventDefault(),t}e.defaultPrevented||0!==e.button||l&&"_self"!==l||function(e){return!!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey)}(e)||(e.preventDefault(),r())}});return s.ref=m!==h&&t||n,o.a.createElement("a",s)}));var g=h((function(e,t){var n=e.component,a=void 0===n?v:n,i=e.replace,l=e.to,f=e.innerRef,g=Object(c.a)(e,["component","replace","to","innerRef"]);return o.a.createElement(r.e.Consumer,null,(function(e){e||Object(s.a)(!1);var n=e.history,r=p(d(l,e.location),e.location),c=r?n.createHref(r):"",v=Object(u.a)({},g,{href:c,navigate:function(){var t=d(l,e.location);(i?n.replace:n.push)(t)}});return m!==h?v.ref=t||f:v.innerRef=f,o.a.createElement(a,v)}))})),y=function(e){return e},b=o.a.forwardRef;"undefined"===typeof b&&(b=y);b((function(e,t){var n=e["aria-current"],a=void 0===n?"page":n,i=e.activeClassName,l=void 0===i?"active":i,f=e.activeStyle,m=e.className,h=e.exact,v=e.isActive,w=e.location,x=e.sensitive,E=e.strict,k=e.style,T=e.to,S=e.innerRef,C=Object(c.a)(e,["aria-current","activeClassName","activeStyle","className","exact","isActive","location","sensitive","strict","style","to","innerRef"]);return o.a.createElement(r.e.Consumer,null,(function(e){e||Object(s.a)(!1);var n=w||e.location,i=p(d(T,n),n),c=i.pathname,P=c&&c.replace(/([.+*?=^!:${}()[\]|/\\])/g,"\\$1"),_=P?Object(r.f)(n.pathname,{path:P,exact:h,sensitive:x,strict:E}):null,O=!!(v?v(_,n):_),N=O?function(){for(var e=arguments.length,t=new Array(e),n=0;n<e;n++)t[n]=arguments[n];return t.filter((function(e){return e})).join(" ")}(m,l):m,M=O?Object(u.a)({},k,{},f):k,R=Object(u.a)({"aria-current":O&&a||null,className:N,style:M,to:i},C);return y!==b?R.ref=t||S:R.innerRef=S,o.a.createElement(g,R)}))}))},function(e,t,n){e.exports=n(37)},,function(e,t,n){"use strict";(function(e){var r=n(0),a=n.n(r),i=n(7),o=n(1),l=n.n(o),u="undefined"!==typeof globalThis?globalThis:"undefined"!==typeof window?window:"undefined"!==typeof e?e:{};function c(e){var t=[];return{on:function(e){t.push(e)},off:function(e){t=t.filter((function(t){return t!==e}))},get:function(){return e},set:function(n,r){e=n,t.forEach((function(t){return t(e,r)}))}}}var s=a.a.createContext||function(e,t){var n,a,o="__create-react-context-"+function(){var e="__global_unique_id__";return u[e]=(u[e]||0)+1}()+"__",s=function(e){function n(){var t;return(t=e.apply(this,arguments)||this).emitter=c(t.props.value),t}Object(i.a)(n,e);var r=n.prototype;return r.getChildContext=function(){var e;return(e={})[o]=this.emitter,e},r.componentWillReceiveProps=function(e){if(this.props.value!==e.value){var n,r=this.props.value,a=e.value;((i=r)===(o=a)?0!==i||1/i===1/o:i!==i&&o!==o)?n=0:(n="function"===typeof t?t(r,a):1073741823,0!==(n|=0)&&this.emitter.set(e.value,n))}var i,o},r.render=function(){return this.props.children},n}(r.Component);s.childContextTypes=((n={})[o]=l.a.object.isRequired,n);var f=function(t){function n(){var e;return(e=t.apply(this,arguments)||this).state={value:e.getValue()},e.onUpdate=function(t,n){0!==((0|e.observedBits)&n)&&e.setState({value:e.getValue()})},e}Object(i.a)(n,t);var r=n.prototype;return r.componentWillReceiveProps=function(e){var t=e.observedBits;this.observedBits=void 0===t||null===t?1073741823:t},r.componentDidMount=function(){this.context[o]&&this.context[o].on(this.onUpdate);var e=this.props.observedBits;this.observedBits=void 0===e||null===e?1073741823:e},r.componentWillUnmount=function(){this.context[o]&&this.context[o].off(this.onUpdate)},r.getValue=function(){return this.context[o]?this.context[o].get():e},r.render=function(){return(e=this.props.children,Array.isArray(e)?e[0]:e)(this.state.value);var e},n}(r.Component);return f.contextTypes=((a={})[o]=l.a.object,a),{Provider:s,Consumer:f}};t.a=s}).call(this,n(56))},function(e,t,n){var r=n(57);e.exports=p,e.exports.parse=i,e.exports.compile=function(e,t){return l(i(e,t),t)},e.exports.tokensToFunction=l,e.exports.tokensToRegExp=d;var a=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function i(e,t){for(var n,r=[],i=0,o=0,l="",s=t&&t.delimiter||"/";null!=(n=a.exec(e));){var f=n[0],d=n[1],p=n.index;if(l+=e.slice(o,p),o=p+f.length,d)l+=d[1];else{var m=e[o],h=n[2],v=n[3],g=n[4],y=n[5],b=n[6],w=n[7];l&&(r.push(l),l="");var x=null!=h&&null!=m&&m!==h,E="+"===b||"*"===b,k="?"===b||"*"===b,T=n[2]||s,S=g||y;r.push({name:v||i++,prefix:h||"",delimiter:T,optional:k,repeat:E,partial:x,asterisk:!!w,pattern:S?c(S):w?".*":"[^"+u(T)+"]+?"})}}return o<e.length&&(l+=e.substr(o)),l&&r.push(l),r}function o(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function l(e,t){for(var n=new Array(e.length),a=0;a<e.length;a++)"object"===typeof e[a]&&(n[a]=new RegExp("^(?:"+e[a].pattern+")$",f(t)));return function(t,a){for(var i="",l=t||{},u=(a||{}).pretty?o:encodeURIComponent,c=0;c<e.length;c++){var s=e[c];if("string"!==typeof s){var f,d=l[s.name];if(null==d){if(s.optional){s.partial&&(i+=s.prefix);continue}throw new TypeError('Expected "'+s.name+'" to be defined')}if(r(d)){if(!s.repeat)throw new TypeError('Expected "'+s.name+'" to not repeat, but received `'+JSON.stringify(d)+"`");if(0===d.length){if(s.optional)continue;throw new TypeError('Expected "'+s.name+'" to not be empty')}for(var p=0;p<d.length;p++){if(f=u(d[p]),!n[c].test(f))throw new TypeError('Expected all "'+s.name+'" to match "'+s.pattern+'", but received `'+JSON.stringify(f)+"`");i+=(0===p?s.prefix:s.delimiter)+f}}else{if(f=s.asterisk?encodeURI(d).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):u(d),!n[c].test(f))throw new TypeError('Expected "'+s.name+'" to match "'+s.pattern+'", but received "'+f+'"');i+=s.prefix+f}}else i+=s}return i}}function u(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function c(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function s(e,t){return e.keys=t,e}function f(e){return e&&e.sensitive?"":"i"}function d(e,t,n){r(t)||(n=t||n,t=[]);for(var a=(n=n||{}).strict,i=!1!==n.end,o="",l=0;l<e.length;l++){var c=e[l];if("string"===typeof c)o+=u(c);else{var d=u(c.prefix),p="(?:"+c.pattern+")";t.push(c),c.repeat&&(p+="(?:"+d+p+")*"),o+=p=c.optional?c.partial?d+"("+p+")?":"(?:"+d+"("+p+"))?":d+"("+p+")"}}var m=u(n.delimiter||"/"),h=o.slice(-m.length)===m;return a||(o=(h?o.slice(0,-m.length):o)+"(?:"+m+"(?=$))?"),o+=i?"$":a&&h?"":"(?="+m+"|$)",s(new RegExp("^"+o,f(n)),t)}function p(e,t,n){return r(t)||(n=t||n,t=[]),n=n||{},e instanceof RegExp?function(e,t){var n=e.source.match(/\((?!\?)/g);if(n)for(var r=0;r<n.length;r++)t.push({name:r,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return s(e,t)}(e,t):r(e)?function(e,t,n){for(var r=[],a=0;a<e.length;a++)r.push(p(e[a],t,n).source);return s(new RegExp("(?:"+r.join("|")+")",f(n)),t)}(e,t,n):function(e,t,n){return d(i(e,n),t,n)}(e,t,n)}},function(e,t,n){"use strict";var r=Object.getOwnPropertySymbols,a=Object.prototype.hasOwnProperty,i=Object.prototype.propertyIsEnumerable;function o(e){if(null===e||void 0===e)throw new TypeError("Object.assign cannot be called with null or undefined");return Object(e)}e.exports=function(){try{if(!Object.assign)return!1;var e=new String("abc");if(e[5]="de","5"===Object.getOwnPropertyNames(e)[0])return!1;for(var t={},n=0;n<10;n++)t["_"+String.fromCharCode(n)]=n;if("0123456789"!==Object.getOwnPropertyNames(t).map((function(e){return t[e]})).join(""))return!1;var r={};return"abcdefghijklmnopqrst".split("").forEach((function(e){r[e]=e})),"abcdefghijklmnopqrst"===Object.keys(Object.assign({},r)).join("")}catch(a){return!1}}()?Object.assign:function(e,t){for(var n,l,u=o(e),c=1;c<arguments.length;c++){for(var s in n=Object(arguments[c]))a.call(n,s)&&(u[s]=n[s]);if(r){l=r(n);for(var f=0;f<l.length;f++)i.call(n,l[f])&&(u[l[f]]=n[l[f]])}}return u}},function(e,t,n){"use strict";e.exports=function(e,t){return function(){for(var n=new Array(arguments.length),r=0;r<n.length;r++)n[r]=arguments[r];return e.apply(t,n)}}},function(e,t,n){"use strict";var r=n(6);function a(e){return encodeURIComponent(e).replace(/%40/gi,"@").replace(/%3A/gi,":").replace(/%24/g,"$").replace(/%2C/gi,",").replace(/%20/g,"+").replace(/%5B/gi,"[").replace(/%5D/gi,"]")}e.exports=function(e,t,n){if(!t)return e;var i;if(n)i=n(t);else if(r.isURLSearchParams(t))i=t.toString();else{var o=[];r.forEach(t,(function(e,t){null!==e&&"undefined"!==typeof e&&(r.isArray(e)?t+="[]":e=[e],r.forEach(e,(function(e){r.isDate(e)?e=e.toISOString():r.isObject(e)&&(e=JSON.stringify(e)),o.push(a(t)+"="+a(e))})))})),i=o.join("&")}if(i){var l=e.indexOf("#");-1!==l&&(e=e.slice(0,l)),e+=(-1===e.indexOf("?")?"?":"&")+i}return e}},function(e,t,n){"use strict";e.exports=function(e){return!(!e||!e.__CANCEL__)}},function(e,t,n){"use strict";(function(t){var r=n(6),a=n(43),i={"Content-Type":"application/x-www-form-urlencoded"};function o(e,t){!r.isUndefined(e)&&r.isUndefined(e["Content-Type"])&&(e["Content-Type"]=t)}var l={adapter:function(){var e;return("undefined"!==typeof XMLHttpRequest||"undefined"!==typeof t&&"[object process]"===Object.prototype.toString.call(t))&&(e=n(22)),e}(),transformRequest:[function(e,t){return a(t,"Accept"),a(t,"Content-Type"),r.isFormData(e)||r.isArrayBuffer(e)||r.isBuffer(e)||r.isStream(e)||r.isFile(e)||r.isBlob(e)?e:r.isArrayBufferView(e)?e.buffer:r.isURLSearchParams(e)?(o(t,"application/x-www-form-urlencoded;charset=utf-8"),e.toString()):r.isObject(e)?(o(t,"application/json;charset=utf-8"),JSON.stringify(e)):e}],transformResponse:[function(e){if("string"===typeof e)try{e=JSON.parse(e)}catch(t){}return e}],timeout:0,xsrfCookieName:"XSRF-TOKEN",xsrfHeaderName:"X-XSRF-TOKEN",maxContentLength:-1,validateStatus:function(e){return e>=200&&e<300},headers:{common:{Accept:"application/json, text/plain, */*"}}};r.forEach(["delete","get","head"],(function(e){l.headers[e]={}})),r.forEach(["post","put","patch"],(function(e){l.headers[e]=r.merge(i)})),e.exports=l}).call(this,n(42))},function(e,t,n){"use strict";var r=n(6),a=n(44),i=n(19),o=n(46),l=n(49),u=n(50),c=n(23);e.exports=function(e){return new Promise((function(t,s){var f=e.data,d=e.headers;r.isFormData(f)&&delete d["Content-Type"];var p=new XMLHttpRequest;if(e.auth){var m=e.auth.username||"",h=e.auth.password||"";d.Authorization="Basic "+btoa(m+":"+h)}var v=o(e.baseURL,e.url);if(p.open(e.method.toUpperCase(),i(v,e.params,e.paramsSerializer),!0),p.timeout=e.timeout,p.onreadystatechange=function(){if(p&&4===p.readyState&&(0!==p.status||p.responseURL&&0===p.responseURL.indexOf("file:"))){var n="getAllResponseHeaders"in p?l(p.getAllResponseHeaders()):null,r={data:e.responseType&&"text"!==e.responseType?p.response:p.responseText,status:p.status,statusText:p.statusText,headers:n,config:e,request:p};a(t,s,r),p=null}},p.onabort=function(){p&&(s(c("Request aborted",e,"ECONNABORTED",p)),p=null)},p.onerror=function(){s(c("Network Error",e,null,p)),p=null},p.ontimeout=function(){var t="timeout of "+e.timeout+"ms exceeded";e.timeoutErrorMessage&&(t=e.timeoutErrorMessage),s(c(t,e,"ECONNABORTED",p)),p=null},r.isStandardBrowserEnv()){var g=n(51),y=(e.withCredentials||u(v))&&e.xsrfCookieName?g.read(e.xsrfCookieName):void 0;y&&(d[e.xsrfHeaderName]=y)}if("setRequestHeader"in p&&r.forEach(d,(function(e,t){"undefined"===typeof f&&"content-type"===t.toLowerCase()?delete d[t]:p.setRequestHeader(t,e)})),r.isUndefined(e.withCredentials)||(p.withCredentials=!!e.withCredentials),e.responseType)try{p.responseType=e.responseType}catch(b){if("json"!==e.responseType)throw b}"function"===typeof e.onDownloadProgress&&p.addEventListener("progress",e.onDownloadProgress),"function"===typeof e.onUploadProgress&&p.upload&&p.upload.addEventListener("progress",e.onUploadProgress),e.cancelToken&&e.cancelToken.promise.then((function(e){p&&(p.abort(),s(e),p=null)})),void 0===f&&(f=null),p.send(f)}))}},function(e,t,n){"use strict";var r=n(45);e.exports=function(e,t,n,a,i){var o=new Error(e);return r(o,t,n,a,i)}},function(e,t,n){"use strict";var r=n(6);e.exports=function(e,t){t=t||{};var n={},a=["url","method","params","data"],i=["headers","auth","proxy"],o=["baseURL","url","transformRequest","transformResponse","paramsSerializer","timeout","withCredentials","adapter","responseType","xsrfCookieName","xsrfHeaderName","onUploadProgress","onDownloadProgress","maxContentLength","validateStatus","maxRedirects","httpAgent","httpsAgent","cancelToken","socketPath"];r.forEach(a,(function(e){"undefined"!==typeof t[e]&&(n[e]=t[e])})),r.forEach(i,(function(a){r.isObject(t[a])?n[a]=r.deepMerge(e[a],t[a]):"undefined"!==typeof t[a]?n[a]=t[a]:r.isObject(e[a])?n[a]=r.deepMerge(e[a]):"undefined"!==typeof e[a]&&(n[a]=e[a])})),r.forEach(o,(function(r){"undefined"!==typeof t[r]?n[r]=t[r]:"undefined"!==typeof e[r]&&(n[r]=e[r])}));var l=a.concat(i).concat(o),u=Object.keys(t).filter((function(e){return-1===l.indexOf(e)}));return r.forEach(u,(function(r){"undefined"!==typeof t[r]?n[r]=t[r]:"undefined"!==typeof e[r]&&(n[r]=e[r])})),n}},function(e,t,n){"use strict";function r(e){this.message=e}r.prototype.toString=function(){return"Cancel"+(this.message?": "+this.message:"")},r.prototype.__CANCEL__=!0,e.exports=r},function(e,t,n){"use strict";e.exports=n(58)},function(e,t,n){"use strict";!function e(){if("undefined"!==typeof __REACT_DEVTOOLS_GLOBAL_HOOK__&&"function"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE){0;try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(e)}catch(t){console.error(t)}}}(),e.exports=n(32)},function(e,t,n){"use strict";var r=n(26),a={childContextTypes:!0,contextType:!0,contextTypes:!0,defaultProps:!0,displayName:!0,getDefaultProps:!0,getDerivedStateFromError:!0,getDerivedStateFromProps:!0,mixins:!0,propTypes:!0,type:!0},i={name:!0,length:!0,prototype:!0,caller:!0,callee:!0,arguments:!0,arity:!0},o={$$typeof:!0,compare:!0,defaultProps:!0,displayName:!0,propTypes:!0,type:!0},l={};function u(e){return r.isMemo(e)?o:l[e.$$typeof]||a}l[r.ForwardRef]={$$typeof:!0,render:!0,defaultProps:!0,displayName:!0,propTypes:!0},l[r.Memo]=o;var c=Object.defineProperty,s=Object.getOwnPropertyNames,f=Object.getOwnPropertySymbols,d=Object.getOwnPropertyDescriptor,p=Object.getPrototypeOf,m=Object.prototype;e.exports=function e(t,n,r){if("string"!==typeof n){if(m){var a=p(n);a&&a!==m&&e(t,a,r)}var o=s(n);f&&(o=o.concat(f(n)));for(var l=u(t),h=u(n),v=0;v<o.length;++v){var g=o[v];if(!i[g]&&(!r||!r[g])&&(!h||!h[g])&&(!l||!l[g])){var y=d(n,g);try{c(t,g,y)}catch(b){}}}}return t}},function(e,t,n){e.exports=n(60)},,function(e,t,n){"use strict";var r=n(17),a="function"===typeof Symbol&&Symbol.for,i=a?Symbol.for("react.element"):60103,o=a?Symbol.for("react.portal"):60106,l=a?Symbol.for("react.fragment"):60107,u=a?Symbol.for("react.strict_mode"):60108,c=a?Symbol.for("react.profiler"):60114,s=a?Symbol.for("react.provider"):60109,f=a?Symbol.for("react.context"):60110,d=a?Symbol.for("react.forward_ref"):60112,p=a?Symbol.for("react.suspense"):60113,m=a?Symbol.for("react.memo"):60115,h=a?Symbol.for("react.lazy"):60116,v="function"===typeof Symbol&&Symbol.iterator;function g(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}var y={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},b={};function w(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}function x(){}function E(e,t,n){this.props=e,this.context=t,this.refs=b,this.updater=n||y}w.prototype.isReactComponent={},w.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error(g(85));this.updater.enqueueSetState(this,e,t,"setState")},w.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},x.prototype=w.prototype;var k=E.prototype=new x;k.constructor=E,r(k,w.prototype),k.isPureReactComponent=!0;var T={current:null},S=Object.prototype.hasOwnProperty,C={key:!0,ref:!0,__self:!0,__source:!0};function P(e,t,n){var r,a={},o=null,l=null;if(null!=t)for(r in void 0!==t.ref&&(l=t.ref),void 0!==t.key&&(o=""+t.key),t)S.call(t,r)&&!C.hasOwnProperty(r)&&(a[r]=t[r]);var u=arguments.length-2;if(1===u)a.children=n;else if(1<u){for(var c=Array(u),s=0;s<u;s++)c[s]=arguments[s+2];a.children=c}if(e&&e.defaultProps)for(r in u=e.defaultProps)void 0===a[r]&&(a[r]=u[r]);return{$$typeof:i,type:e,key:o,ref:l,props:a,_owner:T.current}}function _(e){return"object"===typeof e&&null!==e&&e.$$typeof===i}var O=/\/+/g,N=[];function M(e,t,n,r){if(N.length){var a=N.pop();return a.result=e,a.keyPrefix=t,a.func=n,a.context=r,a.count=0,a}return{result:e,keyPrefix:t,func:n,context:r,count:0}}function R(e){e.result=null,e.keyPrefix=null,e.func=null,e.context=null,e.count=0,10>N.length&&N.push(e)}function j(e,t,n){return null==e?0:function e(t,n,r,a){var l=typeof t;"undefined"!==l&&"boolean"!==l||(t=null);var u=!1;if(null===t)u=!0;else switch(l){case"string":case"number":u=!0;break;case"object":switch(t.$$typeof){case i:case o:u=!0}}if(u)return r(a,t,""===n?"."+z(t,0):n),1;if(u=0,n=""===n?".":n+":",Array.isArray(t))for(var c=0;c<t.length;c++){var s=n+z(l=t[c],c);u+=e(l,s,r,a)}else if(null===t||"object"!==typeof t?s=null:s="function"===typeof(s=v&&t[v]||t["@@iterator"])?s:null,"function"===typeof s)for(t=s.call(t),c=0;!(l=t.next()).done;)u+=e(l=l.value,s=n+z(l,c++),r,a);else if("object"===l)throw r=""+t,Error(g(31,"[object Object]"===r?"object with keys {"+Object.keys(t).join(", ")+"}":r,""));return u}(e,"",t,n)}function z(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+(""+e).replace(/[=:]/g,(function(e){return t[e]}))}(e.key):t.toString(36)}function A(e,t){e.func.call(e.context,t,e.count++)}function L(e,t,n){var r=e.result,a=e.keyPrefix;e=e.func.call(e.context,t,e.count++),Array.isArray(e)?D(e,r,n,(function(e){return e})):null!=e&&(_(e)&&(e=function(e,t){return{$$typeof:i,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(e,a+(!e.key||t&&t.key===e.key?"":(""+e.key).replace(O,"$&/")+"/")+n)),r.push(e))}function D(e,t,n,r,a){var i="";null!=n&&(i=(""+n).replace(O,"$&/")+"/"),j(e,L,t=M(t,i,r,a)),R(t)}var I={current:null};function F(){var e=I.current;if(null===e)throw Error(g(321));return e}var U={ReactCurrentDispatcher:I,ReactCurrentBatchConfig:{suspense:null},ReactCurrentOwner:T,IsSomeRendererActing:{current:!1},assign:r};t.Children={map:function(e,t,n){if(null==e)return e;var r=[];return D(e,r,null,t,n),r},forEach:function(e,t,n){if(null==e)return e;j(e,A,t=M(null,null,t,n)),R(t)},count:function(e){return j(e,(function(){return null}),null)},toArray:function(e){var t=[];return D(e,t,null,(function(e){return e})),t},only:function(e){if(!_(e))throw Error(g(143));return e}},t.Component=w,t.Fragment=l,t.Profiler=c,t.PureComponent=E,t.StrictMode=u,t.Suspense=p,t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=U,t.cloneElement=function(e,t,n){if(null===e||void 0===e)throw Error(g(267,e));var a=r({},e.props),o=e.key,l=e.ref,u=e._owner;if(null!=t){if(void 0!==t.ref&&(l=t.ref,u=T.current),void 0!==t.key&&(o=""+t.key),e.type&&e.type.defaultProps)var c=e.type.defaultProps;for(s in t)S.call(t,s)&&!C.hasOwnProperty(s)&&(a[s]=void 0===t[s]&&void 0!==c?c[s]:t[s])}var s=arguments.length-2;if(1===s)a.children=n;else if(1<s){c=Array(s);for(var f=0;f<s;f++)c[f]=arguments[f+2];a.children=c}return{$$typeof:i,type:e.type,key:o,ref:l,props:a,_owner:u}},t.createContext=function(e,t){return void 0===t&&(t=null),(e={$$typeof:f,_calculateChangedBits:t,_currentValue:e,_currentValue2:e,_threadCount:0,Provider:null,Consumer:null}).Provider={$$typeof:s,_context:e},e.Consumer=e},t.createElement=P,t.createFactory=function(e){var t=P.bind(null,e);return t.type=e,t},t.createRef=function(){return{current:null}},t.forwardRef=function(e){return{$$typeof:d,render:e}},t.isValidElement=_,t.lazy=function(e){return{$$typeof:h,_ctor:e,_status:-1,_result:null}},t.memo=function(e,t){return{$$typeof:m,type:e,compare:void 0===t?null:t}},t.useCallback=function(e,t){return F().useCallback(e,t)},t.useContext=function(e,t){return F().useContext(e,t)},t.useDebugValue=function(){},t.useEffect=function(e,t){return F().useEffect(e,t)},t.useImperativeHandle=function(e,t,n){return F().useImperativeHandle(e,t,n)},t.useLayoutEffect=function(e,t){return F().useLayoutEffect(e,t)},t.useMemo=function(e,t){return F().useMemo(e,t)},t.useReducer=function(e,t,n){return F().useReducer(e,t,n)},t.useRef=function(e){return F().useRef(e)},t.useState=function(e){return F().useState(e)},t.version="16.13.1"},function(e,t,n){"use strict";var r=n(0),a=n(17),i=n(33);function o(e){for(var t="https://reactjs.org/docs/error-decoder.html?invariant="+e,n=1;n<arguments.length;n++)t+="&args[]="+encodeURIComponent(arguments[n]);return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}if(!r)throw Error(o(227));function l(e,t,n,r,a,i,o,l,u){var c=Array.prototype.slice.call(arguments,3);try{t.apply(n,c)}catch(s){this.onError(s)}}var u=!1,c=null,s=!1,f=null,d={onError:function(e){u=!0,c=e}};function p(e,t,n,r,a,i,o,s,f){u=!1,c=null,l.apply(d,arguments)}var m=null,h=null,v=null;function g(e,t,n){var r=e.type||"unknown-event";e.currentTarget=v(n),function(e,t,n,r,a,i,l,d,m){if(p.apply(this,arguments),u){if(!u)throw Error(o(198));var h=c;u=!1,c=null,s||(s=!0,f=h)}}(r,t,void 0,e),e.currentTarget=null}var y=null,b={};function w(){if(y)for(var e in b){var t=b[e],n=y.indexOf(e);if(!(-1<n))throw Error(o(96,e));if(!E[n]){if(!t.extractEvents)throw Error(o(97,e));for(var r in E[n]=t,n=t.eventTypes){var a=void 0,i=n[r],l=t,u=r;if(k.hasOwnProperty(u))throw Error(o(99,u));k[u]=i;var c=i.phasedRegistrationNames;if(c){for(a in c)c.hasOwnProperty(a)&&x(c[a],l,u);a=!0}else i.registrationName?(x(i.registrationName,l,u),a=!0):a=!1;if(!a)throw Error(o(98,r,e))}}}}function x(e,t,n){if(T[e])throw Error(o(100,e));T[e]=t,S[e]=t.eventTypes[n].dependencies}var E=[],k={},T={},S={};function C(e){var t,n=!1;for(t in e)if(e.hasOwnProperty(t)){var r=e[t];if(!b.hasOwnProperty(t)||b[t]!==r){if(b[t])throw Error(o(102,t));b[t]=r,n=!0}}n&&w()}var P=!("undefined"===typeof window||"undefined"===typeof window.document||"undefined"===typeof window.document.createElement),_=null,O=null,N=null;function M(e){if(e=h(e)){if("function"!==typeof _)throw Error(o(280));var t=e.stateNode;t&&(t=m(t),_(e.stateNode,e.type,t))}}function R(e){O?N?N.push(e):N=[e]:O=e}function j(){if(O){var e=O,t=N;if(N=O=null,M(e),t)for(e=0;e<t.length;e++)M(t[e])}}function z(e,t){return e(t)}function A(e,t,n,r,a){return e(t,n,r,a)}function L(){}var D=z,I=!1,F=!1;function U(){null===O&&null===N||(L(),j())}function B(e,t,n){if(F)return e(t,n);F=!0;try{return D(e,t,n)}finally{F=!1,U()}}var $=/^[:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD][:A-Z_a-z\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u02FF\u0370-\u037D\u037F-\u1FFF\u200C-\u200D\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD\-.0-9\u00B7\u0300-\u036F\u203F-\u2040]*$/,W=Object.prototype.hasOwnProperty,V={},H={};function Q(e,t,n,r,a,i){this.acceptsBooleans=2===t||3===t||4===t,this.attributeName=r,this.attributeNamespace=a,this.mustUseProperty=n,this.propertyName=e,this.type=t,this.sanitizeURL=i}var q={};"children dangerouslySetInnerHTML defaultValue defaultChecked innerHTML suppressContentEditableWarning suppressHydrationWarning style".split(" ").forEach((function(e){q[e]=new Q(e,0,!1,e,null,!1)})),[["acceptCharset","accept-charset"],["className","class"],["htmlFor","for"],["httpEquiv","http-equiv"]].forEach((function(e){var t=e[0];q[t]=new Q(t,1,!1,e[1],null,!1)})),["contentEditable","draggable","spellCheck","value"].forEach((function(e){q[e]=new Q(e,2,!1,e.toLowerCase(),null,!1)})),["autoReverse","externalResourcesRequired","focusable","preserveAlpha"].forEach((function(e){q[e]=new Q(e,2,!1,e,null,!1)})),"allowFullScreen async autoFocus autoPlay controls default defer disabled disablePictureInPicture formNoValidate hidden loop noModule noValidate open playsInline readOnly required reversed scoped seamless itemScope".split(" ").forEach((function(e){q[e]=new Q(e,3,!1,e.toLowerCase(),null,!1)})),["checked","multiple","muted","selected"].forEach((function(e){q[e]=new Q(e,3,!0,e,null,!1)})),["capture","download"].forEach((function(e){q[e]=new Q(e,4,!1,e,null,!1)})),["cols","rows","size","span"].forEach((function(e){q[e]=new Q(e,6,!1,e,null,!1)})),["rowSpan","start"].forEach((function(e){q[e]=new Q(e,5,!1,e.toLowerCase(),null,!1)}));var K=/[\-:]([a-z])/g;function X(e){return e[1].toUpperCase()}"accent-height alignment-baseline arabic-form baseline-shift cap-height clip-path clip-rule color-interpolation color-interpolation-filters color-profile color-rendering dominant-baseline enable-background fill-opacity fill-rule flood-color flood-opacity font-family font-size font-size-adjust font-stretch font-style font-variant font-weight glyph-name glyph-orientation-horizontal glyph-orientation-vertical horiz-adv-x horiz-origin-x image-rendering letter-spacing lighting-color marker-end marker-mid marker-start overline-position overline-thickness paint-order panose-1 pointer-events rendering-intent shape-rendering stop-color stop-opacity strikethrough-position strikethrough-thickness stroke-dasharray stroke-dashoffset stroke-linecap stroke-linejoin stroke-miterlimit stroke-opacity stroke-width text-anchor text-decoration text-rendering underline-position underline-thickness unicode-bidi unicode-range units-per-em v-alphabetic v-hanging v-ideographic v-mathematical vector-effect vert-adv-y vert-origin-x vert-origin-y word-spacing writing-mode xmlns:xlink x-height".split(" ").forEach((function(e){var t=e.replace(K,X);q[t]=new Q(t,1,!1,e,null,!1)})),"xlink:actuate xlink:arcrole xlink:role xlink:show xlink:title xlink:type".split(" ").forEach((function(e){var t=e.replace(K,X);q[t]=new Q(t,1,!1,e,"http://www.w3.org/1999/xlink",!1)})),["xml:base","xml:lang","xml:space"].forEach((function(e){var t=e.replace(K,X);q[t]=new Q(t,1,!1,e,"http://www.w3.org/XML/1998/namespace",!1)})),["tabIndex","crossOrigin"].forEach((function(e){q[e]=new Q(e,1,!1,e.toLowerCase(),null,!1)})),q.xlinkHref=new Q("xlinkHref",1,!1,"xlink:href","http://www.w3.org/1999/xlink",!0),["src","href","action","formAction"].forEach((function(e){q[e]=new Q(e,1,!1,e.toLowerCase(),null,!0)}));var Y=r.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function G(e,t,n,r){var a=q.hasOwnProperty(t)?q[t]:null;(null!==a?0===a.type:!r&&(2<t.length&&("o"===t[0]||"O"===t[0])&&("n"===t[1]||"N"===t[1])))||(function(e,t,n,r){if(null===t||"undefined"===typeof t||function(e,t,n,r){if(null!==n&&0===n.type)return!1;switch(typeof t){case"function":case"symbol":return!0;case"boolean":return!r&&(null!==n?!n.acceptsBooleans:"data-"!==(e=e.toLowerCase().slice(0,5))&&"aria-"!==e);default:return!1}}(e,t,n,r))return!0;if(r)return!1;if(null!==n)switch(n.type){case 3:return!t;case 4:return!1===t;case 5:return isNaN(t);case 6:return isNaN(t)||1>t}return!1}(t,n,a,r)&&(n=null),r||null===a?function(e){return!!W.call(H,e)||!W.call(V,e)&&($.test(e)?H[e]=!0:(V[e]=!0,!1))}(t)&&(null===n?e.removeAttribute(t):e.setAttribute(t,""+n)):a.mustUseProperty?e[a.propertyName]=null===n?3!==a.type&&"":n:(t=a.attributeName,r=a.attributeNamespace,null===n?e.removeAttribute(t):(n=3===(a=a.type)||4===a&&!0===n?"":""+n,r?e.setAttributeNS(r,t,n):e.setAttribute(t,n))))}Y.hasOwnProperty("ReactCurrentDispatcher")||(Y.ReactCurrentDispatcher={current:null}),Y.hasOwnProperty("ReactCurrentBatchConfig")||(Y.ReactCurrentBatchConfig={suspense:null});var J=/^(.*)[\\\/]/,Z="function"===typeof Symbol&&Symbol.for,ee=Z?Symbol.for("react.element"):60103,te=Z?Symbol.for("react.portal"):60106,ne=Z?Symbol.for("react.fragment"):60107,re=Z?Symbol.for("react.strict_mode"):60108,ae=Z?Symbol.for("react.profiler"):60114,ie=Z?Symbol.for("react.provider"):60109,oe=Z?Symbol.for("react.context"):60110,le=Z?Symbol.for("react.concurrent_mode"):60111,ue=Z?Symbol.for("react.forward_ref"):60112,ce=Z?Symbol.for("react.suspense"):60113,se=Z?Symbol.for("react.suspense_list"):60120,fe=Z?Symbol.for("react.memo"):60115,de=Z?Symbol.for("react.lazy"):60116,pe=Z?Symbol.for("react.block"):60121,me="function"===typeof Symbol&&Symbol.iterator;function he(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=me&&e[me]||e["@@iterator"])?e:null}function ve(e){if(null==e)return null;if("function"===typeof e)return e.displayName||e.name||null;if("string"===typeof e)return e;switch(e){case ne:return"Fragment";case te:return"Portal";case ae:return"Profiler";case re:return"StrictMode";case ce:return"Suspense";case se:return"SuspenseList"}if("object"===typeof e)switch(e.$$typeof){case oe:return"Context.Consumer";case ie:return"Context.Provider";case ue:var t=e.render;return t=t.displayName||t.name||"",e.displayName||(""!==t?"ForwardRef("+t+")":"ForwardRef");case fe:return ve(e.type);case pe:return ve(e.render);case de:if(e=1===e._status?e._result:null)return ve(e)}return null}function ge(e){var t="";do{e:switch(e.tag){case 3:case 4:case 6:case 7:case 10:case 9:var n="";break e;default:var r=e._debugOwner,a=e._debugSource,i=ve(e.type);n=null,r&&(n=ve(r.type)),r=i,i="",a?i=" (at "+a.fileName.replace(J,"")+":"+a.lineNumber+")":n&&(i=" (created by "+n+")"),n="\n    in "+(r||"Unknown")+i}t+=n,e=e.return}while(e);return t}function ye(e){switch(typeof e){case"boolean":case"number":case"object":case"string":case"undefined":return e;default:return""}}function be(e){var t=e.type;return(e=e.nodeName)&&"input"===e.toLowerCase()&&("checkbox"===t||"radio"===t)}function we(e){e._valueTracker||(e._valueTracker=function(e){var t=be(e)?"checked":"value",n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t),r=""+e[t];if(!e.hasOwnProperty(t)&&"undefined"!==typeof n&&"function"===typeof n.get&&"function"===typeof n.set){var a=n.get,i=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return a.call(this)},set:function(e){r=""+e,i.call(this,e)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return r},setValue:function(e){r=""+e},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}(e))}function xe(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var n=t.getValue(),r="";return e&&(r=be(e)?e.checked?"true":"false":e.value),(e=r)!==n&&(t.setValue(e),!0)}function Ee(e,t){var n=t.checked;return a({},t,{defaultChecked:void 0,defaultValue:void 0,value:void 0,checked:null!=n?n:e._wrapperState.initialChecked})}function ke(e,t){var n=null==t.defaultValue?"":t.defaultValue,r=null!=t.checked?t.checked:t.defaultChecked;n=ye(null!=t.value?t.value:n),e._wrapperState={initialChecked:r,initialValue:n,controlled:"checkbox"===t.type||"radio"===t.type?null!=t.checked:null!=t.value}}function Te(e,t){null!=(t=t.checked)&&G(e,"checked",t,!1)}function Se(e,t){Te(e,t);var n=ye(t.value),r=t.type;if(null!=n)"number"===r?(0===n&&""===e.value||e.value!=n)&&(e.value=""+n):e.value!==""+n&&(e.value=""+n);else if("submit"===r||"reset"===r)return void e.removeAttribute("value");t.hasOwnProperty("value")?Pe(e,t.type,n):t.hasOwnProperty("defaultValue")&&Pe(e,t.type,ye(t.defaultValue)),null==t.checked&&null!=t.defaultChecked&&(e.defaultChecked=!!t.defaultChecked)}function Ce(e,t,n){if(t.hasOwnProperty("value")||t.hasOwnProperty("defaultValue")){var r=t.type;if(!("submit"!==r&&"reset"!==r||void 0!==t.value&&null!==t.value))return;t=""+e._wrapperState.initialValue,n||t===e.value||(e.value=t),e.defaultValue=t}""!==(n=e.name)&&(e.name=""),e.defaultChecked=!!e._wrapperState.initialChecked,""!==n&&(e.name=n)}function Pe(e,t,n){"number"===t&&e.ownerDocument.activeElement===e||(null==n?e.defaultValue=""+e._wrapperState.initialValue:e.defaultValue!==""+n&&(e.defaultValue=""+n))}function _e(e,t){return e=a({children:void 0},t),(t=function(e){var t="";return r.Children.forEach(e,(function(e){null!=e&&(t+=e)})),t}(t.children))&&(e.children=t),e}function Oe(e,t,n,r){if(e=e.options,t){t={};for(var a=0;a<n.length;a++)t["$"+n[a]]=!0;for(n=0;n<e.length;n++)a=t.hasOwnProperty("$"+e[n].value),e[n].selected!==a&&(e[n].selected=a),a&&r&&(e[n].defaultSelected=!0)}else{for(n=""+ye(n),t=null,a=0;a<e.length;a++){if(e[a].value===n)return e[a].selected=!0,void(r&&(e[a].defaultSelected=!0));null!==t||e[a].disabled||(t=e[a])}null!==t&&(t.selected=!0)}}function Ne(e,t){if(null!=t.dangerouslySetInnerHTML)throw Error(o(91));return a({},t,{value:void 0,defaultValue:void 0,children:""+e._wrapperState.initialValue})}function Me(e,t){var n=t.value;if(null==n){if(n=t.children,t=t.defaultValue,null!=n){if(null!=t)throw Error(o(92));if(Array.isArray(n)){if(!(1>=n.length))throw Error(o(93));n=n[0]}t=n}null==t&&(t=""),n=t}e._wrapperState={initialValue:ye(n)}}function Re(e,t){var n=ye(t.value),r=ye(t.defaultValue);null!=n&&((n=""+n)!==e.value&&(e.value=n),null==t.defaultValue&&e.defaultValue!==n&&(e.defaultValue=n)),null!=r&&(e.defaultValue=""+r)}function je(e){var t=e.textContent;t===e._wrapperState.initialValue&&""!==t&&null!==t&&(e.value=t)}var ze="http://www.w3.org/1999/xhtml",Ae="http://www.w3.org/2000/svg";function Le(e){switch(e){case"svg":return"http://www.w3.org/2000/svg";case"math":return"http://www.w3.org/1998/Math/MathML";default:return"http://www.w3.org/1999/xhtml"}}function De(e,t){return null==e||"http://www.w3.org/1999/xhtml"===e?Le(t):"http://www.w3.org/2000/svg"===e&&"foreignObject"===t?"http://www.w3.org/1999/xhtml":e}var Ie,Fe=function(e){return"undefined"!==typeof MSApp&&MSApp.execUnsafeLocalFunction?function(t,n,r,a){MSApp.execUnsafeLocalFunction((function(){return e(t,n)}))}:e}((function(e,t){if(e.namespaceURI!==Ae||"innerHTML"in e)e.innerHTML=t;else{for((Ie=Ie||document.createElement("div")).innerHTML="<svg>"+t.valueOf().toString()+"</svg>",t=Ie.firstChild;e.firstChild;)e.removeChild(e.firstChild);for(;t.firstChild;)e.appendChild(t.firstChild)}}));function Ue(e,t){if(t){var n=e.firstChild;if(n&&n===e.lastChild&&3===n.nodeType)return void(n.nodeValue=t)}e.textContent=t}function Be(e,t){var n={};return n[e.toLowerCase()]=t.toLowerCase(),n["Webkit"+e]="webkit"+t,n["Moz"+e]="moz"+t,n}var $e={animationend:Be("Animation","AnimationEnd"),animationiteration:Be("Animation","AnimationIteration"),animationstart:Be("Animation","AnimationStart"),transitionend:Be("Transition","TransitionEnd")},We={},Ve={};function He(e){if(We[e])return We[e];if(!$e[e])return e;var t,n=$e[e];for(t in n)if(n.hasOwnProperty(t)&&t in Ve)return We[e]=n[t];return e}P&&(Ve=document.createElement("div").style,"AnimationEvent"in window||(delete $e.animationend.animation,delete $e.animationiteration.animation,delete $e.animationstart.animation),"TransitionEvent"in window||delete $e.transitionend.transition);var Qe=He("animationend"),qe=He("animationiteration"),Ke=He("animationstart"),Xe=He("transitionend"),Ye="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),Ge=new("function"===typeof WeakMap?WeakMap:Map);function Je(e){var t=Ge.get(e);return void 0===t&&(t=new Map,Ge.set(e,t)),t}function Ze(e){var t=e,n=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do{0!==(1026&(t=e).effectTag)&&(n=t.return),e=t.return}while(e)}return 3===t.tag?n:null}function et(e){if(13===e.tag){var t=e.memoizedState;if(null===t&&(null!==(e=e.alternate)&&(t=e.memoizedState)),null!==t)return t.dehydrated}return null}function tt(e){if(Ze(e)!==e)throw Error(o(188))}function nt(e){if(!(e=function(e){var t=e.alternate;if(!t){if(null===(t=Ze(e)))throw Error(o(188));return t!==e?null:e}for(var n=e,r=t;;){var a=n.return;if(null===a)break;var i=a.alternate;if(null===i){if(null!==(r=a.return)){n=r;continue}break}if(a.child===i.child){for(i=a.child;i;){if(i===n)return tt(a),e;if(i===r)return tt(a),t;i=i.sibling}throw Error(o(188))}if(n.return!==r.return)n=a,r=i;else{for(var l=!1,u=a.child;u;){if(u===n){l=!0,n=a,r=i;break}if(u===r){l=!0,r=a,n=i;break}u=u.sibling}if(!l){for(u=i.child;u;){if(u===n){l=!0,n=i,r=a;break}if(u===r){l=!0,r=i,n=a;break}u=u.sibling}if(!l)throw Error(o(189))}}if(n.alternate!==r)throw Error(o(190))}if(3!==n.tag)throw Error(o(188));return n.stateNode.current===n?e:t}(e)))return null;for(var t=e;;){if(5===t.tag||6===t.tag)return t;if(t.child)t.child.return=t,t=t.child;else{if(t===e)break;for(;!t.sibling;){if(!t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}}return null}function rt(e,t){if(null==t)throw Error(o(30));return null==e?t:Array.isArray(e)?Array.isArray(t)?(e.push.apply(e,t),e):(e.push(t),e):Array.isArray(t)?[e].concat(t):[e,t]}function at(e,t,n){Array.isArray(e)?e.forEach(t,n):e&&t.call(n,e)}var it=null;function ot(e){if(e){var t=e._dispatchListeners,n=e._dispatchInstances;if(Array.isArray(t))for(var r=0;r<t.length&&!e.isPropagationStopped();r++)g(e,t[r],n[r]);else t&&g(e,t,n);e._dispatchListeners=null,e._dispatchInstances=null,e.isPersistent()||e.constructor.release(e)}}function lt(e){if(null!==e&&(it=rt(it,e)),e=it,it=null,e){if(at(e,ot),it)throw Error(o(95));if(s)throw e=f,s=!1,f=null,e}}function ut(e){return(e=e.target||e.srcElement||window).correspondingUseElement&&(e=e.correspondingUseElement),3===e.nodeType?e.parentNode:e}function ct(e){if(!P)return!1;var t=(e="on"+e)in document;return t||((t=document.createElement("div")).setAttribute(e,"return;"),t="function"===typeof t[e]),t}var st=[];function ft(e){e.topLevelType=null,e.nativeEvent=null,e.targetInst=null,e.ancestors.length=0,10>st.length&&st.push(e)}function dt(e,t,n,r){if(st.length){var a=st.pop();return a.topLevelType=e,a.eventSystemFlags=r,a.nativeEvent=t,a.targetInst=n,a}return{topLevelType:e,eventSystemFlags:r,nativeEvent:t,targetInst:n,ancestors:[]}}function pt(e){var t=e.targetInst,n=t;do{if(!n){e.ancestors.push(n);break}var r=n;if(3===r.tag)r=r.stateNode.containerInfo;else{for(;r.return;)r=r.return;r=3!==r.tag?null:r.stateNode.containerInfo}if(!r)break;5!==(t=n.tag)&&6!==t||e.ancestors.push(n),n=Pn(r)}while(n);for(n=0;n<e.ancestors.length;n++){t=e.ancestors[n];var a=ut(e.nativeEvent);r=e.topLevelType;var i=e.nativeEvent,o=e.eventSystemFlags;0===n&&(o|=64);for(var l=null,u=0;u<E.length;u++){var c=E[u];c&&(c=c.extractEvents(r,t,i,a,o))&&(l=rt(l,c))}lt(l)}}function mt(e,t,n){if(!n.has(e)){switch(e){case"scroll":Kt(t,"scroll",!0);break;case"focus":case"blur":Kt(t,"focus",!0),Kt(t,"blur",!0),n.set("blur",null),n.set("focus",null);break;case"cancel":case"close":ct(e)&&Kt(t,e,!0);break;case"invalid":case"submit":case"reset":break;default:-1===Ye.indexOf(e)&&qt(e,t)}n.set(e,null)}}var ht,vt,gt,yt=!1,bt=[],wt=null,xt=null,Et=null,kt=new Map,Tt=new Map,St=[],Ct="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput close cancel copy cut paste click change contextmenu reset submit".split(" "),Pt="focus blur dragenter dragleave mouseover mouseout pointerover pointerout gotpointercapture lostpointercapture".split(" ");function _t(e,t,n,r,a){return{blockedOn:e,topLevelType:t,eventSystemFlags:32|n,nativeEvent:a,container:r}}function Ot(e,t){switch(e){case"focus":case"blur":wt=null;break;case"dragenter":case"dragleave":xt=null;break;case"mouseover":case"mouseout":Et=null;break;case"pointerover":case"pointerout":kt.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":Tt.delete(t.pointerId)}}function Nt(e,t,n,r,a,i){return null===e||e.nativeEvent!==i?(e=_t(t,n,r,a,i),null!==t&&(null!==(t=_n(t))&&vt(t)),e):(e.eventSystemFlags|=r,e)}function Mt(e){var t=Pn(e.target);if(null!==t){var n=Ze(t);if(null!==n)if(13===(t=n.tag)){if(null!==(t=et(n)))return e.blockedOn=t,void i.unstable_runWithPriority(e.priority,(function(){gt(n)}))}else if(3===t&&n.stateNode.hydrate)return void(e.blockedOn=3===n.tag?n.stateNode.containerInfo:null)}e.blockedOn=null}function Rt(e){if(null!==e.blockedOn)return!1;var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);if(null!==t){var n=_n(t);return null!==n&&vt(n),e.blockedOn=t,!1}return!0}function jt(e,t,n){Rt(e)&&n.delete(t)}function zt(){for(yt=!1;0<bt.length;){var e=bt[0];if(null!==e.blockedOn){null!==(e=_n(e.blockedOn))&&ht(e);break}var t=Jt(e.topLevelType,e.eventSystemFlags,e.container,e.nativeEvent);null!==t?e.blockedOn=t:bt.shift()}null!==wt&&Rt(wt)&&(wt=null),null!==xt&&Rt(xt)&&(xt=null),null!==Et&&Rt(Et)&&(Et=null),kt.forEach(jt),Tt.forEach(jt)}function At(e,t){e.blockedOn===t&&(e.blockedOn=null,yt||(yt=!0,i.unstable_scheduleCallback(i.unstable_NormalPriority,zt)))}function Lt(e){function t(t){return At(t,e)}if(0<bt.length){At(bt[0],e);for(var n=1;n<bt.length;n++){var r=bt[n];r.blockedOn===e&&(r.blockedOn=null)}}for(null!==wt&&At(wt,e),null!==xt&&At(xt,e),null!==Et&&At(Et,e),kt.forEach(t),Tt.forEach(t),n=0;n<St.length;n++)(r=St[n]).blockedOn===e&&(r.blockedOn=null);for(;0<St.length&&null===(n=St[0]).blockedOn;)Mt(n),null===n.blockedOn&&St.shift()}var Dt={},It=new Map,Ft=new Map,Ut=["abort","abort",Qe,"animationEnd",qe,"animationIteration",Ke,"animationStart","canplay","canPlay","canplaythrough","canPlayThrough","durationchange","durationChange","emptied","emptied","encrypted","encrypted","ended","ended","error","error","gotpointercapture","gotPointerCapture","load","load","loadeddata","loadedData","loadedmetadata","loadedMetadata","loadstart","loadStart","lostpointercapture","lostPointerCapture","playing","playing","progress","progress","seeking","seeking","stalled","stalled","suspend","suspend","timeupdate","timeUpdate",Xe,"transitionEnd","waiting","waiting"];function Bt(e,t){for(var n=0;n<e.length;n+=2){var r=e[n],a=e[n+1],i="on"+(a[0].toUpperCase()+a.slice(1));i={phasedRegistrationNames:{bubbled:i,captured:i+"Capture"},dependencies:[r],eventPriority:t},Ft.set(r,t),It.set(r,i),Dt[a]=i}}Bt("blur blur cancel cancel click click close close contextmenu contextMenu copy copy cut cut auxclick auxClick dblclick doubleClick dragend dragEnd dragstart dragStart drop drop focus focus input input invalid invalid keydown keyDown keypress keyPress keyup keyUp mousedown mouseDown mouseup mouseUp paste paste pause pause play play pointercancel pointerCancel pointerdown pointerDown pointerup pointerUp ratechange rateChange reset reset seeked seeked submit submit touchcancel touchCancel touchend touchEnd touchstart touchStart volumechange volumeChange".split(" "),0),Bt("drag drag dragenter dragEnter dragexit dragExit dragleave dragLeave dragover dragOver mousemove mouseMove mouseout mouseOut mouseover mouseOver pointermove pointerMove pointerout pointerOut pointerover pointerOver scroll scroll toggle toggle touchmove touchMove wheel wheel".split(" "),1),Bt(Ut,2);for(var $t="change selectionchange textInput compositionstart compositionend compositionupdate".split(" "),Wt=0;Wt<$t.length;Wt++)Ft.set($t[Wt],0);var Vt=i.unstable_UserBlockingPriority,Ht=i.unstable_runWithPriority,Qt=!0;function qt(e,t){Kt(t,e,!1)}function Kt(e,t,n){var r=Ft.get(t);switch(void 0===r?2:r){case 0:r=Xt.bind(null,t,1,e);break;case 1:r=Yt.bind(null,t,1,e);break;default:r=Gt.bind(null,t,1,e)}n?e.addEventListener(t,r,!0):e.addEventListener(t,r,!1)}function Xt(e,t,n,r){I||L();var a=Gt,i=I;I=!0;try{A(a,e,t,n,r)}finally{(I=i)||U()}}function Yt(e,t,n,r){Ht(Vt,Gt.bind(null,e,t,n,r))}function Gt(e,t,n,r){if(Qt)if(0<bt.length&&-1<Ct.indexOf(e))e=_t(null,e,t,n,r),bt.push(e);else{var a=Jt(e,t,n,r);if(null===a)Ot(e,r);else if(-1<Ct.indexOf(e))e=_t(a,e,t,n,r),bt.push(e);else if(!function(e,t,n,r,a){switch(t){case"focus":return wt=Nt(wt,e,t,n,r,a),!0;case"dragenter":return xt=Nt(xt,e,t,n,r,a),!0;case"mouseover":return Et=Nt(Et,e,t,n,r,a),!0;case"pointerover":var i=a.pointerId;return kt.set(i,Nt(kt.get(i)||null,e,t,n,r,a)),!0;case"gotpointercapture":return i=a.pointerId,Tt.set(i,Nt(Tt.get(i)||null,e,t,n,r,a)),!0}return!1}(a,e,t,n,r)){Ot(e,r),e=dt(e,r,null,t);try{B(pt,e)}finally{ft(e)}}}}function Jt(e,t,n,r){if(null!==(n=Pn(n=ut(r)))){var a=Ze(n);if(null===a)n=null;else{var i=a.tag;if(13===i){if(null!==(n=et(a)))return n;n=null}else if(3===i){if(a.stateNode.hydrate)return 3===a.tag?a.stateNode.containerInfo:null;n=null}else a!==n&&(n=null)}}e=dt(e,r,n,t);try{B(pt,e)}finally{ft(e)}return null}var Zt={animationIterationCount:!0,borderImageOutset:!0,borderImageSlice:!0,borderImageWidth:!0,boxFlex:!0,boxFlexGroup:!0,boxOrdinalGroup:!0,columnCount:!0,columns:!0,flex:!0,flexGrow:!0,flexPositive:!0,flexShrink:!0,flexNegative:!0,flexOrder:!0,gridArea:!0,gridRow:!0,gridRowEnd:!0,gridRowSpan:!0,gridRowStart:!0,gridColumn:!0,gridColumnEnd:!0,gridColumnSpan:!0,gridColumnStart:!0,fontWeight:!0,lineClamp:!0,lineHeight:!0,opacity:!0,order:!0,orphans:!0,tabSize:!0,widows:!0,zIndex:!0,zoom:!0,fillOpacity:!0,floodOpacity:!0,stopOpacity:!0,strokeDasharray:!0,strokeDashoffset:!0,strokeMiterlimit:!0,strokeOpacity:!0,strokeWidth:!0},en=["Webkit","ms","Moz","O"];function tn(e,t,n){return null==t||"boolean"===typeof t||""===t?"":n||"number"!==typeof t||0===t||Zt.hasOwnProperty(e)&&Zt[e]?(""+t).trim():t+"px"}function nn(e,t){for(var n in e=e.style,t)if(t.hasOwnProperty(n)){var r=0===n.indexOf("--"),a=tn(n,t[n],r);"float"===n&&(n="cssFloat"),r?e.setProperty(n,a):e[n]=a}}Object.keys(Zt).forEach((function(e){en.forEach((function(t){t=t+e.charAt(0).toUpperCase()+e.substring(1),Zt[t]=Zt[e]}))}));var rn=a({menuitem:!0},{area:!0,base:!0,br:!0,col:!0,embed:!0,hr:!0,img:!0,input:!0,keygen:!0,link:!0,meta:!0,param:!0,source:!0,track:!0,wbr:!0});function an(e,t){if(t){if(rn[e]&&(null!=t.children||null!=t.dangerouslySetInnerHTML))throw Error(o(137,e,""));if(null!=t.dangerouslySetInnerHTML){if(null!=t.children)throw Error(o(60));if("object"!==typeof t.dangerouslySetInnerHTML||!("__html"in t.dangerouslySetInnerHTML))throw Error(o(61))}if(null!=t.style&&"object"!==typeof t.style)throw Error(o(62,""))}}function on(e,t){if(-1===e.indexOf("-"))return"string"===typeof t.is;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var ln=ze;function un(e,t){var n=Je(e=9===e.nodeType||11===e.nodeType?e:e.ownerDocument);t=S[t];for(var r=0;r<t.length;r++)mt(t[r],e,n)}function cn(){}function sn(e){if("undefined"===typeof(e=e||("undefined"!==typeof document?document:void 0)))return null;try{return e.activeElement||e.body}catch(t){return e.body}}function fn(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function dn(e,t){var n,r=fn(e);for(e=0;r;){if(3===r.nodeType){if(n=e+r.textContent.length,e<=t&&n>=t)return{node:r,offset:t-e};e=n}e:{for(;r;){if(r.nextSibling){r=r.nextSibling;break e}r=r.parentNode}r=void 0}r=fn(r)}}function pn(){for(var e=window,t=sn();t instanceof e.HTMLIFrameElement;){try{var n="string"===typeof t.contentWindow.location.href}catch(r){n=!1}if(!n)break;t=sn((e=t.contentWindow).document)}return t}function mn(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&("input"===t&&("text"===e.type||"search"===e.type||"tel"===e.type||"url"===e.type||"password"===e.type)||"textarea"===t||"true"===e.contentEditable)}var hn=null,vn=null;function gn(e,t){switch(e){case"button":case"input":case"select":case"textarea":return!!t.autoFocus}return!1}function yn(e,t){return"textarea"===e||"option"===e||"noscript"===e||"string"===typeof t.children||"number"===typeof t.children||"object"===typeof t.dangerouslySetInnerHTML&&null!==t.dangerouslySetInnerHTML&&null!=t.dangerouslySetInnerHTML.__html}var bn="function"===typeof setTimeout?setTimeout:void 0,wn="function"===typeof clearTimeout?clearTimeout:void 0;function xn(e){for(;null!=e;e=e.nextSibling){var t=e.nodeType;if(1===t||3===t)break}return e}function En(e){e=e.previousSibling;for(var t=0;e;){if(8===e.nodeType){var n=e.data;if("$"===n||"$!"===n||"$?"===n){if(0===t)return e;t--}else"/$"===n&&t++}e=e.previousSibling}return null}var kn=Math.random().toString(36).slice(2),Tn="__reactInternalInstance$"+kn,Sn="__reactEventHandlers$"+kn,Cn="__reactContainere$"+kn;function Pn(e){var t=e[Tn];if(t)return t;for(var n=e.parentNode;n;){if(t=n[Cn]||n[Tn]){if(n=t.alternate,null!==t.child||null!==n&&null!==n.child)for(e=En(e);null!==e;){if(n=e[Tn])return n;e=En(e)}return t}n=(e=n).parentNode}return null}function _n(e){return!(e=e[Tn]||e[Cn])||5!==e.tag&&6!==e.tag&&13!==e.tag&&3!==e.tag?null:e}function On(e){if(5===e.tag||6===e.tag)return e.stateNode;throw Error(o(33))}function Nn(e){return e[Sn]||null}function Mn(e){do{e=e.return}while(e&&5!==e.tag);return e||null}function Rn(e,t){var n=e.stateNode;if(!n)return null;var r=m(n);if(!r)return null;n=r[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(r=!r.disabled)||(r=!("button"===(e=e.type)||"input"===e||"select"===e||"textarea"===e)),e=!r;break e;default:e=!1}if(e)return null;if(n&&"function"!==typeof n)throw Error(o(231,t,typeof n));return n}function jn(e,t,n){(t=Rn(e,n.dispatchConfig.phasedRegistrationNames[t]))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function zn(e){if(e&&e.dispatchConfig.phasedRegistrationNames){for(var t=e._targetInst,n=[];t;)n.push(t),t=Mn(t);for(t=n.length;0<t--;)jn(n[t],"captured",e);for(t=0;t<n.length;t++)jn(n[t],"bubbled",e)}}function An(e,t,n){e&&n&&n.dispatchConfig.registrationName&&(t=Rn(e,n.dispatchConfig.registrationName))&&(n._dispatchListeners=rt(n._dispatchListeners,t),n._dispatchInstances=rt(n._dispatchInstances,e))}function Ln(e){e&&e.dispatchConfig.registrationName&&An(e._targetInst,null,e)}function Dn(e){at(e,zn)}var In=null,Fn=null,Un=null;function Bn(){if(Un)return Un;var e,t,n=Fn,r=n.length,a="value"in In?In.value:In.textContent,i=a.length;for(e=0;e<r&&n[e]===a[e];e++);var o=r-e;for(t=1;t<=o&&n[r-t]===a[i-t];t++);return Un=a.slice(e,1<t?1-t:void 0)}function $n(){return!0}function Wn(){return!1}function Vn(e,t,n,r){for(var a in this.dispatchConfig=e,this._targetInst=t,this.nativeEvent=n,e=this.constructor.Interface)e.hasOwnProperty(a)&&((t=e[a])?this[a]=t(n):"target"===a?this.target=r:this[a]=n[a]);return this.isDefaultPrevented=(null!=n.defaultPrevented?n.defaultPrevented:!1===n.returnValue)?$n:Wn,this.isPropagationStopped=Wn,this}function Hn(e,t,n,r){if(this.eventPool.length){var a=this.eventPool.pop();return this.call(a,e,t,n,r),a}return new this(e,t,n,r)}function Qn(e){if(!(e instanceof this))throw Error(o(279));e.destructor(),10>this.eventPool.length&&this.eventPool.push(e)}function qn(e){e.eventPool=[],e.getPooled=Hn,e.release=Qn}a(Vn.prototype,{preventDefault:function(){this.defaultPrevented=!0;var e=this.nativeEvent;e&&(e.preventDefault?e.preventDefault():"unknown"!==typeof e.returnValue&&(e.returnValue=!1),this.isDefaultPrevented=$n)},stopPropagation:function(){var e=this.nativeEvent;e&&(e.stopPropagation?e.stopPropagation():"unknown"!==typeof e.cancelBubble&&(e.cancelBubble=!0),this.isPropagationStopped=$n)},persist:function(){this.isPersistent=$n},isPersistent:Wn,destructor:function(){var e,t=this.constructor.Interface;for(e in t)this[e]=null;this.nativeEvent=this._targetInst=this.dispatchConfig=null,this.isPropagationStopped=this.isDefaultPrevented=Wn,this._dispatchInstances=this._dispatchListeners=null}}),Vn.Interface={type:null,target:null,currentTarget:function(){return null},eventPhase:null,bubbles:null,cancelable:null,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:null,isTrusted:null},Vn.extend=function(e){function t(){}function n(){return r.apply(this,arguments)}var r=this;t.prototype=r.prototype;var i=new t;return a(i,n.prototype),n.prototype=i,n.prototype.constructor=n,n.Interface=a({},r.Interface,e),n.extend=r.extend,qn(n),n},qn(Vn);var Kn=Vn.extend({data:null}),Xn=Vn.extend({data:null}),Yn=[9,13,27,32],Gn=P&&"CompositionEvent"in window,Jn=null;P&&"documentMode"in document&&(Jn=document.documentMode);var Zn=P&&"TextEvent"in window&&!Jn,er=P&&(!Gn||Jn&&8<Jn&&11>=Jn),tr=String.fromCharCode(32),nr={beforeInput:{phasedRegistrationNames:{bubbled:"onBeforeInput",captured:"onBeforeInputCapture"},dependencies:["compositionend","keypress","textInput","paste"]},compositionEnd:{phasedRegistrationNames:{bubbled:"onCompositionEnd",captured:"onCompositionEndCapture"},dependencies:"blur compositionend keydown keypress keyup mousedown".split(" ")},compositionStart:{phasedRegistrationNames:{bubbled:"onCompositionStart",captured:"onCompositionStartCapture"},dependencies:"blur compositionstart keydown keypress keyup mousedown".split(" ")},compositionUpdate:{phasedRegistrationNames:{bubbled:"onCompositionUpdate",captured:"onCompositionUpdateCapture"},dependencies:"blur compositionupdate keydown keypress keyup mousedown".split(" ")}},rr=!1;function ar(e,t){switch(e){case"keyup":return-1!==Yn.indexOf(t.keyCode);case"keydown":return 229!==t.keyCode;case"keypress":case"mousedown":case"blur":return!0;default:return!1}}function ir(e){return"object"===typeof(e=e.detail)&&"data"in e?e.data:null}var or=!1;var lr={eventTypes:nr,extractEvents:function(e,t,n,r){var a;if(Gn)e:{switch(e){case"compositionstart":var i=nr.compositionStart;break e;case"compositionend":i=nr.compositionEnd;break e;case"compositionupdate":i=nr.compositionUpdate;break e}i=void 0}else or?ar(e,n)&&(i=nr.compositionEnd):"keydown"===e&&229===n.keyCode&&(i=nr.compositionStart);return i?(er&&"ko"!==n.locale&&(or||i!==nr.compositionStart?i===nr.compositionEnd&&or&&(a=Bn()):(Fn="value"in(In=r)?In.value:In.textContent,or=!0)),i=Kn.getPooled(i,t,n,r),a?i.data=a:null!==(a=ir(n))&&(i.data=a),Dn(i),a=i):a=null,(e=Zn?function(e,t){switch(e){case"compositionend":return ir(t);case"keypress":return 32!==t.which?null:(rr=!0,tr);case"textInput":return(e=t.data)===tr&&rr?null:e;default:return null}}(e,n):function(e,t){if(or)return"compositionend"===e||!Gn&&ar(e,t)?(e=Bn(),Un=Fn=In=null,or=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return er&&"ko"!==t.locale?null:t.data;default:return null}}(e,n))?((t=Xn.getPooled(nr.beforeInput,t,n,r)).data=e,Dn(t)):t=null,null===a?t:null===t?a:[a,t]}},ur={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function cr(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return"input"===t?!!ur[e.type]:"textarea"===t}var sr={change:{phasedRegistrationNames:{bubbled:"onChange",captured:"onChangeCapture"},dependencies:"blur change click focus input keydown keyup selectionchange".split(" ")}};function fr(e,t,n){return(e=Vn.getPooled(sr.change,e,t,n)).type="change",R(n),Dn(e),e}var dr=null,pr=null;function mr(e){lt(e)}function hr(e){if(xe(On(e)))return e}function vr(e,t){if("change"===e)return t}var gr=!1;function yr(){dr&&(dr.detachEvent("onpropertychange",br),pr=dr=null)}function br(e){if("value"===e.propertyName&&hr(pr))if(e=fr(pr,e,ut(e)),I)lt(e);else{I=!0;try{z(mr,e)}finally{I=!1,U()}}}function wr(e,t,n){"focus"===e?(yr(),pr=n,(dr=t).attachEvent("onpropertychange",br)):"blur"===e&&yr()}function xr(e){if("selectionchange"===e||"keyup"===e||"keydown"===e)return hr(pr)}function Er(e,t){if("click"===e)return hr(t)}function kr(e,t){if("input"===e||"change"===e)return hr(t)}P&&(gr=ct("input")&&(!document.documentMode||9<document.documentMode));var Tr={eventTypes:sr,_isInputEventSupported:gr,extractEvents:function(e,t,n,r){var a=t?On(t):window,i=a.nodeName&&a.nodeName.toLowerCase();if("select"===i||"input"===i&&"file"===a.type)var o=vr;else if(cr(a))if(gr)o=kr;else{o=xr;var l=wr}else(i=a.nodeName)&&"input"===i.toLowerCase()&&("checkbox"===a.type||"radio"===a.type)&&(o=Er);if(o&&(o=o(e,t)))return fr(o,n,r);l&&l(e,a,t),"blur"===e&&(e=a._wrapperState)&&e.controlled&&"number"===a.type&&Pe(a,"number",a.value)}},Sr=Vn.extend({view:null,detail:null}),Cr={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function Pr(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):!!(e=Cr[e])&&!!t[e]}function _r(){return Pr}var Or=0,Nr=0,Mr=!1,Rr=!1,jr=Sr.extend({screenX:null,screenY:null,clientX:null,clientY:null,pageX:null,pageY:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,getModifierState:_r,button:null,buttons:null,relatedTarget:function(e){return e.relatedTarget||(e.fromElement===e.srcElement?e.toElement:e.fromElement)},movementX:function(e){if("movementX"in e)return e.movementX;var t=Or;return Or=e.screenX,Mr?"mousemove"===e.type?e.screenX-t:0:(Mr=!0,0)},movementY:function(e){if("movementY"in e)return e.movementY;var t=Nr;return Nr=e.screenY,Rr?"mousemove"===e.type?e.screenY-t:0:(Rr=!0,0)}}),zr=jr.extend({pointerId:null,width:null,height:null,pressure:null,tangentialPressure:null,tiltX:null,tiltY:null,twist:null,pointerType:null,isPrimary:null}),Ar={mouseEnter:{registrationName:"onMouseEnter",dependencies:["mouseout","mouseover"]},mouseLeave:{registrationName:"onMouseLeave",dependencies:["mouseout","mouseover"]},pointerEnter:{registrationName:"onPointerEnter",dependencies:["pointerout","pointerover"]},pointerLeave:{registrationName:"onPointerLeave",dependencies:["pointerout","pointerover"]}},Lr={eventTypes:Ar,extractEvents:function(e,t,n,r,a){var i="mouseover"===e||"pointerover"===e,o="mouseout"===e||"pointerout"===e;if(i&&0===(32&a)&&(n.relatedTarget||n.fromElement)||!o&&!i)return null;(i=r.window===r?r:(i=r.ownerDocument)?i.defaultView||i.parentWindow:window,o)?(o=t,null!==(t=(t=n.relatedTarget||n.toElement)?Pn(t):null)&&(t!==Ze(t)||5!==t.tag&&6!==t.tag)&&(t=null)):o=null;if(o===t)return null;if("mouseout"===e||"mouseover"===e)var l=jr,u=Ar.mouseLeave,c=Ar.mouseEnter,s="mouse";else"pointerout"!==e&&"pointerover"!==e||(l=zr,u=Ar.pointerLeave,c=Ar.pointerEnter,s="pointer");if(e=null==o?i:On(o),i=null==t?i:On(t),(u=l.getPooled(u,o,n,r)).type=s+"leave",u.target=e,u.relatedTarget=i,(n=l.getPooled(c,t,n,r)).type=s+"enter",n.target=i,n.relatedTarget=e,s=t,(r=o)&&s)e:{for(c=s,o=0,e=l=r;e;e=Mn(e))o++;for(e=0,t=c;t;t=Mn(t))e++;for(;0<o-e;)l=Mn(l),o--;for(;0<e-o;)c=Mn(c),e--;for(;o--;){if(l===c||l===c.alternate)break e;l=Mn(l),c=Mn(c)}l=null}else l=null;for(c=l,l=[];r&&r!==c&&(null===(o=r.alternate)||o!==c);)l.push(r),r=Mn(r);for(r=[];s&&s!==c&&(null===(o=s.alternate)||o!==c);)r.push(s),s=Mn(s);for(s=0;s<l.length;s++)An(l[s],"bubbled",u);for(s=r.length;0<s--;)An(r[s],"captured",n);return 0===(64&a)?[u]:[u,n]}};var Dr="function"===typeof Object.is?Object.is:function(e,t){return e===t&&(0!==e||1/e===1/t)||e!==e&&t!==t},Ir=Object.prototype.hasOwnProperty;function Fr(e,t){if(Dr(e,t))return!0;if("object"!==typeof e||null===e||"object"!==typeof t||null===t)return!1;var n=Object.keys(e),r=Object.keys(t);if(n.length!==r.length)return!1;for(r=0;r<n.length;r++)if(!Ir.call(t,n[r])||!Dr(e[n[r]],t[n[r]]))return!1;return!0}var Ur=P&&"documentMode"in document&&11>=document.documentMode,Br={select:{phasedRegistrationNames:{bubbled:"onSelect",captured:"onSelectCapture"},dependencies:"blur contextmenu dragend focus keydown keyup mousedown mouseup selectionchange".split(" ")}},$r=null,Wr=null,Vr=null,Hr=!1;function Qr(e,t){var n=t.window===t?t.document:9===t.nodeType?t:t.ownerDocument;return Hr||null==$r||$r!==sn(n)?null:("selectionStart"in(n=$r)&&mn(n)?n={start:n.selectionStart,end:n.selectionEnd}:n={anchorNode:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection()).anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset},Vr&&Fr(Vr,n)?null:(Vr=n,(e=Vn.getPooled(Br.select,Wr,e,t)).type="select",e.target=$r,Dn(e),e))}var qr={eventTypes:Br,extractEvents:function(e,t,n,r,a,i){if(!(i=!(a=i||(r.window===r?r.document:9===r.nodeType?r:r.ownerDocument)))){e:{a=Je(a),i=S.onSelect;for(var o=0;o<i.length;o++)if(!a.has(i[o])){a=!1;break e}a=!0}i=!a}if(i)return null;switch(a=t?On(t):window,e){case"focus":(cr(a)||"true"===a.contentEditable)&&($r=a,Wr=t,Vr=null);break;case"blur":Vr=Wr=$r=null;break;case"mousedown":Hr=!0;break;case"contextmenu":case"mouseup":case"dragend":return Hr=!1,Qr(n,r);case"selectionchange":if(Ur)break;case"keydown":case"keyup":return Qr(n,r)}return null}},Kr=Vn.extend({animationName:null,elapsedTime:null,pseudoElement:null}),Xr=Vn.extend({clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),Yr=Sr.extend({relatedTarget:null});function Gr(e){var t=e.keyCode;return"charCode"in e?0===(e=e.charCode)&&13===t&&(e=13):e=t,10===e&&(e=13),32<=e||13===e?e:0}var Jr={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},Zr={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},ea=Sr.extend({key:function(e){if(e.key){var t=Jr[e.key]||e.key;if("Unidentified"!==t)return t}return"keypress"===e.type?13===(e=Gr(e))?"Enter":String.fromCharCode(e):"keydown"===e.type||"keyup"===e.type?Zr[e.keyCode]||"Unidentified":""},location:null,ctrlKey:null,shiftKey:null,altKey:null,metaKey:null,repeat:null,locale:null,getModifierState:_r,charCode:function(e){return"keypress"===e.type?Gr(e):0},keyCode:function(e){return"keydown"===e.type||"keyup"===e.type?e.keyCode:0},which:function(e){return"keypress"===e.type?Gr(e):"keydown"===e.type||"keyup"===e.type?e.keyCode:0}}),ta=jr.extend({dataTransfer:null}),na=Sr.extend({touches:null,targetTouches:null,changedTouches:null,altKey:null,metaKey:null,ctrlKey:null,shiftKey:null,getModifierState:_r}),ra=Vn.extend({propertyName:null,elapsedTime:null,pseudoElement:null}),aa=jr.extend({deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:null,deltaMode:null}),ia={eventTypes:Dt,extractEvents:function(e,t,n,r){var a=It.get(e);if(!a)return null;switch(e){case"keypress":if(0===Gr(n))return null;case"keydown":case"keyup":e=ea;break;case"blur":case"focus":e=Yr;break;case"click":if(2===n.button)return null;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":e=jr;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":e=ta;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":e=na;break;case Qe:case qe:case Ke:e=Kr;break;case Xe:e=ra;break;case"scroll":e=Sr;break;case"wheel":e=aa;break;case"copy":case"cut":case"paste":e=Xr;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":e=zr;break;default:e=Vn}return Dn(t=e.getPooled(a,t,n,r)),t}};if(y)throw Error(o(101));y=Array.prototype.slice.call("ResponderEventPlugin SimpleEventPlugin EnterLeaveEventPlugin ChangeEventPlugin SelectEventPlugin BeforeInputEventPlugin".split(" ")),w(),m=Nn,h=_n,v=On,C({SimpleEventPlugin:ia,EnterLeaveEventPlugin:Lr,ChangeEventPlugin:Tr,SelectEventPlugin:qr,BeforeInputEventPlugin:lr});var oa=[],la=-1;function ua(e){0>la||(e.current=oa[la],oa[la]=null,la--)}function ca(e,t){la++,oa[la]=e.current,e.current=t}var sa={},fa={current:sa},da={current:!1},pa=sa;function ma(e,t){var n=e.type.contextTypes;if(!n)return sa;var r=e.stateNode;if(r&&r.__reactInternalMemoizedUnmaskedChildContext===t)return r.__reactInternalMemoizedMaskedChildContext;var a,i={};for(a in n)i[a]=t[a];return r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=t,e.__reactInternalMemoizedMaskedChildContext=i),i}function ha(e){return null!==(e=e.childContextTypes)&&void 0!==e}function va(){ua(da),ua(fa)}function ga(e,t,n){if(fa.current!==sa)throw Error(o(168));ca(fa,t),ca(da,n)}function ya(e,t,n){var r=e.stateNode;if(e=t.childContextTypes,"function"!==typeof r.getChildContext)return n;for(var i in r=r.getChildContext())if(!(i in e))throw Error(o(108,ve(t)||"Unknown",i));return a({},n,{},r)}function ba(e){return e=(e=e.stateNode)&&e.__reactInternalMemoizedMergedChildContext||sa,pa=fa.current,ca(fa,e),ca(da,da.current),!0}function wa(e,t,n){var r=e.stateNode;if(!r)throw Error(o(169));n?(e=ya(e,t,pa),r.__reactInternalMemoizedMergedChildContext=e,ua(da),ua(fa),ca(fa,e)):ua(da),ca(da,n)}var xa=i.unstable_runWithPriority,Ea=i.unstable_scheduleCallback,ka=i.unstable_cancelCallback,Ta=i.unstable_requestPaint,Sa=i.unstable_now,Ca=i.unstable_getCurrentPriorityLevel,Pa=i.unstable_ImmediatePriority,_a=i.unstable_UserBlockingPriority,Oa=i.unstable_NormalPriority,Na=i.unstable_LowPriority,Ma=i.unstable_IdlePriority,Ra={},ja=i.unstable_shouldYield,za=void 0!==Ta?Ta:function(){},Aa=null,La=null,Da=!1,Ia=Sa(),Fa=1e4>Ia?Sa:function(){return Sa()-Ia};function Ua(){switch(Ca()){case Pa:return 99;case _a:return 98;case Oa:return 97;case Na:return 96;case Ma:return 95;default:throw Error(o(332))}}function Ba(e){switch(e){case 99:return Pa;case 98:return _a;case 97:return Oa;case 96:return Na;case 95:return Ma;default:throw Error(o(332))}}function $a(e,t){return e=Ba(e),xa(e,t)}function Wa(e,t,n){return e=Ba(e),Ea(e,t,n)}function Va(e){return null===Aa?(Aa=[e],La=Ea(Pa,Qa)):Aa.push(e),Ra}function Ha(){if(null!==La){var e=La;La=null,ka(e)}Qa()}function Qa(){if(!Da&&null!==Aa){Da=!0;var e=0;try{var t=Aa;$a(99,(function(){for(;e<t.length;e++){var n=t[e];do{n=n(!0)}while(null!==n)}})),Aa=null}catch(n){throw null!==Aa&&(Aa=Aa.slice(e+1)),Ea(Pa,Ha),n}finally{Da=!1}}}function qa(e,t,n){return 1073741821-(1+((1073741821-e+t/10)/(n/=10)|0))*n}function Ka(e,t){if(e&&e.defaultProps)for(var n in t=a({},t),e=e.defaultProps)void 0===t[n]&&(t[n]=e[n]);return t}var Xa={current:null},Ya=null,Ga=null,Ja=null;function Za(){Ja=Ga=Ya=null}function ei(e){var t=Xa.current;ua(Xa),e.type._context._currentValue=t}function ti(e,t){for(;null!==e;){var n=e.alternate;if(e.childExpirationTime<t)e.childExpirationTime=t,null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t);else{if(!(null!==n&&n.childExpirationTime<t))break;n.childExpirationTime=t}e=e.return}}function ni(e,t){Ya=e,Ja=Ga=null,null!==(e=e.dependencies)&&null!==e.firstContext&&(e.expirationTime>=t&&(No=!0),e.firstContext=null)}function ri(e,t){if(Ja!==e&&!1!==t&&0!==t)if("number"===typeof t&&1073741823!==t||(Ja=e,t=1073741823),t={context:e,observedBits:t,next:null},null===Ga){if(null===Ya)throw Error(o(308));Ga=t,Ya.dependencies={expirationTime:0,firstContext:t,responders:null}}else Ga=Ga.next=t;return e._currentValue}var ai=!1;function ii(e){e.updateQueue={baseState:e.memoizedState,baseQueue:null,shared:{pending:null},effects:null}}function oi(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,baseQueue:e.baseQueue,shared:e.shared,effects:e.effects})}function li(e,t){return(e={expirationTime:e,suspenseConfig:t,tag:0,payload:null,callback:null,next:null}).next=e}function ui(e,t){if(null!==(e=e.updateQueue)){var n=(e=e.shared).pending;null===n?t.next=t:(t.next=n.next,n.next=t),e.pending=t}}function ci(e,t){var n=e.alternate;null!==n&&oi(n,e),null===(n=(e=e.updateQueue).baseQueue)?(e.baseQueue=t.next=t,t.next=t):(t.next=n.next,n.next=t)}function si(e,t,n,r){var i=e.updateQueue;ai=!1;var o=i.baseQueue,l=i.shared.pending;if(null!==l){if(null!==o){var u=o.next;o.next=l.next,l.next=u}o=l,i.shared.pending=null,null!==(u=e.alternate)&&(null!==(u=u.updateQueue)&&(u.baseQueue=l))}if(null!==o){u=o.next;var c=i.baseState,s=0,f=null,d=null,p=null;if(null!==u)for(var m=u;;){if((l=m.expirationTime)<r){var h={expirationTime:m.expirationTime,suspenseConfig:m.suspenseConfig,tag:m.tag,payload:m.payload,callback:m.callback,next:null};null===p?(d=p=h,f=c):p=p.next=h,l>s&&(s=l)}else{null!==p&&(p=p.next={expirationTime:1073741823,suspenseConfig:m.suspenseConfig,tag:m.tag,payload:m.payload,callback:m.callback,next:null}),iu(l,m.suspenseConfig);e:{var v=e,g=m;switch(l=t,h=n,g.tag){case 1:if("function"===typeof(v=g.payload)){c=v.call(h,c,l);break e}c=v;break e;case 3:v.effectTag=-4097&v.effectTag|64;case 0:if(null===(l="function"===typeof(v=g.payload)?v.call(h,c,l):v)||void 0===l)break e;c=a({},c,l);break e;case 2:ai=!0}}null!==m.callback&&(e.effectTag|=32,null===(l=i.effects)?i.effects=[m]:l.push(m))}if(null===(m=m.next)||m===u){if(null===(l=i.shared.pending))break;m=o.next=l.next,l.next=u,i.baseQueue=o=l,i.shared.pending=null}}null===p?f=c:p.next=d,i.baseState=f,i.baseQueue=p,ou(s),e.expirationTime=s,e.memoizedState=c}}function fi(e,t,n){if(e=t.effects,t.effects=null,null!==e)for(t=0;t<e.length;t++){var r=e[t],a=r.callback;if(null!==a){if(r.callback=null,r=a,a=n,"function"!==typeof r)throw Error(o(191,r));r.call(a)}}}var di=Y.ReactCurrentBatchConfig,pi=(new r.Component).refs;function mi(e,t,n,r){n=null===(n=n(r,t=e.memoizedState))||void 0===n?t:a({},t,n),e.memoizedState=n,0===e.expirationTime&&(e.updateQueue.baseState=n)}var hi={isMounted:function(e){return!!(e=e._reactInternalFiber)&&Ze(e)===e},enqueueSetState:function(e,t,n){e=e._reactInternalFiber;var r=Ql(),a=di.suspense;(a=li(r=ql(r,e,a),a)).payload=t,void 0!==n&&null!==n&&(a.callback=n),ui(e,a),Kl(e,r)},enqueueReplaceState:function(e,t,n){e=e._reactInternalFiber;var r=Ql(),a=di.suspense;(a=li(r=ql(r,e,a),a)).tag=1,a.payload=t,void 0!==n&&null!==n&&(a.callback=n),ui(e,a),Kl(e,r)},enqueueForceUpdate:function(e,t){e=e._reactInternalFiber;var n=Ql(),r=di.suspense;(r=li(n=ql(n,e,r),r)).tag=2,void 0!==t&&null!==t&&(r.callback=t),ui(e,r),Kl(e,n)}};function vi(e,t,n,r,a,i,o){return"function"===typeof(e=e.stateNode).shouldComponentUpdate?e.shouldComponentUpdate(r,i,o):!t.prototype||!t.prototype.isPureReactComponent||(!Fr(n,r)||!Fr(a,i))}function gi(e,t,n){var r=!1,a=sa,i=t.contextType;return"object"===typeof i&&null!==i?i=ri(i):(a=ha(t)?pa:fa.current,i=(r=null!==(r=t.contextTypes)&&void 0!==r)?ma(e,a):sa),t=new t(n,i),e.memoizedState=null!==t.state&&void 0!==t.state?t.state:null,t.updater=hi,e.stateNode=t,t._reactInternalFiber=e,r&&((e=e.stateNode).__reactInternalMemoizedUnmaskedChildContext=a,e.__reactInternalMemoizedMaskedChildContext=i),t}function yi(e,t,n,r){e=t.state,"function"===typeof t.componentWillReceiveProps&&t.componentWillReceiveProps(n,r),"function"===typeof t.UNSAFE_componentWillReceiveProps&&t.UNSAFE_componentWillReceiveProps(n,r),t.state!==e&&hi.enqueueReplaceState(t,t.state,null)}function bi(e,t,n,r){var a=e.stateNode;a.props=n,a.state=e.memoizedState,a.refs=pi,ii(e);var i=t.contextType;"object"===typeof i&&null!==i?a.context=ri(i):(i=ha(t)?pa:fa.current,a.context=ma(e,i)),si(e,n,a,r),a.state=e.memoizedState,"function"===typeof(i=t.getDerivedStateFromProps)&&(mi(e,t,i,n),a.state=e.memoizedState),"function"===typeof t.getDerivedStateFromProps||"function"===typeof a.getSnapshotBeforeUpdate||"function"!==typeof a.UNSAFE_componentWillMount&&"function"!==typeof a.componentWillMount||(t=a.state,"function"===typeof a.componentWillMount&&a.componentWillMount(),"function"===typeof a.UNSAFE_componentWillMount&&a.UNSAFE_componentWillMount(),t!==a.state&&hi.enqueueReplaceState(a,a.state,null),si(e,n,a,r),a.state=e.memoizedState),"function"===typeof a.componentDidMount&&(e.effectTag|=4)}var wi=Array.isArray;function xi(e,t,n){if(null!==(e=n.ref)&&"function"!==typeof e&&"object"!==typeof e){if(n._owner){if(n=n._owner){if(1!==n.tag)throw Error(o(309));var r=n.stateNode}if(!r)throw Error(o(147,e));var a=""+e;return null!==t&&null!==t.ref&&"function"===typeof t.ref&&t.ref._stringRef===a?t.ref:((t=function(e){var t=r.refs;t===pi&&(t=r.refs={}),null===e?delete t[a]:t[a]=e})._stringRef=a,t)}if("string"!==typeof e)throw Error(o(284));if(!n._owner)throw Error(o(290,e))}return e}function Ei(e,t){if("textarea"!==e.type)throw Error(o(31,"[object Object]"===Object.prototype.toString.call(t)?"object with keys {"+Object.keys(t).join(", ")+"}":t,""))}function ki(e){function t(t,n){if(e){var r=t.lastEffect;null!==r?(r.nextEffect=n,t.lastEffect=n):t.firstEffect=t.lastEffect=n,n.nextEffect=null,n.effectTag=8}}function n(n,r){if(!e)return null;for(;null!==r;)t(n,r),r=r.sibling;return null}function r(e,t){for(e=new Map;null!==t;)null!==t.key?e.set(t.key,t):e.set(t.index,t),t=t.sibling;return e}function a(e,t){return(e=Cu(e,t)).index=0,e.sibling=null,e}function i(t,n,r){return t.index=r,e?null!==(r=t.alternate)?(r=r.index)<n?(t.effectTag=2,n):r:(t.effectTag=2,n):n}function l(t){return e&&null===t.alternate&&(t.effectTag=2),t}function u(e,t,n,r){return null===t||6!==t.tag?((t=Ou(n,e.mode,r)).return=e,t):((t=a(t,n)).return=e,t)}function c(e,t,n,r){return null!==t&&t.elementType===n.type?((r=a(t,n.props)).ref=xi(e,t,n),r.return=e,r):((r=Pu(n.type,n.key,n.props,null,e.mode,r)).ref=xi(e,t,n),r.return=e,r)}function s(e,t,n,r){return null===t||4!==t.tag||t.stateNode.containerInfo!==n.containerInfo||t.stateNode.implementation!==n.implementation?((t=Nu(n,e.mode,r)).return=e,t):((t=a(t,n.children||[])).return=e,t)}function f(e,t,n,r,i){return null===t||7!==t.tag?((t=_u(n,e.mode,r,i)).return=e,t):((t=a(t,n)).return=e,t)}function d(e,t,n){if("string"===typeof t||"number"===typeof t)return(t=Ou(""+t,e.mode,n)).return=e,t;if("object"===typeof t&&null!==t){switch(t.$$typeof){case ee:return(n=Pu(t.type,t.key,t.props,null,e.mode,n)).ref=xi(e,null,t),n.return=e,n;case te:return(t=Nu(t,e.mode,n)).return=e,t}if(wi(t)||he(t))return(t=_u(t,e.mode,n,null)).return=e,t;Ei(e,t)}return null}function p(e,t,n,r){var a=null!==t?t.key:null;if("string"===typeof n||"number"===typeof n)return null!==a?null:u(e,t,""+n,r);if("object"===typeof n&&null!==n){switch(n.$$typeof){case ee:return n.key===a?n.type===ne?f(e,t,n.props.children,r,a):c(e,t,n,r):null;case te:return n.key===a?s(e,t,n,r):null}if(wi(n)||he(n))return null!==a?null:f(e,t,n,r,null);Ei(e,n)}return null}function m(e,t,n,r,a){if("string"===typeof r||"number"===typeof r)return u(t,e=e.get(n)||null,""+r,a);if("object"===typeof r&&null!==r){switch(r.$$typeof){case ee:return e=e.get(null===r.key?n:r.key)||null,r.type===ne?f(t,e,r.props.children,a,r.key):c(t,e,r,a);case te:return s(t,e=e.get(null===r.key?n:r.key)||null,r,a)}if(wi(r)||he(r))return f(t,e=e.get(n)||null,r,a,null);Ei(t,r)}return null}function h(a,o,l,u){for(var c=null,s=null,f=o,h=o=0,v=null;null!==f&&h<l.length;h++){f.index>h?(v=f,f=null):v=f.sibling;var g=p(a,f,l[h],u);if(null===g){null===f&&(f=v);break}e&&f&&null===g.alternate&&t(a,f),o=i(g,o,h),null===s?c=g:s.sibling=g,s=g,f=v}if(h===l.length)return n(a,f),c;if(null===f){for(;h<l.length;h++)null!==(f=d(a,l[h],u))&&(o=i(f,o,h),null===s?c=f:s.sibling=f,s=f);return c}for(f=r(a,f);h<l.length;h++)null!==(v=m(f,a,h,l[h],u))&&(e&&null!==v.alternate&&f.delete(null===v.key?h:v.key),o=i(v,o,h),null===s?c=v:s.sibling=v,s=v);return e&&f.forEach((function(e){return t(a,e)})),c}function v(a,l,u,c){var s=he(u);if("function"!==typeof s)throw Error(o(150));if(null==(u=s.call(u)))throw Error(o(151));for(var f=s=null,h=l,v=l=0,g=null,y=u.next();null!==h&&!y.done;v++,y=u.next()){h.index>v?(g=h,h=null):g=h.sibling;var b=p(a,h,y.value,c);if(null===b){null===h&&(h=g);break}e&&h&&null===b.alternate&&t(a,h),l=i(b,l,v),null===f?s=b:f.sibling=b,f=b,h=g}if(y.done)return n(a,h),s;if(null===h){for(;!y.done;v++,y=u.next())null!==(y=d(a,y.value,c))&&(l=i(y,l,v),null===f?s=y:f.sibling=y,f=y);return s}for(h=r(a,h);!y.done;v++,y=u.next())null!==(y=m(h,a,v,y.value,c))&&(e&&null!==y.alternate&&h.delete(null===y.key?v:y.key),l=i(y,l,v),null===f?s=y:f.sibling=y,f=y);return e&&h.forEach((function(e){return t(a,e)})),s}return function(e,r,i,u){var c="object"===typeof i&&null!==i&&i.type===ne&&null===i.key;c&&(i=i.props.children);var s="object"===typeof i&&null!==i;if(s)switch(i.$$typeof){case ee:e:{for(s=i.key,c=r;null!==c;){if(c.key===s){switch(c.tag){case 7:if(i.type===ne){n(e,c.sibling),(r=a(c,i.props.children)).return=e,e=r;break e}break;default:if(c.elementType===i.type){n(e,c.sibling),(r=a(c,i.props)).ref=xi(e,c,i),r.return=e,e=r;break e}}n(e,c);break}t(e,c),c=c.sibling}i.type===ne?((r=_u(i.props.children,e.mode,u,i.key)).return=e,e=r):((u=Pu(i.type,i.key,i.props,null,e.mode,u)).ref=xi(e,r,i),u.return=e,e=u)}return l(e);case te:e:{for(c=i.key;null!==r;){if(r.key===c){if(4===r.tag&&r.stateNode.containerInfo===i.containerInfo&&r.stateNode.implementation===i.implementation){n(e,r.sibling),(r=a(r,i.children||[])).return=e,e=r;break e}n(e,r);break}t(e,r),r=r.sibling}(r=Nu(i,e.mode,u)).return=e,e=r}return l(e)}if("string"===typeof i||"number"===typeof i)return i=""+i,null!==r&&6===r.tag?(n(e,r.sibling),(r=a(r,i)).return=e,e=r):(n(e,r),(r=Ou(i,e.mode,u)).return=e,e=r),l(e);if(wi(i))return h(e,r,i,u);if(he(i))return v(e,r,i,u);if(s&&Ei(e,i),"undefined"===typeof i&&!c)switch(e.tag){case 1:case 0:throw e=e.type,Error(o(152,e.displayName||e.name||"Component"))}return n(e,r)}}var Ti=ki(!0),Si=ki(!1),Ci={},Pi={current:Ci},_i={current:Ci},Oi={current:Ci};function Ni(e){if(e===Ci)throw Error(o(174));return e}function Mi(e,t){switch(ca(Oi,t),ca(_i,e),ca(Pi,Ci),e=t.nodeType){case 9:case 11:t=(t=t.documentElement)?t.namespaceURI:De(null,"");break;default:t=De(t=(e=8===e?t.parentNode:t).namespaceURI||null,e=e.tagName)}ua(Pi),ca(Pi,t)}function Ri(){ua(Pi),ua(_i),ua(Oi)}function ji(e){Ni(Oi.current);var t=Ni(Pi.current),n=De(t,e.type);t!==n&&(ca(_i,e),ca(Pi,n))}function zi(e){_i.current===e&&(ua(Pi),ua(_i))}var Ai={current:0};function Li(e){for(var t=e;null!==t;){if(13===t.tag){var n=t.memoizedState;if(null!==n&&(null===(n=n.dehydrated)||"$?"===n.data||"$!"===n.data))return t}else if(19===t.tag&&void 0!==t.memoizedProps.revealOrder){if(0!==(64&t.effectTag))return t}else if(null!==t.child){t.child.return=t,t=t.child;continue}if(t===e)break;for(;null===t.sibling;){if(null===t.return||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}function Di(e,t){return{responder:e,props:t}}var Ii=Y.ReactCurrentDispatcher,Fi=Y.ReactCurrentBatchConfig,Ui=0,Bi=null,$i=null,Wi=null,Vi=!1;function Hi(){throw Error(o(321))}function Qi(e,t){if(null===t)return!1;for(var n=0;n<t.length&&n<e.length;n++)if(!Dr(e[n],t[n]))return!1;return!0}function qi(e,t,n,r,a,i){if(Ui=i,Bi=t,t.memoizedState=null,t.updateQueue=null,t.expirationTime=0,Ii.current=null===e||null===e.memoizedState?go:yo,e=n(r,a),t.expirationTime===Ui){i=0;do{if(t.expirationTime=0,!(25>i))throw Error(o(301));i+=1,Wi=$i=null,t.updateQueue=null,Ii.current=bo,e=n(r,a)}while(t.expirationTime===Ui)}if(Ii.current=vo,t=null!==$i&&null!==$i.next,Ui=0,Wi=$i=Bi=null,Vi=!1,t)throw Error(o(300));return e}function Ki(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return null===Wi?Bi.memoizedState=Wi=e:Wi=Wi.next=e,Wi}function Xi(){if(null===$i){var e=Bi.alternate;e=null!==e?e.memoizedState:null}else e=$i.next;var t=null===Wi?Bi.memoizedState:Wi.next;if(null!==t)Wi=t,$i=e;else{if(null===e)throw Error(o(310));e={memoizedState:($i=e).memoizedState,baseState:$i.baseState,baseQueue:$i.baseQueue,queue:$i.queue,next:null},null===Wi?Bi.memoizedState=Wi=e:Wi=Wi.next=e}return Wi}function Yi(e,t){return"function"===typeof t?t(e):t}function Gi(e){var t=Xi(),n=t.queue;if(null===n)throw Error(o(311));n.lastRenderedReducer=e;var r=$i,a=r.baseQueue,i=n.pending;if(null!==i){if(null!==a){var l=a.next;a.next=i.next,i.next=l}r.baseQueue=a=i,n.pending=null}if(null!==a){a=a.next,r=r.baseState;var u=l=i=null,c=a;do{var s=c.expirationTime;if(s<Ui){var f={expirationTime:c.expirationTime,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null};null===u?(l=u=f,i=r):u=u.next=f,s>Bi.expirationTime&&(Bi.expirationTime=s,ou(s))}else null!==u&&(u=u.next={expirationTime:1073741823,suspenseConfig:c.suspenseConfig,action:c.action,eagerReducer:c.eagerReducer,eagerState:c.eagerState,next:null}),iu(s,c.suspenseConfig),r=c.eagerReducer===e?c.eagerState:e(r,c.action);c=c.next}while(null!==c&&c!==a);null===u?i=r:u.next=l,Dr(r,t.memoizedState)||(No=!0),t.memoizedState=r,t.baseState=i,t.baseQueue=u,n.lastRenderedState=r}return[t.memoizedState,n.dispatch]}function Ji(e){var t=Xi(),n=t.queue;if(null===n)throw Error(o(311));n.lastRenderedReducer=e;var r=n.dispatch,a=n.pending,i=t.memoizedState;if(null!==a){n.pending=null;var l=a=a.next;do{i=e(i,l.action),l=l.next}while(l!==a);Dr(i,t.memoizedState)||(No=!0),t.memoizedState=i,null===t.baseQueue&&(t.baseState=i),n.lastRenderedState=i}return[i,r]}function Zi(e){var t=Ki();return"function"===typeof e&&(e=e()),t.memoizedState=t.baseState=e,e=(e=t.queue={pending:null,dispatch:null,lastRenderedReducer:Yi,lastRenderedState:e}).dispatch=ho.bind(null,Bi,e),[t.memoizedState,e]}function eo(e,t,n,r){return e={tag:e,create:t,destroy:n,deps:r,next:null},null===(t=Bi.updateQueue)?(t={lastEffect:null},Bi.updateQueue=t,t.lastEffect=e.next=e):null===(n=t.lastEffect)?t.lastEffect=e.next=e:(r=n.next,n.next=e,e.next=r,t.lastEffect=e),e}function to(){return Xi().memoizedState}function no(e,t,n,r){var a=Ki();Bi.effectTag|=e,a.memoizedState=eo(1|t,n,void 0,void 0===r?null:r)}function ro(e,t,n,r){var a=Xi();r=void 0===r?null:r;var i=void 0;if(null!==$i){var o=$i.memoizedState;if(i=o.destroy,null!==r&&Qi(r,o.deps))return void eo(t,n,i,r)}Bi.effectTag|=e,a.memoizedState=eo(1|t,n,i,r)}function ao(e,t){return no(516,4,e,t)}function io(e,t){return ro(516,4,e,t)}function oo(e,t){return ro(4,2,e,t)}function lo(e,t){return"function"===typeof t?(e=e(),t(e),function(){t(null)}):null!==t&&void 0!==t?(e=e(),t.current=e,function(){t.current=null}):void 0}function uo(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,ro(4,2,lo.bind(null,t,e),n)}function co(){}function so(e,t){return Ki().memoizedState=[e,void 0===t?null:t],e}function fo(e,t){var n=Xi();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Qi(t,r[1])?r[0]:(n.memoizedState=[e,t],e)}function po(e,t){var n=Xi();t=void 0===t?null:t;var r=n.memoizedState;return null!==r&&null!==t&&Qi(t,r[1])?r[0]:(e=e(),n.memoizedState=[e,t],e)}function mo(e,t,n){var r=Ua();$a(98>r?98:r,(function(){e(!0)})),$a(97<r?97:r,(function(){var r=Fi.suspense;Fi.suspense=void 0===t?null:t;try{e(!1),n()}finally{Fi.suspense=r}}))}function ho(e,t,n){var r=Ql(),a=di.suspense;a={expirationTime:r=ql(r,e,a),suspenseConfig:a,action:n,eagerReducer:null,eagerState:null,next:null};var i=t.pending;if(null===i?a.next=a:(a.next=i.next,i.next=a),t.pending=a,i=e.alternate,e===Bi||null!==i&&i===Bi)Vi=!0,a.expirationTime=Ui,Bi.expirationTime=Ui;else{if(0===e.expirationTime&&(null===i||0===i.expirationTime)&&null!==(i=t.lastRenderedReducer))try{var o=t.lastRenderedState,l=i(o,n);if(a.eagerReducer=i,a.eagerState=l,Dr(l,o))return}catch(u){}Kl(e,r)}}var vo={readContext:ri,useCallback:Hi,useContext:Hi,useEffect:Hi,useImperativeHandle:Hi,useLayoutEffect:Hi,useMemo:Hi,useReducer:Hi,useRef:Hi,useState:Hi,useDebugValue:Hi,useResponder:Hi,useDeferredValue:Hi,useTransition:Hi},go={readContext:ri,useCallback:so,useContext:ri,useEffect:ao,useImperativeHandle:function(e,t,n){return n=null!==n&&void 0!==n?n.concat([e]):null,no(4,2,lo.bind(null,t,e),n)},useLayoutEffect:function(e,t){return no(4,2,e,t)},useMemo:function(e,t){var n=Ki();return t=void 0===t?null:t,e=e(),n.memoizedState=[e,t],e},useReducer:function(e,t,n){var r=Ki();return t=void 0!==n?n(t):t,r.memoizedState=r.baseState=t,e=(e=r.queue={pending:null,dispatch:null,lastRenderedReducer:e,lastRenderedState:t}).dispatch=ho.bind(null,Bi,e),[r.memoizedState,e]},useRef:function(e){return e={current:e},Ki().memoizedState=e},useState:Zi,useDebugValue:co,useResponder:Di,useDeferredValue:function(e,t){var n=Zi(e),r=n[0],a=n[1];return ao((function(){var n=Fi.suspense;Fi.suspense=void 0===t?null:t;try{a(e)}finally{Fi.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Zi(!1),n=t[0];return t=t[1],[so(mo.bind(null,t,e),[t,e]),n]}},yo={readContext:ri,useCallback:fo,useContext:ri,useEffect:io,useImperativeHandle:uo,useLayoutEffect:oo,useMemo:po,useReducer:Gi,useRef:to,useState:function(){return Gi(Yi)},useDebugValue:co,useResponder:Di,useDeferredValue:function(e,t){var n=Gi(Yi),r=n[0],a=n[1];return io((function(){var n=Fi.suspense;Fi.suspense=void 0===t?null:t;try{a(e)}finally{Fi.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Gi(Yi),n=t[0];return t=t[1],[fo(mo.bind(null,t,e),[t,e]),n]}},bo={readContext:ri,useCallback:fo,useContext:ri,useEffect:io,useImperativeHandle:uo,useLayoutEffect:oo,useMemo:po,useReducer:Ji,useRef:to,useState:function(){return Ji(Yi)},useDebugValue:co,useResponder:Di,useDeferredValue:function(e,t){var n=Ji(Yi),r=n[0],a=n[1];return io((function(){var n=Fi.suspense;Fi.suspense=void 0===t?null:t;try{a(e)}finally{Fi.suspense=n}}),[e,t]),r},useTransition:function(e){var t=Ji(Yi),n=t[0];return t=t[1],[fo(mo.bind(null,t,e),[t,e]),n]}},wo=null,xo=null,Eo=!1;function ko(e,t){var n=Tu(5,null,null,0);n.elementType="DELETED",n.type="DELETED",n.stateNode=t,n.return=e,n.effectTag=8,null!==e.lastEffect?(e.lastEffect.nextEffect=n,e.lastEffect=n):e.firstEffect=e.lastEffect=n}function To(e,t){switch(e.tag){case 5:var n=e.type;return null!==(t=1!==t.nodeType||n.toLowerCase()!==t.nodeName.toLowerCase()?null:t)&&(e.stateNode=t,!0);case 6:return null!==(t=""===e.pendingProps||3!==t.nodeType?null:t)&&(e.stateNode=t,!0);case 13:default:return!1}}function So(e){if(Eo){var t=xo;if(t){var n=t;if(!To(e,t)){if(!(t=xn(n.nextSibling))||!To(e,t))return e.effectTag=-1025&e.effectTag|2,Eo=!1,void(wo=e);ko(wo,n)}wo=e,xo=xn(t.firstChild)}else e.effectTag=-1025&e.effectTag|2,Eo=!1,wo=e}}function Co(e){for(e=e.return;null!==e&&5!==e.tag&&3!==e.tag&&13!==e.tag;)e=e.return;wo=e}function Po(e){if(e!==wo)return!1;if(!Eo)return Co(e),Eo=!0,!1;var t=e.type;if(5!==e.tag||"head"!==t&&"body"!==t&&!yn(t,e.memoizedProps))for(t=xo;t;)ko(e,t),t=xn(t.nextSibling);if(Co(e),13===e.tag){if(!(e=null!==(e=e.memoizedState)?e.dehydrated:null))throw Error(o(317));e:{for(e=e.nextSibling,t=0;e;){if(8===e.nodeType){var n=e.data;if("/$"===n){if(0===t){xo=xn(e.nextSibling);break e}t--}else"$"!==n&&"$!"!==n&&"$?"!==n||t++}e=e.nextSibling}xo=null}}else xo=wo?xn(e.stateNode.nextSibling):null;return!0}function _o(){xo=wo=null,Eo=!1}var Oo=Y.ReactCurrentOwner,No=!1;function Mo(e,t,n,r){t.child=null===e?Si(t,null,n,r):Ti(t,e.child,n,r)}function Ro(e,t,n,r,a){n=n.render;var i=t.ref;return ni(t,a),r=qi(e,t,n,r,i,a),null===e||No?(t.effectTag|=1,Mo(e,t,r,a),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=a&&(e.expirationTime=0),Ko(e,t,a))}function jo(e,t,n,r,a,i){if(null===e){var o=n.type;return"function"!==typeof o||Su(o)||void 0!==o.defaultProps||null!==n.compare||void 0!==n.defaultProps?((e=Pu(n.type,null,r,null,t.mode,i)).ref=t.ref,e.return=t,t.child=e):(t.tag=15,t.type=o,zo(e,t,o,r,a,i))}return o=e.child,a<i&&(a=o.memoizedProps,(n=null!==(n=n.compare)?n:Fr)(a,r)&&e.ref===t.ref)?Ko(e,t,i):(t.effectTag|=1,(e=Cu(o,r)).ref=t.ref,e.return=t,t.child=e)}function zo(e,t,n,r,a,i){return null!==e&&Fr(e.memoizedProps,r)&&e.ref===t.ref&&(No=!1,a<i)?(t.expirationTime=e.expirationTime,Ko(e,t,i)):Lo(e,t,n,r,i)}function Ao(e,t){var n=t.ref;(null===e&&null!==n||null!==e&&e.ref!==n)&&(t.effectTag|=128)}function Lo(e,t,n,r,a){var i=ha(n)?pa:fa.current;return i=ma(t,i),ni(t,a),n=qi(e,t,n,r,i,a),null===e||No?(t.effectTag|=1,Mo(e,t,n,a),t.child):(t.updateQueue=e.updateQueue,t.effectTag&=-517,e.expirationTime<=a&&(e.expirationTime=0),Ko(e,t,a))}function Do(e,t,n,r,a){if(ha(n)){var i=!0;ba(t)}else i=!1;if(ni(t,a),null===t.stateNode)null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),gi(t,n,r),bi(t,n,r,a),r=!0;else if(null===e){var o=t.stateNode,l=t.memoizedProps;o.props=l;var u=o.context,c=n.contextType;"object"===typeof c&&null!==c?c=ri(c):c=ma(t,c=ha(n)?pa:fa.current);var s=n.getDerivedStateFromProps,f="function"===typeof s||"function"===typeof o.getSnapshotBeforeUpdate;f||"function"!==typeof o.UNSAFE_componentWillReceiveProps&&"function"!==typeof o.componentWillReceiveProps||(l!==r||u!==c)&&yi(t,o,r,c),ai=!1;var d=t.memoizedState;o.state=d,si(t,r,o,a),u=t.memoizedState,l!==r||d!==u||da.current||ai?("function"===typeof s&&(mi(t,n,s,r),u=t.memoizedState),(l=ai||vi(t,n,l,r,d,u,c))?(f||"function"!==typeof o.UNSAFE_componentWillMount&&"function"!==typeof o.componentWillMount||("function"===typeof o.componentWillMount&&o.componentWillMount(),"function"===typeof o.UNSAFE_componentWillMount&&o.UNSAFE_componentWillMount()),"function"===typeof o.componentDidMount&&(t.effectTag|=4)):("function"===typeof o.componentDidMount&&(t.effectTag|=4),t.memoizedProps=r,t.memoizedState=u),o.props=r,o.state=u,o.context=c,r=l):("function"===typeof o.componentDidMount&&(t.effectTag|=4),r=!1)}else o=t.stateNode,oi(e,t),l=t.memoizedProps,o.props=t.type===t.elementType?l:Ka(t.type,l),u=o.context,"object"===typeof(c=n.contextType)&&null!==c?c=ri(c):c=ma(t,c=ha(n)?pa:fa.current),(f="function"===typeof(s=n.getDerivedStateFromProps)||"function"===typeof o.getSnapshotBeforeUpdate)||"function"!==typeof o.UNSAFE_componentWillReceiveProps&&"function"!==typeof o.componentWillReceiveProps||(l!==r||u!==c)&&yi(t,o,r,c),ai=!1,u=t.memoizedState,o.state=u,si(t,r,o,a),d=t.memoizedState,l!==r||u!==d||da.current||ai?("function"===typeof s&&(mi(t,n,s,r),d=t.memoizedState),(s=ai||vi(t,n,l,r,u,d,c))?(f||"function"!==typeof o.UNSAFE_componentWillUpdate&&"function"!==typeof o.componentWillUpdate||("function"===typeof o.componentWillUpdate&&o.componentWillUpdate(r,d,c),"function"===typeof o.UNSAFE_componentWillUpdate&&o.UNSAFE_componentWillUpdate(r,d,c)),"function"===typeof o.componentDidUpdate&&(t.effectTag|=4),"function"===typeof o.getSnapshotBeforeUpdate&&(t.effectTag|=256)):("function"!==typeof o.componentDidUpdate||l===e.memoizedProps&&u===e.memoizedState||(t.effectTag|=4),"function"!==typeof o.getSnapshotBeforeUpdate||l===e.memoizedProps&&u===e.memoizedState||(t.effectTag|=256),t.memoizedProps=r,t.memoizedState=d),o.props=r,o.state=d,o.context=c,r=s):("function"!==typeof o.componentDidUpdate||l===e.memoizedProps&&u===e.memoizedState||(t.effectTag|=4),"function"!==typeof o.getSnapshotBeforeUpdate||l===e.memoizedProps&&u===e.memoizedState||(t.effectTag|=256),r=!1);return Io(e,t,n,r,i,a)}function Io(e,t,n,r,a,i){Ao(e,t);var o=0!==(64&t.effectTag);if(!r&&!o)return a&&wa(t,n,!1),Ko(e,t,i);r=t.stateNode,Oo.current=t;var l=o&&"function"!==typeof n.getDerivedStateFromError?null:r.render();return t.effectTag|=1,null!==e&&o?(t.child=Ti(t,e.child,null,i),t.child=Ti(t,null,l,i)):Mo(e,t,l,i),t.memoizedState=r.state,a&&wa(t,n,!0),t.child}function Fo(e){var t=e.stateNode;t.pendingContext?ga(0,t.pendingContext,t.pendingContext!==t.context):t.context&&ga(0,t.context,!1),Mi(e,t.containerInfo)}var Uo,Bo,$o,Wo={dehydrated:null,retryTime:0};function Vo(e,t,n){var r,a=t.mode,i=t.pendingProps,o=Ai.current,l=!1;if((r=0!==(64&t.effectTag))||(r=0!==(2&o)&&(null===e||null!==e.memoizedState)),r?(l=!0,t.effectTag&=-65):null!==e&&null===e.memoizedState||void 0===i.fallback||!0===i.unstable_avoidThisFallback||(o|=1),ca(Ai,1&o),null===e){if(void 0!==i.fallback&&So(t),l){if(l=i.fallback,(i=_u(null,a,0,null)).return=t,0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,i.child=e;null!==e;)e.return=i,e=e.sibling;return(n=_u(l,a,n,null)).return=t,i.sibling=n,t.memoizedState=Wo,t.child=i,n}return a=i.children,t.memoizedState=null,t.child=Si(t,null,a,n)}if(null!==e.memoizedState){if(a=(e=e.child).sibling,l){if(i=i.fallback,(n=Cu(e,e.pendingProps)).return=t,0===(2&t.mode)&&(l=null!==t.memoizedState?t.child.child:t.child)!==e.child)for(n.child=l;null!==l;)l.return=n,l=l.sibling;return(a=Cu(a,i)).return=t,n.sibling=a,n.childExpirationTime=0,t.memoizedState=Wo,t.child=n,a}return n=Ti(t,e.child,i.children,n),t.memoizedState=null,t.child=n}if(e=e.child,l){if(l=i.fallback,(i=_u(null,a,0,null)).return=t,i.child=e,null!==e&&(e.return=i),0===(2&t.mode))for(e=null!==t.memoizedState?t.child.child:t.child,i.child=e;null!==e;)e.return=i,e=e.sibling;return(n=_u(l,a,n,null)).return=t,i.sibling=n,n.effectTag|=2,i.childExpirationTime=0,t.memoizedState=Wo,t.child=i,n}return t.memoizedState=null,t.child=Ti(t,e,i.children,n)}function Ho(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t),ti(e.return,t)}function Qo(e,t,n,r,a,i){var o=e.memoizedState;null===o?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:r,tail:n,tailExpiration:0,tailMode:a,lastEffect:i}:(o.isBackwards=t,o.rendering=null,o.renderingStartTime=0,o.last=r,o.tail=n,o.tailExpiration=0,o.tailMode=a,o.lastEffect=i)}function qo(e,t,n){var r=t.pendingProps,a=r.revealOrder,i=r.tail;if(Mo(e,t,r.children,n),0!==(2&(r=Ai.current)))r=1&r|2,t.effectTag|=64;else{if(null!==e&&0!==(64&e.effectTag))e:for(e=t.child;null!==e;){if(13===e.tag)null!==e.memoizedState&&Ho(e,n);else if(19===e.tag)Ho(e,n);else if(null!==e.child){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;null===e.sibling;){if(null===e.return||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}r&=1}if(ca(Ai,r),0===(2&t.mode))t.memoizedState=null;else switch(a){case"forwards":for(n=t.child,a=null;null!==n;)null!==(e=n.alternate)&&null===Li(e)&&(a=n),n=n.sibling;null===(n=a)?(a=t.child,t.child=null):(a=n.sibling,n.sibling=null),Qo(t,!1,a,n,i,t.lastEffect);break;case"backwards":for(n=null,a=t.child,t.child=null;null!==a;){if(null!==(e=a.alternate)&&null===Li(e)){t.child=a;break}e=a.sibling,a.sibling=n,n=a,a=e}Qo(t,!0,n,null,i,t.lastEffect);break;case"together":Qo(t,!1,null,null,void 0,t.lastEffect);break;default:t.memoizedState=null}return t.child}function Ko(e,t,n){null!==e&&(t.dependencies=e.dependencies);var r=t.expirationTime;if(0!==r&&ou(r),t.childExpirationTime<n)return null;if(null!==e&&t.child!==e.child)throw Error(o(153));if(null!==t.child){for(n=Cu(e=t.child,e.pendingProps),t.child=n,n.return=t;null!==e.sibling;)e=e.sibling,(n=n.sibling=Cu(e,e.pendingProps)).return=t;n.sibling=null}return t.child}function Xo(e,t){switch(e.tailMode){case"hidden":t=e.tail;for(var n=null;null!==t;)null!==t.alternate&&(n=t),t=t.sibling;null===n?e.tail=null:n.sibling=null;break;case"collapsed":n=e.tail;for(var r=null;null!==n;)null!==n.alternate&&(r=n),n=n.sibling;null===r?t||null===e.tail?e.tail=null:e.tail.sibling=null:r.sibling=null}}function Yo(e,t,n){var r=t.pendingProps;switch(t.tag){case 2:case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return null;case 1:return ha(t.type)&&va(),null;case 3:return Ri(),ua(da),ua(fa),(n=t.stateNode).pendingContext&&(n.context=n.pendingContext,n.pendingContext=null),null!==e&&null!==e.child||!Po(t)||(t.effectTag|=4),null;case 5:zi(t),n=Ni(Oi.current);var i=t.type;if(null!==e&&null!=t.stateNode)Bo(e,t,i,r,n),e.ref!==t.ref&&(t.effectTag|=128);else{if(!r){if(null===t.stateNode)throw Error(o(166));return null}if(e=Ni(Pi.current),Po(t)){r=t.stateNode,i=t.type;var l=t.memoizedProps;switch(r[Tn]=t,r[Sn]=l,i){case"iframe":case"object":case"embed":qt("load",r);break;case"video":case"audio":for(e=0;e<Ye.length;e++)qt(Ye[e],r);break;case"source":qt("error",r);break;case"img":case"image":case"link":qt("error",r),qt("load",r);break;case"form":qt("reset",r),qt("submit",r);break;case"details":qt("toggle",r);break;case"input":ke(r,l),qt("invalid",r),un(n,"onChange");break;case"select":r._wrapperState={wasMultiple:!!l.multiple},qt("invalid",r),un(n,"onChange");break;case"textarea":Me(r,l),qt("invalid",r),un(n,"onChange")}for(var u in an(i,l),e=null,l)if(l.hasOwnProperty(u)){var c=l[u];"children"===u?"string"===typeof c?r.textContent!==c&&(e=["children",c]):"number"===typeof c&&r.textContent!==""+c&&(e=["children",""+c]):T.hasOwnProperty(u)&&null!=c&&un(n,u)}switch(i){case"input":we(r),Ce(r,l,!0);break;case"textarea":we(r),je(r);break;case"select":case"option":break;default:"function"===typeof l.onClick&&(r.onclick=cn)}n=e,t.updateQueue=n,null!==n&&(t.effectTag|=4)}else{switch(u=9===n.nodeType?n:n.ownerDocument,e===ln&&(e=Le(i)),e===ln?"script"===i?((e=u.createElement("div")).innerHTML="<script><\/script>",e=e.removeChild(e.firstChild)):"string"===typeof r.is?e=u.createElement(i,{is:r.is}):(e=u.createElement(i),"select"===i&&(u=e,r.multiple?u.multiple=!0:r.size&&(u.size=r.size))):e=u.createElementNS(e,i),e[Tn]=t,e[Sn]=r,Uo(e,t),t.stateNode=e,u=on(i,r),i){case"iframe":case"object":case"embed":qt("load",e),c=r;break;case"video":case"audio":for(c=0;c<Ye.length;c++)qt(Ye[c],e);c=r;break;case"source":qt("error",e),c=r;break;case"img":case"image":case"link":qt("error",e),qt("load",e),c=r;break;case"form":qt("reset",e),qt("submit",e),c=r;break;case"details":qt("toggle",e),c=r;break;case"input":ke(e,r),c=Ee(e,r),qt("invalid",e),un(n,"onChange");break;case"option":c=_e(e,r);break;case"select":e._wrapperState={wasMultiple:!!r.multiple},c=a({},r,{value:void 0}),qt("invalid",e),un(n,"onChange");break;case"textarea":Me(e,r),c=Ne(e,r),qt("invalid",e),un(n,"onChange");break;default:c=r}an(i,c);var s=c;for(l in s)if(s.hasOwnProperty(l)){var f=s[l];"style"===l?nn(e,f):"dangerouslySetInnerHTML"===l?null!=(f=f?f.__html:void 0)&&Fe(e,f):"children"===l?"string"===typeof f?("textarea"!==i||""!==f)&&Ue(e,f):"number"===typeof f&&Ue(e,""+f):"suppressContentEditableWarning"!==l&&"suppressHydrationWarning"!==l&&"autoFocus"!==l&&(T.hasOwnProperty(l)?null!=f&&un(n,l):null!=f&&G(e,l,f,u))}switch(i){case"input":we(e),Ce(e,r,!1);break;case"textarea":we(e),je(e);break;case"option":null!=r.value&&e.setAttribute("value",""+ye(r.value));break;case"select":e.multiple=!!r.multiple,null!=(n=r.value)?Oe(e,!!r.multiple,n,!1):null!=r.defaultValue&&Oe(e,!!r.multiple,r.defaultValue,!0);break;default:"function"===typeof c.onClick&&(e.onclick=cn)}gn(i,r)&&(t.effectTag|=4)}null!==t.ref&&(t.effectTag|=128)}return null;case 6:if(e&&null!=t.stateNode)$o(0,t,e.memoizedProps,r);else{if("string"!==typeof r&&null===t.stateNode)throw Error(o(166));n=Ni(Oi.current),Ni(Pi.current),Po(t)?(n=t.stateNode,r=t.memoizedProps,n[Tn]=t,n.nodeValue!==r&&(t.effectTag|=4)):((n=(9===n.nodeType?n:n.ownerDocument).createTextNode(r))[Tn]=t,t.stateNode=n)}return null;case 13:return ua(Ai),r=t.memoizedState,0!==(64&t.effectTag)?(t.expirationTime=n,t):(n=null!==r,r=!1,null===e?void 0!==t.memoizedProps.fallback&&Po(t):(r=null!==(i=e.memoizedState),n||null===i||null!==(i=e.child.sibling)&&(null!==(l=t.firstEffect)?(t.firstEffect=i,i.nextEffect=l):(t.firstEffect=t.lastEffect=i,i.nextEffect=null),i.effectTag=8)),n&&!r&&0!==(2&t.mode)&&(null===e&&!0!==t.memoizedProps.unstable_avoidThisFallback||0!==(1&Ai.current)?Pl===wl&&(Pl=xl):(Pl!==wl&&Pl!==xl||(Pl=El),0!==Rl&&null!==Tl&&(ju(Tl,Cl),zu(Tl,Rl)))),(n||r)&&(t.effectTag|=4),null);case 4:return Ri(),null;case 10:return ei(t),null;case 17:return ha(t.type)&&va(),null;case 19:if(ua(Ai),null===(r=t.memoizedState))return null;if(i=0!==(64&t.effectTag),null===(l=r.rendering)){if(i)Xo(r,!1);else if(Pl!==wl||null!==e&&0!==(64&e.effectTag))for(l=t.child;null!==l;){if(null!==(e=Li(l))){for(t.effectTag|=64,Xo(r,!1),null!==(i=e.updateQueue)&&(t.updateQueue=i,t.effectTag|=4),null===r.lastEffect&&(t.firstEffect=null),t.lastEffect=r.lastEffect,r=t.child;null!==r;)l=n,(i=r).effectTag&=2,i.nextEffect=null,i.firstEffect=null,i.lastEffect=null,null===(e=i.alternate)?(i.childExpirationTime=0,i.expirationTime=l,i.child=null,i.memoizedProps=null,i.memoizedState=null,i.updateQueue=null,i.dependencies=null):(i.childExpirationTime=e.childExpirationTime,i.expirationTime=e.expirationTime,i.child=e.child,i.memoizedProps=e.memoizedProps,i.memoizedState=e.memoizedState,i.updateQueue=e.updateQueue,l=e.dependencies,i.dependencies=null===l?null:{expirationTime:l.expirationTime,firstContext:l.firstContext,responders:l.responders}),r=r.sibling;return ca(Ai,1&Ai.current|2),t.child}l=l.sibling}}else{if(!i)if(null!==(e=Li(l))){if(t.effectTag|=64,i=!0,null!==(n=e.updateQueue)&&(t.updateQueue=n,t.effectTag|=4),Xo(r,!0),null===r.tail&&"hidden"===r.tailMode&&!l.alternate)return null!==(t=t.lastEffect=r.lastEffect)&&(t.nextEffect=null),null}else 2*Fa()-r.renderingStartTime>r.tailExpiration&&1<n&&(t.effectTag|=64,i=!0,Xo(r,!1),t.expirationTime=t.childExpirationTime=n-1);r.isBackwards?(l.sibling=t.child,t.child=l):(null!==(n=r.last)?n.sibling=l:t.child=l,r.last=l)}return null!==r.tail?(0===r.tailExpiration&&(r.tailExpiration=Fa()+500),n=r.tail,r.rendering=n,r.tail=n.sibling,r.lastEffect=t.lastEffect,r.renderingStartTime=Fa(),n.sibling=null,t=Ai.current,ca(Ai,i?1&t|2:1&t),n):null}throw Error(o(156,t.tag))}function Go(e){switch(e.tag){case 1:ha(e.type)&&va();var t=e.effectTag;return 4096&t?(e.effectTag=-4097&t|64,e):null;case 3:if(Ri(),ua(da),ua(fa),0!==(64&(t=e.effectTag)))throw Error(o(285));return e.effectTag=-4097&t|64,e;case 5:return zi(e),null;case 13:return ua(Ai),4096&(t=e.effectTag)?(e.effectTag=-4097&t|64,e):null;case 19:return ua(Ai),null;case 4:return Ri(),null;case 10:return ei(e),null;default:return null}}function Jo(e,t){return{value:e,source:t,stack:ge(t)}}Uo=function(e,t){for(var n=t.child;null!==n;){if(5===n.tag||6===n.tag)e.appendChild(n.stateNode);else if(4!==n.tag&&null!==n.child){n.child.return=n,n=n.child;continue}if(n===t)break;for(;null===n.sibling;){if(null===n.return||n.return===t)return;n=n.return}n.sibling.return=n.return,n=n.sibling}},Bo=function(e,t,n,r,i){var o=e.memoizedProps;if(o!==r){var l,u,c=t.stateNode;switch(Ni(Pi.current),e=null,n){case"input":o=Ee(c,o),r=Ee(c,r),e=[];break;case"option":o=_e(c,o),r=_e(c,r),e=[];break;case"select":o=a({},o,{value:void 0}),r=a({},r,{value:void 0}),e=[];break;case"textarea":o=Ne(c,o),r=Ne(c,r),e=[];break;default:"function"!==typeof o.onClick&&"function"===typeof r.onClick&&(c.onclick=cn)}for(l in an(n,r),n=null,o)if(!r.hasOwnProperty(l)&&o.hasOwnProperty(l)&&null!=o[l])if("style"===l)for(u in c=o[l])c.hasOwnProperty(u)&&(n||(n={}),n[u]="");else"dangerouslySetInnerHTML"!==l&&"children"!==l&&"suppressContentEditableWarning"!==l&&"suppressHydrationWarning"!==l&&"autoFocus"!==l&&(T.hasOwnProperty(l)?e||(e=[]):(e=e||[]).push(l,null));for(l in r){var s=r[l];if(c=null!=o?o[l]:void 0,r.hasOwnProperty(l)&&s!==c&&(null!=s||null!=c))if("style"===l)if(c){for(u in c)!c.hasOwnProperty(u)||s&&s.hasOwnProperty(u)||(n||(n={}),n[u]="");for(u in s)s.hasOwnProperty(u)&&c[u]!==s[u]&&(n||(n={}),n[u]=s[u])}else n||(e||(e=[]),e.push(l,n)),n=s;else"dangerouslySetInnerHTML"===l?(s=s?s.__html:void 0,c=c?c.__html:void 0,null!=s&&c!==s&&(e=e||[]).push(l,s)):"children"===l?c===s||"string"!==typeof s&&"number"!==typeof s||(e=e||[]).push(l,""+s):"suppressContentEditableWarning"!==l&&"suppressHydrationWarning"!==l&&(T.hasOwnProperty(l)?(null!=s&&un(i,l),e||c===s||(e=[])):(e=e||[]).push(l,s))}n&&(e=e||[]).push("style",n),i=e,(t.updateQueue=i)&&(t.effectTag|=4)}},$o=function(e,t,n,r){n!==r&&(t.effectTag|=4)};var Zo="function"===typeof WeakSet?WeakSet:Set;function el(e,t){var n=t.source,r=t.stack;null===r&&null!==n&&(r=ge(n)),null!==n&&ve(n.type),t=t.value,null!==e&&1===e.tag&&ve(e.type);try{console.error(t)}catch(a){setTimeout((function(){throw a}))}}function tl(e){var t=e.ref;if(null!==t)if("function"===typeof t)try{t(null)}catch(n){yu(e,n)}else t.current=null}function nl(e,t){switch(t.tag){case 0:case 11:case 15:case 22:return;case 1:if(256&t.effectTag&&null!==e){var n=e.memoizedProps,r=e.memoizedState;t=(e=t.stateNode).getSnapshotBeforeUpdate(t.elementType===t.type?n:Ka(t.type,n),r),e.__reactInternalSnapshotBeforeUpdate=t}return;case 3:case 5:case 6:case 4:case 17:return}throw Error(o(163))}function rl(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.destroy;n.destroy=void 0,void 0!==r&&r()}n=n.next}while(n!==t)}}function al(e,t){if(null!==(t=null!==(t=t.updateQueue)?t.lastEffect:null)){var n=t=t.next;do{if((n.tag&e)===e){var r=n.create;n.destroy=r()}n=n.next}while(n!==t)}}function il(e,t,n){switch(n.tag){case 0:case 11:case 15:case 22:return void al(3,n);case 1:if(e=n.stateNode,4&n.effectTag)if(null===t)e.componentDidMount();else{var r=n.elementType===n.type?t.memoizedProps:Ka(n.type,t.memoizedProps);e.componentDidUpdate(r,t.memoizedState,e.__reactInternalSnapshotBeforeUpdate)}return void(null!==(t=n.updateQueue)&&fi(n,t,e));case 3:if(null!==(t=n.updateQueue)){if(e=null,null!==n.child)switch(n.child.tag){case 5:e=n.child.stateNode;break;case 1:e=n.child.stateNode}fi(n,t,e)}return;case 5:return e=n.stateNode,void(null===t&&4&n.effectTag&&gn(n.type,n.memoizedProps)&&e.focus());case 6:case 4:case 12:return;case 13:return void(null===n.memoizedState&&(n=n.alternate,null!==n&&(n=n.memoizedState,null!==n&&(n=n.dehydrated,null!==n&&Lt(n)))));case 19:case 17:case 20:case 21:return}throw Error(o(163))}function ol(e,t,n){switch("function"===typeof Eu&&Eu(t),t.tag){case 0:case 11:case 14:case 15:case 22:if(null!==(e=t.updateQueue)&&null!==(e=e.lastEffect)){var r=e.next;$a(97<n?97:n,(function(){var e=r;do{var n=e.destroy;if(void 0!==n){var a=t;try{n()}catch(i){yu(a,i)}}e=e.next}while(e!==r)}))}break;case 1:tl(t),"function"===typeof(n=t.stateNode).componentWillUnmount&&function(e,t){try{t.props=e.memoizedProps,t.state=e.memoizedState,t.componentWillUnmount()}catch(n){yu(e,n)}}(t,n);break;case 5:tl(t);break;case 4:sl(e,t,n)}}function ll(e){var t=e.alternate;e.return=null,e.child=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.alternate=null,e.firstEffect=null,e.lastEffect=null,e.pendingProps=null,e.memoizedProps=null,e.stateNode=null,null!==t&&ll(t)}function ul(e){return 5===e.tag||3===e.tag||4===e.tag}function cl(e){e:{for(var t=e.return;null!==t;){if(ul(t)){var n=t;break e}t=t.return}throw Error(o(160))}switch(t=n.stateNode,n.tag){case 5:var r=!1;break;case 3:case 4:t=t.containerInfo,r=!0;break;default:throw Error(o(161))}16&n.effectTag&&(Ue(t,""),n.effectTag&=-17);e:t:for(n=e;;){for(;null===n.sibling;){if(null===n.return||ul(n.return)){n=null;break e}n=n.return}for(n.sibling.return=n.return,n=n.sibling;5!==n.tag&&6!==n.tag&&18!==n.tag;){if(2&n.effectTag)continue t;if(null===n.child||4===n.tag)continue t;n.child.return=n,n=n.child}if(!(2&n.effectTag)){n=n.stateNode;break e}}r?function e(t,n,r){var a=t.tag,i=5===a||6===a;if(i)t=i?t.stateNode:t.stateNode.instance,n?8===r.nodeType?r.parentNode.insertBefore(t,n):r.insertBefore(t,n):(8===r.nodeType?(n=r.parentNode).insertBefore(t,r):(n=r).appendChild(t),null!==(r=r._reactRootContainer)&&void 0!==r||null!==n.onclick||(n.onclick=cn));else if(4!==a&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t):function e(t,n,r){var a=t.tag,i=5===a||6===a;if(i)t=i?t.stateNode:t.stateNode.instance,n?r.insertBefore(t,n):r.appendChild(t);else if(4!==a&&null!==(t=t.child))for(e(t,n,r),t=t.sibling;null!==t;)e(t,n,r),t=t.sibling}(e,n,t)}function sl(e,t,n){for(var r,a,i=t,l=!1;;){if(!l){l=i.return;e:for(;;){if(null===l)throw Error(o(160));switch(r=l.stateNode,l.tag){case 5:a=!1;break e;case 3:case 4:r=r.containerInfo,a=!0;break e}l=l.return}l=!0}if(5===i.tag||6===i.tag){e:for(var u=e,c=i,s=n,f=c;;)if(ol(u,f,s),null!==f.child&&4!==f.tag)f.child.return=f,f=f.child;else{if(f===c)break e;for(;null===f.sibling;){if(null===f.return||f.return===c)break e;f=f.return}f.sibling.return=f.return,f=f.sibling}a?(u=r,c=i.stateNode,8===u.nodeType?u.parentNode.removeChild(c):u.removeChild(c)):r.removeChild(i.stateNode)}else if(4===i.tag){if(null!==i.child){r=i.stateNode.containerInfo,a=!0,i.child.return=i,i=i.child;continue}}else if(ol(e,i,n),null!==i.child){i.child.return=i,i=i.child;continue}if(i===t)break;for(;null===i.sibling;){if(null===i.return||i.return===t)return;4===(i=i.return).tag&&(l=!1)}i.sibling.return=i.return,i=i.sibling}}function fl(e,t){switch(t.tag){case 0:case 11:case 14:case 15:case 22:return void rl(3,t);case 1:return;case 5:var n=t.stateNode;if(null!=n){var r=t.memoizedProps,a=null!==e?e.memoizedProps:r;e=t.type;var i=t.updateQueue;if(t.updateQueue=null,null!==i){for(n[Sn]=r,"input"===e&&"radio"===r.type&&null!=r.name&&Te(n,r),on(e,a),t=on(e,r),a=0;a<i.length;a+=2){var l=i[a],u=i[a+1];"style"===l?nn(n,u):"dangerouslySetInnerHTML"===l?Fe(n,u):"children"===l?Ue(n,u):G(n,l,u,t)}switch(e){case"input":Se(n,r);break;case"textarea":Re(n,r);break;case"select":t=n._wrapperState.wasMultiple,n._wrapperState.wasMultiple=!!r.multiple,null!=(e=r.value)?Oe(n,!!r.multiple,e,!1):t!==!!r.multiple&&(null!=r.defaultValue?Oe(n,!!r.multiple,r.defaultValue,!0):Oe(n,!!r.multiple,r.multiple?[]:"",!1))}}}return;case 6:if(null===t.stateNode)throw Error(o(162));return void(t.stateNode.nodeValue=t.memoizedProps);case 3:return void((t=t.stateNode).hydrate&&(t.hydrate=!1,Lt(t.containerInfo)));case 12:return;case 13:if(n=t,null===t.memoizedState?r=!1:(r=!0,n=t.child,zl=Fa()),null!==n)e:for(e=n;;){if(5===e.tag)i=e.stateNode,r?"function"===typeof(i=i.style).setProperty?i.setProperty("display","none","important"):i.display="none":(i=e.stateNode,a=void 0!==(a=e.memoizedProps.style)&&null!==a&&a.hasOwnProperty("display")?a.display:null,i.style.display=tn("display",a));else if(6===e.tag)e.stateNode.nodeValue=r?"":e.memoizedProps;else{if(13===e.tag&&null!==e.memoizedState&&null===e.memoizedState.dehydrated){(i=e.child.sibling).return=e,e=i;continue}if(null!==e.child){e.child.return=e,e=e.child;continue}}if(e===n)break;for(;null===e.sibling;){if(null===e.return||e.return===n)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}return void dl(t);case 19:return void dl(t);case 17:return}throw Error(o(163))}function dl(e){var t=e.updateQueue;if(null!==t){e.updateQueue=null;var n=e.stateNode;null===n&&(n=e.stateNode=new Zo),t.forEach((function(t){var r=wu.bind(null,e,t);n.has(t)||(n.add(t),t.then(r,r))}))}}var pl="function"===typeof WeakMap?WeakMap:Map;function ml(e,t,n){(n=li(n,null)).tag=3,n.payload={element:null};var r=t.value;return n.callback=function(){Ll||(Ll=!0,Dl=r),el(e,t)},n}function hl(e,t,n){(n=li(n,null)).tag=3;var r=e.type.getDerivedStateFromError;if("function"===typeof r){var a=t.value;n.payload=function(){return el(e,t),r(a)}}var i=e.stateNode;return null!==i&&"function"===typeof i.componentDidCatch&&(n.callback=function(){"function"!==typeof r&&(null===Il?Il=new Set([this]):Il.add(this),el(e,t));var n=t.stack;this.componentDidCatch(t.value,{componentStack:null!==n?n:""})}),n}var vl,gl=Math.ceil,yl=Y.ReactCurrentDispatcher,bl=Y.ReactCurrentOwner,wl=0,xl=3,El=4,kl=0,Tl=null,Sl=null,Cl=0,Pl=wl,_l=null,Ol=1073741823,Nl=1073741823,Ml=null,Rl=0,jl=!1,zl=0,Al=null,Ll=!1,Dl=null,Il=null,Fl=!1,Ul=null,Bl=90,$l=null,Wl=0,Vl=null,Hl=0;function Ql(){return 0!==(48&kl)?1073741821-(Fa()/10|0):0!==Hl?Hl:Hl=1073741821-(Fa()/10|0)}function ql(e,t,n){if(0===(2&(t=t.mode)))return 1073741823;var r=Ua();if(0===(4&t))return 99===r?1073741823:1073741822;if(0!==(16&kl))return Cl;if(null!==n)e=qa(e,0|n.timeoutMs||5e3,250);else switch(r){case 99:e=1073741823;break;case 98:e=qa(e,150,100);break;case 97:case 96:e=qa(e,5e3,250);break;case 95:e=2;break;default:throw Error(o(326))}return null!==Tl&&e===Cl&&--e,e}function Kl(e,t){if(50<Wl)throw Wl=0,Vl=null,Error(o(185));if(null!==(e=Xl(e,t))){var n=Ua();1073741823===t?0!==(8&kl)&&0===(48&kl)?Zl(e):(Gl(e),0===kl&&Ha()):Gl(e),0===(4&kl)||98!==n&&99!==n||(null===$l?$l=new Map([[e,t]]):(void 0===(n=$l.get(e))||n>t)&&$l.set(e,t))}}function Xl(e,t){e.expirationTime<t&&(e.expirationTime=t);var n=e.alternate;null!==n&&n.expirationTime<t&&(n.expirationTime=t);var r=e.return,a=null;if(null===r&&3===e.tag)a=e.stateNode;else for(;null!==r;){if(n=r.alternate,r.childExpirationTime<t&&(r.childExpirationTime=t),null!==n&&n.childExpirationTime<t&&(n.childExpirationTime=t),null===r.return&&3===r.tag){a=r.stateNode;break}r=r.return}return null!==a&&(Tl===a&&(ou(t),Pl===El&&ju(a,Cl)),zu(a,t)),a}function Yl(e){var t=e.lastExpiredTime;if(0!==t)return t;if(!Ru(e,t=e.firstPendingTime))return t;var n=e.lastPingedTime;return 2>=(e=n>(e=e.nextKnownPendingLevel)?n:e)&&t!==e?0:e}function Gl(e){if(0!==e.lastExpiredTime)e.callbackExpirationTime=1073741823,e.callbackPriority=99,e.callbackNode=Va(Zl.bind(null,e));else{var t=Yl(e),n=e.callbackNode;if(0===t)null!==n&&(e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90);else{var r=Ql();if(1073741823===t?r=99:1===t||2===t?r=95:r=0>=(r=10*(1073741821-t)-10*(1073741821-r))?99:250>=r?98:5250>=r?97:95,null!==n){var a=e.callbackPriority;if(e.callbackExpirationTime===t&&a>=r)return;n!==Ra&&ka(n)}e.callbackExpirationTime=t,e.callbackPriority=r,t=1073741823===t?Va(Zl.bind(null,e)):Wa(r,Jl.bind(null,e),{timeout:10*(1073741821-t)-Fa()}),e.callbackNode=t}}}function Jl(e,t){if(Hl=0,t)return Au(e,t=Ql()),Gl(e),null;var n=Yl(e);if(0!==n){if(t=e.callbackNode,0!==(48&kl))throw Error(o(327));if(hu(),e===Tl&&n===Cl||nu(e,n),null!==Sl){var r=kl;kl|=16;for(var a=au();;)try{uu();break}catch(u){ru(e,u)}if(Za(),kl=r,yl.current=a,1===Pl)throw t=_l,nu(e,n),ju(e,n),Gl(e),t;if(null===Sl)switch(a=e.finishedWork=e.current.alternate,e.finishedExpirationTime=n,r=Pl,Tl=null,r){case wl:case 1:throw Error(o(345));case 2:Au(e,2<n?2:n);break;case xl:if(ju(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fu(a)),1073741823===Ol&&10<(a=zl+500-Fa())){if(jl){var i=e.lastPingedTime;if(0===i||i>=n){e.lastPingedTime=n,nu(e,n);break}}if(0!==(i=Yl(e))&&i!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}e.timeoutHandle=bn(du.bind(null,e),a);break}du(e);break;case El:if(ju(e,n),n===(r=e.lastSuspendedTime)&&(e.nextKnownPendingLevel=fu(a)),jl&&(0===(a=e.lastPingedTime)||a>=n)){e.lastPingedTime=n,nu(e,n);break}if(0!==(a=Yl(e))&&a!==n)break;if(0!==r&&r!==n){e.lastPingedTime=r;break}if(1073741823!==Nl?r=10*(1073741821-Nl)-Fa():1073741823===Ol?r=0:(r=10*(1073741821-Ol)-5e3,0>(r=(a=Fa())-r)&&(r=0),(n=10*(1073741821-n)-a)<(r=(120>r?120:480>r?480:1080>r?1080:1920>r?1920:3e3>r?3e3:4320>r?4320:1960*gl(r/1960))-r)&&(r=n)),10<r){e.timeoutHandle=bn(du.bind(null,e),r);break}du(e);break;case 5:if(1073741823!==Ol&&null!==Ml){i=Ol;var l=Ml;if(0>=(r=0|l.busyMinDurationMs)?r=0:(a=0|l.busyDelayMs,r=(i=Fa()-(10*(1073741821-i)-(0|l.timeoutMs||5e3)))<=a?0:a+r-i),10<r){ju(e,n),e.timeoutHandle=bn(du.bind(null,e),r);break}}du(e);break;default:throw Error(o(329))}if(Gl(e),e.callbackNode===t)return Jl.bind(null,e)}}return null}function Zl(e){var t=e.lastExpiredTime;if(t=0!==t?t:1073741823,0!==(48&kl))throw Error(o(327));if(hu(),e===Tl&&t===Cl||nu(e,t),null!==Sl){var n=kl;kl|=16;for(var r=au();;)try{lu();break}catch(a){ru(e,a)}if(Za(),kl=n,yl.current=r,1===Pl)throw n=_l,nu(e,t),ju(e,t),Gl(e),n;if(null!==Sl)throw Error(o(261));e.finishedWork=e.current.alternate,e.finishedExpirationTime=t,Tl=null,du(e),Gl(e)}return null}function eu(e,t){var n=kl;kl|=1;try{return e(t)}finally{0===(kl=n)&&Ha()}}function tu(e,t){var n=kl;kl&=-2,kl|=8;try{return e(t)}finally{0===(kl=n)&&Ha()}}function nu(e,t){e.finishedWork=null,e.finishedExpirationTime=0;var n=e.timeoutHandle;if(-1!==n&&(e.timeoutHandle=-1,wn(n)),null!==Sl)for(n=Sl.return;null!==n;){var r=n;switch(r.tag){case 1:null!==(r=r.type.childContextTypes)&&void 0!==r&&va();break;case 3:Ri(),ua(da),ua(fa);break;case 5:zi(r);break;case 4:Ri();break;case 13:case 19:ua(Ai);break;case 10:ei(r)}n=n.return}Tl=e,Sl=Cu(e.current,null),Cl=t,Pl=wl,_l=null,Nl=Ol=1073741823,Ml=null,Rl=0,jl=!1}function ru(e,t){for(;;){try{if(Za(),Ii.current=vo,Vi)for(var n=Bi.memoizedState;null!==n;){var r=n.queue;null!==r&&(r.pending=null),n=n.next}if(Ui=0,Wi=$i=Bi=null,Vi=!1,null===Sl||null===Sl.return)return Pl=1,_l=t,Sl=null;e:{var a=e,i=Sl.return,o=Sl,l=t;if(t=Cl,o.effectTag|=2048,o.firstEffect=o.lastEffect=null,null!==l&&"object"===typeof l&&"function"===typeof l.then){var u=l;if(0===(2&o.mode)){var c=o.alternate;c?(o.updateQueue=c.updateQueue,o.memoizedState=c.memoizedState,o.expirationTime=c.expirationTime):(o.updateQueue=null,o.memoizedState=null)}var s=0!==(1&Ai.current),f=i;do{var d;if(d=13===f.tag){var p=f.memoizedState;if(null!==p)d=null!==p.dehydrated;else{var m=f.memoizedProps;d=void 0!==m.fallback&&(!0!==m.unstable_avoidThisFallback||!s)}}if(d){var h=f.updateQueue;if(null===h){var v=new Set;v.add(u),f.updateQueue=v}else h.add(u);if(0===(2&f.mode)){if(f.effectTag|=64,o.effectTag&=-2981,1===o.tag)if(null===o.alternate)o.tag=17;else{var g=li(1073741823,null);g.tag=2,ui(o,g)}o.expirationTime=1073741823;break e}l=void 0,o=t;var y=a.pingCache;if(null===y?(y=a.pingCache=new pl,l=new Set,y.set(u,l)):void 0===(l=y.get(u))&&(l=new Set,y.set(u,l)),!l.has(o)){l.add(o);var b=bu.bind(null,a,u,o);u.then(b,b)}f.effectTag|=4096,f.expirationTime=t;break e}f=f.return}while(null!==f);l=Error((ve(o.type)||"A React component")+" suspended while rendering, but no fallback UI was specified.\n\nAdd a <Suspense fallback=...> component higher in the tree to provide a loading indicator or placeholder to display."+ge(o))}5!==Pl&&(Pl=2),l=Jo(l,o),f=i;do{switch(f.tag){case 3:u=l,f.effectTag|=4096,f.expirationTime=t,ci(f,ml(f,u,t));break e;case 1:u=l;var w=f.type,x=f.stateNode;if(0===(64&f.effectTag)&&("function"===typeof w.getDerivedStateFromError||null!==x&&"function"===typeof x.componentDidCatch&&(null===Il||!Il.has(x)))){f.effectTag|=4096,f.expirationTime=t,ci(f,hl(f,u,t));break e}}f=f.return}while(null!==f)}Sl=su(Sl)}catch(E){t=E;continue}break}}function au(){var e=yl.current;return yl.current=vo,null===e?vo:e}function iu(e,t){e<Ol&&2<e&&(Ol=e),null!==t&&e<Nl&&2<e&&(Nl=e,Ml=t)}function ou(e){e>Rl&&(Rl=e)}function lu(){for(;null!==Sl;)Sl=cu(Sl)}function uu(){for(;null!==Sl&&!ja();)Sl=cu(Sl)}function cu(e){var t=vl(e.alternate,e,Cl);return e.memoizedProps=e.pendingProps,null===t&&(t=su(e)),bl.current=null,t}function su(e){Sl=e;do{var t=Sl.alternate;if(e=Sl.return,0===(2048&Sl.effectTag)){if(t=Yo(t,Sl,Cl),1===Cl||1!==Sl.childExpirationTime){for(var n=0,r=Sl.child;null!==r;){var a=r.expirationTime,i=r.childExpirationTime;a>n&&(n=a),i>n&&(n=i),r=r.sibling}Sl.childExpirationTime=n}if(null!==t)return t;null!==e&&0===(2048&e.effectTag)&&(null===e.firstEffect&&(e.firstEffect=Sl.firstEffect),null!==Sl.lastEffect&&(null!==e.lastEffect&&(e.lastEffect.nextEffect=Sl.firstEffect),e.lastEffect=Sl.lastEffect),1<Sl.effectTag&&(null!==e.lastEffect?e.lastEffect.nextEffect=Sl:e.firstEffect=Sl,e.lastEffect=Sl))}else{if(null!==(t=Go(Sl)))return t.effectTag&=2047,t;null!==e&&(e.firstEffect=e.lastEffect=null,e.effectTag|=2048)}if(null!==(t=Sl.sibling))return t;Sl=e}while(null!==Sl);return Pl===wl&&(Pl=5),null}function fu(e){var t=e.expirationTime;return t>(e=e.childExpirationTime)?t:e}function du(e){var t=Ua();return $a(99,pu.bind(null,e,t)),null}function pu(e,t){do{hu()}while(null!==Ul);if(0!==(48&kl))throw Error(o(327));var n=e.finishedWork,r=e.finishedExpirationTime;if(null===n)return null;if(e.finishedWork=null,e.finishedExpirationTime=0,n===e.current)throw Error(o(177));e.callbackNode=null,e.callbackExpirationTime=0,e.callbackPriority=90,e.nextKnownPendingLevel=0;var a=fu(n);if(e.firstPendingTime=a,r<=e.lastSuspendedTime?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:r<=e.firstSuspendedTime&&(e.firstSuspendedTime=r-1),r<=e.lastPingedTime&&(e.lastPingedTime=0),r<=e.lastExpiredTime&&(e.lastExpiredTime=0),e===Tl&&(Sl=Tl=null,Cl=0),1<n.effectTag?null!==n.lastEffect?(n.lastEffect.nextEffect=n,a=n.firstEffect):a=n:a=n.firstEffect,null!==a){var i=kl;kl|=32,bl.current=null,hn=Qt;var l=pn();if(mn(l)){if("selectionStart"in l)var u={start:l.selectionStart,end:l.selectionEnd};else e:{var c=(u=(u=l.ownerDocument)&&u.defaultView||window).getSelection&&u.getSelection();if(c&&0!==c.rangeCount){u=c.anchorNode;var s=c.anchorOffset,f=c.focusNode;c=c.focusOffset;try{u.nodeType,f.nodeType}catch(C){u=null;break e}var d=0,p=-1,m=-1,h=0,v=0,g=l,y=null;t:for(;;){for(var b;g!==u||0!==s&&3!==g.nodeType||(p=d+s),g!==f||0!==c&&3!==g.nodeType||(m=d+c),3===g.nodeType&&(d+=g.nodeValue.length),null!==(b=g.firstChild);)y=g,g=b;for(;;){if(g===l)break t;if(y===u&&++h===s&&(p=d),y===f&&++v===c&&(m=d),null!==(b=g.nextSibling))break;y=(g=y).parentNode}g=b}u=-1===p||-1===m?null:{start:p,end:m}}else u=null}u=u||{start:0,end:0}}else u=null;vn={activeElementDetached:null,focusedElem:l,selectionRange:u},Qt=!1,Al=a;do{try{mu()}catch(C){if(null===Al)throw Error(o(330));yu(Al,C),Al=Al.nextEffect}}while(null!==Al);Al=a;do{try{for(l=e,u=t;null!==Al;){var w=Al.effectTag;if(16&w&&Ue(Al.stateNode,""),128&w){var x=Al.alternate;if(null!==x){var E=x.ref;null!==E&&("function"===typeof E?E(null):E.current=null)}}switch(1038&w){case 2:cl(Al),Al.effectTag&=-3;break;case 6:cl(Al),Al.effectTag&=-3,fl(Al.alternate,Al);break;case 1024:Al.effectTag&=-1025;break;case 1028:Al.effectTag&=-1025,fl(Al.alternate,Al);break;case 4:fl(Al.alternate,Al);break;case 8:sl(l,s=Al,u),ll(s)}Al=Al.nextEffect}}catch(C){if(null===Al)throw Error(o(330));yu(Al,C),Al=Al.nextEffect}}while(null!==Al);if(E=vn,x=pn(),w=E.focusedElem,u=E.selectionRange,x!==w&&w&&w.ownerDocument&&function e(t,n){return!(!t||!n)&&(t===n||(!t||3!==t.nodeType)&&(n&&3===n.nodeType?e(t,n.parentNode):"contains"in t?t.contains(n):!!t.compareDocumentPosition&&!!(16&t.compareDocumentPosition(n))))}(w.ownerDocument.documentElement,w)){null!==u&&mn(w)&&(x=u.start,void 0===(E=u.end)&&(E=x),"selectionStart"in w?(w.selectionStart=x,w.selectionEnd=Math.min(E,w.value.length)):(E=(x=w.ownerDocument||document)&&x.defaultView||window).getSelection&&(E=E.getSelection(),s=w.textContent.length,l=Math.min(u.start,s),u=void 0===u.end?l:Math.min(u.end,s),!E.extend&&l>u&&(s=u,u=l,l=s),s=dn(w,l),f=dn(w,u),s&&f&&(1!==E.rangeCount||E.anchorNode!==s.node||E.anchorOffset!==s.offset||E.focusNode!==f.node||E.focusOffset!==f.offset)&&((x=x.createRange()).setStart(s.node,s.offset),E.removeAllRanges(),l>u?(E.addRange(x),E.extend(f.node,f.offset)):(x.setEnd(f.node,f.offset),E.addRange(x))))),x=[];for(E=w;E=E.parentNode;)1===E.nodeType&&x.push({element:E,left:E.scrollLeft,top:E.scrollTop});for("function"===typeof w.focus&&w.focus(),w=0;w<x.length;w++)(E=x[w]).element.scrollLeft=E.left,E.element.scrollTop=E.top}Qt=!!hn,vn=hn=null,e.current=n,Al=a;do{try{for(w=e;null!==Al;){var k=Al.effectTag;if(36&k&&il(w,Al.alternate,Al),128&k){x=void 0;var T=Al.ref;if(null!==T){var S=Al.stateNode;switch(Al.tag){case 5:x=S;break;default:x=S}"function"===typeof T?T(x):T.current=x}}Al=Al.nextEffect}}catch(C){if(null===Al)throw Error(o(330));yu(Al,C),Al=Al.nextEffect}}while(null!==Al);Al=null,za(),kl=i}else e.current=n;if(Fl)Fl=!1,Ul=e,Bl=t;else for(Al=a;null!==Al;)t=Al.nextEffect,Al.nextEffect=null,Al=t;if(0===(t=e.firstPendingTime)&&(Il=null),1073741823===t?e===Vl?Wl++:(Wl=0,Vl=e):Wl=0,"function"===typeof xu&&xu(n.stateNode,r),Gl(e),Ll)throw Ll=!1,e=Dl,Dl=null,e;return 0!==(8&kl)||Ha(),null}function mu(){for(;null!==Al;){var e=Al.effectTag;0!==(256&e)&&nl(Al.alternate,Al),0===(512&e)||Fl||(Fl=!0,Wa(97,(function(){return hu(),null}))),Al=Al.nextEffect}}function hu(){if(90!==Bl){var e=97<Bl?97:Bl;return Bl=90,$a(e,vu)}}function vu(){if(null===Ul)return!1;var e=Ul;if(Ul=null,0!==(48&kl))throw Error(o(331));var t=kl;for(kl|=32,e=e.current.firstEffect;null!==e;){try{var n=e;if(0!==(512&n.effectTag))switch(n.tag){case 0:case 11:case 15:case 22:rl(5,n),al(5,n)}}catch(r){if(null===e)throw Error(o(330));yu(e,r)}n=e.nextEffect,e.nextEffect=null,e=n}return kl=t,Ha(),!0}function gu(e,t,n){ui(e,t=ml(e,t=Jo(n,t),1073741823)),null!==(e=Xl(e,1073741823))&&Gl(e)}function yu(e,t){if(3===e.tag)gu(e,e,t);else for(var n=e.return;null!==n;){if(3===n.tag){gu(n,e,t);break}if(1===n.tag){var r=n.stateNode;if("function"===typeof n.type.getDerivedStateFromError||"function"===typeof r.componentDidCatch&&(null===Il||!Il.has(r))){ui(n,e=hl(n,e=Jo(t,e),1073741823)),null!==(n=Xl(n,1073741823))&&Gl(n);break}}n=n.return}}function bu(e,t,n){var r=e.pingCache;null!==r&&r.delete(t),Tl===e&&Cl===n?Pl===El||Pl===xl&&1073741823===Ol&&Fa()-zl<500?nu(e,Cl):jl=!0:Ru(e,n)&&(0!==(t=e.lastPingedTime)&&t<n||(e.lastPingedTime=n,Gl(e)))}function wu(e,t){var n=e.stateNode;null!==n&&n.delete(t),0===(t=0)&&(t=ql(t=Ql(),e,null)),null!==(e=Xl(e,t))&&Gl(e)}vl=function(e,t,n){var r=t.expirationTime;if(null!==e){var a=t.pendingProps;if(e.memoizedProps!==a||da.current)No=!0;else{if(r<n){switch(No=!1,t.tag){case 3:Fo(t),_o();break;case 5:if(ji(t),4&t.mode&&1!==n&&a.hidden)return t.expirationTime=t.childExpirationTime=1,null;break;case 1:ha(t.type)&&ba(t);break;case 4:Mi(t,t.stateNode.containerInfo);break;case 10:r=t.memoizedProps.value,a=t.type._context,ca(Xa,a._currentValue),a._currentValue=r;break;case 13:if(null!==t.memoizedState)return 0!==(r=t.child.childExpirationTime)&&r>=n?Vo(e,t,n):(ca(Ai,1&Ai.current),null!==(t=Ko(e,t,n))?t.sibling:null);ca(Ai,1&Ai.current);break;case 19:if(r=t.childExpirationTime>=n,0!==(64&e.effectTag)){if(r)return qo(e,t,n);t.effectTag|=64}if(null!==(a=t.memoizedState)&&(a.rendering=null,a.tail=null),ca(Ai,Ai.current),!r)return null}return Ko(e,t,n)}No=!1}}else No=!1;switch(t.expirationTime=0,t.tag){case 2:if(r=t.type,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,a=ma(t,fa.current),ni(t,n),a=qi(null,t,r,e,a,n),t.effectTag|=1,"object"===typeof a&&null!==a&&"function"===typeof a.render&&void 0===a.$$typeof){if(t.tag=1,t.memoizedState=null,t.updateQueue=null,ha(r)){var i=!0;ba(t)}else i=!1;t.memoizedState=null!==a.state&&void 0!==a.state?a.state:null,ii(t);var l=r.getDerivedStateFromProps;"function"===typeof l&&mi(t,r,l,e),a.updater=hi,t.stateNode=a,a._reactInternalFiber=t,bi(t,r,e,n),t=Io(null,t,r,!0,i,n)}else t.tag=0,Mo(null,t,a,n),t=t.child;return t;case 16:e:{if(a=t.elementType,null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),e=t.pendingProps,function(e){if(-1===e._status){e._status=0;var t=e._ctor;t=t(),e._result=t,t.then((function(t){0===e._status&&(t=t.default,e._status=1,e._result=t)}),(function(t){0===e._status&&(e._status=2,e._result=t)}))}}(a),1!==a._status)throw a._result;switch(a=a._result,t.type=a,i=t.tag=function(e){if("function"===typeof e)return Su(e)?1:0;if(void 0!==e&&null!==e){if((e=e.$$typeof)===ue)return 11;if(e===fe)return 14}return 2}(a),e=Ka(a,e),i){case 0:t=Lo(null,t,a,e,n);break e;case 1:t=Do(null,t,a,e,n);break e;case 11:t=Ro(null,t,a,e,n);break e;case 14:t=jo(null,t,a,Ka(a.type,e),r,n);break e}throw Error(o(306,a,""))}return t;case 0:return r=t.type,a=t.pendingProps,Lo(e,t,r,a=t.elementType===r?a:Ka(r,a),n);case 1:return r=t.type,a=t.pendingProps,Do(e,t,r,a=t.elementType===r?a:Ka(r,a),n);case 3:if(Fo(t),r=t.updateQueue,null===e||null===r)throw Error(o(282));if(r=t.pendingProps,a=null!==(a=t.memoizedState)?a.element:null,oi(e,t),si(t,r,null,n),(r=t.memoizedState.element)===a)_o(),t=Ko(e,t,n);else{if((a=t.stateNode.hydrate)&&(xo=xn(t.stateNode.containerInfo.firstChild),wo=t,a=Eo=!0),a)for(n=Si(t,null,r,n),t.child=n;n;)n.effectTag=-3&n.effectTag|1024,n=n.sibling;else Mo(e,t,r,n),_o();t=t.child}return t;case 5:return ji(t),null===e&&So(t),r=t.type,a=t.pendingProps,i=null!==e?e.memoizedProps:null,l=a.children,yn(r,a)?l=null:null!==i&&yn(r,i)&&(t.effectTag|=16),Ao(e,t),4&t.mode&&1!==n&&a.hidden?(t.expirationTime=t.childExpirationTime=1,t=null):(Mo(e,t,l,n),t=t.child),t;case 6:return null===e&&So(t),null;case 13:return Vo(e,t,n);case 4:return Mi(t,t.stateNode.containerInfo),r=t.pendingProps,null===e?t.child=Ti(t,null,r,n):Mo(e,t,r,n),t.child;case 11:return r=t.type,a=t.pendingProps,Ro(e,t,r,a=t.elementType===r?a:Ka(r,a),n);case 7:return Mo(e,t,t.pendingProps,n),t.child;case 8:case 12:return Mo(e,t,t.pendingProps.children,n),t.child;case 10:e:{r=t.type._context,a=t.pendingProps,l=t.memoizedProps,i=a.value;var u=t.type._context;if(ca(Xa,u._currentValue),u._currentValue=i,null!==l)if(u=l.value,0===(i=Dr(u,i)?0:0|("function"===typeof r._calculateChangedBits?r._calculateChangedBits(u,i):1073741823))){if(l.children===a.children&&!da.current){t=Ko(e,t,n);break e}}else for(null!==(u=t.child)&&(u.return=t);null!==u;){var c=u.dependencies;if(null!==c){l=u.child;for(var s=c.firstContext;null!==s;){if(s.context===r&&0!==(s.observedBits&i)){1===u.tag&&((s=li(n,null)).tag=2,ui(u,s)),u.expirationTime<n&&(u.expirationTime=n),null!==(s=u.alternate)&&s.expirationTime<n&&(s.expirationTime=n),ti(u.return,n),c.expirationTime<n&&(c.expirationTime=n);break}s=s.next}}else l=10===u.tag&&u.type===t.type?null:u.child;if(null!==l)l.return=u;else for(l=u;null!==l;){if(l===t){l=null;break}if(null!==(u=l.sibling)){u.return=l.return,l=u;break}l=l.return}u=l}Mo(e,t,a.children,n),t=t.child}return t;case 9:return a=t.type,r=(i=t.pendingProps).children,ni(t,n),r=r(a=ri(a,i.unstable_observedBits)),t.effectTag|=1,Mo(e,t,r,n),t.child;case 14:return i=Ka(a=t.type,t.pendingProps),jo(e,t,a,i=Ka(a.type,i),r,n);case 15:return zo(e,t,t.type,t.pendingProps,r,n);case 17:return r=t.type,a=t.pendingProps,a=t.elementType===r?a:Ka(r,a),null!==e&&(e.alternate=null,t.alternate=null,t.effectTag|=2),t.tag=1,ha(r)?(e=!0,ba(t)):e=!1,ni(t,n),gi(t,r,a),bi(t,r,a,n),Io(null,t,r,!0,e,n);case 19:return qo(e,t,n)}throw Error(o(156,t.tag))};var xu=null,Eu=null;function ku(e,t,n,r){this.tag=e,this.key=n,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=r,this.effectTag=0,this.lastEffect=this.firstEffect=this.nextEffect=null,this.childExpirationTime=this.expirationTime=0,this.alternate=null}function Tu(e,t,n,r){return new ku(e,t,n,r)}function Su(e){return!(!(e=e.prototype)||!e.isReactComponent)}function Cu(e,t){var n=e.alternate;return null===n?((n=Tu(e.tag,t,e.key,e.mode)).elementType=e.elementType,n.type=e.type,n.stateNode=e.stateNode,n.alternate=e,e.alternate=n):(n.pendingProps=t,n.effectTag=0,n.nextEffect=null,n.firstEffect=null,n.lastEffect=null),n.childExpirationTime=e.childExpirationTime,n.expirationTime=e.expirationTime,n.child=e.child,n.memoizedProps=e.memoizedProps,n.memoizedState=e.memoizedState,n.updateQueue=e.updateQueue,t=e.dependencies,n.dependencies=null===t?null:{expirationTime:t.expirationTime,firstContext:t.firstContext,responders:t.responders},n.sibling=e.sibling,n.index=e.index,n.ref=e.ref,n}function Pu(e,t,n,r,a,i){var l=2;if(r=e,"function"===typeof e)Su(e)&&(l=1);else if("string"===typeof e)l=5;else e:switch(e){case ne:return _u(n.children,a,i,t);case le:l=8,a|=7;break;case re:l=8,a|=1;break;case ae:return(e=Tu(12,n,t,8|a)).elementType=ae,e.type=ae,e.expirationTime=i,e;case ce:return(e=Tu(13,n,t,a)).type=ce,e.elementType=ce,e.expirationTime=i,e;case se:return(e=Tu(19,n,t,a)).elementType=se,e.expirationTime=i,e;default:if("object"===typeof e&&null!==e)switch(e.$$typeof){case ie:l=10;break e;case oe:l=9;break e;case ue:l=11;break e;case fe:l=14;break e;case de:l=16,r=null;break e;case pe:l=22;break e}throw Error(o(130,null==e?e:typeof e,""))}return(t=Tu(l,n,t,a)).elementType=e,t.type=r,t.expirationTime=i,t}function _u(e,t,n,r){return(e=Tu(7,e,r,t)).expirationTime=n,e}function Ou(e,t,n){return(e=Tu(6,e,null,t)).expirationTime=n,e}function Nu(e,t,n){return(t=Tu(4,null!==e.children?e.children:[],e.key,t)).expirationTime=n,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}function Mu(e,t,n){this.tag=t,this.current=null,this.containerInfo=e,this.pingCache=this.pendingChildren=null,this.finishedExpirationTime=0,this.finishedWork=null,this.timeoutHandle=-1,this.pendingContext=this.context=null,this.hydrate=n,this.callbackNode=null,this.callbackPriority=90,this.lastExpiredTime=this.lastPingedTime=this.nextKnownPendingLevel=this.lastSuspendedTime=this.firstSuspendedTime=this.firstPendingTime=0}function Ru(e,t){var n=e.firstSuspendedTime;return e=e.lastSuspendedTime,0!==n&&n>=t&&e<=t}function ju(e,t){var n=e.firstSuspendedTime,r=e.lastSuspendedTime;n<t&&(e.firstSuspendedTime=t),(r>t||0===n)&&(e.lastSuspendedTime=t),t<=e.lastPingedTime&&(e.lastPingedTime=0),t<=e.lastExpiredTime&&(e.lastExpiredTime=0)}function zu(e,t){t>e.firstPendingTime&&(e.firstPendingTime=t);var n=e.firstSuspendedTime;0!==n&&(t>=n?e.firstSuspendedTime=e.lastSuspendedTime=e.nextKnownPendingLevel=0:t>=e.lastSuspendedTime&&(e.lastSuspendedTime=t+1),t>e.nextKnownPendingLevel&&(e.nextKnownPendingLevel=t))}function Au(e,t){var n=e.lastExpiredTime;(0===n||n>t)&&(e.lastExpiredTime=t)}function Lu(e,t,n,r){var a=t.current,i=Ql(),l=di.suspense;i=ql(i,a,l);e:if(n){t:{if(Ze(n=n._reactInternalFiber)!==n||1!==n.tag)throw Error(o(170));var u=n;do{switch(u.tag){case 3:u=u.stateNode.context;break t;case 1:if(ha(u.type)){u=u.stateNode.__reactInternalMemoizedMergedChildContext;break t}}u=u.return}while(null!==u);throw Error(o(171))}if(1===n.tag){var c=n.type;if(ha(c)){n=ya(n,c,u);break e}}n=u}else n=sa;return null===t.context?t.context=n:t.pendingContext=n,(t=li(i,l)).payload={element:e},null!==(r=void 0===r?null:r)&&(t.callback=r),ui(a,t),Kl(a,i),i}function Du(e){if(!(e=e.current).child)return null;switch(e.child.tag){case 5:default:return e.child.stateNode}}function Iu(e,t){null!==(e=e.memoizedState)&&null!==e.dehydrated&&e.retryTime<t&&(e.retryTime=t)}function Fu(e,t){Iu(e,t),(e=e.alternate)&&Iu(e,t)}function Uu(e,t,n){var r=new Mu(e,t,n=null!=n&&!0===n.hydrate),a=Tu(3,null,null,2===t?7:1===t?3:0);r.current=a,a.stateNode=r,ii(a),e[Cn]=r.current,n&&0!==t&&function(e,t){var n=Je(t);Ct.forEach((function(e){mt(e,t,n)})),Pt.forEach((function(e){mt(e,t,n)}))}(0,9===e.nodeType?e:e.ownerDocument),this._internalRoot=r}function Bu(e){return!(!e||1!==e.nodeType&&9!==e.nodeType&&11!==e.nodeType&&(8!==e.nodeType||" react-mount-point-unstable "!==e.nodeValue))}function $u(e,t,n,r,a){var i=n._reactRootContainer;if(i){var o=i._internalRoot;if("function"===typeof a){var l=a;a=function(){var e=Du(o);l.call(e)}}Lu(t,o,e,a)}else{if(i=n._reactRootContainer=function(e,t){if(t||(t=!(!(t=e?9===e.nodeType?e.documentElement:e.firstChild:null)||1!==t.nodeType||!t.hasAttribute("data-reactroot"))),!t)for(var n;n=e.lastChild;)e.removeChild(n);return new Uu(e,0,t?{hydrate:!0}:void 0)}(n,r),o=i._internalRoot,"function"===typeof a){var u=a;a=function(){var e=Du(o);u.call(e)}}tu((function(){Lu(t,o,e,a)}))}return Du(o)}function Wu(e,t,n){var r=3<arguments.length&&void 0!==arguments[3]?arguments[3]:null;return{$$typeof:te,key:null==r?null:""+r,children:e,containerInfo:t,implementation:n}}function Vu(e,t){var n=2<arguments.length&&void 0!==arguments[2]?arguments[2]:null;if(!Bu(t))throw Error(o(200));return Wu(e,t,null,n)}Uu.prototype.render=function(e){Lu(e,this._internalRoot,null,null)},Uu.prototype.unmount=function(){var e=this._internalRoot,t=e.containerInfo;Lu(null,e,null,(function(){t[Cn]=null}))},ht=function(e){if(13===e.tag){var t=qa(Ql(),150,100);Kl(e,t),Fu(e,t)}},vt=function(e){13===e.tag&&(Kl(e,3),Fu(e,3))},gt=function(e){if(13===e.tag){var t=Ql();Kl(e,t=ql(t,e,null)),Fu(e,t)}},_=function(e,t,n){switch(t){case"input":if(Se(e,n),t=n.name,"radio"===n.type&&null!=t){for(n=e;n.parentNode;)n=n.parentNode;for(n=n.querySelectorAll("input[name="+JSON.stringify(""+t)+'][type="radio"]'),t=0;t<n.length;t++){var r=n[t];if(r!==e&&r.form===e.form){var a=Nn(r);if(!a)throw Error(o(90));xe(r),Se(r,a)}}}break;case"textarea":Re(e,n);break;case"select":null!=(t=n.value)&&Oe(e,!!n.multiple,t,!1)}},z=eu,A=function(e,t,n,r,a){var i=kl;kl|=4;try{return $a(98,e.bind(null,t,n,r,a))}finally{0===(kl=i)&&Ha()}},L=function(){0===(49&kl)&&(function(){if(null!==$l){var e=$l;$l=null,e.forEach((function(e,t){Au(t,e),Gl(t)})),Ha()}}(),hu())},D=function(e,t){var n=kl;kl|=2;try{return e(t)}finally{0===(kl=n)&&Ha()}};var Hu={Events:[_n,On,Nn,C,k,Dn,function(e){at(e,Ln)},R,j,Gt,lt,hu,{current:!1}]};!function(e){var t=e.findFiberByHostInstance;(function(e){if("undefined"===typeof __REACT_DEVTOOLS_GLOBAL_HOOK__)return!1;var t=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(t.isDisabled||!t.supportsFiber)return!0;try{var n=t.inject(e);xu=function(e){try{t.onCommitFiberRoot(n,e,void 0,64===(64&e.current.effectTag))}catch(r){}},Eu=function(e){try{t.onCommitFiberUnmount(n,e)}catch(r){}}}catch(r){}})(a({},e,{overrideHookState:null,overrideProps:null,setSuspenseHandler:null,scheduleUpdate:null,currentDispatcherRef:Y.ReactCurrentDispatcher,findHostInstanceByFiber:function(e){return null===(e=nt(e))?null:e.stateNode},findFiberByHostInstance:function(e){return t?t(e):null},findHostInstancesForRefresh:null,scheduleRefresh:null,scheduleRoot:null,setRefreshHandler:null,getCurrentFiber:null}))}({findFiberByHostInstance:Pn,bundleType:0,version:"16.13.1",rendererPackageName:"react-dom"}),t.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=Hu,t.createPortal=Vu,t.findDOMNode=function(e){if(null==e)return null;if(1===e.nodeType)return e;var t=e._reactInternalFiber;if(void 0===t){if("function"===typeof e.render)throw Error(o(188));throw Error(o(268,Object.keys(e)))}return e=null===(e=nt(t))?null:e.stateNode},t.flushSync=function(e,t){if(0!==(48&kl))throw Error(o(187));var n=kl;kl|=1;try{return $a(99,e.bind(null,t))}finally{kl=n,Ha()}},t.hydrate=function(e,t,n){if(!Bu(t))throw Error(o(200));return $u(null,e,t,!0,n)},t.render=function(e,t,n){if(!Bu(t))throw Error(o(200));return $u(null,e,t,!1,n)},t.unmountComponentAtNode=function(e){if(!Bu(e))throw Error(o(40));return!!e._reactRootContainer&&(tu((function(){$u(null,null,e,!1,(function(){e._reactRootContainer=null,e[Cn]=null}))})),!0)},t.unstable_batchedUpdates=eu,t.unstable_createPortal=function(e,t){return Vu(e,t,2<arguments.length&&void 0!==arguments[2]?arguments[2]:null)},t.unstable_renderSubtreeIntoContainer=function(e,t,n,r){if(!Bu(n))throw Error(o(200));if(null==e||void 0===e._reactInternalFiber)throw Error(o(38));return $u(e,t,n,!1,r)},t.version="16.13.1"},function(e,t,n){"use strict";e.exports=n(34)},function(e,t,n){"use strict";var r,a,i,o,l;if("undefined"===typeof window||"function"!==typeof MessageChannel){var u=null,c=null,s=function e(){if(null!==u)try{var n=t.unstable_now();u(!0,n),u=null}catch(r){throw setTimeout(e,0),r}},f=Date.now();t.unstable_now=function(){return Date.now()-f},r=function(e){null!==u?setTimeout(r,0,e):(u=e,setTimeout(s,0))},a=function(e,t){c=setTimeout(e,t)},i=function(){clearTimeout(c)},o=function(){return!1},l=t.unstable_forceFrameRate=function(){}}else{var d=window.performance,p=window.Date,m=window.setTimeout,h=window.clearTimeout;if("undefined"!==typeof console){var v=window.cancelAnimationFrame;"function"!==typeof window.requestAnimationFrame&&console.error("This browser doesn't support requestAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills"),"function"!==typeof v&&console.error("This browser doesn't support cancelAnimationFrame. Make sure that you load a polyfill in older browsers. https://fb.me/react-polyfills")}if("object"===typeof d&&"function"===typeof d.now)t.unstable_now=function(){return d.now()};else{var g=p.now();t.unstable_now=function(){return p.now()-g}}var y=!1,b=null,w=-1,x=5,E=0;o=function(){return t.unstable_now()>=E},l=function(){},t.unstable_forceFrameRate=function(e){0>e||125<e?console.error("forceFrameRate takes a positive int between 0 and 125, forcing framerates higher than 125 fps is not unsupported"):x=0<e?Math.floor(1e3/e):5};var k=new MessageChannel,T=k.port2;k.port1.onmessage=function(){if(null!==b){var e=t.unstable_now();E=e+x;try{b(!0,e)?T.postMessage(null):(y=!1,b=null)}catch(n){throw T.postMessage(null),n}}else y=!1},r=function(e){b=e,y||(y=!0,T.postMessage(null))},a=function(e,n){w=m((function(){e(t.unstable_now())}),n)},i=function(){h(w),w=-1}}function S(e,t){var n=e.length;e.push(t);e:for(;;){var r=n-1>>>1,a=e[r];if(!(void 0!==a&&0<_(a,t)))break e;e[r]=t,e[n]=a,n=r}}function C(e){return void 0===(e=e[0])?null:e}function P(e){var t=e[0];if(void 0!==t){var n=e.pop();if(n!==t){e[0]=n;e:for(var r=0,a=e.length;r<a;){var i=2*(r+1)-1,o=e[i],l=i+1,u=e[l];if(void 0!==o&&0>_(o,n))void 0!==u&&0>_(u,o)?(e[r]=u,e[l]=n,r=l):(e[r]=o,e[i]=n,r=i);else{if(!(void 0!==u&&0>_(u,n)))break e;e[r]=u,e[l]=n,r=l}}}return t}return null}function _(e,t){var n=e.sortIndex-t.sortIndex;return 0!==n?n:e.id-t.id}var O=[],N=[],M=1,R=null,j=3,z=!1,A=!1,L=!1;function D(e){for(var t=C(N);null!==t;){if(null===t.callback)P(N);else{if(!(t.startTime<=e))break;P(N),t.sortIndex=t.expirationTime,S(O,t)}t=C(N)}}function I(e){if(L=!1,D(e),!A)if(null!==C(O))A=!0,r(F);else{var t=C(N);null!==t&&a(I,t.startTime-e)}}function F(e,n){A=!1,L&&(L=!1,i()),z=!0;var r=j;try{for(D(n),R=C(O);null!==R&&(!(R.expirationTime>n)||e&&!o());){var l=R.callback;if(null!==l){R.callback=null,j=R.priorityLevel;var u=l(R.expirationTime<=n);n=t.unstable_now(),"function"===typeof u?R.callback=u:R===C(O)&&P(O),D(n)}else P(O);R=C(O)}if(null!==R)var c=!0;else{var s=C(N);null!==s&&a(I,s.startTime-n),c=!1}return c}finally{R=null,j=r,z=!1}}function U(e){switch(e){case 1:return-1;case 2:return 250;case 5:return 1073741823;case 4:return 1e4;default:return 5e3}}var B=l;t.unstable_IdlePriority=5,t.unstable_ImmediatePriority=1,t.unstable_LowPriority=4,t.unstable_NormalPriority=3,t.unstable_Profiling=null,t.unstable_UserBlockingPriority=2,t.unstable_cancelCallback=function(e){e.callback=null},t.unstable_continueExecution=function(){A||z||(A=!0,r(F))},t.unstable_getCurrentPriorityLevel=function(){return j},t.unstable_getFirstCallbackNode=function(){return C(O)},t.unstable_next=function(e){switch(j){case 1:case 2:case 3:var t=3;break;default:t=j}var n=j;j=t;try{return e()}finally{j=n}},t.unstable_pauseExecution=function(){},t.unstable_requestPaint=B,t.unstable_runWithPriority=function(e,t){switch(e){case 1:case 2:case 3:case 4:case 5:break;default:e=3}var n=j;j=e;try{return t()}finally{j=n}},t.unstable_scheduleCallback=function(e,n,o){var l=t.unstable_now();if("object"===typeof o&&null!==o){var u=o.delay;u="number"===typeof u&&0<u?l+u:l,o="number"===typeof o.timeout?o.timeout:U(e)}else o=U(e),u=l;return e={id:M++,callback:n,priorityLevel:e,startTime:u,expirationTime:o=u+o,sortIndex:-1},u>l?(e.sortIndex=u,S(N,e),null===C(O)&&e===C(N)&&(L?i():L=!0,a(I,u-l))):(e.sortIndex=o,S(O,e),A||z||(A=!0,r(F))),e},t.unstable_shouldYield=function(){var e=t.unstable_now();D(e);var n=C(O);return n!==R&&null!==R&&null!==n&&null!==n.callback&&n.startTime<=e&&n.expirationTime<R.expirationTime||o()},t.unstable_wrapCallback=function(e){var t=j;return function(){var n=j;j=t;try{return e.apply(this,arguments)}finally{j=n}}}},function(e,t,n){},,function(e,t,n){"use strict";var r=n(6),a=n(18),i=n(38),o=n(24);function l(e){var t=new i(e),n=a(i.prototype.request,t);return r.extend(n,i.prototype,t),r.extend(n,t),n}var u=l(n(21));u.Axios=i,u.create=function(e){return l(o(u.defaults,e))},u.Cancel=n(25),u.CancelToken=n(52),u.isCancel=n(20),u.all=function(e){return Promise.all(e)},u.spread=n(53),e.exports=u,e.exports.default=u},function(e,t,n){"use strict";var r=n(6),a=n(19),i=n(39),o=n(40),l=n(24);function u(e){this.defaults=e,this.interceptors={request:new i,response:new i}}u.prototype.request=function(e){"string"===typeof e?(e=arguments[1]||{}).url=arguments[0]:e=e||{},(e=l(this.defaults,e)).method?e.method=e.method.toLowerCase():this.defaults.method?e.method=this.defaults.method.toLowerCase():e.method="get";var t=[o,void 0],n=Promise.resolve(e);for(this.interceptors.request.forEach((function(e){t.unshift(e.fulfilled,e.rejected)})),this.interceptors.response.forEach((function(e){t.push(e.fulfilled,e.rejected)}));t.length;)n=n.then(t.shift(),t.shift());return n},u.prototype.getUri=function(e){return e=l(this.defaults,e),a(e.url,e.params,e.paramsSerializer).replace(/^\?/,"")},r.forEach(["delete","get","head","options"],(function(e){u.prototype[e]=function(t,n){return this.request(r.merge(n||{},{method:e,url:t}))}})),r.forEach(["post","put","patch"],(function(e){u.prototype[e]=function(t,n,a){return this.request(r.merge(a||{},{method:e,url:t,data:n}))}})),e.exports=u},function(e,t,n){"use strict";var r=n(6);function a(){this.handlers=[]}a.prototype.use=function(e,t){return this.handlers.push({fulfilled:e,rejected:t}),this.handlers.length-1},a.prototype.eject=function(e){this.handlers[e]&&(this.handlers[e]=null)},a.prototype.forEach=function(e){r.forEach(this.handlers,(function(t){null!==t&&e(t)}))},e.exports=a},function(e,t,n){"use strict";var r=n(6),a=n(41),i=n(20),o=n(21);function l(e){e.cancelToken&&e.cancelToken.throwIfRequested()}e.exports=function(e){return l(e),e.headers=e.headers||{},e.data=a(e.data,e.headers,e.transformRequest),e.headers=r.merge(e.headers.common||{},e.headers[e.method]||{},e.headers),r.forEach(["delete","get","head","post","put","patch","common"],(function(t){delete e.headers[t]})),(e.adapter||o.adapter)(e).then((function(t){return l(e),t.data=a(t.data,t.headers,e.transformResponse),t}),(function(t){return i(t)||(l(e),t&&t.response&&(t.response.data=a(t.response.data,t.response.headers,e.transformResponse))),Promise.reject(t)}))}},function(e,t,n){"use strict";var r=n(6);e.exports=function(e,t,n){return r.forEach(n,(function(n){e=n(e,t)})),e}},function(e,t){var n,r,a=e.exports={};function i(){throw new Error("setTimeout has not been defined")}function o(){throw new Error("clearTimeout has not been defined")}function l(e){if(n===setTimeout)return setTimeout(e,0);if((n===i||!n)&&setTimeout)return n=setTimeout,setTimeout(e,0);try{return n(e,0)}catch(t){try{return n.call(null,e,0)}catch(t){return n.call(this,e,0)}}}!function(){try{n="function"===typeof setTimeout?setTimeout:i}catch(e){n=i}try{r="function"===typeof clearTimeout?clearTimeout:o}catch(e){r=o}}();var u,c=[],s=!1,f=-1;function d(){s&&u&&(s=!1,u.length?c=u.concat(c):f=-1,c.length&&p())}function p(){if(!s){var e=l(d);s=!0;for(var t=c.length;t;){for(u=c,c=[];++f<t;)u&&u[f].run();f=-1,t=c.length}u=null,s=!1,function(e){if(r===clearTimeout)return clearTimeout(e);if((r===o||!r)&&clearTimeout)return r=clearTimeout,clearTimeout(e);try{r(e)}catch(t){try{return r.call(null,e)}catch(t){return r.call(this,e)}}}(e)}}function m(e,t){this.fun=e,this.array=t}function h(){}a.nextTick=function(e){var t=new Array(arguments.length-1);if(arguments.length>1)for(var n=1;n<arguments.length;n++)t[n-1]=arguments[n];c.push(new m(e,t)),1!==c.length||s||l(p)},m.prototype.run=function(){this.fun.apply(null,this.array)},a.title="browser",a.browser=!0,a.env={},a.argv=[],a.version="",a.versions={},a.on=h,a.addListener=h,a.once=h,a.off=h,a.removeListener=h,a.removeAllListeners=h,a.emit=h,a.prependListener=h,a.prependOnceListener=h,a.listeners=function(e){return[]},a.binding=function(e){throw new Error("process.binding is not supported")},a.cwd=function(){return"/"},a.chdir=function(e){throw new Error("process.chdir is not supported")},a.umask=function(){return 0}},function(e,t,n){"use strict";var r=n(6);e.exports=function(e,t){r.forEach(e,(function(n,r){r!==t&&r.toUpperCase()===t.toUpperCase()&&(e[t]=n,delete e[r])}))}},function(e,t,n){"use strict";var r=n(23);e.exports=function(e,t,n){var a=n.config.validateStatus;!a||a(n.status)?e(n):t(r("Request failed with status code "+n.status,n.config,null,n.request,n))}},function(e,t,n){"use strict";e.exports=function(e,t,n,r,a){return e.config=t,n&&(e.code=n),e.request=r,e.response=a,e.isAxiosError=!0,e.toJSON=function(){return{message:this.message,name:this.name,description:this.description,number:this.number,fileName:this.fileName,lineNumber:this.lineNumber,columnNumber:this.columnNumber,stack:this.stack,config:this.config,code:this.code}},e}},function(e,t,n){"use strict";var r=n(47),a=n(48);e.exports=function(e,t){return e&&!r(t)?a(e,t):t}},function(e,t,n){"use strict";e.exports=function(e){return/^([a-z][a-z\d\+\-\.]*:)?\/\//i.test(e)}},function(e,t,n){"use strict";e.exports=function(e,t){return t?e.replace(/\/+$/,"")+"/"+t.replace(/^\/+/,""):e}},function(e,t,n){"use strict";var r=n(6),a=["age","authorization","content-length","content-type","etag","expires","from","host","if-modified-since","if-unmodified-since","last-modified","location","max-forwards","proxy-authorization","referer","retry-after","user-agent"];e.exports=function(e){var t,n,i,o={};return e?(r.forEach(e.split("\n"),(function(e){if(i=e.indexOf(":"),t=r.trim(e.substr(0,i)).toLowerCase(),n=r.trim(e.substr(i+1)),t){if(o[t]&&a.indexOf(t)>=0)return;o[t]="set-cookie"===t?(o[t]?o[t]:[]).concat([n]):o[t]?o[t]+", "+n:n}})),o):o}},function(e,t,n){"use strict";var r=n(6);e.exports=r.isStandardBrowserEnv()?function(){var e,t=/(msie|trident)/i.test(navigator.userAgent),n=document.createElement("a");function a(e){var r=e;return t&&(n.setAttribute("href",r),r=n.href),n.setAttribute("href",r),{href:n.href,protocol:n.protocol?n.protocol.replace(/:$/,""):"",host:n.host,search:n.search?n.search.replace(/^\?/,""):"",hash:n.hash?n.hash.replace(/^#/,""):"",hostname:n.hostname,port:n.port,pathname:"/"===n.pathname.charAt(0)?n.pathname:"/"+n.pathname}}return e=a(window.location.href),function(t){var n=r.isString(t)?a(t):t;return n.protocol===e.protocol&&n.host===e.host}}():function(){return!0}},function(e,t,n){"use strict";var r=n(6);e.exports=r.isStandardBrowserEnv()?{write:function(e,t,n,a,i,o){var l=[];l.push(e+"="+encodeURIComponent(t)),r.isNumber(n)&&l.push("expires="+new Date(n).toGMTString()),r.isString(a)&&l.push("path="+a),r.isString(i)&&l.push("domain="+i),!0===o&&l.push("secure"),document.cookie=l.join("; ")},read:function(e){var t=document.cookie.match(new RegExp("(^|;\\s*)("+e+")=([^;]*)"));return t?decodeURIComponent(t[3]):null},remove:function(e){this.write(e,"",Date.now()-864e5)}}:{write:function(){},read:function(){return null},remove:function(){}}},function(e,t,n){"use strict";var r=n(25);function a(e){if("function"!==typeof e)throw new TypeError("executor must be a function.");var t;this.promise=new Promise((function(e){t=e}));var n=this;e((function(e){n.reason||(n.reason=new r(e),t(n.reason))}))}a.prototype.throwIfRequested=function(){if(this.reason)throw this.reason},a.source=function(){var e;return{token:new a((function(t){e=t})),cancel:e}},e.exports=a},function(e,t,n){"use strict";e.exports=function(e){return function(t){return e.apply(null,t)}}},function(e,t,n){"use strict";var r=n(55);function a(){}function i(){}i.resetWarningCache=a,e.exports=function(){function e(e,t,n,a,i,o){if(o!==r){var l=new Error("Calling PropTypes validators directly is not supported by the `prop-types` package. Use PropTypes.checkPropTypes() to call them. Read more at http://fb.me/use-check-prop-types");throw l.name="Invariant Violation",l}}function t(){return e}e.isRequired=e;var n={array:e,bool:e,func:e,number:e,object:e,string:e,symbol:e,any:e,arrayOf:t,element:e,elementType:e,instanceOf:t,node:e,objectOf:t,oneOf:t,oneOfType:t,shape:t,exact:t,checkPropTypes:i,resetWarningCache:a};return n.PropTypes=n,n}},function(e,t,n){"use strict";e.exports="SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED"},function(e,t){var n;n=function(){return this}();try{n=n||new Function("return this")()}catch(r){"object"===typeof window&&(n=window)}e.exports=n},function(e,t){e.exports=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)}},function(e,t,n){"use strict";var r="function"===typeof Symbol&&Symbol.for,a=r?Symbol.for("react.element"):60103,i=r?Symbol.for("react.portal"):60106,o=r?Symbol.for("react.fragment"):60107,l=r?Symbol.for("react.strict_mode"):60108,u=r?Symbol.for("react.profiler"):60114,c=r?Symbol.for("react.provider"):60109,s=r?Symbol.for("react.context"):60110,f=r?Symbol.for("react.async_mode"):60111,d=r?Symbol.for("react.concurrent_mode"):60111,p=r?Symbol.for("react.forward_ref"):60112,m=r?Symbol.for("react.suspense"):60113,h=r?Symbol.for("react.suspense_list"):60120,v=r?Symbol.for("react.memo"):60115,g=r?Symbol.for("react.lazy"):60116,y=r?Symbol.for("react.block"):60121,b=r?Symbol.for("react.fundamental"):60117,w=r?Symbol.for("react.responder"):60118,x=r?Symbol.for("react.scope"):60119;function E(e){if("object"===typeof e&&null!==e){var t=e.$$typeof;switch(t){case a:switch(e=e.type){case f:case d:case o:case u:case l:case m:return e;default:switch(e=e&&e.$$typeof){case s:case p:case g:case v:case c:return e;default:return t}}case i:return t}}}function k(e){return E(e)===d}t.AsyncMode=f,t.ConcurrentMode=d,t.ContextConsumer=s,t.ContextProvider=c,t.Element=a,t.ForwardRef=p,t.Fragment=o,t.Lazy=g,t.Memo=v,t.Portal=i,t.Profiler=u,t.StrictMode=l,t.Suspense=m,t.isAsyncMode=function(e){return k(e)||E(e)===f},t.isConcurrentMode=k,t.isContextConsumer=function(e){return E(e)===s},t.isContextProvider=function(e){return E(e)===c},t.isElement=function(e){return"object"===typeof e&&null!==e&&e.$$typeof===a},t.isForwardRef=function(e){return E(e)===p},t.isFragment=function(e){return E(e)===o},t.isLazy=function(e){return E(e)===g},t.isMemo=function(e){return E(e)===v},t.isPortal=function(e){return E(e)===i},t.isProfiler=function(e){return E(e)===u},t.isStrictMode=function(e){return E(e)===l},t.isSuspense=function(e){return E(e)===m},t.isValidElementType=function(e){return"string"===typeof e||"function"===typeof e||e===o||e===d||e===u||e===l||e===m||e===h||"object"===typeof e&&null!==e&&(e.$$typeof===g||e.$$typeof===v||e.$$typeof===c||e.$$typeof===s||e.$$typeof===p||e.$$typeof===b||e.$$typeof===w||e.$$typeof===x||e.$$typeof===y)},t.typeOf=E},,function(e,t,n){var r,a,i;a=[t,n(0),n(1),n(61)],void 0===(i="function"===typeof(r=function(e,t,n,r){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.default=s;var a=o(t),i=o(n);function o(e){return e&&e.__esModule?e:{default:e}}var l=Object.assign||function(e){for(var t=1;t<arguments.length;t++){var n=arguments[t];for(var r in n)Object.prototype.hasOwnProperty.call(n,r)&&(e[r]=n[r])}return e},u=function(e,t){if(Array.isArray(e))return e;if(Symbol.iterator in Object(e))return function(e,t){var n=[],r=!0,a=!1,i=void 0;try{for(var o,l=e[Symbol.iterator]();!(r=(o=l.next()).done)&&(n.push(o.value),!t||n.length!==t);r=!0);}catch(u){a=!0,i=u}finally{try{!r&&l.return&&l.return()}finally{if(a)throw i}}return n}(e,t);throw new TypeError("Invalid attempt to destructure non-iterable instance")},c=["Audio","BallTriangle","Bars","Circles","Grid","Hearts","Oval","Puff","Rings","TailSpin","ThreeDots","Watch","RevolvingDot","Triangle","Plane","MutatingDots","CradleLoader"];function s(e){var n,i=(0,t.useState)(!0),o=u(i,2),s=o[0],f=o[1];return(0,t.useEffect)((function(){var t=void 0;return e.timeout&&e.timeout>0&&(t=setTimeout((function(){f(!1)}),e.timeout)),function(){t&&clearTimeout(t)}})),e.visible&&"false"!==e.visible&&s?a.default.createElement("div",{"aria-busy":"true",className:e.className,style:e.style},a.default.createElement((n=e.type,c.includes(n)?r.Spinner[n]:r.Spinner.Audio),l({},e))):null}s.propTypes={type:i.default.oneOf([].concat(c)),style:i.default.objectOf(i.default.string),className:i.default.string,visible:i.default.oneOfType([i.default.bool,i.default.string]),timeout:i.default.number},s.defaultProps={type:"Audio",style:{},className:"",visible:!0,timeout:0}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(62),n(63),n(64),n(65),n(66),n(67),n(68),n(69),n(70),n(71),n(72),n(73),n(74),n(75),n(76),n(77),n(78)],void 0===(i="function"===typeof(r=function(e,t,n,r,a,i,o,l,u,c,s,f,d,p,m,h,v,g){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Spinner=void 0,e.Spinner={Circles:t.Circles,Audio:r.Audio,BallTriangle:a.BallTriangle,Bars:i.Bars,CradleLoader:o.CradleLoader,Grid:l.Grid,Hearts:u.Hearts,MutatingDots:c.MutatingDots,Oval:s.Oval,Plane:f.Plane,Puff:d.Puff,RevolvingDot:p.RevolvingDot,Rings:m.Rings,TailSpin:h.TailSpin,ThreeDots:v.ThreeDots,Triangle:g.Triangle,Watch:n.Watch}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Circles=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Circles=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 135 135",xmlns:"http://www.w3.org/2000/svg",fill:e.color,"aria-label":e.label},r.default.createElement("path",{d:"M67.447 58c5.523 0 10-4.477 10-10s-4.477-10-10-10-10 4.477-10 10 4.477 10 10 10zm9.448 9.447c0 5.523 4.477 10 10 10 5.522 0 10-4.477 10-10s-4.478-10-10-10c-5.523 0-10 4.477-10 10zm-9.448 9.448c-5.523 0-10 4.477-10 10 0 5.522 4.477 10 10 10s10-4.478 10-10c0-5.523-4.477-10-10-10zM58 67.447c0-5.523-4.477-10-10-10s-10 4.477-10 10 4.477 10 10 10 10-4.477 10-10z"},r.default.createElement("animateTransform",{attributeName:"transform",type:"rotate",from:"0 67 67",to:"-360 67 67",dur:"2.5s",repeatCount:"indefinite"})),r.default.createElement("path",{d:"M28.19 40.31c6.627 0 12-5.374 12-12 0-6.628-5.373-12-12-12-6.628 0-12 5.372-12 12 0 6.626 5.372 12 12 12zm30.72-19.825c4.686 4.687 12.284 4.687 16.97 0 4.686-4.686 4.686-12.284 0-16.97-4.686-4.687-12.284-4.687-16.97 0-4.687 4.686-4.687 12.284 0 16.97zm35.74 7.705c0 6.627 5.37 12 12 12 6.626 0 12-5.373 12-12 0-6.628-5.374-12-12-12-6.63 0-12 5.372-12 12zm19.822 30.72c-4.686 4.686-4.686 12.284 0 16.97 4.687 4.686 12.285 4.686 16.97 0 4.687-4.686 4.687-12.284 0-16.97-4.685-4.687-12.283-4.687-16.97 0zm-7.704 35.74c-6.627 0-12 5.37-12 12 0 6.626 5.373 12 12 12s12-5.374 12-12c0-6.63-5.373-12-12-12zm-30.72 19.822c-4.686-4.686-12.284-4.686-16.97 0-4.686 4.687-4.686 12.285 0 16.97 4.686 4.687 12.284 4.687 16.97 0 4.687-4.685 4.687-12.283 0-16.97zm-35.74-7.704c0-6.627-5.372-12-12-12-6.626 0-12 5.373-12 12s5.374 12 12 12c6.628 0 12-5.373 12-12zm-19.823-30.72c4.687-4.686 4.687-12.284 0-16.97-4.686-4.686-12.284-4.686-16.97 0-4.687 4.686-4.687 12.284 0 16.97 4.686 4.687 12.284 4.687 16.97 0z"},r.default.createElement("animateTransform",{attributeName:"transform",type:"rotate",from:"0 67 67",to:"360 67 67",dur:"8s",repeatCount:"indefinite"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Watch=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Watch=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,version:"1.1",id:"L2",xmlns:"http://www.w3.org/2000/svg",x:"0px",y:"0px",viewBox:"0 0 100 100",enableBackground:"new 0 0 100 100",xmlSpace:"preserve","aria-label":e.label},r.default.createElement("circle",{fill:"none",stroke:e.color,strokeWidth:"4",strokeMiterlimit:"10",cx:"50",cy:"50",r:e.radius}),r.default.createElement("line",{fill:"none",strokeLinecap:"round",stroke:e.color,strokeWidth:"4",strokeMiterlimit:"10",x1:"50",y1:"50",x2:"85",y2:"50.5"},r.default.createElement("animateTransform",{attributeName:"transform",dur:"2s",type:"rotate",from:"0 50 50",to:"360 50 50",repeatCount:"indefinite"})),r.default.createElement("line",{fill:"none",strokeLinecap:"round",stroke:e.color,strokeWidth:"4",strokeMiterlimit:"10",x1:"50",y1:"50",x2:"49.5",y2:"74"},r.default.createElement("animateTransform",{attributeName:"transform",dur:"15s",type:"rotate",from:"0 50 50",to:"360 50 50",repeatCount:"indefinite"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading",radius:48}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Audio=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Audio=function(e){return r.default.createElement("svg",{height:e.height,width:e.width,fill:e.color,viewBox:"0 0 55 80",xmlns:"http://www.w3.org/2000/svg","aria-label":e.label},r.default.createElement("g",{transform:"matrix(1 0 0 -1 0 80)"},r.default.createElement("rect",{width:"10",height:"20",rx:"3"},r.default.createElement("animate",{attributeName:"height",begin:"0s",dur:"4.3s",values:"20;45;57;80;64;32;66;45;64;23;66;13;64;56;34;34;2;23;76;79;20",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"15",width:"10",height:"80",rx:"3"},r.default.createElement("animate",{attributeName:"height",begin:"0s",dur:"2s",values:"80;55;33;5;75;23;73;33;12;14;60;80",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"30",width:"10",height:"50",rx:"3"},r.default.createElement("animate",{attributeName:"height",begin:"0s",dur:"1.4s",values:"50;34;78;23;56;23;34;76;80;54;21;50",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"45",width:"10",height:"30",rx:"3"},r.default.createElement("animate",{attributeName:"height",begin:"0s",dur:"2s",values:"30;45;13;80;56;72;45;76;34;23;67;30",calcMode:"linear",repeatCount:"indefinite"}))))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.BallTriangle=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.BallTriangle=function(e){return r.default.createElement("svg",{height:e.height,width:e.width,stroke:e.color,viewBox:"0 0 57 57",xmlns:"http://www.w3.org/2000/svg","aria-label":e.label},r.default.createElement("g",{fill:"none",fillRule:"evenodd"},r.default.createElement("g",{transform:"translate(1 1)",strokeWidth:"2"},r.default.createElement("circle",{cx:"5",cy:"50",r:e.radius},r.default.createElement("animate",{attributeName:"cy",begin:"0s",dur:"2.2s",values:"50;5;50;50",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"cx",begin:"0s",dur:"2.2s",values:"5;27;49;5",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"27",cy:"5",r:e.radius},r.default.createElement("animate",{attributeName:"cy",begin:"0s",dur:"2.2s",from:"5",to:"5",values:"5;50;50;5",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"cx",begin:"0s",dur:"2.2s",from:"27",to:"27",values:"27;49;5;27",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"49",cy:"50",r:e.radius},r.default.createElement("animate",{attributeName:"cy",begin:"0s",dur:"2.2s",values:"50;50;5;50",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"cx",from:"49",to:"49",begin:"0s",dur:"2.2s",values:"49;5;27;49",calcMode:"linear",repeatCount:"indefinite"})))))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",radius:5,label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Bars=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Bars=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,fill:e.color,viewBox:"0 0 135 140",xmlns:"http://www.w3.org/2000/svg","aria-label":e.label},r.default.createElement("rect",{y:"10",width:"15",height:"120",rx:"6"},r.default.createElement("animate",{attributeName:"height",begin:"0.5s",dur:"1s",values:"120;110;100;90;80;70;60;50;40;140;120",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"y",begin:"0.5s",dur:"1s",values:"10;15;20;25;30;35;40;45;50;0;10",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"30",y:"10",width:"15",height:"120",rx:"6"},r.default.createElement("animate",{attributeName:"height",begin:"0.25s",dur:"1s",values:"120;110;100;90;80;70;60;50;40;140;120",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"y",begin:"0.25s",dur:"1s",values:"10;15;20;25;30;35;40;45;50;0;10",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"60",width:"15",height:"140",rx:"6"},r.default.createElement("animate",{attributeName:"height",begin:"0s",dur:"1s",values:"120;110;100;90;80;70;60;50;40;140;120",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"y",begin:"0s",dur:"1s",values:"10;15;20;25;30;35;40;45;50;0;10",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"90",y:"10",width:"15",height:"120",rx:"6"},r.default.createElement("animate",{attributeName:"height",begin:"0.25s",dur:"1s",values:"120;110;100;90;80;70;60;50;40;140;120",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"y",begin:"0.25s",dur:"1s",values:"10;15;20;25;30;35;40;45;50;0;10",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("rect",{x:"120",y:"10",width:"15",height:"120",rx:"6"},r.default.createElement("animate",{attributeName:"height",begin:"0.5s",dur:"1s",values:"120;110;100;90;80;70;60;50;40;140;120",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"y",begin:"0.5s",dur:"1s",values:"10;15;20;25;30;35;40;45;50;0;10",calcMode:"linear",repeatCount:"indefinite"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.CradleLoader=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.CradleLoader=function(e){return r.default.createElement("div",{"aria-label":e.label,role:"presentation",className:"container"},r.default.createElement("div",{className:"react-spinner-loader-swing"},r.default.createElement("div",{className:"react-spinner-loader-swing-l"}),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",{className:"react-spinner-loader-swing-r"})),r.default.createElement("div",{className:"react-spinner-loader-shadow"},r.default.createElement("div",{className:"react-spinner-loader-shadow-l"}),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",null),r.default.createElement("div",{className:"react-spinner-loader-shadow-r"})))};o.propTypes={label:a.default.string},o.defaultProps={label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Grid=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Grid=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 105 105",fill:e.color,"aria-label":e.label},r.default.createElement("circle",{cx:"12.5",cy:"12.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"0s",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"12.5",cy:"52.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"100ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"52.5",cy:"12.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"300ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"52.5",cy:"52.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"600ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"92.5",cy:"12.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"800ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"92.5",cy:"52.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"400ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"12.5",cy:"92.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"700ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"52.5",cy:"92.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"500ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"92.5",cy:"92.5",r:e.radius},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"200ms",dur:"1s",values:"1;.2;1",calcMode:"linear",repeatCount:"indefinite"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",radius:12.5,label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Hearts=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Hearts=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 140 64",xmlns:"http://www.w3.org/2000/svg",fill:e.color,"aria-label":e.label},r.default.createElement("path",{d:"M30.262 57.02L7.195 40.723c-5.84-3.976-7.56-12.06-3.842-18.063 3.715-6 11.467-7.65 17.306-3.68l4.52 3.76 2.6-5.274c3.717-6.002 11.47-7.65 17.305-3.68 5.84 3.97 7.56 12.054 3.842 18.062L34.49 56.118c-.897 1.512-2.793 1.915-4.228.9z",attributeName:"fill-opacity",from:"0",to:".5"},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"0s",dur:"1.4s",values:"0.5;1;0.5",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("path",{d:"M105.512 56.12l-14.44-24.272c-3.716-6.008-1.996-14.093 3.843-18.062 5.835-3.97 13.588-2.322 17.306 3.68l2.6 5.274 4.52-3.76c5.84-3.97 13.592-2.32 17.307 3.68 3.718 6.003 1.998 14.088-3.842 18.064L109.74 57.02c-1.434 1.014-3.33.61-4.228-.9z",attributeName:"fill-opacity",from:"0",to:".5"},r.default.createElement("animate",{attributeName:"fill-opacity",begin:"0.7s",dur:"1.4s",values:"0.5;1;0.5",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("path",{d:"M67.408 57.834l-23.01-24.98c-5.864-6.15-5.864-16.108 0-22.248 5.86-6.14 15.37-6.14 21.234 0L70 16.168l4.368-5.562c5.863-6.14 15.375-6.14 21.235 0 5.863 6.14 5.863 16.098 0 22.247l-23.007 24.98c-1.43 1.556-3.757 1.556-5.188 0z"}))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.MutatingDots=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.MutatingDots=function(e){return r.default.createElement("svg",{id:"goo-loader",width:e.width,height:e.height,"aria-label":e.label},r.default.createElement("filter",{id:"fancy-goo"},r.default.createElement("feGaussianBlur",{in:"SourceGraphic",stdDeviation:"6",result:"blur"}),r.default.createElement("feColorMatrix",{in:"blur",mode:"matrix",values:"1 0 0 0 0  0 1 0 0 0  0 0 1 0 0  0 0 0 19 -9",result:"goo"}),r.default.createElement("feComposite",{in:"SourceGraphic",in2:"goo",operator:"atop"})),r.default.createElement("g",{filter:"url(#fancy-goo)"},r.default.createElement("animateTransform",{id:"mainAnim",attributeName:"transform",attributeType:"XML",type:"rotate",from:"0 50 50",to:"359 50 50",dur:"1.2s",repeatCount:"indefinite"}),r.default.createElement("circle",{cx:"50%",cy:"40",r:e.radius,fill:e.color},r.default.createElement("animate",{id:"cAnim1",attributeType:"XML",attributeName:"cy",dur:"0.6s",begin:"0;cAnim1.end+0.2s",calcMode:"spline",values:"40;20;40",keyTimes:"0;0.3;1",keySplines:"0.175, 0.885, 0.320, 1.5; 0.175, 0.885, 0.320, 1.5"})),r.default.createElement("circle",{cx:"50%",cy:"60",r:e.radius,fill:e.secondaryColor},r.default.createElement("animate",{id:"cAnim2",attributeType:"XML",attributeName:"cy",dur:"0.6s",begin:"0.4s;cAnim2.end+0.2s",calcMode:"spline",values:"60;80;60",keyTimes:"0;0.3;1",keySplines:"0.175, 0.885, 0.320, 1.5;0.175, 0.885, 0.320, 1.5"}))))};o.propTypes={width:a.default.number,secondaryColor:a.default.string,height:a.default.number,color:a.default.string,radius:a.default.number,label:a.default.string},o.defaultProps={width:80,height:90,color:"green",radius:11,secondaryColor:"green",label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Oval=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Oval=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 38 38",xmlns:"http://www.w3.org/2000/svg",stroke:e.color,"aria-label":e.label},r.default.createElement("g",{fill:"none",fillRule:"evenodd"},r.default.createElement("g",{transform:"translate(1 1)",strokeWidth:"2"},r.default.createElement("circle",{strokeOpacity:".5",cx:"18",cy:"18",r:e.radius}),r.default.createElement("path",{d:"M36 18c0-9.94-8.06-18-18-18"},r.default.createElement("animateTransform",{attributeName:"transform",type:"rotate",from:"0 18 18",to:"360 18 18",dur:"1s",repeatCount:"indefinite"})))))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading",radius:18}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Plane=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Plane=function(e){return r.default.createElement("svg",{className:"react-spinner-loader-svg-calLoader",xmlns:"http://www.w3.org/2000/svg",width:"230",height:"230","aria-label":e.label},r.default.createElement("desc",null,"Plane animation. Loading "),r.default.createElement("path",{className:"react-spinner-loader-cal-loader__path",style:{stroke:e.secondaryColor},d:"M86.429 40c63.616-20.04 101.511 25.08 107.265 61.93 6.487 41.54-18.593 76.99-50.6 87.643-59.46 19.791-101.262-23.577-107.142-62.616C29.398 83.441 59.945 48.343 86.43 40z",fill:"none",stroke:"#0099cc",strokeWidth:"4",strokeLinecap:"round",strokeLinejoin:"round",strokeDasharray:"10 10 10 10 10 10 10 432",strokeDashoffset:"77"}),r.default.createElement("path",{className:"cal-loader__plane",style:{fill:e.color},d:"M141.493 37.93c-1.087-.927-2.942-2.002-4.32-2.501-2.259-.824-3.252-.955-9.293-1.172-4.017-.146-5.197-.23-5.47-.37-.766-.407-1.526-1.448-7.114-9.773-4.8-7.145-5.344-7.914-6.327-8.976-1.214-1.306-1.396-1.378-3.79-1.473-1.036-.04-2-.043-2.153-.002-.353.1-.87.586-1 .952-.139.399-.076.71.431 2.22.241.72 1.029 3.386 1.742 5.918 1.644 5.844 2.378 8.343 2.863 9.705.206.601.33 1.1.275 1.125-.24.097-10.56 1.066-11.014 1.032a3.532 3.532 0 0 1-1.002-.276l-.487-.246-2.044-2.613c-2.234-2.87-2.228-2.864-3.35-3.309-.717-.287-2.82-.386-3.276-.163-.457.237-.727.644-.737 1.152-.018.39.167.805 1.916 4.373 1.06 2.166 1.964 4.083 1.998 4.27.04.179.004.521-.076.75-.093.228-1.109 2.064-2.269 4.088-1.921 3.34-2.11 3.711-2.123 4.107-.008.25.061.557.168.725.328.512.72.644 1.966.676 1.32.029 2.352-.236 3.05-.762.222-.171 1.275-1.313 2.412-2.611 1.918-2.185 2.048-2.32 2.45-2.505.241-.111.601-.232.82-.271.267-.058 2.213.201 5.912.8 3.036.48 5.525.894 5.518.914 0 .026-.121.306-.27.638-.54 1.198-1.515 3.842-3.35 9.021-1.029 2.913-2.107 5.897-2.4 6.62-.703 1.748-.725 1.833-.594 2.286.137.46.45.833.872 1.012.41.177 3.823.24 4.37.085.852-.25 1.44-.688 2.312-1.724 1.166-1.39 3.169-3.948 6.771-8.661 5.8-7.583 6.561-8.49 7.387-8.702.233-.065 2.828-.056 5.784.011 5.827.138 6.64.09 8.62-.5 2.24-.67 4.035-1.65 5.517-3.016 1.136-1.054 1.135-1.014.207-1.962-.357-.38-.767-.777-.902-.893z",fill:"#000033"}))};o.propTypes={secondaryColor:a.default.string,color:a.default.string,label:a.default.string},o.defaultProps={secondaryColor:"grey",color:"#FFA500",label:"async-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Puff=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Puff=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 44 44",xmlns:"http://www.w3.org/2000/svg",stroke:e.color,"aria-label":e.label},r.default.createElement("g",{fill:"none",fillRule:"evenodd",strokeWidth:"2"},r.default.createElement("circle",{cx:"22",cy:"22",r:e.radius},r.default.createElement("animate",{attributeName:"r",begin:"0s",dur:"1.8s",values:"1; 20",calcMode:"spline",keyTimes:"0; 1",keySplines:"0.165, 0.84, 0.44, 1",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"strokeOpacity",begin:"0s",dur:"1.8s",values:"1; 0",calcMode:"spline",keyTimes:"0; 1",keySplines:"0.3, 0.61, 0.355, 1",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"22",cy:"22",r:e.radius},r.default.createElement("animate",{attributeName:"r",begin:"-0.9s",dur:"1.8s",values:"1; 20",calcMode:"spline",keyTimes:"0; 1",keySplines:"0.165, 0.84, 0.44, 1",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"strokeOpacity",begin:"-0.9s",dur:"1.8s",values:"1; 0",calcMode:"spline",keyTimes:"0; 1",keySplines:"0.3, 0.61, 0.355, 1",repeatCount:"indefinite"}))))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading",radius:1}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.RevolvingDot=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.RevolvingDot=function(e){return r.default.createElement("svg",{version:"1.1",width:e.width,height:e.height,xmlns:"http://www.w3.org/2000/svg",x:"0px",y:"0px","aria-label":e.label},r.default.createElement("circle",{fill:"none",stroke:e.color,strokeWidth:"4",cx:"50",cy:"50",r:e.radius+38,style:{opacity:.5}}),r.default.createElement("circle",{fill:e.color,stroke:e.color,strokeWidth:"3",cx:"8",cy:"54",r:e.radius},r.default.createElement("animateTransform",{attributeName:"transform",dur:"2s",type:"rotate",from:"0 50 48",to:"360 50 52",repeatCount:"indefinite"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading",radius:6}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Rings=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Rings=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 45 45",xmlns:"http://www.w3.org/2000/svg",stroke:e.color,"aria-label":e.label},r.default.createElement("g",{fill:"none",fillRule:"evenodd",transform:"translate(1 1)",strokeWidth:"2"},r.default.createElement("circle",{cx:"22",cy:"22",r:e.radius,strokeOpacity:"0"},r.default.createElement("animate",{attributeName:"r",begin:"1.5s",dur:"3s",values:"6;22",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"stroke-opacity",begin:"1.5s",dur:"3s",values:"1;0",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"stroke-width",begin:"1.5s",dur:"3s",values:"2;0",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"22",cy:"22",r:e.radius,strokeOpacity:"0"},r.default.createElement("animate",{attributeName:"r",begin:"3s",dur:"3s",values:"6;22",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"strokeOpacity",begin:"3s",dur:"3s",values:"1;0",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"strokeWidth",begin:"3s",dur:"3s",values:"2;0",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"22",cy:"22",r:e.radius+2},r.default.createElement("animate",{attributeName:"r",begin:"0s",dur:"1.5s",values:"6;1;2;3;4;5;6",calcMode:"linear",repeatCount:"indefinite"}))))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",radius:6,label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.TailSpin=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.TailSpin=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 38 38",xmlns:"http://www.w3.org/2000/svg","aria-label":e.label},r.default.createElement("defs",null,r.default.createElement("linearGradient",{x1:"8.042%",y1:"0%",x2:"65.682%",y2:"23.865%",id:"a"},r.default.createElement("stop",{stopColor:e.color,stopOpacity:"0",offset:"0%"}),r.default.createElement("stop",{stopColor:e.color,stopOpacity:".631",offset:"63.146%"}),r.default.createElement("stop",{stopColor:e.color,offset:"100%"}))),r.default.createElement("g",{fill:"none",fillRule:"evenodd"},r.default.createElement("g",{transform:"translate(1 1)"},r.default.createElement("path",{d:"M36 18c0-9.94-8.06-18-18-18",id:"Oval-2",stroke:e.color,strokeWidth:"2"},r.default.createElement("animateTransform",{attributeName:"transform",type:"rotate",from:"0 18 18",to:"360 18 18",dur:"0.9s",repeatCount:"indefinite"})),r.default.createElement("circle",{fill:"#fff",cx:"36",cy:"18",r:e.radius},r.default.createElement("animateTransform",{attributeName:"transform",type:"rotate",from:"0 18 18",to:"360 18 18",dur:"0.9s",repeatCount:"indefinite"})))))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",radius:1,label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.ThreeDots=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.ThreeDots=function(e){return r.default.createElement("svg",{width:e.width,height:e.height,viewBox:"0 0 120 30",xmlns:"http://www.w3.org/2000/svg",fill:e.color,"aria-label":e.label},r.default.createElement("circle",{cx:"15",cy:"15",r:e.radius+6},r.default.createElement("animate",{attributeName:"r",from:"15",to:"15",begin:"0s",dur:"0.8s",values:"15;9;15",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"fillOpacity",from:"1",to:"1",begin:"0s",dur:"0.8s",values:"1;.5;1",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"60",cy:"15",r:e.radius,attributeName:"fillOpacity",from:"1",to:"0.3"},r.default.createElement("animate",{attributeName:"r",from:"9",to:"9",begin:"0s",dur:"0.8s",values:"9;15;9",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"fillOpacity",from:"0.5",to:"0.5",begin:"0s",dur:"0.8s",values:".5;1;.5",calcMode:"linear",repeatCount:"indefinite"})),r.default.createElement("circle",{cx:"105",cy:"15",r:e.radius+6},r.default.createElement("animate",{attributeName:"r",from:"15",to:"15",begin:"0s",dur:"0.8s",values:"15;9;15",calcMode:"linear",repeatCount:"indefinite"}),r.default.createElement("animate",{attributeName:"fillOpacity",from:"1",to:"1",begin:"0s",dur:"0.8s",values:"1;.5;1",calcMode:"linear",repeatCount:"indefinite"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string,radius:a.default.number},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading",radius:9}})?r.apply(t,a):r)||(e.exports=i)},function(e,t,n){var r,a,i;a=[t,n(0),n(1)],void 0===(i="function"===typeof(r=function(e,t,n){"use strict";Object.defineProperty(e,"__esModule",{value:!0}),e.Triangle=void 0;var r=i(t),a=i(n);function i(e){return e&&e.__esModule?e:{default:e}}var o=e.Triangle=function(e){return r.default.createElement("div",{className:"react-spinner-loader-svg"},r.default.createElement("svg",{id:"triangle",width:e.width,height:e.height,viewBox:"-3 -4 39 39","aria-label":e.label},r.default.createElement("polygon",{fill:"transparent",stroke:e.color,strokeWidth:"1",points:"16,0 32,32 0,32"})))};o.propTypes={height:a.default.oneOfType([a.default.string,a.default.number]),width:a.default.oneOfType([a.default.string,a.default.number]),color:a.default.string,label:a.default.string},o.defaultProps={height:80,width:80,color:"green",label:"audio-loading"}})?r.apply(t,a):r)||(e.exports=i)},,,function(e,t,n){"use strict";var r=n(2),a=n(4),i=n(0),o=n.n(i),l=n(1),u=n.n(l),c=n(10),s=n.n(c),f=n(5),d={tag:f.b,inverse:u.a.bool,color:u.a.string,body:u.a.bool,outline:u.a.bool,className:u.a.string,cssModule:u.a.object,innerRef:u.a.oneOfType([u.a.object,u.a.string,u.a.func])},p=function(e){var t=e.className,n=e.cssModule,i=e.color,l=e.body,u=e.inverse,c=e.outline,d=e.tag,p=e.innerRef,m=Object(a.a)(e,["className","cssModule","color","body","inverse","outline","tag","innerRef"]),h=Object(f.a)(s()(t,"card",!!u&&"text-white",!!l&&"card-body",!!i&&(c?"border":"bg")+"-"+i),n);return o.a.createElement(d,Object(r.a)({},m,{className:h,ref:p}))};p.propTypes=d,p.defaultProps={tag:"div"},t.a=p},function(e,t,n){"use strict";var r=n(2),a=n(4),i=n(0),o=n.n(i),l=n(1),u=n.n(l),c=n(10),s=n.n(c),f=n(5),d={tag:f.b,className:u.a.string,cssModule:u.a.object,innerRef:u.a.oneOfType([u.a.object,u.a.string,u.a.func])},p=function(e){var t=e.className,n=e.cssModule,i=e.innerRef,l=e.tag,u=Object(a.a)(e,["className","cssModule","innerRef","tag"]),c=Object(f.a)(s()(t,"card-body"),n);return o.a.createElement(l,Object(r.a)({},u,{className:c,ref:i}))};p.propTypes=d,p.defaultProps={tag:"div"},t.a=p},function(e,t,n){"use strict";var r=n(2),a=n(4),i=n(0),o=n.n(i),l=n(1),u=n.n(l),c=n(10),s=n.n(c),f=n(5),d={tag:f.b,className:u.a.string,cssModule:u.a.object},p=function(e){var t=e.className,n=e.cssModule,i=e.tag,l=Object(a.a)(e,["className","cssModule","tag"]),u=Object(f.a)(s()(t,"card-title"),n);return o.a.createElement(i,Object(r.a)({},l,{className:u}))};p.propTypes=d,p.defaultProps={tag:"div"},t.a=p},function(e,t,n){"use strict";var r=n(2),a=n(4),i=n(0),o=n.n(i),l=n(1),u=n.n(l),c=n(10),s=n.n(c),f=n(5),d={tag:f.b,className:u.a.string,cssModule:u.a.object},p=function(e){var t=e.className,n=e.cssModule,i=e.tag,l=Object(a.a)(e,["className","cssModule","tag"]),u=Object(f.a)(s()(t,"card-text"),n);return o.a.createElement(i,Object(r.a)({},l,{className:u}))};p.propTypes=d,p.defaultProps={tag:"p"},t.a=p},function(e,t,n){"use strict";var r=n(2),a=n(4),i=n(0),o=n.n(i),l=n(1),u=n.n(l),c=n(10),s=n.n(c),f=n(5),d={tag:f.b,top:u.a.bool,bottom:u.a.bool,className:u.a.string,cssModule:u.a.object},p=function(e){var t=e.className,n=e.cssModule,i=e.top,l=e.bottom,u=e.tag,c=Object(a.a)(e,["className","cssModule","top","bottom","tag"]),d="card-img";i&&(d="card-img-top"),l&&(d="card-img-bottom");var p=Object(f.a)(s()(t,d),n);return o.a.createElement(u,Object(r.a)({},c,{className:p}))};p.propTypes=d,p.defaultProps={tag:"img"},t.a=p}]]);
PATH: omdb-backend/public/static/js/2.fb38bfe7.chunk.js
LINES: 3-3

//# sourceMappingURL=2.fb38bfe7.chunk.js.map
PATH: omdb-backend/public/static/js/main.66ea98dc.chunk.js
LINES: 1-1

(this["webpackJsonpomdb-ui"]=this["webpackJsonpomdb-ui"]||[]).push([[0],{30:function(e,t,a){e.exports=a(79)},36:function(e,t,a){},59:function(e,t,a){},79:function(e,t,a){"use strict";a.r(t);var n=a(0),r=a.n(n),c=a(27),l=a.n(c),o=(a(35),a(12)),m=a(3),i=(a(36),a(11)),s=a(13),u=a.n(s),d=r.a.memo((function(e){var t=e.movie,a=Object(m.g)();return r.a.createElement("div",{className:"col-md-4"},r.a.createElement("div",{className:"card",style:{width:"18rem",margin:"auto"}},r.a.createElement("div",{className:"bd-placeholder-img card-img-top",style:{width:"18rem",height:"18rem",overflow:"hidden"}},r.a.createElement("img",{src:t.Poster,style:{width:"18rem"},alt:"Poster"})),r.a.createElement("div",{className:"card-body"},r.a.createElement("h5",{className:"card-title"},t.Title),r.a.createElement("p",{className:"card-text"}," Year : ",t.Year," , Type: ",t.Type," "),r.a.createElement("button",{className:"btn btn-primary",onClick:function(){a.push("/details/".concat(t.imdbID))}},"View"))),r.a.createElement("br",null))}));function E(e){var t=e.elements||[],a=e.component;return r.a.createElement("div",{className:"row"},t.map((function(e){return t=e,r.a.createElement(a,{movie:t,key:t.imdbID});var t})))}a(59);function h(e){var t=e.value;return r.a.createElement("div",{className:"background"},r.a.createElement("input",{className:"search clear-margin-top",type:"text",placeholder:"Enter text to search",value:t,onChange:function(t){return e.onChange(t.target.value)},onKeyDown:function(a){"Enter"===a.key&&e.onSubmit(t)}}),r.a.createElement("button",{className:"btn btn-success clear-margin-top",onClick:function(){return e.onSubmit(t)}},"Search"))}var b="http://localhost:3001/",v={headers:{"Access-Control-Allow-Origin":"*"}};function f(){var e=Object(n.useState)(""),t=Object(i.a)(e,2),a=t[0],c=t[1],l=Object(n.useState)([]),o=Object(i.a)(l,2),m=o[0],s=o[1];return r.a.createElement(r.a.Fragment,null,r.a.createElement(h,{value:a,onChange:c,onSubmit:function(e){var t="".concat(b,"omdb/search?title=").concat(e);u.a.get(t,v).then((function(e){return function(e){e&&e.data&&e.data.Search instanceof Array&&s(e.data.Search)}(e)})).catch((function(e){return console.error(e)}))}}),r.a.createElement("br",null),r.a.createElement(E,{elements:m,component:d}))}var p=a(81),g=a(82),w=a(83),y=a(84),N=a(85),S=a(29),j=a.n(S);function O(){var e=Object(m.h)(),t=Object(n.useState)({}),a=Object(i.a)(t,2),c=a[0],l=a[1],o=Object(n.useState)(!0),s=Object(i.a)(o,2),d=s[0],E=s[1],h=e.imdbID,f="".concat(b,"omdb/imdb/").concat(h);return Object(n.useEffect)((function(){u.a.get(f,v).then((function(e){l(e.data),E(!1)}))})),d?r.a.createElement("div",{style:{width:"100%",height:"100",display:"flex",justifyContent:"center",alignItems:"center"}},r.a.createElement(j.a,{type:"ThreeDots",color:"#2BAD60",height:"100",width:"100"})):r.a.createElement("div",{className:"container-md"},r.a.createElement(p.a,null,r.a.createElement(g.a,null,r.a.createElement(w.a,null,r.a.createElement("h4",null,c.Title," (",c.Year,")")),r.a.createElement(y.a,null,c.Rated," | ",c.Runtime," | ",c.Genre," | ",c.Released),r.a.createElement(N.a,{src:c.Poster,style:{width:"18rem"},alt:"Hello"}),r.a.createElement(y.a,null,r.a.createElement("br",null),r.a.createElement("p",null,c.Plot)),r.a.createElement(y.a,null,r.a.createElement("b",null,"Director(s):")," ",c.Director),r.a.createElement(y.a,null,r.a.createElement("b",null,"Writers:")," ",c.Writer),r.a.createElement(y.a,null,r.a.createElement("b",null,"Stars:")," ",c.Actors),r.a.createElement(y.a,null,r.a.createElement("b",null,"MetaScore:")," ",c.Metascore," | ",r.a.createElement("b",null,"Rating:")," ",c.imdbRating),r.a.createElement(y.a,null,r.a.createElement("b",null,"Awards:")," ",c.Awards))))}var k=function(){return r.a.createElement("div",{className:"container-fluid remove-padding"},r.a.createElement(o.a,null,r.a.createElement(m.d,null,r.a.createElement(m.b,{path:"/search",component:f}),r.a.createElement(m.b,{path:"/details/:imdbID",component:O}),r.a.createElement(m.a,{to:"/search"}))))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));l.a.render(r.a.createElement(r.a.StrictMode,null,r.a.createElement(k,null)),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then((function(e){e.unregister()})).catch((function(e){console.error(e.message)}))}},[[30,1,2]]]);
PATH: omdb-backend/public/static/js/main.66ea98dc.chunk.js
LINES: 2-2

//# sourceMappingURL=main.66ea98dc.chunk.js.map
PATH: omdb-backend/public/static/js/runtime-main.1d58dd66.js
LINES: 1-1

!function(e){function r(r){for(var n,i,l=r[0],f=r[1],a=r[2],c=0,s=[];c<l.length;c++)i=l[c],Object.prototype.hasOwnProperty.call(o,i)&&o[i]&&s.push(o[i][0]),o[i]=0;for(n in f)Object.prototype.hasOwnProperty.call(f,n)&&(e[n]=f[n]);for(p&&p(r);s.length;)s.shift()();return u.push.apply(u,a||[]),t()}function t(){for(var e,r=0;r<u.length;r++){for(var t=u[r],n=!0,l=1;l<t.length;l++){var f=t[l];0!==o[f]&&(n=!1)}n&&(u.splice(r--,1),e=i(i.s=t[0]))}return e}var n={},o={1:0},u=[];function i(r){if(n[r])return n[r].exports;var t=n[r]={i:r,l:!1,exports:{}};return e[r].call(t.exports,t,t.exports,i),t.l=!0,t.exports}i.m=e,i.c=n,i.d=function(e,r,t){i.o(e,r)||Object.defineProperty(e,r,{enumerable:!0,get:t})},i.r=function(e){"undefined"!==typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.t=function(e,r){if(1&r&&(e=i(e)),8&r)return e;if(4&r&&"object"===typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(i.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&r&&"string"!=typeof e)for(var n in e)i.d(t,n,function(r){return e[r]}.bind(null,n));return t},i.n=function(e){var r=e&&e.__esModule?function(){return e.default}:function(){return e};return i.d(r,"a",r),r},i.o=function(e,r){return Object.prototype.hasOwnProperty.call(e,r)},i.p="/";var l=this["webpackJsonpomdb-ui"]=this["webpackJsonpomdb-ui"]||[],f=l.push.bind(l);l.push=r,l=l.slice();for(var a=0;a<l.length;a++)r(l[a]);var p=f;t()}([]);
PATH: omdb-backend/public/static/js/runtime-main.1d58dd66.js
LINES: 2-2

//# sourceMappingURL=runtime-main.1d58dd66.js.map
PATH: omdb-backend/routes/omdb.js
LINES: 1-49

const express = require('express');
const router = express.Router();

const axios = require('axios');

const json = require('../config.json')

const OMDB_API_KEY = json['OMDB_API_KEY']; 
const OMDB_URL = json['OMDB_URL'];

/* Hello World */
router.get('/', function(req, res, next) {
    // A middleware should either return a response
    // Via the response object, or it should call next()
    next();
});

/* Hello World */
router.get('/', function (req, res, next) {
    res.send("Hello World");
});

/* IMDB Search */
router.get('/search', (req, res, next) => {
  const title = req.query.title
  const url = `${OMDB_URL}?apikey=${OMDB_API_KEY}&s=${title}`
  axios.get(url)
    .then(response => {
      res.send(response.data);
    })
    .catch(err => {
      res.send(err)
    })
});

/* Single IMDB Movie */
router.get('/imdb/:imdbId', (req, res, next) => {
  const imdbId = req.params.imdbId
  const url = `${OMDB_URL}?apikey=${OMDB_API_KEY}&i=${imdbId}`
  axios.get(url)
    .then(response => {
      res.send(response.data);
    })
    .catch(err => {
      res.send(err)
    })
})

module.exports = router;
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>React App</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
</head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>
PATH: omdb-ui/src/App.js
LINES: 1-13

import React from 'react';
import {
  BrowserRouter as Router,
  Switch,
  Route,
  Redirect
} from "react-router-dom";
import './App.css';
import SearchPage from './pages/SearchPage/SearchPage';
import DetailsPage from './pages/DetailsPage/DetailsPage';

/**
 * The Router maps a url to a component
PATH: omdb-ui/src/App.js
LINES: 14-29

*/
function App() {
  return (
    <div className="container-fluid remove-padding">
      <Router>
        <Switch>
          <Route path="/search" component={SearchPage} />
          <Route path="/details/:imdbID" component={DetailsPage} />
          <Redirect to = "/search"/>
        </Switch>
      </Router>
    </div>
  );
}

export default App;
PATH: omdb-ui/src/App.test.js
LINES: 1-9

import React from 'react';
import { render } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  const { getByText } = render(<App />);
  const linkElement = getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});
PATH: omdb-ui/src/components/Card/Card.js
LINES: 1-25

import React from 'react';
import { useHistory } from 'react-router-dom';

/**
 * useState(), useHistory() => Both are react hooks.
 * React hooks help to change state.
 * State change via hooks also informs React that
 * re-rendering is required.
 * React.memo is a special function. It prevents re-rendering
 * unless this component's input has changed.
 * Usually all child components re-rendered.
 * With React.memo() only if this component input has updated
 * then re-rendering will occur.
 */
const Card = React.memo(props => {
  /**
   * movie = {
   *  Poster: String,
   *  Title: String,
   *  Year: String,
   *  Type: String,
   *  imdbID: String
   * }
   */
  const movie = props.movie;
PATH: omdb-ui/src/components/Card/Card.js
LINES: 26-49

const history = useHistory();

  const viewButtonClick = () => {
    history.push(`/details/${movie.imdbID}`);
  }
  
  return (
    <div className="col-md-4">
      <div className="card" style={{ width: '18rem', margin: 'auto' }}>
        <div className="bd-placeholder-img card-img-top" style={{ width: '18rem', height: '18rem', overflow: 'hidden' }}>
          <img src={movie.Poster} style={{ width: '18rem' }} alt="Poster" />
        </div>
        <div className="card-body">
          <h5 className="card-title">{movie.Title}</h5>
          <p className="card-text"> Year : {movie.Year} , Type: {movie.Type} </p>
          <button className="btn btn-primary" onClick={viewButtonClick}>View</button>
        </div>
      </div>
      <br />
    </div>
  )
})

export default Card;
PATH: omdb-ui/src/components/List/List.js
LINES: 1-4

import React from 'react';

export default function List(props) {
  const elements = props.elements || []; // Array of movies
PATH: omdb-ui/src/components/List/List.js
LINES: 5-21

const component = props.component; // Card component

  const createReactElement = (element) => {
    /**
     * This is a specific use case where you have to use the compiled version directly
     * Second argument is input for card component.
     * A list in react has to have a unique input property of key.
     */
    return React.createElement(component, { movie: element, key: element.imdbID })
  }

  return (
    <div className="row">
      {elements.map(e => createReactElement(e))}
    </div>
  )
}
PATH: omdb-ui/src/components/Search/Search.js
LINES: 1-1

import React from 'react';
PATH: omdb-ui/src/components/Search/Search.js
LINES: 2-4

import './Search.css';

export default function Search(props) {
PATH: omdb-ui/src/components/Search/Search.js
LINES: 5-5

const value = props.value;
PATH: omdb-ui/src/components/Search/Search.js
LINES: 6-6

const handleChange = (e) => props.onChange(e.target.value);
PATH: omdb-ui/src/components/Search/Search.js
LINES: 7-20

const handleSubmit = () => props.onSubmit(value);
  const handleEnterPress = (e) => {
    if(e.key==='Enter'){
      props.onSubmit(value);
    }
  }

  return (
    <div className="background">
      <input className="search clear-margin-top" type="text" placeholder="Enter text to search" value={value} onChange={handleChange} onKeyDown={handleEnterPress}/>
      <button className="btn btn-success clear-margin-top" onClick={handleSubmit} >Search</button>
    </div>
  )
}
PATH: omdb-ui/src/index.js
LINES: 1-18

import React from 'react';
import ReactDOM from 'react-dom';
import 'bootstrap/dist/css/bootstrap.min.css';

import App from './App';
import * as serviceWorker from './serviceWorker';

ReactDOM.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>,
  document.getElementById('root')
);

// If you want your app to work offline and load faster, you can change
// unregister() to register() below. Note this comes with some pitfalls.
// Learn more about service workers: https://bit.ly/CRA-PWA
serviceWorker.unregister();
PATH: omdb-ui/src/pages/DetailsPage/DetailsPage.js
LINES: 1-5

import React, { useState, useEffect } from 'react';
import { useParams } from 'react-router-dom';
import {Card, CardImg, CardBody, CardTitle, CardText} from 'reactstrap';
import { baseUrl, axiosHeaders } from '../../utils/constants';
import axios from 'axios';
PATH: omdb-ui/src/pages/DetailsPage/DetailsPage.js
LINES: 6-66

import Loader from 'react-loader-spinner';


export default function DetailsPage() {
    const params = useParams();
    const [data, setData] = useState({})
    const [isLoading, setLoadingState]=useState(true)
    const imdbID = params.imdbID;
    const uri = `${baseUrl}omdb/imdb/${imdbID}`;

    useEffect(()=>{
        axios.get(uri, axiosHeaders)
        .then(
            res=>{
                setData(res.data);
                setLoadingState(false);
            }
        )
    });
    /**
     * Call server with imdbID
     * URL Format : http://localhost:3001/omdb/imdb/tt0944947
     * const imdbID = params.imdbID;
     */

    if(isLoading){
        return (
            <div
                style={{
                width: "100%",
                height: "100",
                display: "flex",
                justifyContent: "center",
                alignItems: "center"
            }}>

        <Loader type="ThreeDots" color="#2BAD60" height="100" width="100" />
        </div>
    )
    }

    return (
        
        <div className="container-md">
        <Card>
          <CardBody>
            <CardTitle><h4>{data.Title} ({data.Year})</h4></CardTitle>
            <CardText>{data.Rated} | {data.Runtime} | {data.Genre} | {data.Released}</CardText>
            <CardImg  src={data.Poster} style={{ width: '18rem'}} alt="Hello" />
            <CardText><br/><p>{data.Plot}</p></CardText>
            <CardText><b>Director(s):</b> {data.Director}</CardText>
            <CardText><b>Writers:</b> {data.Writer}</CardText>
            <CardText><b>Stars:</b> {data.Actors}</CardText>
            <CardText><b>MetaScore:</b> {data.Metascore} | <b>Rating:</b> {data.imdbRating}</CardText>
            <CardText><b>Awards:</b> {data.Awards}</CardText>
          </CardBody>
        </Card>
    </div>
       
    )
}
PATH: omdb-ui/src/pages/HomePage/HomePage.js
LINES: 1-4

import React, { useState } from 'react';

function Button(props) {
    return <button onClick={props.incrementCounter}> Click me! </button>
PATH: omdb-ui/src/pages/HomePage/HomePage.js
LINES: 5-8

}

function Counter(props) {
    return <p> Count = {props.value} </p>
PATH: omdb-ui/src/pages/HomePage/HomePage.js
LINES: 9-20

}

export default function HomePage() {
    // useState -> A function provided by react
    // Input to useState -> Initial value of counter
    // Returns an array of 2 items
    // 1st is counter
    // 2nd is function to set value of counter
    // Long syntax
    // const returnArr = useState(0);
    // const counter = returnArr[0];
    // const setCounter = returnArr[1];
PATH: omdb-ui/src/pages/HomePage/HomePage.js
LINES: 21-30

const [counter, setCounter] = useState(0);
    const incrementCounter = () => { setCounter(counter + 1); }
    return (
        <>
            <p> Hello World </p>
            <Button incrementCounter={incrementCounter} />
            <Counter value={counter} />
        </>
    )
}
PATH: omdb-ui/src/pages/SearchPage/SearchPage.js
LINES: 1-7

import React, { useState } from 'react';
import axios from 'axios';

import Card from '../../components/Card/Card';
import List from '../../components/List/List';
import Search from '../../components/Search/Search';
PATH: omdb-ui/src/pages/SearchPage/SearchPage.js
LINES: 8-11

import { baseUrl, axiosHeaders } from '../../utils/constants';

export default function SearchPage() {
  const [searchTerm, setSearchTerm] = useState('');
PATH: omdb-ui/src/pages/SearchPage/SearchPage.js
LINES: 12-18

const [searchResult, setSearchResult] = useState([]);

  const onSubmitSuccess = (res) => {
    // Data validation before setting searchResult
    if (res && res.data && res.data.Search instanceof Array) {
      setSearchResult(res.data.Search);
    }
PATH: omdb-ui/src/pages/SearchPage/SearchPage.js
LINES: 19-35

}

  const onSubmit = (searchTerm) => {
    const uri = `${baseUrl}omdb/search?title=${searchTerm}`;
    axios.get(uri, axiosHeaders)
      .then(res => onSubmitSuccess(res))
      .catch(err => console.error(err));
  }

  return (
    <>
      <Search value={searchTerm} onChange={setSearchTerm} onSubmit={onSubmit} />
      <br />
      <List elements={searchResult} component={Card} />
    </>
  )
}
PATH: omdb-ui/src/serviceWorker.js
LINES: 1-20

// This optional code is used to register a service worker.
// register() is not called by default.

// This lets the app load faster on subsequent visits in production, and gives
// it offline capabilities. However, it also means that developers (and users)
// will only see deployed updates on subsequent visits to a page, after all the
// existing tabs open on the page have been closed, since previously cached
// resources are updated in the background.

// To learn more about the benefits of this model and instructions on how to
// opt-in, read https://bit.ly/CRA-PWA

const isLocalhost = Boolean(
  window.location.hostname === 'localhost' ||
    // [::1] is the IPv6 localhost address.
    window.location.hostname === '[::1]' ||
    // 127.0.0.0/8 are considered localhost for IPv4.
    window.location.hostname.match(
      /^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/
    )
PATH: omdb-ui/src/serviceWorker.js
LINES: 21-54

);

export function register(config) {
  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {
    // The URL constructor is available in all browsers that support SW.
    const publicUrl = new URL(process.env.PUBLIC_URL, window.location.href);
    if (publicUrl.origin !== window.location.origin) {
      // Our service worker won't work if PUBLIC_URL is on a different origin
      // from what our page is served on. This might happen if a CDN is used to
      // serve assets; see https://github.com/facebook/create-react-app/issues/2374
      return;
    }

    window.addEventListener('load', () => {
      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;

      if (isLocalhost) {
        // This is running on localhost. Let's check if a service worker still exists or not.
        checkValidServiceWorker(swUrl, config);

        // Add some additional logging to localhost, pointing developers to the
        // service worker/PWA documentation.
        navigator.serviceWorker.ready.then(() => {
          console.log(
            'This web app is being served cache-first by a service ' +
              'worker. To learn more, visit https://bit.ly/CRA-PWA'
          );
        });
      } else {
        // Is not localhost. Just register service worker
        registerValidSW(swUrl, config);
      }
    });
  }
PATH: omdb-ui/src/serviceWorker.js
LINES: 55-98

}

function registerValidSW(swUrl, config) {
  navigator.serviceWorker
    .register(swUrl)
    .then(registration => {
      registration.onupdatefound = () => {
        const installingWorker = registration.installing;
        if (installingWorker == null) {
          return;
        }
        installingWorker.onstatechange = () => {
          if (installingWorker.state === 'installed') {
            if (navigator.serviceWorker.controller) {
              // At this point, the updated precached content has been fetched,
              // but the previous service worker will still serve the older
              // content until all client tabs are closed.
              console.log(
                'New content is available and will be used when all ' +
                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'
              );

              // Execute callback
              if (config && config.onUpdate) {
                config.onUpdate(registration);
              }
            } else {
              // At this point, everything has been precached.
              // It's the perfect time to display a
              // "Content is cached for offline use." message.
              console.log('Content is cached for offline use.');

              // Execute callback
              if (config && config.onSuccess) {
                config.onSuccess(registration);
              }
            }
          }
        };
      };
    })
    .catch(error => {
      console.error('Error during service worker registration:', error);
    });
PATH: omdb-ui/src/serviceWorker.js
LINES: 99-128

}

function checkValidServiceWorker(swUrl, config) {
  // Check if the service worker can be found. If it can't reload the page.
  fetch(swUrl, {
    headers: { 'Service-Worker': 'script' },
  })
    .then(response => {
      // Ensure service worker exists, and that we really are getting a JS file.
      const contentType = response.headers.get('content-type');
      if (
        response.status === 404 ||
        (contentType != null && contentType.indexOf('javascript') === -1)
      ) {
        // No service worker found. Probably a different app. Reload the page.
        navigator.serviceWorker.ready.then(registration => {
          registration.unregister().then(() => {
            window.location.reload();
          });
        });
      } else {
        // Service worker found. Proceed as normal.
        registerValidSW(swUrl, config);
      }
    })
    .catch(() => {
      console.log(
        'No internet connection found. App is running in offline mode.'
      );
    });
PATH: omdb-ui/src/serviceWorker.js
LINES: 129-141

}

export function unregister() {
  if ('serviceWorker' in navigator) {
    navigator.serviceWorker.ready
      .then(registration => {
        registration.unregister();
      })
      .catch(error => {
        console.error(error.message);
      });
  }
}
PATH: omdb-ui/src/setupTests.js
LINES: 1-5

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom/extend-expect';
PATH: omdb-ui/src/utils/constants.js
LINES: 1-7

const baseUrl = 'http://localhost:3001/';
const axiosHeaders = {
	headers: {
	  'Access-Control-Allow-Origin': '*',
	}
};
export { baseUrl, axiosHeaders }
# Portfolio Chat  Copilot Instructions

## Big picture
- **Client-side React 19 portfolio** with RAG chatbot; **secure backend proxy** protects OpenAI API key. Chat flow: [src/services/chatAPIClient.js](src/services/chatAPIClient.js)  [src/services/ragService.js](src/services/ragService.js)  [api/embed.js](api/embed.js) + [api/chat.js](api/chat.js)  UI in [src/components/ChatHistory.js](src/components/ChatHistory.js).
- **Agentic architecture**: LLM can call 6 resume tools on-demand (`get_experience`, `get_education`, `get_projects`, `get_skills`, `get_certifications`, `get_contact_info`) via OpenAI function calling with proper JSON Schema validation. Tools in [src/services/ragService.js#L76](src/services/ragService.js#L76) with `emptySchema` ({type: 'object', properties: {}, required: []}). Agentic loop runs max 3 iterations: LLM  detect tool calls  execute tools  inject results  LLM final response.
- **Binary vector index**: Pre-computed embeddings stored in [public/rag/](public/rag/) (meta.json for metadata, vectors.f32 for Float32 embeddings, texts.txt for chunk content). Resume content is single source of truth in [public/data/resume.json](public/data/resume.json).
## Big picture
- **Client-side React 19 portfolio** with RAG chatbot; **secure backend proxy** protects OpenAI API key. Chat flow: [src/services/chatAPIClient.js](src/services/chatAPIClient.js)  [src/services/ragService.js](src/services/ragService.js)  [api/embed.js](api/embed.js) + [api/chat.js](api/chat.js)  UI in [src/components/ChatHistory.js](src/components/ChatHistory.js).
- **Agentic architecture**: LLM can call 6 resume tools on-demand (`get_experience`, `get_education`, `get_projects`, `get_skills`, `get_certifications`, `get_contact_info`) via OpenAI function calling with proper JSON Schema validation. Tools in [src/services/ragService.js#L76](src/services/ragService.js#L76) with `emptySchema` ({type: 'object', properties: {}, required: []}). Agentic loop runs max 3 iterations: LLM  detect tool calls  execute tools  inject results  LLM final response.
- **Binary vector index**: Pre-computed embeddings stored in [public/rag/](public/rag/) (meta.json for metadata, vectors.f32 for Float32 embeddings, texts.txt for chunk content). Resume content is single source of truth in [public/data/resume.json](public/data/resume.json).
- **Action tags**: Responses ending with `>` trigger auto-scroll to page sections. Parsed/stripped by [src/components/ChatHistory.js](src/components/ChatHistory.js) regex; target sections have IDs in [src/pages/](src/pages/) components.
## Big picture
- **Client-side React 19 portfolio** with RAG chatbot; **secure backend proxy** protects OpenAI API key. Chat flow: [src/services/chatAPIClient.js](src/services/chatAPIClient.js)  [src/services/ragService.js](src/services/ragService.js)  [api/embed.js](api/embed.js) + [api/chat.js](api/chat.js)  UI in [src/components/ChatHistory.js](src/components/ChatHistory.js).
- **Agentic architecture**: LLM can call 6 resume tools on-demand (`get_experience`, `get_education`, `get_projects`, `get_skills`, `get_certifications`, `get_contact_info`) via OpenAI function calling with proper JSON Schema validation. Tools in [src/services/ragService.js#L76](src/services/ragService.js#L76) with `emptySchema` ({type: 'object', properties: {}, required: []}). Agentic loop runs max 3 iterations: LLM  detect tool calls  execute tools  inject results  LLM final response.
- **Binary vector index**: Pre-computed embeddings stored in [public/rag/](public/rag/) (meta.json for metadata, vectors.f32 for Float32 embeddings, texts.txt for chunk content). Resume content is single source of truth in [public/data/resume.json](public/data/resume.json).
- **Action tags**: Responses ending with `>` trigger auto-scroll to page sections. Parsed/stripped by [src/components/ChatHistory.js](src/components/ChatHistory.js) regex; target sections have IDs in [src/pages/](src/pages/) components.
- **Client-side React 19 portfolio** with RAG chatbot; **secure backend proxy** protects OpenAI API key. Chat flow: [src/services/chatAPIClient.js](src/services/chatAPIClient.js)  [src/services/ragService.js](src/services/ragService.js)  [api/embed.js](api/embed.js) + [api/chat.js](api/chat.js)  UI in [src/components/ChatHistory.js](src/components/ChatHistory.js).
- **Agentic architecture**: LLM can call 6 resume tools on-demand (`get_experience`, `get_education`, `get_projects`, `get_skills`, `get_certifications`, `get_contact_info`) via OpenAI function calling with proper JSON Schema validation. Tools in [src/services/ragService.js#L76](src/services/ragService.js#L76) with `emptySchema` ({type: 'object', properties: {}, required: []}). Agentic loop runs max 3 iterations: LLM  detect tool calls  execute tools  inject results  LLM final response.
- **Binary vector index**: Pre-computed embeddings stored in [public/rag/](public/rag/) (meta.json for metadata, vectors.f32 for Float32 embeddings, texts.txt for chunk content). Resume content is single source of truth in [public/data/resume.json](public/data/resume.json).
- **Action tags**: Responses ending with `>` trigger auto-scroll to page sections. Parsed/stripped by [src/components/ChatHistory.js](src/components/ChatHistory.js) regex; target sections have IDs in [src/pages/](src/pages/) components.
- **Agentic architecture**: LLM can call 6 resume tools on-demand (`get_experience`, `get_education`, `get_projects`, `get_skills`, `get_certifications`, `get_contact_info`) via OpenAI function calling with proper JSON Schema validation. Tools in [src/services/ragService.js#L76](src/services/ragService.js#L76) with `emptySchema` ({type: 'object', properties: {}, required: []}). Agentic loop runs max 3 iterations: LLM  detect tool calls  execute tools  inject results  LLM final response.
- **Binary vector index**: Pre-computed embeddings stored in [public/rag/](public/rag/) (meta.json for metadata, vectors.f32 for Float32 embeddings, texts.txt for chunk content). Resume content is single source of truth in [public/data/resume.json](public/data/resume.json).
- **Action tags**: Responses ending with `>` trigger auto-scroll to page sections. Parsed/stripped by [src/components/ChatHistory.js](src/components/ChatHistory.js) regex; target sections have IDs in [src/pages/](src/pages/) components.
- **Security model**: Vercel Serverless Functions (`/api/embed`, `/api/chat`) proxy OpenAI requests. API key stored as `OPENAI_API_KEY` (no `REACT_APP_` prefix) in Vercel environment variables only. Client never sees API key.

## Project-specific patterns
- **Binary vector index**: Pre-computed embeddings stored in [public/rag/](public/rag/) (meta.json for metadata, vectors.f32 for Float32 embeddings, texts.txt for chunk content). Resume content is single source of truth in [public/data/resume.json](public/data/resume.json).
- **Action tags**: Responses ending with `>` trigger auto-scroll to page sections. Parsed/stripped by [src/components/ChatHistory.js](src/components/ChatHistory.js) regex; target sections have IDs in [src/pages/](src/pages/) components.
- **Security model**: Vercel Serverless Functions (`/api/embed`, `/api/chat`) proxy OpenAI requests. API key stored as `OPENAI_API_KEY` (no `REACT_APP_` prefix) in Vercel environment variables only. Client never sees API key.

## Project-specific patterns
- **Backend proxy architecture**: Client calls `/api/embed` and `/api/chat` Vercel serverless functions instead of OpenAI directly. API key secured server-side as `OPENAI_API_KEY` (no `REACT_APP_` prefix).
- **RAG pipeline**: Client-side embedding via `/api/embed`  dot-product similarity search (cosine for L2-normalized vectors)  format top-K chunks with citations  inject into system prompt. Scoped retrieval: project detection via Jaccard similarity on aliases OR LLM fallback, then filter vectors by `repo` field (k=8 for scoped, k=10 for broad).
- **Action tags**: Responses ending with `>` trigger auto-scroll to page sections. Parsed/stripped by [src/components/ChatHistory.js](src/components/ChatHistory.js) regex; target sections have IDs in [src/pages/](src/pages/) components.
- **Security model**: Vercel Serverless Functions (`/api/embed`, `/api/chat`) proxy OpenAI requests. API key stored as `OPENAI_API_KEY` (no `REACT_APP_` prefix) in Vercel environment variables only. Client never sees API key.

## Project-specific patterns
- **Backend proxy architecture**: Client calls `/api/embed` and `/api/chat` Vercel serverless functions instead of OpenAI directly. API key secured server-side as `OPENAI_API_KEY` (no `REACT_APP_` prefix).
- **RAG pipeline**: Client-side embedding via `/api/embed`  dot-product similarity search (cosine for L2-normalized vectors)  format top-K chunks with citations  inject into system prompt. Scoped retrieval: project detection via Jaccard similarity on aliases OR LLM fallback, then filter vectors by `repo` field (k=8 for scoped, k=10 for broad).
- **Message formatting**: [ChatHistory.js](src/components/ChatHistory.js) pipeline: parse `[label](url)` links  bold `**text**`  strip action tags regex `/?>?/g`.
- **Security model**: Vercel Serverless Functions (`/api/embed`, `/api/chat`) proxy OpenAI requests. API key stored as `OPENAI_API_KEY` (no `REACT_APP_` prefix) in Vercel environment variables only. Client never sees API key.

## Project-specific patterns
- **Backend proxy architecture**: Client calls `/api/embed` and `/api/chat` Vercel serverless functions instead of OpenAI directly. API key secured server-side as `OPENAI_API_KEY` (no `REACT_APP_` prefix).
- **RAG pipeline**: Client-side embedding via `/api/embed`  dot-product similarity search (cosine for L2-normalized vectors)  format top-K chunks with citations  inject into system prompt. Scoped retrieval: project detection via Jaccard similarity on aliases OR LLM fallback, then filter vectors by `repo` field (k=8 for scoped, k=10 for broad).
- **Message formatting**: [ChatHistory.js](src/components/ChatHistory.js) pipeline: parse `[label](url)` links  bold `**text**`  strip action tags regex `/?>?/g`.
- **Theme system**: CSS variables only ([src/index.css](src/index.css)). No CSS-in-JS. Toggle via `document.documentElement.dataset.theme` (dark|light). Set in [src/App.js](src/App.js) localStorage + [src/components/TopNav.js](src/components/TopNav.js).
## Project-specific patterns
- **Backend proxy architecture**: Client calls `/api/embed` and `/api/chat` Vercel serverless functions instead of OpenAI directly. API key secured server-side as `OPENAI_API_KEY` (no `REACT_APP_` prefix).
- **RAG pipeline**: Client-side embedding via `/api/embed`  dot-product similarity search (cosine for L2-normalized vectors)  format top-K chunks with citations  inject into system prompt. Scoped retrieval: project detection via Jaccard similarity on aliases OR LLM fallback, then filter vectors by `repo` field (k=8 for scoped, k=10 for broad).
- **Message formatting**: [ChatHistory.js](src/components/ChatHistory.js) pipeline: parse `[label](url)` links  bold `**text**`  strip action tags regex `/?>?/g`.
- **Theme system**: CSS variables only ([src/index.css](src/index.css)). No CSS-in-JS. Toggle via `document.documentElement.dataset.theme` (dark|light). Set in [src/App.js](src/App.js) localStorage + [src/components/TopNav.js](src/components/TopNav.js).
- **Factory pattern**: Reusable handlers via factory functions (e.g., `createScrollHandler`, `createMessageFormatter`). Avoid inline styles; use shared utilities in [src/App.css](src/App.css) (`.section`, `.card`, `.btn`).

## Workflows
- **RAG pipeline**: Client-side embedding via `/api/embed`  dot-product similarity search (cosine for L2-normalized vectors)  format top-K chunks with citations  inject into system prompt. Scoped retrieval: project detection via Jaccard similarity on aliases OR LLM fallback, then filter vectors by `repo` field (k=8 for scoped, k=10 for broad).
- **Message formatting**: [ChatHistory.js](src/components/ChatHistory.js) pipeline: parse `[label](url)` links  bold `**text**`  strip action tags regex `/?>?/g`.
- **Theme system**: CSS variables only ([src/index.css](src/index.css)). No CSS-in-JS. Toggle via `document.documentElement.dataset.theme` (dark|light). Set in [src/App.js](src/App.js) localStorage + [src/components/TopNav.js](src/components/TopNav.js).
- **Factory pattern**: Reusable handlers via factory functions (e.g., `createScrollHandler`, `createMessageFormatter`). Avoid inline styles; use shared utilities in [src/App.css](src/App.css) (`.section`, `.card`, `.btn`).

## Workflows
- **Local dev with Vercel Functions**: `npm install` then `vercel dev` (NOT `npm start`CRA dev server doesn't handle `/api` routes). Tests: `npm test`. Production build: `npm run build`.
- **Environment variables**:
 - Local: Create `.env` file (not `.env.local`) with `OPENAI_API_KEY=sk-...` for `vercel dev`
 - Or pass via CLI: `OPENAI_API_KEY=sk-... vercel dev`
## Workflows
- **Local dev with Vercel Functions**: `npm install` then `vercel dev` (NOT `npm start`CRA dev server doesn't handle `/api` routes). Tests: `npm test`. Production build: `npm run build`.
- **Environment variables**:
 - Local: Create `.env` file (not `.env.local`) with `OPENAI_API_KEY=sk-...` for `vercel dev`
 - Or pass via CLI: `OPENAI_API_KEY=sk-... vercel dev`
 - Or sync from Vercel: `vercel env pull` then `vercel dev`
 - Client-side (safe to expose): `REACT_APP_LLM_MODEL`, `REACT_APP_EMBEDDING_MODEL`, `REACT_APP_RAG_ENABLED`
- **Vercel deployment**: Set `OPENAI_API_KEY` in Vercel Dashboard (Project Settings  Environment Variables). API routes in `/api` auto-deploy.
- **Adding/changing resume data**: Edit [public/data/resume.json](public/data/resume.json) and regenerate the RAG index assets in [public/rag/](public/rag/) (see [CLIENT_SIDE_MIGRATION.md](CLIENT_SIDE_MIGRATION.md)).

## Gotchas
- `server.js` is deprecated; keep changes client-side only.
- Action tag must be the final characters in the model reply (e.g., `... >`) to trigger scroll.
- Section ids expected by scroll tags: `about-me`, `education`, `experience`, `projects`, `certifications`, `contact`.
-  Tool schemas MUST be valid JSON Schema objects: `{type: 'object', properties: {}, required: []}`. Invalid schemas cause OpenAI 400 errors.
## Gotchas
- `server.js` is deprecated; keep changes client-side only.
- Action tag must be the final characters in the model reply (e.g., `... >`) to trigger scroll.
- Section ids expected by scroll tags: `about-me`, `education`, `experience`, `projects`, `certifications`, `contact`.
-  Tool schemas MUST be valid JSON Schema objects: `{type: 'object', properties: {}, required: []}`. Invalid schemas cause OpenAI 400 errors.
-  Initialize RAG once in `RAGService.initialize()` on App mount
-  Always pass conversation history (last 6 messages) to maintain context
-  Wrap LLM calls in try-catch with fallback to resume-only context
-  Normalize emails in LLM responses via `ragService.normalizeEmails()`
-  NEVER expose API key in frontend code (use server-side env vars only)
-  Use `OPENAI_API_KEY` (no REACT_APP_ prefix) for backend serverless functions
-  Backend `/api` routes handle all OpenAI requests; client never calls OpenAI directly
-  Use `vercel dev` for local testing with API routes (not `npm start`)
### Accessibility
-  Semantic HTML (``, ``, ``)
-  ARIA labels where needed (`aria-label`, `aria-expanded`)
-  Touch targets: 4444px minimum
-  Alt text for images; empty alt (`alt=""`) for decorative icons
-  Keyboard navigation: Tab through interactive elements (buttons, links, inputs)

## Debugging Checklist
### RAG Not Initializing
- [ ] `.env.local` has `OPENAI_API_KEY=sk-...` (no REACT_APP_ prefix)
- [ ] Vercel environment variables: `OPENAI_API_KEY` set (Project Settings)
- [ ] `/rag/meta.json` exists and valid JSON (check Network tab)
- [ ] `/rag/vectors.f32` size = count  1536  4 bytes (binary Float32)
- [ ] `/rag/texts.txt` size matches total text_length sum in meta.json
- [ ] Browser console shows " RAG initialized successfully (secure proxy mode)"
- [ ] Network tab: requests to `/api/embed` and `/api/chat` return 200 (not direct `api.openai.com` calls)
### Action Tags Visible in Chat (Not Executing)
- [ ] ChatHistory.js regex correctly strips: `/?>?/g`
- [ ] LLM response has tag at **very end** (e.g., "...project. >")
- [ ] Target section has correct `id` attribute (e.g., ``)
- [ ] Browser console: check for scroll errors, verify `document.getElementById()` finds element
- [ ] Test: manually paste tag in chat input to verify strip logic works
### Theme Not Switching
- [ ] `document.documentElement.dataset.theme` is either "dark" or "light"
- [ ] CSS rules exist for both `:root[data-theme="dark"]` and `:root[data-theme="light"]`
- [ ] Component CSS imports color tokens: `color: var(--text)`
- [ ] TopNav.js detects system preference: `window.matchMedia('(prefers-color-scheme)')`
- [ ] Check DevTools: computed styles show correct CSS variable values
### Chat Not Responding
- [ ] OpenAI API key valid and set in `.env` (for `vercel dev`) or Vercel Dashboard
- [ ] Using `vercel dev` (NOT `npm start`required for `/api` routes)
- [ ] Billing/quota not exceeded on OpenAI account
- [ ] Browser console: check for errors in chatAPIClient.sendMessage()
- [ ] Network tab: `/api/chat` endpoint returns 200 (check response body for error details)
- [ ] API key is string type (check type validation in `/api/chat.js`)
- [ ] Verify RAG initialized before sending message
- [ ] Check Vercel function logs for backend errors
### Tools Not Being Called or 400 Schema Errors
- [ ] LLM model supports function calling (gpt-3.5-turbo-1106+, gpt-4+)
- [ ] Tool schemas are valid JSON Schema: `{type: 'object', properties: {}, required: []}` (required for OpenAI)
- [ ] Check `buildTools()` in [src/services/ragService.js#L92](src/services/ragService.js#L92)all 6 tools must have `schema: emptySchema`
- [ ] Tool descriptions are clear and specific
- [ ] Check console for " LLM called X tool(s)" logs
- [ ] Agentic loop completing iterations (max 3) without errors
- [ ] Tool execution returns valid JSON-serializable data
- [ ] `/api/chat` endpoint correctly converts tools: `function: { name, description, parameters: t.schema }`

## Project Structure
```
src/
 App.js # Root component: state (messages, theme, chat), RAG init
 App.css # Chat UI, layout utilities (.section, .card, .btn)
 index.css # Design tokens (colors, spacing, typography)
 components/
  ChatHistory.js # Message list: action tag detection, link/bold parsing, auto-scroll
  InputArea.js # Text input + debounced send
  SectionHeader.js # Section title component
  SuggestionChips.js # Quick action chips for common queries
  TopNav.js # Header: theme toggle
 pages/
  AboutMe.js/css # Profile section with image
  Education.js/css # Education timeline with GPA
  SectionHeader.js # Section title component
  SuggestionChips.js # Quick action chips for common queries
  TopNav.js # Header: theme toggle
 pages/
  AboutMe.js/css # Profile section with image
  Education.js/css # Education timeline with GPA
  Experience.js/css # Work history (timeline layout)
  Projects.js/css # Project cards grid with expand/collapse
  Certifications.js/css # License & certification cards
  ContactMe.js/css # Contact info + social links
 services/
  ragService.js # Core: RAG retrieval, tool definitions, binary index loading, LLM calls via secure proxy
  chatAPIClient.js # Wrapper: ensures RAG ready before queries

api/
 embed.js # Vercel serverless: proxy OpenAI embeddings API
 chat.js # Vercel serverless: proxy OpenAI chat completions API

public/
 data/
  resume.json # Single source of truth: experience, education, projects, skills, socials
  site_data.json # Legacy, may be deprecated
 rag/
  meta.json # Index metadata: count, dim, items[] (file_path, repo, chunk_id, text offsets)
  vectors.f32 # Binary Float32 embeddings (pre-computed L2-normalized, dim=1536)
  texts.txt # Concatenated chunk texts (UTF-8), indexed by text_offset/text_length
```

## Key Insights
 vectors.f32 # Binary Float32 embeddings (pre-computed L2-normalized, dim=1536)
  texts.txt # Concatenated chunk texts (UTF-8), indexed by text_offset/text_length
```

## Key Insights

1. **Client-side architecture** - No backend complexity, OpenAI API called directly from browser (fine for portfolio/demo, not production apps with sensitive API keys)

2. **Binary index design** - Custom retrieval optimized for small datasets (>, <>, or no brackets).

4. **CSS variable architecture** - Theme switching is O(1) via single `data-theme` attribute change. No component re-renders.

5. **Factory functions** - Core React pattern used throughout (createScrollHandler, createMessageFormatter, etc.)

6. **Project context detection** - Two-tier approach: (1) alias matching with Jaccard similarity, (2) LLM fallback for fuzzy queries. Scopes retrieval to 8 chunks when project detected vs 10 for broad search.

7. **Email normalization** - Idempotent regex in ragService handles `plain@email.com`, `[email](url)`, and existing `[email](mailto:...)` links without duplication.

8. **Resume as API contract** - Single JSON file drives all portfolio sections + RAG context. Adding skills/projects only requires JSON edit + RAG index regeneration.
6. **Project context detection** - Two-tier approach: (1) alias matching with Jaccard similarity, (2) LLM fallback for fuzzy queries. Scopes retrieval to 8 chunks when project detected vs 10 for broad search.

7. **Email normalization** - Idempotent regex in ragService handles `plain@email.com`, `[email](url)`, and existing `[email](mailto:...)` links without duplication.

8. **Resume as API contract** - Single JSON file drives all portfolio sections + RAG context. Adding skills/projects only requires JSON edit + RAG index regeneration.

9. **Conversation memory** - sessionStorage-based rolling summary compresses multi-turn conversations into concise bullets, reducing token bloat while preserving context continuity. Summary stored in sessionStorage, updated async via fire-and-forget LLM calls.

## Documentation

- `BACKEND_PROXY_SECURITY.md` - Complete migration guide for secure backend proxy architecture
- `CLIENT_SIDE_MIGRATION.md` - Server  client-side Langchain migration notes
- `DESIGN_TOKENS.md` - Complete CSS variable system reference (468 lines)
- `LANGCHAIN_SETUP.md` / `LANGCHAIN_INTEGRATION.md` - Langchain config details
- `README.md` - Standard CRA instructions (not project-specific)

---

**Last Updated:** January 30, 2026
**Status:** Production-ready (client-side RAG + conversation memory + secure backend proxy)
#  Client-Side Langchain Migration Complete

## Summary of Changes

You've successfully migrated from a backend-based Langchain setup to a **fully client-side implementation**. No server.js needed!

## What Changed
###  Removed
- Backend API endpoints (`/api/chat`, `/api/health`, etc.)
- Express.js server dependency
- Node.js backend complexity
- Backend .env configuration
###  Added/Updated
- **Client-side Langchain RAG** in `ragService.js`
- **Direct API client** in `chatAPIClient.js` (no backend proxy)
- **Simplified `.env.local`** (only frontend config needed)
- **Direct browser access** to OpenAI API (via Langchain)

## Architecture
```
React App (browser)
 
ChatAPIClient
 
RAGService (Langchain)
 
OpenAI API
```

## Key Advantages

 **Simpler Setup** - Just `npm start`, no server.js
 **No Network Latency** - Direct API calls from browser
 **Faster Prototyping** - No backend code to maintain
 **Self-Contained** - Everything in one React app
 **Cost Effective** - No server infrastructure needed

## Setup (5 minutes)

1. **Install dependencies**
 ```bash
 npm install langchain openai
 ```

2. **Configure `.env.local`**
 ```
 REACT_APP_OPENAI_API_KEY=sk-...
 REACT_APP_LLM_MODEL=gpt-3.5-turbo
 REACT_APP_RAG_ENABLED=true
 ```

3. **Copy data to public**
 ```bash
 cp src/data/site_data.json public/data/site_data.json
 ```

4. **Run**
 ```bash
 npm start
 ```

## Files Overview

| File | Purpose |
|------|---------|
| `src/services/ragService.js` | Langchain RAG chain setup & queries |
| `src/services/chatAPIClient.js` | LLM interface (uses ragService) |
| `src/App.js` | State management & message handling |
| `LANGCHAIN_SETUP.md` | Complete setup guide |
| `server.js` | DEPRECATED (kept as reference only) |

## First Message Flow
| `src/App.js` | State management & message handling |
| `LANGCHAIN_SETUP.md` | Complete setup guide |
| `server.js` | DEPRECATED (kept as reference only) |

## First Message Flow

1. **User types & hits send**
2. **App calls `chatAPIClient.sendMessage()`**
3. **RAGService initializes** (first time only):
 - Fetches `public/data/site_data.json`
 - Creates OpenAI embeddings
 - Builds in-memory vector store
4. **Langchain queries RAG chain**
5. **OpenAI returns response** (with portfolio context)
6. **Message appears in chat** with " Thinking..." indicator

## Performance

- **First message**: ~2-3 seconds (Langchain init)
- **Subsequent messages**: ~1-2 seconds (direct API)
- **No server overhead**: Pure client-side execution

## Security Notes

 Your OpenAI API key is exposed in the browser
- Fine for personal/portfolio projects
- For production apps, add a backend proxy to hide the key
- Use environment variables on deployment platforms

## Next Steps

-  Test the chat with your portfolio data
-  Optional: Add streaming for real-time typing
-  Optional: Use Vercel/Netlify for hosting (they handle env vars securely)
-  Optional: Migrate to vector DB (Pinecone) for production scale

**You're all set! **
# Design Token System

This document describes the comprehensive design token system implemented across the Portfolio Chat application.

## Overview

The design token system provides a single source of truth for all visual design decisions, enabling:
- **Consistent theming** across dark and light modes
- **Maintainable styles** with semantic naming
- **Accessible focus states** with keyboard navigation support
- **Reduced motion** support for users with vestibular disorders
- **Scalable spacing** based on a consistent scale

---

## Token Categories
###  Color Tokens

#### Background Layers
```css
--background /* Base canvas (dark: #0B0F19, light: #F8FAFC) */
--surface /* Cards, panels (dark: #0F172A, light: #FFFFFF) */
--surface-2 /* Elevated surfaces (dark: #1E293B, light: #F1F5F9) */
```

#### Borders
```css
--border /* Subtle borders (dark: rgba(31,41,55,0.3), light: #E2E8F0) */
--border-strong /* Emphasized borders (dark: rgba(31,41,55,0.5), light: #CBD5E1) */
```

#### Text
```css
--text /* Primary text (dark: #E5E7EB, light: #0F172A) */
--text-muted /* Secondary text (dark: rgba(156,163,175,0.85), light: rgba(71,85,105,0.85)) */
--text-subtle /* De-emphasized text (dark: rgba(156,163,175,0.5), light: rgba(71,85,105,0.5)) */
```

#### Primary Colors
```css
--primary /* Neon cyan accent (dark: #22D3EE, light: #0EA5E9) */
--primary-hover /* Primary hover state (dark: #06B6D4, light: #0284C7) */
--primary-2 /* Violet secondary accent (#8B5CF6) */
--primary-2-hover /* Violet hover (#7C3AED) */
```

#### Status Colors
```css
--success /* Green (#10B981 dark, #059669 light) */
--success-muted /* Success background (rgba with 15% or 10% opacity) */
--warning /* Amber (#F59E0B dark, #D97706 light) */
--warning-muted /* Warning background */
--error /* Red (#EF4444 dark, #DC2626 light) */
--error-muted /* Error background */
```

#### Shadows
```css
--shadow-sm /* Subtle shadow for cards */
--error-muted /* Error background */
```

#### Shadows
```css
--shadow-sm /* Subtle shadow for cards */
--shadow-md /* Medium elevation shadow */
--shadow-lg /* High elevation shadow */
--shadow-focus /* Cyan glow for focus states (0 0 0 3px rgba(cyan, 0.4)) */
```

---
###  Geometry Tokens

#### Border Radius
```css
--radius-sm: 4px;
--radius-md: 6px;
--radius-lg: 8px;
--radius-xl: 12px;
--radius-full: 9999px; /* Perfect circles */
```

#### Spacing Scale (0.25rem = 4px base)
```css
--space-1: 0.25rem; /* 4px */
--space-2: 0.5rem; /* 8px */
--space-3: 0.75rem; /* 12px */
--space-4: 1rem; /* 16px */
--space-5: 1.25rem; /* 20px */
--space-6: 1.5rem; /* 24px */
--space-8: 2rem; /* 32px */
--space-10: 2.5rem; /* 40px */
--space-12: 3rem; /* 48px */
--space-16: 4rem; /* 64px */
--space-20: 5rem; /* 80px */
--space-24: 6rem; /* 96px */
```

**Usage Guidelines:**
- Component padding: `--space-4` to `--space-8`
- Section padding: `--space-8` to `--space-16`
- Gaps between elements: `--space-2` to `--space-6`
- Large vertical spacing: `--space-12` to `--space-24`

---
###  Typography Tokens

#### Font Families
```css
--font-ui: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
--font-mono: 'JetBrains Mono', 'Monaco', 'Courier New', monospace;
```

#### Font Weights
```css
--font-light: 300;
--font-normal: 400;
--font-medium: 500;
--font-semibold: 600;
--font-bold: 700;
```

---
###  Motion Tokens

#### Transition Durations
```css
--transition-fast: 150ms; /* Quick interactions (hover, button press) */
--transition-base: 200ms; /* Standard transitions (most UI changes) */
--transition-slow: 300ms; /* Slower animations (panel open/close) */
```

#### Transition Timing Functions
```css
--ease-in-out: cubic-bezier(0.4, 0, 0.2, 1); /* Smooth start and end */
--ease-out: cubic-bezier(0, 0, 0.2, 1); /* Smooth deceleration */
--ease-in: cubic-bezier(0.4, 0, 1, 1); /* Smooth acceleration */
```

**Usage:**
```css
transition: color var(--transition-base) var(--ease-in-out),
 opacity var(--transition-base) var(--ease-in-out);
```

---

## Theme Switching
### Implementation

Themes are controlled via `data-theme` attribute on `` or ``:

```javascript
// Set dark theme
document.documentElement.setAttribute('data-theme', 'dark');

// Set light theme
document.documentElement.setAttribute('data-theme', 'light');
```
### System Preference Detection

The design system automatically respects system preferences:

```css
@media (prefers-color-scheme: light) {
 :root:not([data-theme="dark"]) {
 /* Light theme tokens automatically applied */
 }
}
```

**Behavior:**
- If user manually sets theme  respect that choice
- If no manual theme set  follow system preference
- Persists across sessions (App.js handles localStorage)

---

## Accessibility Features
### Focus-Visible States

All interactive elements have enhanced focus rings for **keyboard navigation only** (not mouse clicks):

```css
*:focus-visible {
 outline: 2px solid var(--primary);
 outline-offset: 2px;
 border-radius: var(--radius-sm);
}

button:focus-visible,
a:focus-visible,
input:focus-visible {
 outline: 2px solid var(--primary);
 outline-offset: 2px;
 box-shadow: var(--shadow-focus); /* Cyan glow */
}
```

**Why focus-visible?**
- Mouse users: No visible focus ring (cleaner UX)
- Keyboard users: Clear focus indicators (better a11y)
### Reduced Motion Support

Respects user preference for reduced motion:

```css
@media (prefers-reduced-motion: reduce) {
 *,
 *::before,
 *::after {
 animation-duration: 0.01ms !important;
 animation-iteration-count: 1 !important;
 transition-duration: 0.01ms !important;
 scroll-behavior: auto !important;
 }
}
```

**Applied to:**
- All animations (cursor blink, fade-ins, etc.)
- All transitions
- Smooth scrolling

---

## Usage Examples
### Component Styling

#### Before (hardcoded values):
```css
.card {
 padding: 16px 24px;
 background: rgba(15, 23, 42, 0.3);
 border: 1px solid rgba(31, 41, 55, 0.25);
 border-radius: 4px;
 color: #E5E7EB;
 transition: border-color 0.2s ease;
}

.card:hover {
 border-color: rgba(31, 41, 55, 0.5);
}
```

#### After (design tokens):
```css
.card {
 padding: var(--space-4) var(--space-6);
 background: var(--surface);
 border: 1px solid var(--border);
 border-radius: var(--radius-sm);
 color: var(--text);
 transition: border-color var(--transition-base) var(--ease-in-out),
 box-shadow var(--transition-base) var(--ease-in-out);
}

.card:hover {
 border-color: var(--border-strong);
 box-shadow: var(--shadow-sm);
}

.card:focus-visible {
 outline: 2px solid var(--primary);
 outline-offset: 2px;
 box-shadow: var(--shadow-focus);
}
```
### Interactive Elements

```css
.button {
 padding: var(--space-2) var(--space-4);
 background: var(--primary);
 color: var(--surface);
 border: none;
 border-radius: var(--radius-md);
 font-weight: var(--font-medium);
 transition: background-color var(--transition-fast) var(--ease-in-out),
 transform var(--transition-fast) var(--ease-out);
}

.button:hover {
 background: var(--primary-hover);
 transform: translateY(-1px);
 box-shadow: var(--shadow-md);
}

.button:focus-visible {
 outline: 2px solid var(--primary);
 outline-offset: 2px;
 box-shadow: var(--shadow-focus);
}

.button:active {
 transform: translateY(0);
}
```
### Tech Badges (with hover effects)

```css
.tech-badge {
 padding: var(--space-1) var(--space-3);
 background: var(--surface-2);
 color: var(--text-muted);
 border: 1px solid var(--border);
 border-radius: var(--radius-sm);
 font-family: var(--font-mono);
 font-weight: var(--font-medium);
 font-size: 0.75rem;
 transition: background-color var(--transition-fast) var(--ease-in-out),
 color var(--transition-fast) var(--ease-in-out),
 border-color var(--transition-fast) var(--ease-in-out);
}

.tech-badge:hover {
 background: var(--primary-2); /* Violet */
 color: var(--surface);
 border-color: var(--primary-2);
}
```

---

## Utility Classes

Commonly used utilities for quick styling:
### Spacing
```css
.mt-4 { margin-top: var(--space-4); }
.mb-6 { margin-bottom: var(--space-6); }
.p-4 { padding: var(--space-4); }
```
### Text
```css
.text-muted { color: var(--text-muted); }
.text-subtle { color: var(--text-subtle); }
.text-primary-color { color: var(--primary); }
```
### Surfaces
```css
.surface { background-color: var(--surface); }
.surface-2 { background-color: var(--surface-2); }
```
### Borders
```css
.border { border: 1px solid var(--border); }
.border-strong { border: 1px solid var(--border-strong); }
.border-radius { border-radius: var(--radius-md); }
```

---

## Migration Checklist

When migrating existing components to the design token system:

- [ ] Replace hardcoded colors with semantic tokens (`--text`, `--surface`, `--primary`)
- [ ] Replace px spacing with spacing scale (`var(--space-4)`)
- [ ] Replace hardcoded transitions with motion tokens (`var(--transition-base)`)
- [ ] Replace hardcoded border-radius with radius tokens (`var(--radius-md)`)
- [ ] Replace `:focus` with `:focus-visible` for keyboard-only focus rings
- [ ] Add `box-shadow: var(--shadow-focus)` to focus-visible states
- [ ] Test in both dark and light themes
- [ ] Verify reduced motion media query disables animations

---

## File Locations
### Token Definitions
- **Primary definition:** `src/index.css` (lines 1-200)
- **Dark theme:** `:root` selector (default)
- **Light theme:** `html[data-theme="light"]` and `@media (prefers-color-scheme: light)`
### Component Styles (Updated)
- `src/App.css` - Main app, nav, chat, input, suggestions
- `src/pages/Experience.css` - Work experience cards
- `src/pages/Projects.css` - Project grid cards
- `src/pages/Education.css` - Education timeline
- `src/pages/ContactMe.css` - Contact section, social links
- `src/pages/AboutMe.css` - Hero section, typing animation

---

## Best Practices
###  Do
- Use semantic token names (`var(--text)`, not `var(--gray-200)`)
- Use spacing scale consistently (`var(--space-4)` not `16px`)
- Layer shadows for depth (`var(--shadow-sm)`  `var(--shadow-md)` on hover)
- Use `:focus-visible` for interactive elements
- Test both dark and light themes
- Use motion tokens for consistent timing
###  Don't
- Hardcode colors (`#22D3EE`  use `var(--primary)`)
- Hardcode spacing (`padding: 16px`  use `var(--space-4)`)
- Use `:focus` instead of `:focus-visible` (creates mouse focus rings)
- Ignore reduced motion preferences
- Mix token scales (e.g., `calc(var(--space-3) * 2.5)`  use `var(--space-8)`)

---

## Browser Support

- Modern browsers (Chrome, Firefox, Safari, Edge)
- CSS variables (100% coverage in modern browsers)
- `:focus-visible` (100% coverage via polyfill if needed)
- `prefers-color-scheme` (95%+ coverage)
- `prefers-reduced-motion` (95%+ coverage)

---

## Future Enhancements

Potential additions to the token system:

1. **Breakpoint tokens** for responsive design
 ```css
 --breakpoint-sm: 640px;
 --breakpoint-md: 768px;
 --breakpoint-lg: 1024px;
 ```

2. **Z-index scale** for layering
 ```css
 --z-base: 1;
 --z-dropdown: 100;
 --z-modal: 1000;
 --z-toast: 2000;
 ```

3. **Animation presets** for common patterns
 ```css
 --animation-fade-in: fadeIn 200ms ease-out;
 --animation-slide-up: slideUp 300ms ease-out;
 ```

4. **Dark mode variant tokens** for specific overrides
 ```css
 --surface-hover-dark: rgba(31, 41, 55, 0.4);
 --surface-hover-light: rgba(241, 245, 249, 1);
 ```

---

## References

- **CSS Variables Spec:** https://www.w3.org/TR/css-variables/
- **Focus-visible Spec:** https://drafts.csswg.org/selectors-4/#the-focus-visible-pseudo
---

## References

- **CSS Variables Spec:** https://www.w3.org/TR/css-variables/
- **Focus-visible Spec:** https://drafts.csswg.org/selectors-4/#the-focus-visible-pseudo
- **Prefers Reduced Motion:** https://www.w3.org/TR/mediaqueries-5/#prefers-reduced-motion
- **WCAG Focus Guidelines:** https://www.w3.org/WAI/WCAG21/Understanding/focus-visible.html

---

**Last Updated:** January 2026
**Status:**  Production-ready
# Fix Langchain Import Errors

## Issue
The Langchain imports need `@langchain/openai` package which was not installed.

## Solution

Run this command to install the required Langchain integration:

```bash
npm install @langchain/openai
```

Then restart your dev server:

```bash
npm start
```

## What This Does

The `@langchain/openai` package provides browser-compatible exports for:
- `ChatOpenAI` - LLM interface for chat
- `OpenAIEmbeddings` - Vector embeddings generation

This is separate from the main `langchain` package and needs to be installed explicitly.

## After Installation

The errors should be resolved and you'll be able to:
1. Initialize Langchain RAG on first message
2. Load your portfolio data from `public/data/site_data.json`
3. Generate context-aware responses with OpenAI

Enjoy! 
# Langchain RAG Integration - Summary

##  Completed Changes
### 1. Frontend Components Updated

#### App.js
-  Added `isLoading` and `error` state for async operations
-  Integrated `chatAPIClient.sendMessage()` with client-side Langchain
-  Added 30-second request timeout with auto-reset failsafe
-  Proper error handling with user-facing messages
-  Request throttling (disabled inputs while loading)

#### ChatHistory.js
-  Added `isLoading` prop support
-  Typing indicator (" Thinking...") with pulse animation

#### InputArea.js
-  Added `disabled` prop to prevent input during API calls
-  Prevent Enter key send while loading

#### SuggestionChips.js
-  Added `disabled` prop to prevent clicks during loading
### 2. Services Created

#### src/services/chatAPIClient.js
-  Client-side LLM interface
-  `sendMessage()` method with conversation history support
-  RAG service integration
-  Environment-aware configuration

#### src/services/ragService.js
-  Langchain RAG implementation (client-side)
-  Document loading from `public/data/` folder
-  Vector embeddings via OpenAI
-  RetrievalQAChain for context-aware responses
-  One-time initialization on first message
### 3. Configuration Files

#### .env.local
```
REACT_APP_OPENAI_API_KEY=...
REACT_APP_LLM_MODEL=gpt-3.5-turbo
REACT_APP_RAG_ENABLED=true
```

#### LANGCHAIN_SETUP.md
- Complete setup instructions
- Architecture diagram (client-side)
- Troubleshooting guide
- Security notes and best practices
- Performance considerations
### 4. Styling Enhancements

#### App.css
-  Disabled state styling for inputs/buttons
-  Typing indicator animation
-  Error message styling
-  Visual feedback for loading states
### 5. Documentation

#### .github/copilot-instructions.md
-  Updated with client-side Langchain architecture
-  RAG service documentation
-  Setup instructions integrated
-  No backend server required noted

##  Quick Start
### Setup (one-time)
```bash
npm install
npm install langchain openai

# Create .env.local with your OpenAI API key
# Copy src/data/site_data.json to public/data/site_data.json
```
### Run
```bash
npm start
```

Visit http://localhost:3000 - that's it! No backend needed! 

##  File Structure
```
portfolio-chat/
 .env.local (new - add your API keys)
 .env.example (new - reference template)
 server.js (deprecated - now just a placeholder)
 LANGCHAIN_SETUP.md (updated - client-side setup)
 LANGCHAIN_INTEGRATION.md (new - this file)
 src/
  App.js (updated - client-side RAG integration)
  App.css (updated - loading/error states)
  components/
   ChatHistory.js (updated - loading indicator)
   InputArea.js (updated - disabled state)
   SuggestionChips.js (updated - disabled state)
   HeroSection.js
   TopNav.js
  services/ (new)
   chatAPIClient.js (updated - client-side)
   ragService.js (updated - Langchain implementation)
  data/
  site_data.json (existing - your portfolio data)
  Gauransh_Sawhney_SDE_Resume.pdf (existing)
 public/
  data/
  site_data.json (copy from src/data for browser access)
 .github/
  copilot-instructions.md (updated - client-side)
```

##  Key Features

1. **Langchain RAG**: Context-aware responses using your portfolio data
2. **OpenAI Integration**: GPT-3.5-turbo (configurable to GPT-4)
3. **Error Handling**: Graceful failures with user-friendly messages
4. **Loading States**: Visual feedback while waiting for API responses
##  Key Features

1. **Langchain RAG**: Context-aware responses using your portfolio data
2. **OpenAI Integration**: GPT-3.5-turbo (configurable to GPT-4)
3. **Error Handling**: Graceful failures with user-friendly messages
4. **Loading States**: Visual feedback while waiting for API responses
5. **Request Throttling**: Prevents duplicate API calls
6. **Timeout Management**: 30-second failsafe with auto-reset
7. **Client-Side Processing**: No backend needed, runs entirely in browser
8. **Fast Initial Load**: Only first message triggers Langchain init (~2-3s)

##  Important

- **API Key Setup**: Add your OpenAI API key to `.env.local`
- **Data Location**: Copy `src/data/site_data.json` to `public/data/` (browsers can only access public folder)
- **No Backend**: Don't run `server.js` - it's deprecated
- **First Message**: Slightly slower (~2-3s) as Langchain initializes embeddings

##  Customization

- Change LLM model: Update `REACT_APP_LLM_MODEL` in `.env.local`
- Disable RAG: Set `REACT_APP_RAG_ENABLED=false`
- Adjust timeout: Modify timeout value in `App.js` line ~52
- Add more data: Drop files in both `src/data/` and `public/data/`

##  Next Steps

1.  Fill in `.env.local` with your OpenAI API key
2.  Copy portfolio data to `public/data/`
3.  Run `npm install langchain openai`
4.  Run `npm start`
5.  Optional: Add streaming responses for real-time typing
1.  Fill in `.env.local` with your OpenAI API key
2.  Copy portfolio data to `public/data/`
3.  Run `npm install langchain openai`
4.  Run `npm start`
5.  Optional: Add streaming responses for real-time typing
6.  Optional: Implement vector database (Pinecone) for production

**Happy chatting! **
# Langchain RAG Setup Guide

## Overview
This portfolio chat application uses Langchain for Retrieval-Augmented Generation (RAG). It reads your portfolio data from `src/data/` folder and uses it to provide context-aware responses via an LLM. **All processing happens client-side in the browser!**

## Architecture
```
React Frontend
 
ChatAPIClient (src/services/chatAPIClient.js)
 
RAG Service (src/services/ragService.js)
 
Langchain (in-browser)
  OpenAI Embeddings (text-embedding-ada-002)
  MemoryVectorStore (in-memory vector DB)
  RetrievalQAChain (retrieval + LLM generation)
 
OpenAI API
```

## Prerequisites
- Node.js 16+ and npm
- OpenAI API key (get one at https://platform.openai.com/api-keys)

## Setup Steps
### Step 1: Frontend Configuration
Create `.env.local` in project root:
```
REACT_APP_OPENAI_API_KEY=sk-...your_api_key...
REACT_APP_LLM_MODEL=gpt-3.5-turbo
REACT_APP_RAG_ENABLED=true
```
### Step 2: Install Dependencies
```bash
npm install
npm install langchain openai
```
### Step 3: Portfolio Data
Ensure your data files are in both locations:
- **Source**: `src/data/site_data.json` (for development)
- **Serve**: `public/data/site_data.json` (for browser access - copy from src/data)

**Example site_data.json structure:**
```json
{
 "name": "Gauransh Sawhney",
 "title": "Software Development Engineer",
 "background": "Wireless Communications & Development",
 "experience": [
 {
 "company": "Company Name",
 "role": "SDE II",
 "duration": "2020 - Present",
 "description": "..."
 }
 ],
 "skills": ["Python", "React", "Wireless Systems", "..."],
 "projects": [...]
}
```
### Step 4: Run Application
```bash
npm start
```
Opens http://localhost:3000

**That's it! No backend server needed.** 

## How RAG Works

1. **Load**: First message triggers `ragService.initialize()`
2. **Fetch**: Portfolio data loaded from `public/data/site_data.json`
3. **Embed**: Document text converted to vectors using OpenAI embeddings
4. **Store**: Vectors stored in in-memory MemoryVectorStore
5. **Retrieve**: User query embedded and matched against documents
6. **Generate**: LLM receives query + relevant context to generate response

Example flow:
```
User: "What's your experience with wireless systems?"
 
Retrieve: Finds relevant experience/project sections
 
Prompt: "Based on: [portfolio context], answer: What's your experience with wireless systems?"
 
Response: "I have 5+ years in wireless communications including..."
```

## Troubleshooting
### "REACT_APP_OPENAI_API_KEY not found"
- Create `.env.local` file in project root
- Add your OpenAI API key
- Restart `npm start` to reload environment variables
### "API error: 401"
- Verify API key in `.env.local` is correct
- Ensure API key has available credits
### "RAG not initialized"
- Check browser console for Langchain initialization errors
- Ensure `public/data/site_data.json` exists
- Verify JSON is valid format
### "Network error fetching site_data.json"
- Copy `src/data/site_data.json` to `public/data/site_data.json`
- Browser can only access files in `public/` folder
- Restart dev server after copying files
### Long initial delay on first message
- Normal! Langchain initializes embeddings (~2-3 seconds)
- Subsequent messages are faster (cached embeddings)

## Customization
### Change LLM Model
In `.env.local`:
```
REACT_APP_LLM_MODEL=gpt-4
```
### Disable RAG (use base LLM only)
In `.env.local`:
```
REACT_APP_RAG_ENABLED=false
```
### Add More Data Sources
1. Add `.json` or `.txt` files to `src/data/`
2. Copy to `public/data/`
3. Restart dev server
4. Data automatically loaded on next chat message

## Security Notes

 **API Key Exposure**: REACT_APP_OPENAI_API_KEY is exposed in browser code
- This is fine for personal/portfolio projects
- For production with real traffic, use a backend proxy to hide the key
- Implement rate limiting and cost controls on backend

 **Never commit `.env.local`**: Add to `.gitignore`
- API keys should only be in local environment
- For production, use deployment platform's environment variables (Vercel, Netlify, etc.)

## Browser Compatibility

-  Chrome 90+
-  Firefox 88+
-  Safari 14+
-  IE 11 (not supported)

Langchain requires modern JavaScript features (ES2020+).

## Performance Considerations

- **First message**: ~2-3s (initializing embeddings)
- **Subsequent messages**: ~1-2s (API call + response)
- **Memory usage**: Depends on portfolio size (usually <50MB)
- **Bundle size**: Langchain adds ~800KB to bundle (gzipped)

For production optimization:
- Consider service worker caching for embeddings
- Implement streaming responses for perceived speed
- Use Vercel Edge Functions or Cloudflare Workers for cost optimization

## Next Steps

1.  Fill in `.env.local` with your OpenAI API key
2.  Copy portfolio data to `public/data/`
3.  Run `npm start`
## Next Steps

1.  Fill in `.env.local` with your OpenAI API key
2.  Copy portfolio data to `public/data/`
3.  Run `npm start`
4.  Test with suggestion chips or free-form questions
5.  Optional: Add streaming responses for real-time typing
6.  Optional: Integrate vector database (Pinecone) for production scale

**Happy chatting! **
# Portfolio Chat

An interactive AI-powered portfolio with a client-side RAG chatbot. Ask questions about my projects, experience, and skillsno backend required.

** Live:** [gauranshsawhney.site](https://gauranshsawhney.site)

##  Features

- **AI Chat Assistant** powered by OpenAI GPT
- **Smart Navigation** - Auto-scrolls to relevant sections based on your questions
- **Client-Side RAG** - Binary-indexed retrieval for instant responses
- **Dark/Light Themes** with system preference detection
- **Fully Responsive** - Mobile-first design

##  Quick Start (Local Development)

**Important:** Use `vercel dev` (not `npm start`) to run the backend API proxy locally.

```bash
# Install
npm install

# Create .env file (or set via CLI)
echo "OPENAI_API_KEY=sk-your-api-key-here" > .env

# Run with Vercel Functions
vercel dev
```

Visit at: `http://localhost:3000`

**Alternative:** Pass API key via CLI instead of `.env`:
```bash
OPENAI_API_KEY=sk-your-api-key-here vercel dev
```

##  Tech Stack

- **React 19** - UI framework
- **Vercel Serverless Functions** - Backend proxy for OpenAI API (`/api/embed`, `/api/chat`)
- **Langchain.js** - Tool definitions for agentic function calling
- **OpenAI API** - LLM (text-embedding-3-large + GPT-3.5/4)
- **Binary Vector Index** - Custom Float32 storage for fast retrieval
- **CSS Variables** - Theme system (no CSS-in-JS)

##  Key Files
- **OpenAI API** - LLM (text-embedding-3-large + GPT-3.5/4)
- **Binary Vector Index** - Custom Float32 storage for fast retrieval
- **CSS Variables** - Theme system (no CSS-in-JS)

##  Key Files

- `src/services/ragService.js` - RAG engine with binary index retrieval & agentic tool calling
- `api/embed.js` - Backend proxy for OpenAI embeddings
- `api/chat.js` - Backend proxy for OpenAI chat with tool calling support
- `src/components/ChatHistory.js` - Message display + action tag parser
- `public/data/resume.json` - Portfolio content (single source of truth)
- `public/rag/` - Binary index assets (meta.json, vectors.f32, texts.txt)

##  Customization

Edit `public/data/resume.json` to update:
- Experience, education, projects, skills
- Contact info and social links
- Certifications

Theme colors in `src/index.css`:
```css
:root[data-theme="dark"] {
 --primary: #22D3EE; /* Neon cyan */
}
```

##  Deployment

Currently hosted on **[gauranshsawhney.site](https://gauranshsawhney.site)**.

To deploy your own version:
```bash
# Build
npm run build

# Set environment variable in Vercel Dashboard
# Project Settings  Environment Variables
# OPENAI_API_KEY: sk-your-key

# Deploy to Vercel (auto-deploys from GitHub)
vercel --prod
```

**Important:** API key must be set in Vercel Dashboard, not in code. `/api` routes auto-deploy.

---

**Author:** Gauransh Sawhney
**Important:** API key must be set in Vercel Dashboard, not in code. `/api` routes auto-deploy.

---

**Author:** Gauransh Sawhney
**Contact:** [gauransh30@gmail.com](mailto:gauransh30@gmail.com)
PATH: api/chat.js
LINES: 1-12

/**
 * Vercel Serverless Function - Chat Proxy
 * Securely handles OpenAI chat completions with tool calling support
 */

import { OpenAI } from 'openai';

// CORS helper
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
  'Access-Control-Allow-Headers': 'Content-Type',
PATH: api/chat.js
LINES: 13-62

};

export default async function handler(req, res) {
  // Handle preflight OPTIONS request
  if (req.method === 'OPTIONS') {
    return res.status(200).json({});
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const {
      messages,
      model = 'gpt-3.5-turbo',
      temperature = 0.2,
      maxTokens = 900,
      tools = null,
    } = req.body;

    // Validate messages
    if (!messages || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({ error: 'Invalid messages: must provide non-empty array' });
    }

    // Validate API key exists (server-side only)
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey || typeof apiKey !== 'string') {
      console.error('OPENAI_API_KEY not configured or invalid:', typeof apiKey);
      return res.status(500).json({ error: 'Server configuration error' });
    }

    // Initialize OpenAI client with explicit string conversion
    const openai = new OpenAI({ apiKey: String(apiKey) });

    // Build chat completion params
    const params = {
      model,
      messages,
      temperature,
      max_tokens: maxTokens,
    };

    // Add tools if provided (for function calling)
    if (tools && Array.isArray(tools) && tools.length > 0) {
      params.tools = tools;
    }

    // Create chat completion
PATH: api/chat.js
LINES: 63-95

const response = await openai.chat.completions.create(params);

    // Return response with tool calls if present
    const message = response.choices[0].message;
    return res.status(200).json({
      content: message.content,
      role: message.role,
      tool_calls: message.tool_calls || null,
      model: response.model,
      usage: response.usage,
    });

  } catch (error) {
    console.error('Chat API error:', error.message);
    
    // Handle rate limits
    if (error.status === 429) {
      return res.status(429).json({ error: 'Rate limit exceeded. Please try again.' });
    }
    
    // Handle invalid API key
    if (error.status === 401) {
      return res.status(500).json({ error: 'Authentication error' });
    }

    // Handle context length errors
    if (error.code === 'context_length_exceeded') {
      return res.status(400).json({ error: 'Message too long. Please shorten your query.' });
    }

    return res.status(500).json({ error: 'Failed to generate response' });
  }
}
PATH: api/embed.js
LINES: 1-12

/**
 * Vercel Serverless Function - Embedding Proxy
 * Securely handles OpenAI embedding requests without exposing API key to client
 */

import { OpenAI } from 'openai';

// CORS helper
const corsHeaders = {
  'Access-Control-Allow-Origin': '*',
  'Access-Control-Allow-Methods': 'POST, OPTIONS',
  'Access-Control-Allow-Headers': 'Content-Type',
PATH: api/embed.js
LINES: 13-70

};

export default async function handler(req, res) {
  // Handle preflight OPTIONS request
  if (req.method === 'OPTIONS') {
    return res.status(200).json({});
  }

  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { input, model = 'text-embedding-3-large' } = req.body;

    if (!input || typeof input !== 'string') {
      return res.status(400).json({ error: 'Invalid input: must provide string input' });
    }

    // Validate API key exists (server-side only, never exposed to client)
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey || typeof apiKey !== 'string') {
      console.error('OPENAI_API_KEY not configured or invalid:', typeof apiKey);
      return res.status(500).json({ error: 'Server configuration error' });
    }

    // Initialize OpenAI client with explicit string conversion
    const openai = new OpenAI({ apiKey: String(apiKey) });

    // Create embedding
    const response = await openai.embeddings.create({
      model,
      input,
    });

    // Return embedding vector
    return res.status(200).json({
      embedding: response.data[0].embedding,
      model: response.model,
      usage: response.usage,
    });

  } catch (error) {
    console.error('Embedding API error:', error.message);
    
    // Handle rate limits
    if (error.status === 429) {
      return res.status(429).json({ error: 'Rate limit exceeded. Please try again.' });
    }
    
    // Handle invalid API key
    if (error.status === 401) {
      return res.status(500).json({ error: 'Authentication error' });
    }

    return res.status(500).json({ error: 'Failed to generate embedding' });
  }
}
<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link id="favicon" rel="icon" href="%PUBLIC_URL%/dark_logo.png" type="image/png" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Gauransh Sawhney's portfolio website"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <!--
      manifest.json provides metadata used when your web app is installed on a
      user's mobile device or desktop. See https://developers.google.com/web/fundamentals/web-app-manifest/
    -->
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <!--
      Notice the use of %PUBLIC_URL% in the tags above.
      It will be replaced with the URL of the `public` folder during the build.
      Only files inside the `public` folder can be referenced from the HTML.

      Unlike "/favicon.ico" or "favicon.ico", "%PUBLIC_URL%/favicon.ico" will
      work correctly both with client-side routing and a non-root public URL.
      Learn how to configure a non-root public URL by running `npm run build`.
    -->
    <title>Gauransh Sawhney</title>
  </head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
</head>
  <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>
    <!--
      This HTML file is a template.
      If you open it directly in the browser, you will see an empty page.

      You can add webfonts, meta tags, or analytics to this file.
      The build step will place the bundled scripts into the <body> tag.

      To begin the development, run `npm start` or `yarn start`.
      To create a production bundle, use `npm run build` or `yarn build`.
    -->
  </body>
</html>
PATH: server.js
LINES: 1-14

/**
 * DEPRECATED: Backend Server Not Required
 * 
 * As of the client-side Langchain implementation, this file is no longer needed.
 * Langchain RAG runs entirely in the browser using:
 * - src/services/ragService.js (Langchain chain initialization)
 * - src/services/chatAPIClient.js (Client-side API client)
 * 
 * Just run: npm start
 * 
 * No backend server required! 
 */

console.log(' This file is deprecated. Langchain now runs client-side.');
PATH: src/App.js
LINES: 1-17

import React, { useState, useEffect, useRef } from 'react';
import './App.css';
import { Analytics } from '@vercel/analytics/react';
import TopNav from './components/TopNav';
import ChatHistory from './components/ChatHistory';
import InputArea from './components/InputArea';
import SuggestionChips from './components/SuggestionChips';

import chatAPIClient from './services/chatAPIClient';

import AboutMe from './pages/AboutMe';
import Education from './pages/Education';
import Experience from './pages/Experience';
import Projects from './pages/Projects';
import Certifications from './pages/Certifications';
import ContactMe from './pages/ContactMe';
PATH: src/App.js
LINES: 18-45

/* ===== Custom Hook: useInputNudge ===== */
const useInputNudge = () => {
  const [showNudge, setShowNudge] = useState(false);
  const inputRef = useRef(null);
  const nudgeTimeoutRef = useRef(null);

  useEffect(() => {
    // Check if nudge has already been shown in this session
    const hasShownNudge = sessionStorage.getItem('inputNudgeShown');

    // Check if user prefers reduced motion
    const prefersReducedMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches;

    if (hasShownNudge || prefersReducedMotion) {
      return;
    }

    // Set nudge to show after 800ms
    nudgeTimeoutRef.current = setTimeout(() => {
      setShowNudge(true);
      sessionStorage.setItem('inputNudgeShown', 'true');

      // Stop nudge after 1s
      setTimeout(() => {
        setShowNudge(false);
      }, 1000);
    }, 800);
PATH: src/App.js
LINES: 46-73

// Handle early stop: clear nudge on input focus or typing
    const handleInputFocus = () => {
      if (nudgeTimeoutRef.current) {
        clearTimeout(nudgeTimeoutRef.current);
      }
      setShowNudge(false);
      sessionStorage.setItem('inputNudgeShown', 'true');
    };

    // Add listeners to textarea if it exists
    const inputElement = inputRef.current?.querySelector('.chat-input-area');
    if (inputElement) {
      inputElement.addEventListener('focus', handleInputFocus);
      inputElement.addEventListener('input', handleInputFocus);
    }

    return () => {
      if (inputElement) {
        inputElement.removeEventListener('focus', handleInputFocus);
        inputElement.removeEventListener('input', handleInputFocus);
      }
      if (nudgeTimeoutRef.current) {
        clearTimeout(nudgeTimeoutRef.current);
      }
    };
  }, []);

  return { showNudge, inputRef };
PATH: src/App.js
LINES: 74-130

};

const App = () => {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [error, setError] = useState(null);
  const [isDarkMode, setIsDarkMode] = useState(true); // Default dark
  const [resumeData, setResumeData] = useState(null);
  const suggestionChipsRef = useRef(null);
  const hasScrolledOnFirstChatRef = useRef(false);

  // Use the nudge hook
  const { showNudge, inputRef } = useInputNudge();

  const suggestions = [
    { label: "Is Gauransh a good fit for my role?", icon: "" },
    { label: "How does this website work?", icon: "" },
    { label: "How can I contact Gauransh?", icon: "" }
  ];

  // Initialize theme from localStorage or system preference
  useEffect(() => {
    const savedTheme = localStorage.getItem('theme');
    if (savedTheme) {
      setIsDarkMode(savedTheme === 'dark');
      document.documentElement.dataset.theme = savedTheme;
    } else {
      const prefersDark = window.matchMedia('(prefers-color-scheme: dark)').matches;
      setIsDarkMode(prefersDark);
      document.documentElement.dataset.theme = prefersDark ? 'dark' : 'light';
    }
  }, []);

  // Update data-theme and localStorage when isDarkMode changes
  useEffect(() => {
    const theme = isDarkMode ? 'dark' : 'light';
    document.documentElement.dataset.theme = theme;
    localStorage.setItem('theme', theme);

    // Update favicon based on theme
    const favicon = document.getElementById('favicon');
    if (favicon) {
      favicon.href = isDarkMode ? '/dark_logo.png' : '/light_logo.png';
    }
  }, [isDarkMode]);

  useEffect(() => {
    fetch('/data/resume.json') 
      .then(res => res.json())
      .then(data => setResumeData(data))
      .catch(err => console.error("Failed to load resume data:", err));
  }, []);

  useEffect(() => {
    if (messages.length === 0 || hasScrolledOnFirstChatRef.current) return;
PATH: src/App.js
LINES: 131-154

const prefersReducedMotion = window.matchMedia('(prefers-reduced-motion: reduce)').matches;
    const scrollToChips = () => {
      const target = suggestionChipsRef.current;
      if (!target) return;

      target.scrollIntoView({
        behavior: prefersReducedMotion ? 'auto' : 'smooth',
        block: 'end'
      });

      const rect = target.getBoundingClientRect();
      const bottom = rect.bottom + window.scrollY;
      const desiredTop = bottom - window.innerHeight + 16;
      window.scrollTo({
        top: Math.max(desiredTop, 0),
        behavior: prefersReducedMotion ? 'auto' : 'smooth'
      });

      hasScrolledOnFirstChatRef.current = true;
    };

    requestAnimationFrame(() => requestAnimationFrame(scrollToChips));
    const retryId = setTimeout(scrollToChips, 250);
    return () => clearTimeout(retryId);
PATH: src/App.js
LINES: 155-199

}, [messages.length]);

  const handleSend = (text) => {
    if (!text.trim() || isLoading) return;
    
    // Add user message immediately
    setMessages(prev => [...prev, { text, sender: 'user' }]);
    setInput('');
    setIsLoading(true);
    setError(null);

    const conversationHistory = messages.map(msg => ({
      role: msg.sender === 'user' ? 'user' : 'assistant',
      content: msg.text
    }));

    chatAPIClient.sendMessage(text, conversationHistory)
        .then(reply => {
           setIsLoading(false);
           setMessages(prev => [...prev, { text: reply, sender: 'bot' }]);
        })
        .catch(err => {
            setIsLoading(false);
            setError(err.message);
        });
  };

  // We no longer need handleCloseChat because the chat is inline/permanent

  return (
    <div className="app-container appBackground">
      <TopNav isDarkMode={isDarkMode} onToggleTheme={() => setIsDarkMode(!isDarkMode)} />

      <main className="main-content">
        
        {/* 1. HERO / LANDING AREA */}
        <div className="landing-wrapper">
          
          {/* Header Section */}
          <AboutMe resumeData={resumeData} />
          
          {/* INLINE CHAT AREA: Only renders if there are messages */}
          {messages.length > 0 && (
            <div className="inline-chat-container">
              <ChatHistory
PATH: src/App.js
LINES: 200-236

messages={messages} 
                isLoading={isLoading} 
                // No onClose needed for inline
              />
            </div>
          )}
          
          {/* Input Section */}
          <div className={`input-wrapper${showNudge ? ' input-nudge' : ''}`} ref={inputRef}>
             <InputArea input={input} onInputChange={setInput} onSend={handleSend} disabled={isLoading} />
          </div>
          
          {/* Chips */}
          <div ref={suggestionChipsRef}>
            <SuggestionChips suggestions={suggestions} onChipClick={handleSend} disabled={isLoading} />
          </div>
          
          {error && <div className="error-message">{error}</div>}
        </div>

        {/* 2. SCROLLABLE SECTIONS */}
        {resumeData && (
          <>
            <Experience resumeExperience={resumeData.experience} />
            <Projects resumeProjects={resumeData.projects} />
            <Education resumeEducation={resumeData.education} />
            <Certifications resumeCertifications={resumeData.certifications} />
            <ContactMe socialLinks={resumeData.socials} />
          </>
        )}
      </main>
      <Analytics />
    </div>
  );
};

export default App;
PATH: src/App.test.js
LINES: 1-8

import { render, screen } from '@testing-library/react';
import App from './App';

test('renders learn react link', () => {
  render(<App />);
  const linkElement = screen.getByText(/learn react/i);
  expect(linkElement).toBeInTheDocument();
});
PATH: src/components/ChatHistory.js
LINES: 1-1

/* src/components/ChatHistory.js */
PATH: src/components/ChatHistory.js
LINES: 2-19

import React, { useEffect, useRef } from 'react';

const ChatHistory = ({ messages, isLoading, chatEndRef, onCloseChat }) => {

  const containerRef = useRef(null);

  // 2. Auto-scroll to bottom whenever messages or loading state changes
  useEffect(() => {
    if (containerRef.current) {
      const { scrollHeight, clientHeight } = containerRef.current;
      // This sets the scroll position to the very bottom immediately
      containerRef.current.scrollTop = scrollHeight - clientHeight;
      
      // Optional: If you want it smooth, use this instead:
      containerRef.current.scrollTo({ top: scrollHeight, behavior: 'smooth' });
    }
  }, [messages, isLoading]);
PATH: src/components/ChatHistory.js
LINES: 20-34

// --- HANDLER FACTORY: Create scroll handlers to avoid repetition ---
  const createScrollHandler = (sectionId) => () => {
    if (onCloseChat) onCloseChat();
    setTimeout(() => {
      document.getElementById(sectionId)?.scrollIntoView({ behavior: 'smooth' });
    }, 300);
  };

  // const handleScrollToAbout = createScrollHandler('about-me');
  const handleScrollToEducation = createScrollHandler('education');
  const handleScrollToExperience = createScrollHandler('experience');
  const handleScrollToProjects = createScrollHandler('projects');
  const handleScrollToCertifications = createScrollHandler('certifications');
  const handleScrollToContact = createScrollHandler('contact');
PATH: src/components/ChatHistory.js
LINES: 35-41

// --- FORMATTER 1: Parse Bold Text (**text**) ---
  const parseBold = (text) => {
    return text.split('**').map((part, index) => {
      return index % 2 === 1 ? <strong key={index}>{part}</strong> : part;
    });
  };
PATH: src/components/ChatHistory.js
LINES: 42-73

// --- FORMATTER 2: Parse Links ([text](url)) THEN Bold ---
  const parseFormattedText = (text) => {
    // Regex matches [label](url)
    const linkRegex = /\[([^\]]+)\]\(([^)]+)\)/g;
    const parts = text.split(linkRegex); 
    
    const result = [];
    for (let i = 0; i < parts.length; i += 3) {
      // 1. Handle plain text
      if (parts[i]) {
        result.push(<React.Fragment key={`text-${i}`}>{parseBold(parts[i])}</React.Fragment>);
      }
      // 2. Handle link
      if (i + 1 < parts.length) {
        const label = parts[i+1];
        const url = parts[i+2];
        result.push(
          <a 
            key={`link-${i}`} 
            href={url} 
            target="_blank" 
            rel="noopener noreferrer"
            className="chat-link"
          >
            {label}
          </a>
        );
      }
    }
    return result;
  };
PATH: src/components/ChatHistory.js
LINES: 74-118

// --- FORMATTER 3: Main Layout (Newlines, Bullets, Lists) ---
  const formatMessage = (text) => {
    return text.split('\n').map((line, i) => {
      const trimmed = line.trim();
      if (!trimmed) return <div key={i} style={{ height: '8px' }} />; // Paragraph break

      // Detect Bullet Points (*, -, )
      const isBullet = /^[*-]\s/.test(trimmed);
      
      // Detect Numbered Lists (1., 2., 10.)
      const isNumbered = /^\d+\.\s/.test(trimmed);

      if (isBullet) {
        const cleanLine = trimmed.replace(/^[*-]\s+/, '');
        return (
          <div key={i} className="chat-list-item bullet">
            <span className="list-marker"></span>
            <span>{parseFormattedText(cleanLine)}</span>
          </div>
        );
      }

      if (isNumbered) {
        const match = trimmed.match(/^(\d+\.)\s+(.*)/);
        if (match) {
          const number = match[1];
          const content = match[2];
          return (
            <div key={i} className="chat-list-item numbered">
              <span className="list-marker">{number}</span>
              <span>{parseFormattedText(content)}</span>
            </div>
          );
        }
      }

      // Standard Line
      return (
        <div key={i} className="chat-paragraph">
          {parseFormattedText(line)}
        </div>
      );
    });
  };
PATH: src/components/ChatHistory.js
LINES: 119-155

// --- RENDERER: Combines Logic + Formatting ---
  const renderMessageContent = (text) => {
    // 1. Detect Actions (Check your original tags)
    
    const hasEducationAction = text.includes("ACTION:SCROLL_EDUCATION");
    const hasExperienceAction = text.includes("ACTION:SCROLL_EXPERIENCE");
    const hasProjectsAction = text.includes("ACTION:SCROLL_PROJECTS");
    const hasCertificationsAction = text.includes("ACTION:SCROLL_CERTIFICATIONS");
    const hasContactAction = text.includes("ACTION:SCROLL_CONTACT");

    // 2. Clean Tags (Remove all action tags using Regex)
    let cleanText = text
      .replace(/<?<?ACTION:SCROLL_[A-Z_]+>?>?/g, "") 
      .trim();

    return (
      <div className="message-content">
        {/* Render Formatted Text */}
        <div className="text-wrapper">
          {formatMessage(cleanText)}
        </div>
        
        {/* Render Original Buttons */}
        {/* {hasAboutAction && (
          <button className="chat-action-btn" onClick={handleScrollToAbout}>
             Read more in About Me section
          </button>
        )} */}

        {hasEducationAction && (
          <button className="chat-action-btn" onClick={handleScrollToEducation}>
             View Education Details
          </button>
        )}

        {hasExperienceAction && (
          <button className="chat-action-btn" onClick={handleScrollToExperience}>
PATH: src/components/ChatHistory.js
LINES: 156-199

 View Work History
          </button>
        )}

        {hasProjectsAction && (
          <button className="chat-action-btn" onClick={handleScrollToProjects}>
             View Projects
          </button>
        )}

        {hasCertificationsAction && (
          <button className="chat-action-btn" onClick={handleScrollToCertifications}>
             View Certifications
          </button>
        )}

        {hasContactAction && (
          <button className="chat-action-btn" onClick={handleScrollToContact}>
              Get In Touch
          </button>
        )}
      </div>
    );
  };

  return (
    <div className="chat-history" ref={containerRef}>
      {messages.map((msg, idx) => (
        <div key={idx} className={`message ${msg.sender}`}>
          {/* Bot messages get parsed, User messages are plain text */}
          {msg.sender === 'bot' ? renderMessageContent(msg.text) : msg.text}
        </div>
      ))}
      {isLoading && (
        <div className="message bot">
          <span className="typing-indicator">Thinking...</span>
        </div>
      )}
      <div ref={chatEndRef} />
    </div>
  );
};

export default ChatHistory;
PATH: src/components/InputArea.js
LINES: 1-4

import React, { useState } from 'react';

const InputArea = ({ input, onInputChange, onSend, disabled }) => {
  const [isTyping, setIsTyping] = useState(false);
PATH: src/components/InputArea.js
LINES: 5-13

const typingTimeoutRef = React.useRef(null);
  
  const handleKeyDown = (e) => {
    // If Enter is pressed WITHOUT Shift -> Send Message
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault(); // Prevents creating a new line
      onSend(input);
    }
    // If Enter IS pressed WITH Shift -> Do nothing (let browser insert new line)
PATH: src/components/InputArea.js
LINES: 14-61

};

  const handleInputChange = (e) => {
    const value = e.target.value;
    onInputChange(value);
    
    // Set typing state
    setIsTyping(true);
    
    // Clear existing timeout
    if (typingTimeoutRef.current) {
      clearTimeout(typingTimeoutRef.current);
    }
    
    // Reset typing state after 1 second of inactivity
    typingTimeoutRef.current = setTimeout(() => {
      setIsTyping(false);
    }, 1000);
  };

  return (
    <div className={`input-wrapper console-input${isTyping ? ' is-typing' : ''}`}>
      <textarea
        className="chat-input-area"
        value={input}
        onChange={handleInputChange}
        onKeyDown={handleKeyDown}
        placeholder="Type a message..."
        disabled={disabled}
        rows={1} // Starts as a single line
      />
      <button
        className="console-send-btn"
        onClick={() => onSend(input)}
        disabled={disabled || !input.trim()}
        aria-label="Send message"
        title="Send (Enter)"
      >
        <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" aria-hidden="true" focusable="false">
          <path d="M4 12L20 4l-4.5 16-3.5-5-5-3.5z" fill="currentColor" opacity="0.12" />
          <path d="M4 12L20 4l-4.5 16-3.5-5-5-3.5z" />
        </svg>
      </button>
    </div>
  );
};

export default InputArea;
PATH: src/components/SectionHeader.js
LINES: 1-13

import React from 'react';

const SectionHeader = ({ title, className = '' }) => {
  return (
    <div className={`sectionHeaderGroup ${className}`}>
      <div className="sectionHeader">
        <h2 className="sectionTitle">{title}</h2>
      </div>
    </div>
  );
};

export default SectionHeader;
PATH: src/components/SuggestionChips.js
LINES: 1-21

import React from 'react';

const SuggestionChips = ({ suggestions, onChipClick, disabled }) => {
  return (
    <div className="suggestions-row">
      {suggestions.map((chip, idx) => (
        <button
          key={idx}
          className="chip"
          onClick={() => onChipClick(chip.label)}
          disabled={disabled}
        >
          <span className="chip-icon">{chip.icon}</span>
          {chip.label}
        </button>
      ))}
    </div>
  );
};

export default SuggestionChips;
PATH: src/components/TopNav.js
LINES: 1-2

import React, { useEffect, useState, useRef } from 'react';
PATH: src/components/TopNav.js
LINES: 3-12

const sectionIds = ['about-me', 'experience', 'projects', 'education', 'certifications', 'contact'];

const TopNav = ({ isDarkMode, onToggleTheme }) => {
  const [activeSection, setActiveSection] = useState('about-me');
  const [isScrolled, setIsScrolled] = useState(false);
  const [isManualNavigation, setIsManualNavigation] = useState(false);
  const navLinksRef = useRef(null);
  const manualNavTimeoutRef = useRef(null);

  // Detect scroll for nav background blur enhancement
PATH: src/components/TopNav.js
LINES: 13-60

useEffect(() => {
    const handleScroll = () => {
      setIsScrolled(window.scrollY > 20);
    };

    window.addEventListener('scroll', handleScroll);
    return () => window.removeEventListener('scroll', handleScroll);
  }, []);

  // Cleanup timeout on unmount
  useEffect(() => {
    return () => {
      if (manualNavTimeoutRef.current) {
        clearTimeout(manualNavTimeoutRef.current);
      }
    };
  }, []);

  useEffect(() => {
    const observer = new IntersectionObserver(
      (entries) => {
        // Skip if user is manually navigating
        if (isManualNavigation) return;

        const visibleEntries = entries.filter((entry) => entry.isIntersecting);
        if (visibleEntries.length > 0) {
          visibleEntries.sort((a, b) => b.intersectionRatio - a.intersectionRatio);
          setActiveSection(visibleEntries[0].target.id);
        }
      },
      {
        root: null,
        rootMargin: '0px',
        threshold: 0.2,
      }
    );

    const elements = sectionIds
      .map((id) => document.getElementById(id))
      .filter(Boolean);

    elements.forEach((el) => observer.observe(el));

    return () => observer.disconnect();
  }, [isManualNavigation]);

  // Fallback: on scroll, set active section based on closest section to viewport top
  useEffect(() => {
PATH: src/components/TopNav.js
LINES: 61-101

let ticking = false;

    const handleScroll = () => {
      // Skip if user is manually navigating
      if (isManualNavigation) return;

      if (ticking) return;
      ticking = true;
      requestAnimationFrame(() => {
        ticking = false;
        const sections = sectionIds
          .map((id) => {
            const el = document.getElementById(id);
            if (!el) return null;
            const rect = el.getBoundingClientRect();
            return { id, top: rect.top };
          })
          .filter(Boolean);

        // Choose the section whose top is closest to viewport top but not below 70% viewport
        const viewportGuard = window.innerHeight * 0.7;
        let best = sections[0];
        sections.forEach((s) => {
          if (s.top < viewportGuard) {
            if (!best || Math.abs(s.top) < Math.abs(best.top)) {
              best = s;
            }
          }
        });

        if (best && best.id !== activeSection) {
          setActiveSection(best.id);
        }
      });
    };

    window.addEventListener('scroll', handleScroll, { passive: true });
    return () => window.removeEventListener('scroll', handleScroll);
  }, [activeSection, isManualNavigation]);

  // Update highlighter position when active section changes
PATH: src/components/TopNav.js
LINES: 102-136

useEffect(() => {
    const updatePillPosition = () => {
      if (navLinksRef.current) {
        const activeLink = navLinksRef.current.querySelector('.nav-link.active');
        if (activeLink) {
          const rect = activeLink.getBoundingClientRect();
          const containerRect = navLinksRef.current.getBoundingClientRect();
          const offsetLeft = rect.left - containerRect.left;
          navLinksRef.current.style.setProperty('--pill-left', `${offsetLeft}px`);
          navLinksRef.current.style.setProperty('--pill-width', `${rect.width}px`);
        }
      }
    };

    // run after paint
    requestAnimationFrame(updatePillPosition);
  }, [activeSection]);

  // Initialize pill position on mount
  useEffect(() => {
    requestAnimationFrame(() => {
      if (navLinksRef.current) {
        const activeLink = navLinksRef.current.querySelector('.nav-link.active');
        if (activeLink) {
          const rect = activeLink.getBoundingClientRect();
          const containerRect = navLinksRef.current.getBoundingClientRect();
          const offsetLeft = rect.left - containerRect.left;
          navLinksRef.current.style.setProperty('--pill-left', `${offsetLeft}px`);
          navLinksRef.current.style.setProperty('--pill-width', `${rect.width}px`);
        }
      }
    });
  }, []);
  // Initialize pill position on mount
  useEffect(() => {
PATH: src/components/TopNav.js
LINES: 137-157

const timer = setTimeout(() => {
      if (navLinksRef.current) {
        const activeLink = navLinksRef.current.querySelector('.nav-link.active');
        console.log('Initial mount - checking active link:', activeLink);
        if (activeLink) {
          const rect = activeLink.getBoundingClientRect();
          const containerRect = navLinksRef.current.getBoundingClientRect();
          const offsetLeft = rect.left - containerRect.left;
          
          console.log('Initial mount - setting pill:', { offsetLeft, width: rect.width });
          
          navLinksRef.current.style.setProperty('--pill-left', `${offsetLeft}px`);
          navLinksRef.current.style.setProperty('--pill-width', `${rect.width}px`);
        }
      }
    }, 200);
    
    return () => clearTimeout(timer);
  }, []);

  // Recalculate on window resize for responsive behavior
PATH: src/components/TopNav.js
LINES: 158-174

useEffect(() => {
    const handleResize = () => {
      if (navLinksRef.current) {
        const activeLink = navLinksRef.current.querySelector('.nav-link.active');
        if (activeLink) {
          const rect = activeLink.getBoundingClientRect();
          const containerRect = navLinksRef.current.getBoundingClientRect();
          const offsetLeft = rect.left - containerRect.left;
          
          navLinksRef.current.style.setProperty('--pill-left', `${offsetLeft}px`);
          navLinksRef.current.style.setProperty('--pill-width', `${rect.width}px`);
        }
      }
    };

    window.addEventListener('resize', handleResize);
    return () => window.removeEventListener('resize', handleResize);
PATH: src/components/TopNav.js
LINES: 175-218

}, []);

  const scrollToTop = () => {
    window.scrollTo({ top: 0, behavior: 'smooth' });
  };

  const navItems = [
    { id: 'about-me', label: 'About' },
    { id: 'experience', label: 'Experience' },
    { id: 'projects', label: 'Projects' },
    { id: 'education', label: 'Education' },
    { id: 'certifications', label: 'Certifications' },
    { id: 'contact', label: 'Contact' },
  ];

  return (
    <nav className={`top-nav ${isScrolled ? 'is-scrolled' : ''}`}>
      <span
        className="logo"
        onClick={scrollToTop}
        role="button"
        tabIndex="0"
        onKeyDown={(e) => (e.key === 'Enter' || e.key === ' ') && scrollToTop()}
        title="Scroll to top"
      >
        GS
      </span>
      <div className="nav-links" ref={navLinksRef}>
        {navItems.map((item) => (
          <a
            key={item.id}
            href={`#${item.id}`}
            className={`nav-link ${activeSection === item.id ? 'active' : ''}`}
            aria-current={activeSection === item.id ? 'page' : undefined}
            onClick={(e) => {
              // Override automatic tracking during manual navigation
              setActiveSection(item.id);
              setIsManualNavigation(true);

              // Clear any previous timeout
              if (manualNavTimeoutRef.current) {
                clearTimeout(manualNavTimeoutRef.current);
              }
PATH: src/components/TopNav.js
LINES: 219-253

// Re-enable tracking after scroll completes (800ms for smooth scroll)
              manualNavTimeoutRef.current = setTimeout(() => {
                setIsManualNavigation(false);
              }, 800);
            }}
          >
            {item.label}
          </a>
        ))}
      </div>
      <button
        className="theme-toggle"
        onClick={onToggleTheme}
        aria-label={isDarkMode ? 'Switch to light mode' : 'Switch to dark mode'}
        aria-pressed={isDarkMode}
        title={isDarkMode ? 'Switch to light mode' : 'Switch to dark mode'}
        type="button"
      >
        {isDarkMode ? (
          <svg viewBox="0 0 24 24" aria-hidden="true" focusable="false">
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79Z" fill="currentColor" opacity="0.18" />
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79Z" />
          </svg>
        ) : (
          <svg viewBox="0 0 24 24" aria-hidden="true" focusable="false">
            <circle cx="12" cy="12" r="5.5" fill="currentColor" opacity="0.18" />
            <circle cx="12" cy="12" r="5.5" />
            <path d="M12 2.5v2.5m0 14v2.5m9-9h-2.5m-14 0H2.5m15.01-6.51-1.77 1.77M8.26 17.74l-1.77 1.77m12.02 0-1.77-1.77M8.26 6.26 6.49 4.49" stroke="currentColor" strokeWidth="1.6" strokeLinecap="round" />
          </svg>
        )}
      </button>
    </nav>
  );
};
PATH: src/components/TopNav.js
LINES: 254-254

export default TopNav;
PATH: src/index.js
LINES: 1-17

import React from 'react';
import ReactDOM from 'react-dom/client';
import './index.css';
import App from './App';
import reportWebVitals from './reportWebVitals';

const root = ReactDOM.createRoot(document.getElementById('root'));
root.render(
  <React.StrictMode>
    <App />
  </React.StrictMode>
);

// If you want to start measuring performance in your app, pass a function
// to log results (for example: reportWebVitals(console.log))
// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals
reportWebVitals();
PATH: src/pages/AboutMe.js
LINES: 1-1

import React, { useState, useEffect } from 'react';
PATH: src/pages/AboutMe.js
LINES: 2-62

import './AboutMe.css';

const AboutMe = ({ resumeData }) => {
  const name = resumeData?.basic_info?.name || "Gauransh Sawhney";
  // The full text we want to type out
  const fullText = `Hi, I'm ${name}.`;
  
  const [typedText, setTypedText] = useState('');

  useEffect(() => {
    let index = 0;
    const typeInterval = setInterval(() => {
      if (index <= fullText.length) {
        setTypedText(fullText.slice(0, index));
        index++;
      } else {
        clearInterval(typeInterval);
      }
    }, 100); // Speed: 100ms per character

    return () => clearInterval(typeInterval);
  }, [fullText]);

  const role = resumeData?.basic_info?.role || "Full Stack Engineer";

  return (
    <section id="about-me" className="hero-section-clean">
      <div className="hero-content">
        
        {/* 1. Typewriter Headline */}
        <h1 className="hero-title">
          {typedText}<span className="cursor">|</span>
        </h1>
        
        {/* 2. Sub-headline */}
        <h2 className="hero-subtitle">
          {role}
        </h2>

        {/* 3. The "Chat Prompt" Line */}
        <div className="ai-prompt-wrapper">
          <p className="ai-prompt-text">
            Chat with an AI trained on my work.
            <br />
            <strong>Ask questions about my work below.</strong>
          </p>
          
          {/* 4. Visual Cue (Animated Arrow) */}
          <div className="arrow-indicator">
            <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
              <path d="M12 5v14M19 12l-7 7-7-7"/>
            </svg>
          </div>
        </div>

      </div>
    </section>
  );
};

export default AboutMe;
PATH: src/pages/Certifications.js
LINES: 1-2

import React from 'react';
import './Certifications.css';
PATH: src/pages/Certifications.js
LINES: 3-5

import SectionHeader from '../components/SectionHeader';

const Certifications = ({ resumeCertifications = [] }) => {
PATH: src/pages/Certifications.js
LINES: 6-10

if (!resumeCertifications || resumeCertifications.length === 0) return null;

  const extractYear = (dateStr) => {
    const match = dateStr.match(/(\d{4})/);
    return match ? match[1] : dateStr;
PATH: src/pages/Certifications.js
LINES: 11-23

};

  const extractBadgeText = (organization) => {
    // Extract 12 letter badge from organization name
    if (!organization) return '?';
    const cleaned = organization.trim().toUpperCase();
    // Prefer first 2 letters; fallback to first letter
    if (cleaned.length >= 2) {
      return cleaned.substring(0, 2);
    }
    return cleaned.charAt(0);
  };
PATH: src/pages/Certifications.js
LINES: 24-72

// Inline external link SVG icon
  const ExternalLinkIcon = () => (
    <svg className="cert-external-icon" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2">
      <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6" />
      <polyline points="15 3 21 3 21 9" />
      <line x1="10" y1="14" x2="21" y2="3" />
    </svg>
  );

  return (
    <section id="certifications" className="section certifications-section">
      <div className="container certifications-container">
        <SectionHeader title="Licenses & Certifications" />

        <div className="certifications-grid">
          {resumeCertifications.map((cert, index) => (
            <a
              key={index}
              href={cert.url || '#'}
              target="_blank"
              rel="noopener noreferrer"
              className="cert-chip"
              title={cert.title}
            >
              {/* Badge */}
              <div className="cert-badge">
                {extractBadgeText(cert.organization)}
              </div>

              {/* Content: Title + Organization */}
              <div className="cert-content">
                <div className="cert-title">{cert.title}</div>
                <div className="cert-organization">{cert.organization}</div>
              </div>

              {/* Year + External Icon */}
              <div className="cert-meta">
                <div className="cert-year">{extractYear(cert.date)}</div>
                <ExternalLinkIcon />
              </div>
            </a>
          ))}
        </div>
      </div>
    </section>
  );
};

export default Certifications;
PATH: src/pages/ContactMe.js
LINES: 1-2

import React, { useState } from 'react';
import './ContactMe.css';
PATH: src/pages/ContactMe.js
LINES: 3-34

import SectionHeader from '../components/SectionHeader';

const ContactMe = ({ socialLinks }) => {
  // Initialize hook before any conditional returns
  const [copied, setCopied] = useState(false);

  if (!socialLinks || socialLinks.length === 0) return null;

  // Extract email from socials if present
  const emailLink = socialLinks.find(link => link.type === 'email');
  const emailAddress = emailLink ? emailLink.url.replace('mailto:', '') : null;

  const handleCopy = async () => {
    if (!emailAddress) return;
    try {
      await navigator.clipboard.writeText(emailAddress);
      setCopied(true);
      setTimeout(() => setCopied(false), 1200);
    } catch (err) {
      // Fallback: select text by creating a temporary textarea
      const textarea = document.createElement('textarea');
      textarea.value = emailAddress;
      textarea.setAttribute('readonly', '');
      textarea.style.position = 'absolute';
      textarea.style.left = '-9999px';
      document.body.appendChild(textarea);
      textarea.select();
      document.execCommand('copy');
      document.body.removeChild(textarea);
      setCopied(true);
      setTimeout(() => setCopied(false), 1200);
    }
PATH: src/pages/ContactMe.js
LINES: 35-41

};

  const handleEmailKeyDown = (event) => {
    if (event.key === 'Enter' || event.key === ' ') {
      event.preventDefault();
      handleCopy();
    }
PATH: src/pages/ContactMe.js
LINES: 42-56

};

  const getIcon = (type) => {
    switch (type.toLowerCase()) {
      case 'linkedin':
        return (
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
          </svg>
        );
      case 'github':
        return (
          <svg viewBox="0 0 24 24" fill="currentColor">
             <path d="M12 0c-6.626 0-12 5.373-12 12 0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931 0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176 0 0 1.008-.322 3.301 1.23.957-.266 1.983-.399 3.003-.404 1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221 0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576 4.765-1.589 8.199-6.086 8.199-11.386 0-6.627-5.373-12-12-12z"/>
          </svg>
PATH: src/pages/ContactMe.js
LINES: 57-91

);
      case 'email':
        return (
          <svg viewBox="0 0 24 24" fill="currentColor">
            <path d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2zm0 4l-8 5-8-5V6l8 5 8-5v2z"/>
          </svg>
        );
      default:
        return (
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round">
            <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path>
            <path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
          </svg>
        );
    }
  };

  const socialWithoutEmail = socialLinks.filter(link => link.type.toLowerCase() !== 'email');
  const showEmailLabel = socialWithoutEmail.length > 0;

  return (
    <section id="contact" className="section contact-section">
      <div className="container contact-container">
        <SectionHeader title="Contact Me" />

        <div className="card contact-panel">
          <p className="contact-pitch">Open to SWE / AI roles  happy to chat.</p>

          {emailAddress && (
            <div className="email-stack">
              {showEmailLabel && (
                <div className="contact-label-muted">Email</div>
              )}
              <div
                className={`email-row-clean ${copied ? 'copied' : ''}`}
PATH: src/pages/ContactMe.js
LINES: 92-120

aria-label="Email address (click to copy)"
                role="button"
                tabIndex={0}
                onClick={handleCopy}
                onKeyDown={handleEmailKeyDown}
              >
                <div className="email-text" title={emailAddress}>{emailAddress}</div>
                <button
                  type="button"
                  className={`copy-icon-btn ${copied ? 'copied' : ''}`}
                  onClick={(event) => {
                    event.stopPropagation();
                    handleCopy();
                  }}
                  aria-label={copied ? 'Email copied to clipboard' : 'Copy email to clipboard'}
                  aria-live="polite"
                  aria-atomic="true"
                  title={copied ? 'Copied' : 'Copy email'}
                >
                  {copied ? (
                    <svg viewBox="0 0 24 24" aria-hidden="true" focusable="false">
                      <path d="M9.5 16.5 5 12l1.4-1.4 3.1 3.1 7.1-7.2L18 8.1z" />
                    </svg>
                  ) : (
                    <svg viewBox="0 0 24 24" aria-hidden="true" focusable="false">
                      <path d="M8 7a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v12a1 1 0 0 1-1 1H9a1 1 0 0 1-1-1V7Zm-3 3a1 1 0 0 1 1-1h1v2H6v8h9v1a1 1 0 0 1-1 1H4a1 1 0 0 1-1-1V10Z" />
                    </svg>
                  )}
                </button>
PATH: src/pages/ContactMe.js
LINES: 121-156

<span
                  className={`copy-feedback ${copied ? 'visible' : ''}`}
                  aria-live="polite"
                >
                  Copied
                </span>
              </div>
            </div>
          )}

          <div className="contact-row social-row">
            <div className="contact-label">Find me on</div>
            <div className="social-icons-wrapper">
              {socialWithoutEmail.map((link, index) => (
                <a
                  key={index}
                  href={link.url}
                  target="_blank"
                  rel="noopener noreferrer"
                  className="icon-button"
                  aria-label={link.name || link.type}
                  title={link.name}
                >
                  {getIcon(link.type)}
                </a>
              ))}
            </div>
          </div>
        </div>

      </div>
    </section>
  );
};

export default ContactMe;
PATH: src/pages/Education.js
LINES: 1-2

import React from 'react';
import './Education.css';
PATH: src/pages/Education.js
LINES: 3-39

import SectionHeader from '../components/SectionHeader';

const Education = ({ resumeEducation }) => {
  // Guard clause: if data hasn't loaded yet, don't render anything
  if (!resumeEducation) return null;

  return (
    <section id="education" className="section education-section">
      <div className="container education-container">
        <SectionHeader title="Education" />
        
        <div className="education-list">
          {resumeEducation.map((edu, index) => (
            <div key={index} className="card education-card">
              <div className="edu-logo-wrapper">
                <img src={edu.logoUrl} alt={`${edu.school} logo`} className="edu-logo" />
              </div>
              
              <div className="edu-details">
                <h3 className="edu-school">{edu.school}</h3>
                <p className="edu-degree">{edu.degree}</p>
                <div className="edu-metadata-row">
                  <span className="edu-gpa-label">GPA</span>
                  <span className="edu-gpa-value">{edu.gpa}</span>
                  <span className="edu-separator" aria-hidden="true"></span>
                  <span className="edu-date">{edu.date}</span>
                </div>
              </div>
            </div>
          ))}
        </div>
      </div>
    </section>
  );
};

export default Education;
PATH: src/pages/Experience.js
LINES: 1-2

import React from 'react';
import './Experience.css';
PATH: src/pages/Experience.js
LINES: 3-34

import SectionHeader from '../components/SectionHeader';

const Experience = ({ resumeExperience }) => {
  if (!resumeExperience || resumeExperience.length === 0) return null;

  return (
    <section id="experience" className="section experience-section">
      <div className="container experience-container">
        <SectionHeader title="Experience" />
        
        <div className="experience-list">
          {resumeExperience.map((job, index) => {
            const isCurrent = index === 0;
            return (
              <div key={index} className={`experience-item ${isCurrent ? 'current' : ''}`}>
                <div className="timeline-col">
                  <span className="timeline-dot" aria-hidden="true" />
                </div>

                <div className="card experience-card">
                  {/* Left Column: Logo */}
                  <div className="exp-logo-container">
                    {job.logoUrl ? (
                      <img src={job.logoUrl} alt={`${job.company} logo`} className="exp-logo" />
                    ) : (
                      <div className="exp-logo-fallback">{job.company.charAt(0)}</div>
                    )}
                  </div>

                  {/* Right Column: Content */}
                  <div className="exp-content">
                    <div className="exp-header">
PATH: src/pages/Experience.js
LINES: 35-67

<div className="exp-top-row">
                        <div className="exp-title-wrapper">
                          <h3 className="exp-company">{job.company}</h3>
                          {isCurrent && <span className="current-badge">Current</span>}
                        </div>
                        <span className="exp-date">{job.years}</span>
                      </div>
                      <div className="exp-role">{job.title}</div>
                    </div>

                    {job.description && (
                      <p className="exp-description">{job.description}</p>
                    )}

                    {job.technologies && job.technologies.length > 0 && (
                      <div className="exp-tech-stack">
                        {job.technologies.map((tech, i) => (
                          <span key={i} className="chip">{tech}</span>
                        ))}
                      </div>
                    )}
                  </div>
                </div>
              </div>
            );
          })}
        </div>
      </div>
    </section>
  );
};

export default Experience;
PATH: src/pages/Projects.js
LINES: 1-2

import React, { useState } from 'react';
import './Projects.css';
PATH: src/pages/Projects.js
LINES: 3-7

import SectionHeader from '../components/SectionHeader';

const Projects = ({ resumeProjects }) => {
  const [expandedProjects, setExpandedProjects] = useState({});
PATH: src/pages/Projects.js
LINES: 8-42

if (!resumeProjects || resumeProjects.length === 0) return null;

  const toggleExpand = (index) => {
    setExpandedProjects(prev => ({
      ...prev,
      [index]: !prev[index]
    }));
  };

  return (
    <section id="projects" className="section projects-section">
      <div className="container projects-container">
        <SectionHeader title="Projects" />
        
        <div className="projects-grid">
          {resumeProjects.map((project, index) => (
            <div key={index} className={`card project-card ${project.featured ? 'featured' : ''}`}>
              
              {/* Top Row: Title + Date */}
              <div className="project-header">
                <h3 className="project-title">{project.title}</h3>
                <span className="project-date">{project.date}</span>
              </div>
              
              {/* Highlight: One-line impact summary */}
              {project.highlight && (
                <p className="project-highlight">{project.highlight}</p>
              )}
              
              {/* Body: Clamped Description */}
              <div className="project-description-wrapper">
                <p className={`project-description ${expandedProjects[index] ? 'expanded' : ''}`}>
                  {project.description}
                </p>
                {project.description && project.description.length > 120 && (
PATH: src/pages/Projects.js
LINES: 43-69

<button 
                    className={`expand-btn ${expandedProjects[index] ? 'expanded' : ''}`}
                    onClick={() => toggleExpand(index)}
                    aria-label={expandedProjects[index] ? 'Show less' : 'Show more'}
                    aria-expanded={expandedProjects[index]}
                  >
                    <svg className="expand-icon" xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2.5" strokeLinecap="round" strokeLinejoin="round" aria-hidden="true">
                      <polyline points="6 9 12 15 18 9"></polyline>
                    </svg>
                    <span>{expandedProjects[index] ? 'Show less' : 'Show more'}</span>
                  </button>
                )}
              </div>
              
              {/* Tech Stack: Chip Elements */}
              <div className="project-tech">
                {project.technologies && project.technologies.map((tech, i) => (
                  <span key={i} className="chip">{tech}</span>
                ))}
              </div>

              {/* Actions Row: Clear call-to-action */}
              <div className="project-actions">
                {project.url && (
                  <a 
                    href={project.url} 
                    target="_blank"
PATH: src/pages/Projects.js
LINES: 70-88

rel="noopener noreferrer" 
                    className="project-action-btn"
                  >
                    <svg xmlns="http://www.w3.org/2000/svg" width="18" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" strokeWidth="2" strokeLinecap="round" strokeLinejoin="round" aria-hidden="true">
                      <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path>
                    </svg>
                    <span>View on GitHub</span>
                  </a>
                )}
              </div>
            </div>
          ))}
        </div>
      </div>
    </section>
  );
};

export default Projects;
PATH: src/reportWebVitals.js
LINES: 1-13

const reportWebVitals = onPerfEntry => {
  if (onPerfEntry && onPerfEntry instanceof Function) {
    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {
      getCLS(onPerfEntry);
      getFID(onPerfEntry);
      getFCP(onPerfEntry);
      getLCP(onPerfEntry);
      getTTFB(onPerfEntry);
    });
  }
};

export default reportWebVitals;
PATH: src/services/chatAPIClient.js
LINES: 1-6

/**
 * Chat API Client - Client-Side Langchain
 * Handles LLM interactions directly in the browser using Langchain
 * No backend server required
 */
PATH: src/services/chatAPIClient.js
LINES: 7-55

import ragService from './ragService';

export class ChatAPIClient {
  constructor() {
    this.ragEnabled = process.env.REACT_APP_RAG_ENABLED !== 'false';
  }

  /**
   * Send message to LLM with RAG context
   * @param {string} message - User message
   * @param {Array} conversationHistory - Previous messages for context
   * @returns {Promise<string>} - LLM response
   */
  async sendMessage(message, conversationHistory = []) {
    try {
      if (!message || typeof message !== 'string') {
        throw new Error('Invalid message');
      }

      // Ensure RAG is initialized
      if (!ragService.initialized) {
        console.log(' Initializing RAG service...');
        await ragService.initialize();
      }

      // Check if initialization succeeded
      if (!ragService.isReady()) {
        const status = ragService.getStatus();
        throw new Error(`RAG initialization failed. Status: ${JSON.stringify(status)}`);
      }

      // Query RAG
      const result = await ragService.query(message, conversationHistory);
      return result.text;
    } catch (err) {
      console.error('Chat API error:', err);
      throw err;
    }
  }

  /**
   * Get RAG status for debugging
   */
  getStatus() {
    return ragService.getStatus();
  }
}

export default new ChatAPIClient();
PATH: src/services/ragService.js
LINES: 1-6

/**
 * Client-Side RAG Service - Secure Backend Proxy Version
 * Uses backend API endpoints to protect OpenAI API key
 * API calls: /api/embed (embeddings), /api/chat (LLM inference)
 */
PATH: src/services/ragService.js
LINES: 7-57

import { tool } from '@langchain/core/tools';

export class RAGChatService {
  constructor() {
    this.initialized = false;
    this.resumeData = null;
    
    // Binary index assets (cached after first load)
    this.indexMeta = null;
    this.vectors = null;
    this.textsBuffer = null;
    this.indexLoaded = false;
    
    // Project detection
    this.repoNames = [];
    this.projectAliases = new Map();

    // Conversation memory
    this.conversationSummary = '';

    // Tool definitions
    this.tools = [];
    
    // API endpoints (configure based on environment)
    this.apiBaseUrl = process.env.REACT_APP_API_BASE_URL || '';
  }

  async loadResumeData() {
    if (this.resumeData) return this.resumeData;
    try {
      const response = await fetch('/data/resume.json');
      if (!response.ok) throw new Error('Failed to load resume data');
      this.resumeData = await response.json();
      return this.resumeData;
    } catch (err) {
      console.warn('Could not load resume data:', err.message);
      return null;
    }
  }

  async initialize() {
    if (this.initialized) return;

    try {
      // No API key needed on client side - handled by backend
      console.log(' Using secure backend proxy for OpenAI API');

      // Build tools that access resume data
      this.buildTools();

      // Load resume data for system context
PATH: src/services/ragService.js
LINES: 58-106

await this.loadResumeData();

      // Load binary RAG index assets
      await this.loadBinaryIndex();

      if (!this.indexLoaded || !this.indexMeta) {
        throw new Error('Failed to initialize binary RAG index');
      }
      
      // Build project alias map
      this.buildProjectAliases();

      this.initialized = true;
      console.log(' RAG initialized successfully (secure proxy mode)');
    } catch (err) {
      console.error(' Failed to initialize RAG:', err.message);
      this.initialized = false;
      throw err;
    }
  }

  // --- Tool Definitions ---
  buildTools() {
    // Schema template for tools with no parameters
    const emptySchema = {
      type: 'object',
      properties: {},
      required: []
    };

    const getExperienceTool = tool(
      () => {
        const experience = this.resumeData?.experience || [];
        return JSON.stringify(experience, null, 2);
      },
      {
        name: 'get_experience',
        description: 'Retrieve Gauransh\'s work experience, including companies, positions, years, descriptions, and technologies used.',
        schema: emptySchema
      }
    );

    const getEducationTool = tool(
      () => {
        const education = this.resumeData?.education || [];
        return JSON.stringify(education, null, 2);
      },
      {
        name: 'get_education',
PATH: src/services/ragService.js
LINES: 107-149

description: 'Retrieve Gauransh\'s education history, including schools, degrees, dates, GPA, and relevant coursework.',
        schema: emptySchema
      }
    );

    const getCertificationsTool = tool(
      () => {
        const certifications = this.resumeData?.certifications || [];
        return JSON.stringify(certifications, null, 2);
      },
      {
        name: 'get_certifications',
        description: 'Retrieve Gauransh\'s licenses and certifications, including title, organization, date issued, and credential ID.',
        schema: emptySchema
      }
    );

    const getProjectsTool = tool(
      () => {
        const projects = this.resumeData?.projects || [];
        return JSON.stringify(projects, null, 2);
      },
      {
        name: 'get_projects',
        description: 'Retrieve Gauransh\'s projects, including titles, descriptions, dates, technologies, and repository links.',
        schema: emptySchema
      }
    );

    const getSkillsTool = tool(
      () => {
        const skills = this.resumeData?.skills || [];
        return JSON.stringify(skills, null, 2);
      },
      {
        name: 'get_skills',
        description: 'Retrieve Gauransh\'s technical skills organized by category (e.g., languages, frameworks, tools).',
        schema: emptySchema
      }
    );

    const getContactInfoTool = tool(
      () => {
PATH: src/services/ragService.js
LINES: 150-198

const socials = this.resumeData?.socials || [];
        const contact = {
          socials,
          basicInfo: this.resumeData?.basic_info || {}
        };
        return JSON.stringify(contact, null, 2);
      },
      {
        name: 'get_contact_info',
        description: 'Retrieve Gauransh\'s contact information and social media profiles (email, LinkedIn, GitHub, etc.).',
        schema: emptySchema
      }
    );

    this.tools = [
      getExperienceTool,
      getEducationTool,
      getCertificationsTool,
      getProjectsTool,
      getSkillsTool,
      getContactInfoTool
    ];

    console.log(' Built 6 tools for resume subsections');
  }

  // --- Backend API Wrappers ---
  
  /**
   * Call backend /api/embed endpoint for secure embedding generation
   */
  async embedQuery(userMessage) {
    try {
      const response = await fetch(`${this.apiBaseUrl}/api/embed`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          input: userMessage,
          model: process.env.REACT_APP_EMBEDDING_MODEL || 'text-embedding-3-large'
        })
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.error || 'Embedding failed');
      }

      const data = await response.json();
      const vec = new Float32Array(data.embedding);
PATH: src/services/ragService.js
LINES: 199-247

return this.normalize(vec);
    } catch (err) {
      console.error(' Embedding API error:', err.message);
      throw err;
    }
  }

  /**
   * Call backend /api/chat endpoint for secure LLM inference
   * Supports tool calling via OpenAI function calling format
   */
  async callLLM(messages, { tools = null } = {}) {
    try {
      const body = {
        messages,
        model: process.env.REACT_APP_LLM_MODEL || 'gpt-3.5-turbo',
        temperature: 0.2,
        maxTokens: 900
      };

      // Convert Langchain tools to OpenAI function calling format
      if (tools && tools.length > 0) {
        body.tools = tools.map(t => ({
          type: 'function',
          function: {
            name: t.name,
            description: t.description,
            parameters: t.schema || { type: 'object', properties: {}, required: [] }
          }
        }));
      }

      const response = await fetch(`${this.apiBaseUrl}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(body)
      });

      if (!response.ok) {
        const error = await response.json();
        throw new Error(error.error || 'Chat API failed');
      }

      const data = await response.json();
      
      // Transform response to match Langchain format
      return {
        content: data.content,
        role: data.role,
PATH: src/services/ragService.js
LINES: 248-285

tool_calls: data.tool_calls?.map(tc => ({
          name: tc.function.name,
          args: JSON.parse(tc.function.arguments || '{}')
        })) || []
      };
    } catch (err) {
      console.error(' Chat API error:', err.message);
      throw err;
    }
  }

  async loadBinaryIndex() {
    if (this.indexLoaded && this.indexMeta && this.vectors && this.textsBuffer) {
      console.log(' Binary index already cached, reusing...');
      return;
    }

    const loadStart = Date.now();
    try {
      const metaResponse = await fetch('/rag/meta.json');
      if (!metaResponse.ok) throw new Error('Failed to load /rag/meta.json');
      this.indexMeta = await metaResponse.json();

      const { count, dim, items } = this.indexMeta;
      if (!count || !dim || !items || items.length !== count) {
        throw new Error('Invalid meta.json');
      }

      const vectorsResponse = await fetch('/rag/vectors.f32');
      if (!vectorsResponse.ok) throw new Error('Failed to load /rag/vectors.f32');
      const arrayBuffer = await vectorsResponse.arrayBuffer();
      this.vectors = new Float32Array(arrayBuffer);

      if (this.vectors.length !== count * dim) {
        throw new Error(`Vector size mismatch: got ${this.vectors.length}, expected ${count * dim}`);
      }

      const textsResponse = await fetch('/rag/texts.txt');
PATH: src/services/ragService.js
LINES: 286-331

if (!textsResponse.ok) throw new Error('Failed to load /rag/texts.txt');
      this.textsBuffer = new Uint8Array(await textsResponse.arrayBuffer());

      this.indexLoaded = true;
      const loadTime = Date.now() - loadStart;
      console.log(` Loaded binary index: ${count} items, dim=${dim}, in ${loadTime}ms`);
    } catch (err) {
      console.error('Failed to load binary index:', err.message);
      this.indexLoaded = false;
      throw err;
    }
  }

  normalizeText(text) {
    return text
      .toLowerCase()
      .replace(/[_-]/g, ' ')
      .replace(/[^a-z0-9\s]/g, '')
      .replace(/\s+/g, ' ')
      .trim();
  }

  buildProjectAliases() {
    this.repoNames = [];
    this.projectAliases = new Map();

    const repoSet = new Set();
    if (this.indexMeta?.items) {
      this.indexMeta.items.forEach(item => {
        const repo = item.repo || item.project_name;
        if (repo) repoSet.add(repo);
      });
    }
    this.repoNames = Array.from(repoSet);

    this.repoNames.forEach(repo => {
      const normalized = this.normalizeText(repo);
      this.projectAliases.set(normalized, repo);
      
      const withoutYear = normalized.replace(/\s+(19|20)\d{2}\s*$/, '').trim();
      if (withoutYear !== normalized) {
        this.projectAliases.set(withoutYear, repo);
      }
    });

    if (this.resumeData?.projects) {
PATH: src/services/ragService.js
LINES: 332-362

this.resumeData.projects.forEach(proj => {
        if (proj.title && proj.repo_name) {
          const normalized = this.normalizeText(proj.title);
          this.projectAliases.set(normalized, proj.repo_name);
        }
      });
    }

    const portfolioAliases = [
      'this website', 'this project', 'this portfolio',
      'portfolio chat', 'ai portfolio', 'rag portfolio', 'portfolio assistant'
    ];
    portfolioAliases.forEach(alias => {
      this.projectAliases.set(alias, 'portfolio-chat');
    });

    console.log(' Built project aliases:', this.projectAliases.size, 'aliases for', this.repoNames.length, 'repos');
  }

  async detectProjectContextViaLLM(userMessage) {
    const projectList = this.resumeData?.projects
      ? JSON.stringify(this.resumeData.projects.map(p => ({ project: p.title, repo: p.repo_name })))
      : '[]';
    
    if (this.repoNames.length === 0) return null;

    try {
      const prompt = `User query: "${userMessage}"\n\nProject-to-repo mapping: ${projectList}\n\nWhich repo does the user's query refer to? \nRespond with ONLY the exact repo name from the list above, or "NONE" if no repo is mentioned.\nDo not add explanation.`;

      const result = await this.callLLM([
        { role: 'user', content: prompt }
PATH: src/services/ragService.js
LINES: 363-414

]);

      const response = (result.content || '').trim();
      
      if (response !== 'NONE' && this.repoNames.includes(response)) {
        console.log(` Detected project via LLM: "${response}"`);
        return response;
      }
      
      console.log(` LLM fallback: no project detected (response: "${response}")`);
      return null;
    } catch (err) {
      console.warn(' LLM project detection failed:', err.message);
      return null;
    }
  }

  normalize(vec) {
    let norm = 0;
    for (let i = 0; i < vec.length; i++) {
      norm += vec[i] * vec[i];
    }
    norm = Math.sqrt(norm);
    if (norm === 0) return vec;
    const normalized = new Float32Array(vec.length);
    for (let i = 0; i < vec.length; i++) {
      normalized[i] = vec[i] / norm;
    }
    return normalized;
  }

  dotProduct(a, b) {
    let sum = 0;
    for (let i = 0; i < a.length; i++) {
      sum += a[i] * b[i];
    }
    return sum;
  }

  getChunkText(itemIndex) {
    const item = this.indexMeta.items[itemIndex];
    if (!item) return '';
    
    const { text_offset, text_length } = item;
    const slice = this.textsBuffer.slice(text_offset, text_offset + text_length);
    return new TextDecoder('utf-8').decode(slice);
  }

  retrieveTopK(queryVector, { repoFilter = null, k = 10 } = {}) {
    const { count, dim, items } = this.indexMeta;
    const similarities = [];
PATH: src/services/ragService.js
LINES: 415-460

for (let i = 0; i < count; i++) {
      const itemRepo = items[i].repo || items[i].project_name;

      if (repoFilter && itemRepo !== repoFilter) {
        continue;
      }

      const vectorStart = i * dim;
      let sim = 0;
      for (let j = 0; j < dim; j++) {
        sim += queryVector[j] * this.vectors[vectorStart + j];
      }
      
      similarities.push({ index: i, similarity: sim });
    }

    similarities.sort((a, b) => b.similarity - a.similarity);
    return similarities.slice(0, k);
  }

  formatChunks(chunkIndices) {
    const formatted = [];

    for (const idx of chunkIndices) {
      const item = this.indexMeta.items[idx];
      const projectName = item.repo || item.project_name || 'Unknown';
      const filePath = item.file_path || 'N/A';
      const startLine = item.start_line;
      const endLine = item.end_line;

      let sourceHeader = `[Source: ${projectName} | ${filePath}`;
      if (startLine && endLine) {
        sourceHeader += ` | L${startLine}${endLine}`;
      }
      sourceHeader += ']';

      const chunkText = this.getChunkText(idx);
      const maxLength = 1200;
      const truncated = chunkText.length > maxLength
        ? chunkText.substring(0, maxLength) + '...'
        : chunkText;

      formatted.push(`${sourceHeader}\n${truncated}`);
    }
    
    console.log(' Formatted context chunks:', formatted.length);
PATH: src/services/ragService.js
LINES: 461-481

return formatted.join('\n\n---\n\n');
  }

  buildSystemPrompt(retrievedContext) {
    const projectList = this.resumeData?.projects
      ? JSON.stringify(this.resumeData.projects.map(p => ({ project: p.title, repo: p.repo_name })))
      : '[]';

    return `You are the official AI Portfolio Assistant for **Gauransh Sawhney**, a Full-stack / AI / ML software engineer and graduate student.
      Your role is to represent Gauransh professionally, accurately, and conservatively.

      ##  Context Recognition
      **This Website / This Project refers to:** "portfolio-chat" (the AI-powered portfolio RAG system you're running in)
      When users ask about "this website", "this project", "portfolio chat", or the portfolio assistant itself, they're asking about the portfolio-chat project.
      Use the get_projects tool to reference its details if needed.

      ##  Core Rules
      1. **Ground everything:** Only cite information explicitly available via tools, retrieved context, or your training knowledge about public projects.
      2. **Be honest:** If you don't have a detail, say "I don't have that specific information."
      3. **Cite sources:** When referencing code or technical details, mention the project name and source.
      4. **Stay professional:** Confident and enthusiastic, but never exaggerated.
PATH: src/services/ragService.js
LINES: 482-510

5. **Be concise:** Keep responses to 34 sentences unless asked for more.
      6. **No speculation:** Don't infer or assume beyond what's explicitly available.

      ##  Available Tools
      Call tools to access resume details on-demand:
      - **get_experience**: Work history and positions
      - **get_education**: Degrees, schools, GPA
      - **get_projects**: Project titles, descriptions, repos
      - **get_skills**: Technical skills by category
      - **get_certifications**: Licenses and credentials
      - **get_contact_info**: Email, LinkedIn, GitHub, socials

      ##  Retrieved Context (Code & Architecture)
      ${retrievedContext || 'No code context available for this query.'}

       ##  Navigation & Action Rules
          Append **EXACTLY ONE** action tag at the end **ONLY IF** the response clearly maps to a navigation intent.
          If no navigation intent applies, append **nothing**.

          **Education keywords:** "Degree", "University", "GPA"  
           <<ACTION:SCROLL_EDUCATION>>

          **Experience keywords:** "Work", "Job", "Internship"  
           <<ACTION:SCROLL_EXPERIENCE>>

          **Projects keywords:** "Projects", "GitHub", "Code", or ${projectList}  
           <<ACTION:SCROLL_PROJECTS>>  

          **Certifications keywords:** "Licenses", "Certifications", "Credentials", "Certificate"
PATH: src/services/ragService.js
LINES: 511-548

 <<ACTION:SCROLL_CERTIFICATIONS>>

          **Contact keywords:** "Contact", "Email", "LinkedIn"  
              <<ACTION:SCROLL_CONTACT>>

          **DO NOT narrate actions.**  
          Correct:  
          "Gauransh has worked on projects such as Image Coloration. <<ACTION:SCROLL_PROJECTS>>"

          Incorrect:  
          "Let me scroll you to his projects. <<ACTION:SCROLL_PROJECTS>>"`;
  }

  detectProjectContext(message) {
    if (!this.resumeData?.projects || this.projectAliases.size === 0) return null;
    
    const normalizedMsg = this.normalizeText(message);
    const msgTokens = new Set(normalizedMsg.split(' ').filter(t => t.length > 0));

    let bestMatch = null;
    let bestScore = 0;
    let matchReason = '';

    for (const [alias, repoName] of this.projectAliases.entries()) {
      if (normalizedMsg.includes(alias)) {
        if (!bestMatch || alias.length > matchReason.length) {
          bestMatch = repoName;
          matchReason = `exact match: "${alias}"`;
          bestScore = 1.0;
        }
        continue;
      }

      const aliasTokens = new Set(alias.split(' ').filter(t => t.length > 0));
      const intersection = new Set([...msgTokens].filter(t => aliasTokens.has(t)));
      const union = new Set([...msgTokens, ...aliasTokens]);
      const jaccard = intersection.size / union.size;
PATH: src/services/ragService.js
LINES: 549-594

if (jaccard >= 0.5 && jaccard > bestScore) {
        bestMatch = repoName;
        bestScore = jaccard;
        matchReason = `token overlap (${jaccard.toFixed(2)}): "${alias}"`;
      }
    }

    if (bestMatch) {
      console.log(` Detected project: "${bestMatch}" via ${matchReason}`);
      return bestMatch;
    }
    
    return null;
  }

  async executeToolCall(toolName, toolInput) {
    const tool = this.tools.find(t => t.name === toolName);
    if (!tool) {
      console.warn(` Tool not found: ${toolName}`);
      return `Tool "${toolName}" not available.`;
    }

    try {
      const result = await tool.invoke(toolInput || {});
      return result;
    } catch (err) {
      console.error(` Tool execution failed: ${toolName}`, err.message);
      return `Error executing tool: ${err.message}`;
    }
  }

  async agenticLoop(systemPrompt, messages, maxIterations = 3) {
    let currentMessages = [...messages];
    let iteration = 0;

    while (iteration < maxIterations) {
      iteration++;
      console.log(` Agentic loop iteration ${iteration}/${maxIterations}`);

      const response = await this.callLLM(
        [{ role: 'system', content: systemPrompt }, ...currentMessages],
        { tools: this.tools }
      );

      if (response.tool_calls && response.tool_calls.length > 0) {
        console.log(` LLM called ${response.tool_calls.length} tool(s)`);
PATH: src/services/ragService.js
LINES: 595-641

currentMessages.push({
          role: 'assistant',
          content: response.content || ''
        });

        for (const toolCall of response.tool_calls) {
          const toolName = toolCall.name;
          const toolInput = toolCall.args || {};

          console.log(`   Executing: ${toolName}`);
          const toolResult = await this.executeToolCall(toolName, toolInput);

          currentMessages.push({
            role: 'user',
            content: `[Tool Result from ${toolName}]:\n${toolResult}`
          });
        }
      } else {
        console.log(` Agentic loop complete (no more tool calls)`);
        return {
          text: response.content || '',
          iterations: iteration
        };
      }
    }

    console.warn(` Max iterations (${maxIterations}) reached`);
    const lastResponse = currentMessages[currentMessages.length - 1];
    return {
      text: lastResponse?.content || 'Unable to generate response.',
      iterations: iteration
    };
  }

  async query(userMessage, history = []) {
    if (!this.initialized || !this.indexLoaded) {
      console.warn(' RAG not ready; using fallback');
      return this.queryFallback(userMessage, history);
    }

    try {
      const queryStart = Date.now();
      
      console.log(' User query:', userMessage);
      let targetRepo = this.detectProjectContext(userMessage);
PATH: src/services/ragService.js
LINES: 642-678

if (!targetRepo) {
        console.log(' No alias match; trying LLM fallback...');
        targetRepo = await this.detectProjectContextViaLLM(userMessage);
      }
      
      console.log(' Detected project:', targetRepo || 'none');
      console.log(' Retrieval mode:', targetRepo ? `scoped (repo: ${targetRepo})` : 'broad');

      const queryVector = await this.embedQuery(userMessage);
      const topKResults = this.retrieveTopK(queryVector, {
        repoFilter: targetRepo,
        k: 10
      });
      
      console.log(' Raw retrieval hits:', topKResults.length);

      let formattedContext = '';
      let sourceDocuments = [];
      if (topKResults.length > 0) {
        const topKIndices = topKResults.map(r => r.index);
        formattedContext = this.formatChunks(topKIndices);
        sourceDocuments = topKIndices.map(idx => ({
          pageContent: this.getChunkText(idx),
          metadata: this.indexMeta.items[idx]
        }));
      }

      const systemPrompt = this.buildSystemPrompt(formattedContext);
      const recentHistory = history.slice(-6);

      const summary = this.getConversationSummary();
      if (summary) {
        recentHistory.unshift({ role: 'system', content: `Conversation Summary:\n${summary}` });
      }

      const agenticResult = await this.agenticLoop(systemPrompt, [
        ...recentHistory,
PATH: src/services/ragService.js
LINES: 679-714

{ role: 'user', content: userMessage }
      ]);

      const queryTime = Date.now() - queryStart;
      console.log(` Query completed in ${queryTime}ms, retrieved ${topKResults.length} chunks, agentic iterations: ${agenticResult.iterations}`);

      const responseText = this.normalizeEmails(agenticResult.text);

      this.updateConversationSummaryAsync([
        ...recentHistory.filter(m => m.role !== 'system'),
        { role: 'user', content: userMessage },
        { role: 'assistant', content: responseText }
      ]);

      return {
        text: responseText,
        sourceDocuments
      };
    } catch (err) {
      console.error(' RAG query failed:', err.message);
      return this.queryFallback(userMessage, history);
    }
  }

  async queryFallback(userMessage, history = []) {
    console.log(' Using fallback mode (tools available, no RAG context)');
    try {
      const fallbackSystemPrompt = `You are Gauransh Sawhney's portfolio assistant. 
        Answer questions about Gauransh's experience, education, projects, and skills.
        Use the available tools to access resume details on demand.
        Be professional, concise, and honest. If you don't have info, say so.`;

      const recentHistory = history.slice(-4);

      const agenticResult = await this.agenticLoop(fallbackSystemPrompt, [
        ...recentHistory,
PATH: src/services/ragService.js
LINES: 715-754

{ role: 'user', content: userMessage }
      ]);

      return { 
        text: this.normalizeEmails(agenticResult.text), 
        sourceDocuments: [] 
      };
    } catch (err) {
      console.error(' Fallback query failed:', err.message);
      return { 
        text: 'I\'m having trouble responding right now. Please try again.', 
        sourceDocuments: [] 
      };
    }
  }

  normalizeEmails(text) {
    const parts = text.split(/(```[\s\S]*?```)/);
    
    return parts.map((part, index) => {
      if (index % 2 === 1) return part;
      
      let result = part;
      
      const existingMailtoEmails = new Set();
      const mailtoRegex = /\(mailto:([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\)/g;
      let mailtoMatch;
      while ((mailtoMatch = mailtoRegex.exec(result)) !== null) {
        existingMailtoEmails.add(mailtoMatch[1]);
      }
      
      result = result.replace(/\[([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\](?!\(mailto:)/g, (match, email) => {
        if (existingMailtoEmails.has(email)) return match;
        return `[${email}](mailto:${email})`;
      });
      
      result = result.replace(/\b([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})\b/g, (match, email) => {
        if (existingMailtoEmails.has(email)) return match;
        
        const idx = result.indexOf(match);
PATH: src/services/ragService.js
LINES: 755-796

const before = result.substring(Math.max(0, idx - 8), idx);
        if (before.includes('mailto:') || before.endsWith('](')) return match;
        
        return `[${email}](mailto:${email})`;
      });
      
      return result;
    }).join('');
  }

  getConversationSummary() {
    try {
      if (!this.conversationSummary) {
        const stored = sessionStorage.getItem('conversationSummary') || '';
        this.conversationSummary = stored;
      }
      return this.conversationSummary;
    } catch (_) {
      return this.conversationSummary || '';
    }
  }

  setConversationSummary(summary) {
    this.conversationSummary = summary || '';
    try {
      sessionStorage.setItem('conversationSummary', this.conversationSummary);
    } catch (_) {}
  }

  buildSummaryPrompt(existingSummary, messages) {
    const header = `You are maintaining a concise conversation summary for a portfolio assistant.\n` +
      `Update the summary to capture enduring context, user goals, constraints, and decisions.\n` +
      `Keep it under 200 tokens. Exclude chit-chat. Prefer bullet points.\n`;

    const summaryBlock = existingSummary
      ? `Current summary:\n${existingSummary}\n\n`
      : '';

    const historyText = messages.map(m => `${m.role.toUpperCase()}: ${m.content}`).join('\n');
    return `${header}${summaryBlock}Recent turns:\n${historyText}\n\nNew summary:`;
  }
PATH: src/services/ragService.js
LINES: 797-804

updateConversationSummaryAsync(messages) {
    try {
      const existing = this.getConversationSummary();
      const prompt = this.buildSummaryPrompt(existing, messages);
      
      this.callLLM([
        { role: 'system', content: 'You are a summarizer that outputs ONLY the updated summary.' },
        { role: 'user', content: prompt }
PATH: src/services/ragService.js
LINES: 805-826

]).then(res => {
        const updated = (res?.content || '').trim();
        if (updated) this.setConversationSummary(updated);
      }).catch(() => {});
    } catch (_) {}
  }

  isReady() {
    return this.initialized && this.indexLoaded;
  }

  getStatus() {
    return {
      initialized: this.initialized,
      ready: this.isReady(),
      indexLoaded: this.indexLoaded,
      proxyMode: true
    };
  }
}

export default new RAGChatService();
PATH: src/setupTests.js
LINES: 1-5

// jest-dom adds custom jest matchers for asserting on DOM nodes.
// allows you to do things like:
// expect(element).toHaveTextContent(/react/i)
// learn more: https://github.com/testing-library/jest-dom
import '@testing-library/jest-dom';
# Twitter-Tweets-Classsification
##  Vision-Controlled Flappy Bird (MATLAB)

This is a **MATLAB-based implementation** of the classic *Flappy Bird* game  controlled entirely by **computer vision**. Instead of pressing buttons, players use **colored objects (your "wings")** and perform **flapping motions** to control the bird. This hands-free interaction brings a fun, physical twist to the nostalgic game!

##  How to Play

1. **Clone the Repository**
 ```bash
 git clone https://github.com/garrysawyers3007/Vision_Bird.git
 ```
2. **Install MATLAB**
 If you haven't already, [download and install MATLAB](https://www.mathworks.com/help/install/ug/install-products-with-internet-connection.html). Make sure to include the following toolboxes:
 - *Image Acquisition Toolbox*
 - *Computer Vision Toolbox*

3. **Set Up Your Webcam**
 The game uses your default webcam by default. To use a different camera, run `webcamlist` in the MATLAB command window and copy the desired camera name. Update this line in `calibrate_color.m`:
 ```
 cam = webcam("Your Camera Name");
 ```

4. **Launch the Game**
 In MATLAB, navigate to the cloned folder and run:
 ```
 flappy_bird_main
 ```

5. **Choose Your Bird**
 You'll be prompted to either:
 - Use the default bird sprite, or
 - Use your **own face** as the bird!
 If using your face, draw a bounding box around it when prompted.

6. **Calibrate Your Wings**
You'll be prompted to either:
 - Use the default bird sprite, or
 - Use your **own face** as the bird!
 If using your face, draw a bounding box around it when prompted.

6. **Calibrate Your Wings**
 - Hold two colored objects of the same color in your hands (your "wings").
 - Use the calibration window to draw a bounding box within one wing to detect its color.
 - Make sure the color **contrasts well with the background** for accurate detection.

7. **Start Flapping!**
 After a 321 countdown:
 - Flap your "wings" (move the colored objects up and down).
 - Ensure they **exit the red detection boxes** to register a flap.
 - Each flap gives the bird a vertical velocy to fly higher.

8. **Avoid Obstacles**
 Just like in the original game:
 - If you stop flapping, the bird falls due to gravity.
 - Dodge the pipes and try to survive as long as possible!

##  Implementation Details
### User Interface (UI)
The game window includes:
- A live webcam feed showing the player's movements.
- A mask preview displaying the detected colored wings, helping users track their wings during gameplay.
- A main gameplay area displaying the bird, pipes, and score.
- After the game ends, the interface shows **Game Over** along with **Restart** and **Exit** buttons.
- The layout is built using MATLABs UI components such as axes, rectangles, and text.
### Computer Vision
- The program captures video frames using MATLABs webcam interface, with the feed mirrored for natural interaction.
- The player selects a color to represent their "wings" during a calibration step.
- Frames are converted to HSV color space for robust color detection under varying lighting conditions.
- Binary masks isolate regions matching the selected wing color.
- Detection zones are defined on each side of the camera feed. The algorithm uses bitwise operations to determine how much of the wing is within these zones.
- A **flap** is triggered when:
 1. Wings were simultaneously inside the detection zones in the previous frame, and
 2. They are no longer in the detection zones in the current frame.

##  Demo and Credits
Here's a flapping demo. Note how the bird hops everytime his wings flap **out** of the detection zones:

If you want a rundown of how to run the game and go through the callibration phase, check it out here: [Demo](https://drive.google.com/file/d/1Mr8nJcGTsq-wVExSaDvRlMApsTDyj2di/view?usp=drivesdk)

Meet the team:
Here's a flapping demo. Note how the bird hops everytime his wings flap **out** of the detection zones:

If you want a rundown of how to run the game and go through the callibration phase, check it out here: [Demo](https://drive.google.com/file/d/1Mr8nJcGTsq-wVExSaDvRlMApsTDyj2di/view?usp=drivesdk)

Meet the team:

[Niranjan Sundararajan](https://www.linkedin.com/in/sundararajann) - Flappiest Bird; [Seher Bhaskar](https://www.linkedin.com/in/seher-bhaskar) - Documentation Duchess; [Akanksha Tanwar](https://www.linkedin.com/in/akankshatanwar17) - Project Manager (self-declared); [Hamsitha Challagundla](https://www.linkedin.com/in/hamsithachallagundla) - Marketing Magician;
[Vaibhav Mahapatra](https://www.linkedin.com/in/vaibhav-mahapatra-aa0a591a8) - Komedy King; [Gauransh Sawhney](https://www.linkedin.com/in/gauransh3007) - Consolidation King;
PATH: calibrate_color.m
LINES: 1-40

function [cam, Hmin, Hmax, Smin, Smax, Vmin, Vmax, face_img] = calibrate_color(use_face)
    % cam = webcam('Logitech Webcam C925e');
    cam = webcam();
    
    face_img = zeros(2);
    if use_face
        clf;
        % Show a frame and let user draw a rectangle
        frame = snapshot(cam);
        h = imshow(frame);
        title('Draw a box around your face');
        rect = drawrectangle();

        % Crop and store face image
        face_img = imcrop(frame, rect.Position);
    end

    pause(1);
    frame = snapshot(cam);

    hCalib = figure('Name', 'Calibration - Draw box in the colored object', ...
                    'Position', [200 200 640 480]);
    imshow(frame);
    title('Draw box around the colored object');
    roi = drawrectangle('Color', 'r');
    pos = round(roi.Position);
    objectROI = imcrop(frame, pos);
    close(hCalib);

    objectHSV = rgb2hsv(objectROI);
    H = objectHSV(:,:,1);
    S = objectHSV(:,:,2);
    % V = objectHSV(:,:,3);

    Hmin = max(min(H(:)) - 0.05, 0); Hmax = min(max(H(:)) + 0.05, 1);
    Smin = max(min(S(:)) - 0.2, 0);  Smax = 1;
    Vmin = 0.2; Vmax = 1;

   
end
PATH: check_collision.m
LINES: 1-22

function result = check_collision(obj1, obj2)
    r1 = get_bounds(obj1);
    r2 = get_bounds(obj2);

    result = ~(r1(1) + r1(3) < r2(1) || ...
               r1(1) > r2(1) + r2(3) || ...
               r1(2) + r1(4) < r2(2) || ...
               r1(2) > r2(2) + r2(4));
end

function bounds = get_bounds(obj)
    if isprop(obj, 'Position')
        pos = get(obj, 'Position');
        bounds = [pos(1), pos(2), pos(3), pos(4)];
    else
        xdata = get(obj, 'XData');
        ydata = get(obj, 'YData');
        width = abs(xdata(2) - xdata(1));
        height = abs(ydata(2) - ydata(1));
        bounds = [xdata(1), ydata(1), width, height];
    end
end
PATH: create_pipe.m
LINES: 1-8

function [top, bottom] = create_pipe(ax, x, gap_y, gap_size, pipe_width)
    top_height = 700 - (gap_y + gap_size / 2);
    bottom_height = gap_y - gap_size / 2;
    top = rectangle(ax, 'Position', [x, gap_y + gap_size/2, pipe_width, top_height], ...
                    'FaceColor', 'green', 'EdgeColor', 'none');
    bottom = rectangle(ax, 'Position', [x, 0, pipe_width, bottom_height], ...
                       'FaceColor', 'green', 'EdgeColor', 'none');
end
PATH: flappy_bird_main.m
LINES: 1-41

function flappy_bird_main
    persistent high_score; %global
    if isempty(high_score)
        high_score = 0;
    end
    addpath('.\images\');

    fig = figure('Color', 'white', ...
                 'MenuBar', 'none', 'ToolBar', 'none', ...
                 'NumberTitle', 'off', 'Name', 'Flappy Bird', ...
                 'Position', [100 100 1000 500]);

    show_start_screen();

    function show_start_screen()
        clf(fig);

        uicontrol('Style', 'text', ...
                  'String', ['High Score: ' num2str(high_score)], ...
                  'FontSize', 14, ...
                  'Position', [350 300 300 60], ...
                  'BackgroundColor', 'white');
        
        uicontrol('Style', 'pushbutton', 'String', 'Start Game', ...
                  'FontSize', 14, ...
                  'Position', [420 200 160 50], ...
                  'Callback', @(~,~) ask_for_custom_bird());
    end

    function ask_for_custom_bird()
        clf(fig);

        uicontrol('Style', 'text', ...
                  'String', 'Do you want to use your face as the bird?', ...
                  'FontSize', 14, ...
                  'Position', [300 300 400 60], ...
                  'BackgroundColor', 'white');

        uicontrol('Style', 'pushbutton', 'String', 'Yes', ...
                  'FontSize', 14, ...
                  'Position', [300 200 150 50], ...
PATH: flappy_bird_main.m
LINES: 42-54

'Callback', @(~,~) start_game_from_calibration(true));

        uicontrol('Style', 'pushbutton', 'String', 'No', ...
                  'FontSize', 14, ...
                  'Position', [550 200 150 50], ...
                  'Callback', @(~,~) start_game_from_calibration(false));
    end

    function start_game_from_calibration(use_face)
        [cam, Hmin, Hmax, Smin, Smax, Vmin, Vmax, face_img] = calibrate_color(use_face);
        start_game(fig, cam, Hmin, Hmax, Smin, Smax, Vmin, Vmax, @show_start_screen, face_img, use_face);
    end
end
PATH: start_game.m
LINES: 1-40

function start_game(fig, cam, Hmin, Hmax, Smin, Smax, Vmin, Vmax, show_start_screen, face_img, use_face)
    persistent high_score;
    if isempty(high_score), high_score = 0; end

    clf(fig);

    bg = imread('images\bg.png');

    % Axes layout
    axGame = axes('Parent', fig, 'Units', 'normalized', 'Position', [0.05 0.1 0.55 0.85]);
    image(axGame, [0 500], [0 700], flipud(bg));
    set(axGame, 'YDir', 'normal'); 
    hold(axGame, 'on');
    axis(axGame, [0 500 0 700]);
    axis manual;
    daspect(axGame, [1 1 1]);

    axCam  = axes('Parent', fig, 'Units', 'normalized', 'Position', [0.65 0.55 0.3 0.35]);
    axMask = axes('Parent', fig, 'Units', 'normalized', 'Position', [0.65 0.1 0.3 0.35]);

    axis(axGame, [0 500 0 700]); axis manual; daspect(axGame, [1 1 1]);
    set(axGame, 'XColor', 'none', 'YColor', 'none', 'box', 'off', 'Color', 'white');
    set(axGame, 'YDir', 'normal');  % (0,0) is bottom-left

    hold(axGame, 'on');
    rectangle(axGame, 'Position', [0 0 500 700], 'EdgeColor', [0.2 0.2 0.2], 'LineWidth', 2);

    % Gameplay constants
    speeding_factor = 2;
    gravity = -0.6 * speeding_factor^2;
    jump_velocity = 10 * speeding_factor;
    pipe_speed = 2.5 * speeding_factor;

    pipe_width = 60; pipe_spacing = 300;
    min_gap_y = 180; max_gap_y = 520;
    min_gap_size = 200; max_gap_size = 240;

    bird_logic = true;

    % Bird
PATH: start_game.m
LINES: 41-73

bird_x = 100; bird_y = 350; bird_vy = 0;
    bird_size = [30 35];

    if ~bird_logic
        bird = rectangle(axGame, 'Position', [bird_x bird_y bird_size], 'FaceColor', 'yellow');
    else
        [bird_img, ~, bird_alpha] = imread('images\flappy_mathworks.png');  % Load image and alpha channel
        if use_face
            bird_img = face_img;
        end
        bird_img = flipud(imresize(bird_img, bird_size));
        bird_alpha = flipud(imresize(bird_alpha, bird_size));
        bird_magnification_factor = 1;
        bird = image('CData', bird_img, ...
                 'XData', bird_x + [0 bird_magnification_factor*bird_size(1)], ...
                 'YData', bird_y + [0 bird_magnification_factor*bird_size(2)], ...
                 'Parent', axGame);
        set(bird, 'AlphaData', bird_alpha);               % Set transparency
    end

    % Pipes
    num_pipes = 3;
    pipes = gobjects(num_pipes, 2); pipe_xs = zeros(1, num_pipes); gap_sizes = zeros(1, num_pipes);
    for i = 1:num_pipes
        x = 500 + (i-1)*pipe_spacing;
        gap_size = randi([min_gap_size max_gap_size]);
        gap_y = randi([min_gap_y max_gap_y]);
        [top, bottom] = create_pipe(axGame, x, gap_y, gap_size, pipe_width);
        pipes(i,:) = [top, bottom]; pipe_xs(i) = x; gap_sizes(i) = gap_size;
    end

    % Score
    score = 0;
PATH: start_game.m
LINES: 74-102

scoreText = text(axGame, 20, 660, "Score: 0", 'FontSize', 14, 'FontWeight', 'bold');
    highScoreText = text(axGame, 20, 630, ['High Score: ' num2str(high_score)], ...
                         'FontSize', 12, 'Color', [0.4 0.4 0.4]);

    % Flap detection setup
    frame = snapshot(cam);
    [frameH, frameW, ~] = size(frame);
    zoneW = round(frameW * 0.3); zoneH = round(frameH * 0.5);
    mirroredLeftZone = [frameW - zoneW + 1, 1, zoneW, zoneH];
    mirroredRightZone = [1, 1, zoneW, zoneH];
    pixelThreshold = 10; inZonePreviously = false;

    % Main loop
    running = true;
    framecount = 0;
    pause(3)
    while running && ishandle(fig)
        % ---- Webcam input and flap detection ----
        frame = fliplr(snapshot(cam));
        hsvFrame = rgb2hsv(frame);
        mask = (hsvFrame(:,:,1) >= Hmin & hsvFrame(:,:,1) <= Hmax) & ...
               (hsvFrame(:,:,2) >= Smin & hsvFrame(:,:,2) <= Smax) & ...
               (hsvFrame(:,:,3) >= Vmin & hsvFrame(:,:,3) <= Vmax);
        mask = bwareaopen(mask, 300); % basic cleanup

        leftMask  = mask(1:zoneH, mirroredLeftZone(1):mirroredLeftZone(1)+zoneW-1);
        rightMask = mask(1:zoneH, mirroredRightZone(1):mirroredRightZone(1)+zoneW-1);
        leftActive = nnz(leftMask) >= pixelThreshold;
        rightActive = nnz(rightMask) >= pixelThreshold;
PATH: start_game.m
LINES: 103-141

bothIn = leftActive && rightActive; bothOut = ~leftActive && ~rightActive;

        if inZonePreviously && bothOut
            bird_vy = jump_velocity;
            inZonePreviously = false;
        elseif bothIn
            inZonePreviously = true;
        end

        % ---- Visual update: webcam + mask ----
        framecount = framecount + 1;
        if framecount >= 3
            framecount = 0;
            frame = insertShape(frame, 'Rectangle', [mirroredLeftZone; mirroredRightZone], ...
                                'Color', 'red', 'LineWidth', 10);
            % stats = regionprops(mask, 'BoundingBox');
            % for i = 1:numel(stats)
            %     frame = insertShape(frame, 'Rectangle', stats(i).BoundingBox, 'Color', 'green');
            % end

            imshow(frame, 'Parent', axCam);
            imshow(mask, 'Parent', axMask);
            
            % if ~exist('frameImg', 'var')
            %     frameImg = flipud(image(axCam, 'CData', frame));
            %     maskImg = flipud(image(axMask, 'CData', mask));
            % else
            %     set(frameImg, 'CData', frame);
            %     set(maskImg, 'CData', mask);
            % end

        end

        % ---- Game logic ----
        bird_vy = bird_vy + gravity;
        bird_y = bird_y + bird_vy;

        if ~bird_logic
            set(bird, 'Position', [bird_x bird_y bird_size]);
PATH: start_game.m
LINES: 142-181

else
            set(bird, 'XData', bird_x + [0 bird_size(1)], ...
              'YData', bird_y + [0 bird_size(2)]);
        end

        for i = 1:num_pipes
            pipe_xs(i) = pipe_xs(i) - pipe_speed;
            if pipe_xs(i) + pipe_width < 0
                pipe_xs(i) = max(pipe_xs) + pipe_spacing;
                gap_sizes(i) = randi([min_gap_size max_gap_size]);
                gap_y = randi([min_gap_y max_gap_y]);
                update_pipe(pipes(i,1), pipes(i,2), pipe_xs(i), gap_y, gap_sizes(i), pipe_width);
                score = score + 1;
                scoreText.String = ['Score: ' num2str(score)];
                if score > high_score
                    high_score = score;
                    highScoreText.String = ['High Score: ' num2str(high_score)];
                end
            else
                update_pipe_x(pipes(i,1), pipes(i,2), pipe_xs(i), pipe_width);
            end
        end

        for i = 1:num_pipes
            if check_collision(bird, pipes(i,1)) || check_collision(bird, pipes(i,2))
                show_game_over(face_img,use_face);
                return;
            end
        end

        if bird_y <= 0 || bird_y + bird_size(2) >= 700
            show_game_over(face_img,use_face);
            return;
        end

        drawnow limitrate;
        pause(0.01);
    end
    
    function show_game_over(face_img,use_face)
PATH: start_game.m
LINES: 182-190

text(axGame, 180, 350, "Game Over", 'FontSize', 24, 'Color', 'r');
        uicontrol('Style', 'pushbutton', 'String', 'Restart', ...
                  'Position', [200 10 100 30], ...
                  'Callback', @(~,~) start_game(fig, cam, Hmin, Hmax, Smin, Smax, Vmin, Vmax, show_start_screen,face_img,use_face));
        uicontrol('Style', 'pushbutton', 'String', 'Exit', ...
                  'Position', [320 10 100 30], ...
                  'Callback', @(~,~) show_start_screen());
    end
end
PATH: update_pipe.m
LINES: 1-6

function update_pipe(top, bottom, new_x, gap_y, gap_size, pipe_width)
    top_height = 700 - (gap_y + gap_size / 2);
    bottom_height = gap_y - gap_size / 2;
    set(top, 'Position', [new_x, gap_y + gap_size/2, pipe_width, top_height]);
    set(bottom, 'Position', [new_x, 0, pipe_width, bottom_height]);
end
PATH: update_pipe_x.m
LINES: 1-6

function update_pipe_x(top, bottom, new_x, pipe_width)
    top_pos = get(top, 'Position');
    bottom_pos = get(bottom, 'Position');
    set(top, 'Position', [new_x, top_pos(2), pipe_width, top_pos(4)]);
    set(bottom, 'Position', [new_x, bottom_pos(2), pipe_width, bottom_pos(4)]);
end
